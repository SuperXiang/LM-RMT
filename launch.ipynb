{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('pytorch')\n",
    "\n",
    "from pytorch.mem_transformer import *\n",
    "from pytorch import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    cuda = False\n",
    "    n_layer = 2\n",
    "    n_rel_layer = None #Nothing\n",
    "    n_head = 4\n",
    "    d_head = 200      # dim of Query\n",
    "    d_model = 202    # = d_proj - projection\n",
    "    d_embed = 198    # dim of embedding b4 attn?\n",
    "    d_inner = 197    # hidden dim of FF\n",
    "    dropout = 0\n",
    "    seed = 42\n",
    "\n",
    "    n_token = 10000 # dict\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "B = 4\n",
    "tgt_len, mem_len, ext_len = 36, 36, 0\n",
    "data_len = tgt_len * 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.LongTensor(data_len*B).random_(0, args.n_token).to(device)\n",
    "diter = data_utils.LMOrderedIterator(data, B, tgt_len, device=device, ext_len=ext_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 4, 20])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reshape(tgt_len, B, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MemTransformerLM_mem(nn.Module):\n",
    "#     def __init__(self, n_token, n_layer, n_head, d_model, d_head, d_inner,\n",
    "#                  dropout, dropatt, tie_weight=True, d_embed=None, \n",
    "#                  div_val=1, tie_projs=[False], pre_lnorm=False,\n",
    "#                  tgt_len=None, ext_len=None, mem_len=None, num_mem_tokens=None,\n",
    "#                  cutoffs=[], adapt_inp=False,\n",
    "#                  same_length=False, attn_type=0, clamp_len=-1, \n",
    "#                  sample_softmax=-1):\n",
    "#         super(MemTransformerLM_mem, self).__init__()\n",
    "#         self.n_token = n_token\n",
    "\n",
    "#         d_embed = d_model if d_embed is None else d_embed\n",
    "#         self.d_embed = d_embed\n",
    "#         self.d_model = d_model\n",
    "#         self.n_head = n_head\n",
    "#         self.d_head = d_head\n",
    "\n",
    "#         self.word_emb = AdaptiveEmbedding(n_token, d_embed, d_model, cutoffs, \n",
    "#                                           div_val=div_val)\n",
    "\n",
    "#         self.drop = nn.Dropout(dropout)\n",
    "\n",
    "#         self.n_layer = n_layer\n",
    "\n",
    "#         self.tgt_len = tgt_len\n",
    "#         self.mem_len = mem_len\n",
    "#         self.ext_len = ext_len\n",
    "#         self.max_klen = tgt_len + ext_len + mem_len\n",
    "#         self.num_mem_tokens = num_mem_tokens\n",
    "\n",
    "#         self.attn_type = attn_type\n",
    "\n",
    "#         self.layers = nn.ModuleList()\n",
    "#         if attn_type == 0: # the default attention\n",
    "#             for i in range(n_layer):\n",
    "#                 self.layers.append(\n",
    "#                     RelPartialLearnableDecoderLayer(\n",
    "#                         n_head, d_model, d_head, d_inner, dropout,\n",
    "#                         tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,\n",
    "#                         dropatt=dropatt, pre_lnorm=pre_lnorm)\n",
    "#                 )\n",
    "#         elif attn_type == 1: # learnable embeddings\n",
    "#             for i in range(n_layer):\n",
    "#                 self.layers.append(\n",
    "#                     RelLearnableDecoderLayer(\n",
    "#                         n_head, d_model, d_head, d_inner, dropout,\n",
    "#                         tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,\n",
    "#                         dropatt=dropatt, pre_lnorm=pre_lnorm)\n",
    "#                 )\n",
    "#         elif attn_type in [2, 3]: # absolute embeddings\n",
    "#             for i in range(n_layer):\n",
    "#                 self.layers.append(\n",
    "#                     DecoderLayer(\n",
    "#                         n_head, d_model, d_head, d_inner, dropout,\n",
    "#                         dropatt=dropatt, pre_lnorm=pre_lnorm)\n",
    "#                 )\n",
    "\n",
    "#         self.sample_softmax = sample_softmax\n",
    "#         # use sampled softmax\n",
    "#         if sample_softmax > 0:\n",
    "#             self.out_layer = nn.Linear(d_model, n_token)\n",
    "#             if tie_weight:\n",
    "#                 self.out_layer.weight = self.word_emb.weight\n",
    "#             self.tie_weight = tie_weight\n",
    "#             self.sampler = LogUniformSampler(n_token, sample_softmax)\n",
    "\n",
    "#         # use adaptive softmax (including standard softmax)\n",
    "#         else:\n",
    "#             self.crit = ProjectedAdaptiveLogSoftmax(n_token, d_embed, d_model, \n",
    "#                                                     cutoffs, div_val=div_val)\n",
    "\n",
    "#             if tie_weight:\n",
    "#                 for i in range(len(self.crit.out_layers)):\n",
    "#                     self.crit.out_layers[i].weight = self.word_emb.emb_layers[i].weight\n",
    "\n",
    "#             if tie_projs:\n",
    "#                 for i, tie_proj in enumerate(tie_projs):\n",
    "#                     if tie_proj and div_val == 1 and d_model != d_embed:\n",
    "#                         self.crit.out_projs[i] = self.word_emb.emb_projs[0]\n",
    "#                     elif tie_proj and div_val != 1:\n",
    "#                         self.crit.out_projs[i] = self.word_emb.emb_projs[i]\n",
    "\n",
    "#         self.same_length = same_length\n",
    "#         self.clamp_len = clamp_len\n",
    "\n",
    "#         self._create_params()\n",
    "#         self.init_mem_tokens()\n",
    "\n",
    "#     def backward_compatible(self):\n",
    "#         self.sample_softmax = -1\n",
    "\n",
    "#     def _create_params(self):\n",
    "#         if self.attn_type == 0: # default attention\n",
    "#             self.pos_emb = PositionalEmbedding(self.d_model)\n",
    "#             self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n",
    "#             self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n",
    "#         elif self.attn_type == 1: # learnable\n",
    "#             self.r_emb = nn.Parameter(torch.Tensor(\n",
    "#                     self.n_layer, self.max_klen, self.n_head, self.d_head))\n",
    "#             self.r_w_bias = nn.Parameter(torch.Tensor(\n",
    "#                     self.n_layer, self.n_head, self.d_head))\n",
    "#             self.r_bias = nn.Parameter(torch.Tensor(\n",
    "#                     self.n_layer, self.max_klen, self.n_head))\n",
    "#         elif self.attn_type == 2: # absolute standard\n",
    "#             self.pos_emb = PositionalEmbedding(self.d_model)\n",
    "#         elif self.attn_type == 3: # absolute deeper SA\n",
    "#             self.r_emb = nn.Parameter(torch.Tensor(\n",
    "#                     self.n_layer, self.max_klen, self.n_head, self.d_head))\n",
    "\n",
    "#     def reset_length(self, tgt_len, ext_len, mem_len):\n",
    "#         self.tgt_len = tgt_len\n",
    "#         self.mem_len = mem_len\n",
    "#         self.ext_len = ext_len\n",
    "\n",
    "#     def init_mems(self):\n",
    "#         if self.mem_len > 0:\n",
    "#             mems = []\n",
    "#             param = next(self.parameters())\n",
    "#             for i in range(self.n_layer+1):\n",
    "#                 empty = torch.empty(0, dtype=param.dtype, device=param.device)\n",
    "#                 mems.append(empty)\n",
    "\n",
    "#             return mems\n",
    "#         else:\n",
    "#             return None\n",
    "\n",
    "#     def init_mem_tokens(self):\n",
    "#         if self.num_mem_tokens in (None, 0):\n",
    "#             self.mem_tokens = None\n",
    "#         else:\n",
    "#             mem_tokens = [torch.randn(1, self.d_model)] * self.num_mem_tokens\n",
    "#             # self.mem_tokens = nn.Parameter(torch.cat(mem_tokens, dim=0))\n",
    "#             self.mem_tokens = torch.cat(mem_tokens, dim=0)\n",
    "\n",
    "#     def _update_mems(self, hids, mems, qlen, mlen):\n",
    "#         # does not deal with None\n",
    "#         if mems is None: return None\n",
    "\n",
    "#         # mems is not None\n",
    "#         assert len(hids) == len(mems), 'len(hids) != len(mems)'\n",
    "\n",
    "#         # There are `mlen + qlen` steps that can be cached into mems\n",
    "#         # For the next step, the last `ext_len` of the `qlen` tokens\n",
    "#         # will be used as the extended context. Hence, we only cache\n",
    "#         # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n",
    "#         # to `mlen + qlen - self.ext_len`.\n",
    "#         with torch.no_grad():\n",
    "#             new_mems = []\n",
    "#             end_idx = mlen + max(0, qlen - 0 - self.ext_len)\n",
    "#             beg_idx = max(0, end_idx - self.mem_len)\n",
    "#             for i in range(len(hids)):\n",
    "\n",
    "#                 cat = torch.cat([mems[i], hids[i]], dim=0)\n",
    "#                 new_mems.append(cat[beg_idx:end_idx].detach())\n",
    "\n",
    "#         return new_mems\n",
    "\n",
    "#     def _forward(self, dec_inp, mems=None):\n",
    "\n",
    "#         word_emb = self.word_emb(dec_inp)\n",
    "#         print( word_emb.isnan().sum())\n",
    "\n",
    "#         mlen = mems[0].size(0) if mems is not None else 0\n",
    "        \n",
    "#         # Concat with mem_tokens\n",
    "#         if self.num_mem_tokens not in (0, None):\n",
    "#             mem_tokens = self.mem_tokens.expand(1, *self.mem_tokens.shape).clone()\n",
    "#             memory = torch.cat([mem_tokens] * word_emb.shape[0], dim=0)\n",
    "#             word_emb = torch.cat((memory, word_emb), dim=1)\n",
    "\n",
    "#         qlen, bsz = dec_inp.size()\n",
    "#         klen = mlen + qlen\n",
    "#         if self.same_length:\n",
    "#             all_ones = word_emb.new_ones(qlen, klen)\n",
    "#             mask_len = klen - self.mem_len\n",
    "#             if mask_len > 0:\n",
    "#                 mask_shift_len = qlen - mask_len\n",
    "#             else:\n",
    "#                 mask_shift_len = qlen\n",
    "#             dec_attn_mask = (torch.triu(all_ones, 1+mlen)\n",
    "#                     + torch.tril(all_ones, -mask_shift_len)).byte()[:, :, None] # -1\n",
    "#         else:\n",
    "#             dec_attn_mask = torch.triu(\n",
    "#                 word_emb.new_ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None]\n",
    "        \n",
    "#         hids = []\n",
    "#         if self.attn_type == 0: # default\n",
    "#             pos_seq = torch.arange(klen-1, -1, -1.0, device=word_emb.device, \n",
    "#                                    dtype=word_emb.dtype)\n",
    "#             if self.clamp_len > 0:\n",
    "#                 pos_seq.clamp_(max=self.clamp_len)\n",
    "#             pos_emb = self.pos_emb(pos_seq)\n",
    "\n",
    "#             core_out = self.drop(word_emb)\n",
    "#             pos_emb = self.drop(pos_emb)\n",
    "\n",
    "#             hids.append(core_out)\n",
    "#             print('core out isna ', core_out.isnan().sum())\n",
    "#             for i, layer in enumerate(self.layers):\n",
    "#                 mems_i = None if mems is None else mems[i]\n",
    "#                 core_out = layer(core_out, pos_emb, self.r_w_bias,\n",
    "#                         self.r_r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i)\n",
    "#                 hids.append(core_out)\n",
    "#                 print('core out isna ', core_out.isnan().sum())\n",
    "#         elif self.attn_type == 1: # learnable\n",
    "#             core_out = self.drop(word_emb)\n",
    "#             hids.append(core_out)\n",
    "#             for i, layer in enumerate(self.layers):\n",
    "#                 if self.clamp_len > 0:\n",
    "#                     r_emb = self.r_emb[i][-self.clamp_len :]\n",
    "#                     r_bias = self.r_bias[i][-self.clamp_len :]\n",
    "#                 else:\n",
    "#                     r_emb, r_bias = self.r_emb[i], self.r_bias[i]\n",
    "\n",
    "#                 mems_i = None if mems is None else mems[i]\n",
    "#                 core_out = layer(core_out, r_emb, self.r_w_bias[i],\n",
    "#                         r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i)\n",
    "#                 hids.append(core_out)\n",
    "#         elif self.attn_type == 2: # absolute\n",
    "#             pos_seq = torch.arange(klen - 1, -1, -1.0, device=word_emb.device,\n",
    "#                                    dtype=word_emb.dtype)\n",
    "#             if self.clamp_len > 0:\n",
    "#                 pos_seq.clamp_(max=self.clamp_len)\n",
    "#             pos_emb = self.pos_emb(pos_seq)\n",
    "\n",
    "#             core_out = self.drop(word_emb + pos_emb[-qlen:])\n",
    "\n",
    "#             hids.append(core_out)\n",
    "#             for i, layer in enumerate(self.layers):\n",
    "#                 mems_i = None if mems is None else mems[i]\n",
    "#                 if mems_i is not None and i == 0:\n",
    "#                     mems_i += pos_emb[:mlen]\n",
    "#                 core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n",
    "#                                  mems=mems_i)\n",
    "#                 hids.append(core_out)\n",
    "#         elif self.attn_type == 3:\n",
    "#             core_out = self.drop(word_emb)\n",
    "\n",
    "#             hids.append(core_out)\n",
    "#             for i, layer in enumerate(self.layers):\n",
    "#                 mems_i = None if mems is None else mems[i]\n",
    "#                 if mems_i is not None and mlen > 0:\n",
    "#                     cur_emb = self.r_emb[i][:-qlen]\n",
    "#                     cur_size = cur_emb.size(0)\n",
    "#                     if cur_size < mlen:\n",
    "#                         cur_emb_pad = cur_emb[0:1].expand(mlen-cur_size, -1, -1)\n",
    "#                         cur_emb = torch.cat([cur_emb_pad, cur_emb], 0)\n",
    "#                     else:\n",
    "#                         cur_emb = cur_emb[-mlen:]\n",
    "#                     mems_i += cur_emb.view(mlen, 1, -1)\n",
    "#                 core_out += self.r_emb[i][-qlen:].view(qlen, 1, -1)\n",
    "\n",
    "#                 core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n",
    "#                                  mems=mems_i)\n",
    "#                 hids.append(core_out)\n",
    "\n",
    "#         core_out = self.drop(core_out)\n",
    "\n",
    "#         new_mems = self._update_mems(hids, mems, mlen, qlen)\n",
    "\n",
    "#         return core_out, new_mems\n",
    "\n",
    "#     def forward(self, data, target, *mems):\n",
    "#         # nn.DataParallel does not allow size(0) tensors to be broadcasted.\n",
    "#         # So, have to initialize size(0) mems inside the model forward.\n",
    "#         # Moreover, have to return new_mems to allow nn.DataParallel to piece\n",
    "#         # them together.\n",
    "#         if not mems: mems = self.init_mems()\n",
    "\n",
    "#         # if self.num_mem_tokens not in (0, None):\n",
    "#         #     mem_tokens = self.mem_tokens.expand(1, *self.mem_tokens.shape).clone()\n",
    "#         #     memory = torch.cat([mem_tokens] * data.shape[0], dim=0)\n",
    "#         #     data = torch.cat((memory, data), dim=1)\n",
    "\n",
    "#         tgt_len = target.size(0)\n",
    "#         hidden, new_mems = self._forward(data, mems=mems)\n",
    "\n",
    "#         print('hidden isnan ', torch.isnan(hidden).sum())\n",
    "#         print('mems ', torch.isnan(mems[0]).sum())\n",
    "#         # print('hidden, new mems: ', (len(hidden), len(new_mems)))\n",
    "#         pred_hid = hidden[-tgt_len:]\n",
    "#         if self.sample_softmax > 0 and self.training:\n",
    "#             assert self.tie_weight\n",
    "#             logit = sample_logits(self.word_emb,\n",
    "#                 self.out_layer.bias, target, pred_hid, self.sampler)\n",
    "#             loss = -F.log_softmax(logit, -1)[:, :, 0]\n",
    "#         else:\n",
    "#             # print('pred_hid ', pred_hid.shape)\n",
    "#             if self.num_mem_tokens not in (0, None):\n",
    "#                 mem_tokens, pred_hid = pred_hid[:, :self.num_mem_tokens].clone(), pred_hid[:, self.num_mem_tokens:].clone()\n",
    "#             # print('pred_hid ', pred_hid.shape)\n",
    "#             # print('target',  target.shape)\n",
    "#             # print('view pred', pred_hid.view(-1, pred_hid.size(-1)).shape)\n",
    "#             # print('view target ', target.view(-1).shape)\n",
    "#             loss = self.crit(pred_hid.view(-1, pred_hid.size(-1)), target.view(-1))\n",
    "#             # print('loss ', loss.shape)\n",
    "#             loss = loss.view(tgt_len, -1)\n",
    "#             # print('loss ', loss.shape)\n",
    "\n",
    "#            # pred_hid = torch.cat((mem_tokens, pred_hid), dim=1)\n",
    "\n",
    "#         output = [loss]\n",
    "\n",
    "#         if new_mems is not None:\n",
    "#             output += new_mems\n",
    "#         if self.num_mem_tokens not in (0, None):\n",
    "#             output = [mem_tokens] + output\n",
    "            \n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mem_tokens = 16\n",
    "\n",
    "div_val = 2\n",
    "d_embed = 100\n",
    "\n",
    "cutoffs = [args.n_token // 2]\n",
    "tie_projs = [False] + [True] * len(cutoffs)\n",
    "\n",
    "model = MemTransformerLM(args.n_token, args.n_layer, args.n_head,\n",
    "                        args.d_model, args.d_head, args.d_inner, args.dropout,\n",
    "                        dropatt=args.dropout, tie_weight=True, \n",
    "                        d_embed=d_embed, div_val=div_val, \n",
    "                        tie_projs=tie_projs, pre_lnorm=True,\n",
    "                        tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,# num_mem_tokens=num_mem_tokens,\n",
    "                        cutoffs=cutoffs, attn_type=0).to(device)\n",
    "\n",
    "\n",
    "# model_mem = MemTransformerLM_mem(args.n_token, args.n_layer, args.n_head,\n",
    "                        # args.d_model, args.d_head, args.d_inner, args.dropout,\n",
    "                        # dropatt=args.dropout, tie_weight=True, \n",
    "                        # d_embed=d_embed, div_val=div_val, \n",
    "                        # tie_projs=tie_projs, pre_lnorm=True,\n",
    "                        # tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len, num_mem_tokens=num_mem_tokens,\n",
    "                        # cutoffs=cutoffs, attn_type=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters():\n",
    "#     print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayd98/Desktop/MIPT/TransformerXL/transformer-xl/pytorch/mem_transformer.py:269: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1104.)\n",
      "  attn_score = attn_score.float().masked_fill(\n"
     ]
    }
   ],
   "source": [
    "mems = tuple()\n",
    "idx, (inp, tgt, seqlen) = list(enumerate(diter))[0]\n",
    "out = model(inp, tgt, *mems, )\n",
    "print(len(out))\n",
    "mem_tokens, enc, mems = out[0], out[1], out[1:]\n",
    "\n",
    "# print(out[0].shape, [m.shape for m in out[1:]], len(out[1:]))\n",
    "\n",
    "# # print(mems[-1])\n",
    "\n",
    "# # idx, (inp, tgt, seqlen) = list(enumerate(diter))[0]\n",
    "# # out = model(inp, tgt, *mems, )\n",
    "# # enc, mems = out[0], out[1:]\n",
    "# # print(out[0].shape, [m.shape for m in out[1:]], len(out[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "mems = tuple()\n",
    "idx, (inp, tgt, seqlen) = list(enumerate(diter))[0]\n",
    "out = model(inp, tgt, *mems, )\n",
    "print(len(out))\n",
    "enc = out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (20) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-1c05bb328751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout_mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (20) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "out_mem[i+1] - out[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(144)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(enc).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8456, 6490, 7959, 8298],\n",
       "        [ 778, 9517, 3080, 2191],\n",
       "        [8833, 6181, 2880, 2668],\n",
       "        [5183, 6818, 7858, 6641],\n",
       "        [6983, 5891, 3529, 2450],\n",
       "        [1939, 7080, 2023, 7603],\n",
       "        [2674, 7028, 3830, 2783],\n",
       "        [7772, 8202, 2321, 7247],\n",
       "        [3748, 5586, 4165, 6088],\n",
       "        [4639, 2829, 1936, 7271],\n",
       "        [8479, 4443, 9749, 6767],\n",
       "        [5289, 9458, 8027, 8781],\n",
       "        [8850, 1741, 1721, 4034],\n",
       "        [4498, 9962, 9225, 1992],\n",
       "        [8376, 9134, 3195, 7744],\n",
       "        [2113, 1467,  754, 8523],\n",
       "        [4358, 9983, 1189, 3312],\n",
       "        [9569, 7361, 5904,  688],\n",
       "        [9976, 7937, 1024, 9200],\n",
       "        [1928, 4606, 2899, 1081],\n",
       "        [4204, 3628, 7964, 4120],\n",
       "        [3171, 8327, 8265, 3809],\n",
       "        [4507, 4815, 7846, 4235],\n",
       "        [4829, 1950, 6476, 9920],\n",
       "        [7774, 7801, 4840, 4767],\n",
       "        [9865, 4135, 3519, 3951],\n",
       "        [7903,  403, 3695, 9756],\n",
       "        [1005, 1072, 7007, 1459],\n",
       "        [3238, 6822, 1936, 8168],\n",
       "        [3985, 2869, 5446, 7292],\n",
       "        [6805, 2069, 5144, 4148],\n",
       "        [4748, 6222, 4970,  628],\n",
       "        [6689,  689, 1479, 9702],\n",
       "        [2817, 7429, 2777,  104],\n",
       "        [5709, 9889, 7480, 9580],\n",
       "        [6816, 1845, 2070, 6572]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan]], grad_fn=<ViewBackward>),\n",
       " tensor([[[-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          [-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          [-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          ...,\n",
       "          [ 1.2929e-25,  1.0689e-25, -6.7834e-26,  ...,  1.0867e-25,\n",
       "            6.1135e-27, -7.3969e-26],\n",
       "          [ 8.4918e-25, -1.8220e-25, -2.2351e-26,  ...,  1.1887e-25,\n",
       "            8.2710e-26, -4.2568e-26],\n",
       "          [-1.1537e-25,  3.2263e-26,  1.4970e-25,  ..., -6.7934e-26,\n",
       "            1.0818e-25, -7.5086e-26]],\n",
       " \n",
       "         [[-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          [-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          [-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          ...,\n",
       "          [ 6.6588e-25,  7.0981e-26,  4.9511e-26,  ...,  4.2248e-26,\n",
       "            9.9378e-27,  9.5020e-26],\n",
       "          [-1.8437e+36,         nan, -4.5406e+37,  ..., -5.7714e+37,\n",
       "           -9.2333e+37, -6.9907e+37],\n",
       "          [ 1.3564e+36,         nan, -3.8505e+37,  ...,  1.0742e+38,\n",
       "            2.0349e+38,  1.7811e+38]],\n",
       " \n",
       "         [[-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          [-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          [-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          ...,\n",
       "          [-1.3047e-25, -4.3851e-26, -3.9968e-25,  ...,  2.0543e-26,\n",
       "           -5.3353e-27, -8.9231e-26],\n",
       "          [-1.0938e+36,         nan,  3.5823e+37,  ..., -4.5176e+37,\n",
       "           -2.1026e+37, -2.8053e+37],\n",
       "          [ 2.0442e+35,         nan, -3.0774e+37,  ...,  1.4836e+38,\n",
       "            2.8393e+38,  1.5980e+38]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          [-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          [-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          ...,\n",
       "          [-4.5299e-25, -6.2260e-26,  1.6495e-25,  ..., -3.5660e-26,\n",
       "            1.2994e-25,  1.0113e-25],\n",
       "          [ 9.6350e+35,         nan,  3.4390e+37,  ..., -3.6196e+36,\n",
       "            3.8308e+37,  9.9296e+37],\n",
       "          [ 7.1148e+35,         nan, -5.6233e+37,  ..., -7.5288e+37,\n",
       "           -9.0379e+37, -1.0352e+38]],\n",
       " \n",
       "         [[-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          [-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          [-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          ...,\n",
       "          [-2.6828e-25, -2.1022e-27,  4.3594e-27,  ..., -8.8641e-27,\n",
       "           -1.7790e-26,  1.6852e-26],\n",
       "          [-2.5621e-25,  6.4541e-27,  1.9247e-25,  ...,  1.0521e-25,\n",
       "            1.5502e-25, -7.3123e-26],\n",
       "          [-4.7138e-25,  4.7068e-26,  2.1326e-25,  ...,  8.0236e-26,\n",
       "            1.1811e-25,  1.2978e-25]],\n",
       " \n",
       "         [[-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          [-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          [-9.6105e-01,  1.9480e-01, -1.3662e+00,  ..., -9.9629e-01,\n",
       "           -2.9737e-01, -9.1085e-01],\n",
       "          ...,\n",
       "          [ 1.6943e+35,         nan,  1.8793e+37,  ..., -1.1990e+38,\n",
       "           -9.1630e+37, -1.0604e+38],\n",
       "          [-6.6501e+35,         nan,  1.5865e+38,  ...,  5.4808e+37,\n",
       "            1.7784e+38,  1.3923e+38],\n",
       "          [-3.1881e-25,  7.3690e-26,  2.4855e-25,  ...,  2.0052e-26,\n",
       "            2.5186e-25,  2.1987e-25]]]),\n",
       " tensor([[[-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          [-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          [-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          [-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          [-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          [-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          [-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          [-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          [-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          [-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          [-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          [-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          [-0.6248, -0.0210, -1.4244,  ..., -0.0739, -0.2143, -0.9807],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]]),\n",
       " tensor([[[-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          [-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          [-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          [-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          [-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          [-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          [-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          [-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          [-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          [-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          [-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          [-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          [-0.4878, -0.6474, -1.5870,  ...,  0.2091, -0.6138, -0.1948],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]])]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 4]) torch.Size([36, 4]) 36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([8456, 6490, 7959, 8298]), tensor([ 778, 9517, 3080, 2191]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inp.shape, tgt.shape, seqlen)\n",
    "inp[0], tgt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([36, 4]),\n",
       " [torch.Size([36, 4, 202]),\n",
       "  torch.Size([36, 4, 202]),\n",
       "  torch.Size([36, 4, 202])],\n",
       " 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape, [m.shape for m in out[1:]], len(out[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum(p.numel() for p in model.parameters()))\n",
    "# print([p.numel() for p in model.parameters()])\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(enumerate(diter))[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, v in enumerate(diter):\n",
    "#     print(i, v[0].shape, v[1].shape, v[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2351009\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "1390909\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "1951009\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n",
      "1150909\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "batch 9\n",
      "batch 10\n",
      "batch 11\n",
      "batch 12\n",
      "batch 13\n",
      "batch 14\n",
      "batch 15\n",
      "batch 16\n",
      "batch 17\n",
      "batch 18\n",
      "batch 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = torch.LongTensor(data_len*B).random_(0, args.n_token).to(device)\n",
    "diter = data_utils.LMOrderedIterator(data, B, tgt_len, device=device, ext_len=ext_len)\n",
    "\n",
    "cutoffs = [args.n_token // 2]\n",
    "tie_projs = [False] + [True] * len(cutoffs)\n",
    "\n",
    "for div_val in [1, 2]:\n",
    "    for d_embed in [200, 100]:\n",
    "        model = MemTransformerLM(args.n_token, args.n_layer, args.n_head,\n",
    "                        args.d_model, args.d_head, args.d_inner, args.dropout,\n",
    "                        dropatt=args.dropout, tie_weight=True, \n",
    "                        d_embed=d_embed, div_val=div_val, \n",
    "                        tie_projs=tie_projs, pre_lnorm=True,\n",
    "                        tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len, \n",
    "                        cutoffs=cutoffs, attn_type=0).to(device)\n",
    "\n",
    "        print(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "        mems = tuple()\n",
    "        for idx, (inp, tgt, seqlen) in enumerate(diter):\n",
    "            print('batch {}'.format(idx))\n",
    "            out = model(inp, tgt, *mems)\n",
    "            mems = out[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
