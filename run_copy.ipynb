{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9dfb631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('pytorch')\n",
    "sys.path.append('pytorch/utils')\n",
    "\n",
    "from pytorch.mem_transformer import *\n",
    "from pytorch import data_utils\n",
    "\n",
    "from experiment_utils.run_experiment import *\n",
    "from experiment_utils.generate_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e6986",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de49706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "TAG = '10tkn_len24_ext'\n",
    "\n",
    "TASK_NAME = 'copy'\n",
    "TRAIN_SIZE = 1000\n",
    "VAL_SIZE = 200\n",
    "TEST_SIZE = 100\n",
    "NUM_INITS = 1\n",
    "\n",
    "\n",
    "NUM_BATCHES = int(4e5)\n",
    "BATCH_SIZE = 128\n",
    "GENERATE_EVERY  = 10000\n",
    "NUM_TOKENS = 10 + 2\n",
    "ENC_SEQ_LEN = 24\n",
    "DEC_SEQ_LEN = 48\n",
    "\n",
    "INPUT_LEN = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87cb25",
   "metadata": {},
   "source": [
    "#### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b7097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !mkdir data24\n",
    "# np.random.seed(42)\n",
    "\n",
    "# generator = copy_generator(batch_size=BATCH_SIZE, enc_seq_len=ENC_SEQ_LEN, dec_seq_len=DEC_SEQ_LEN, num_tokens=NUM_TOKENS)\n",
    "# generate_data(generator, path=f'data{INPUT_LEN}', task_name=TASK_NAME, train_size=TRAIN_SIZE, test_size=TEST_SIZE, val_size=VAL_SIZE, batch_size=BATCH_SIZE)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17253635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader:\n",
    "    def __init__(self, task_name, path='data', batch_size=32, none_mask=True):\n",
    "        self.X, self.y = np.load(f'{path}/{task_name}_X.npy'), np.load(f'{path}/{task_name}_y.npy')\n",
    "        self.data_size = self.X.shape[0]\n",
    "        self.data_ptr = 0\n",
    "\n",
    "        if none_mask:\n",
    "            self.src_mask, self.tgt_mask = None, None\n",
    "        else:\n",
    "            self.src_masks, self.tgt_mask = np.load(f'{path}/{task_name}_mask.npy'), None\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.none_mask = none_mask\n",
    "\n",
    "    def __next__(self):\n",
    "        X = self.X[self.data_ptr: self.data_ptr+self.batch_size]\n",
    "        y = self.y[self.data_ptr: self.data_ptr+self.batch_size]\n",
    "        \n",
    "        if not self.none_mask:\n",
    "            sm = self.src_masks[self.data_ptr: self.data_ptr+self.batch_size]\n",
    "            sm = torch.tensor(sm).cuda()\n",
    "        else:\n",
    "            sm = None\n",
    "            \n",
    "        self.data_ptr = (self.data_ptr + self.batch_size) % self.data_size\n",
    "\n",
    "        return torch.tensor(X),\\\n",
    "                torch.tensor(y),\\\n",
    "                sm, self.tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c118128",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082c59ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = ParameterGrid({\n",
    "                'n_layer': [2],\n",
    "                'n_head': [4],\n",
    "                'd_head': [128],\n",
    "                'num_mem_tokens': [9, 0], \n",
    "                'mem_len': [0]})\n",
    "\n",
    "param = list(model_parameters)[0]\n",
    "\n",
    "fixed_parameters = {'n_token': NUM_TOKENS,\n",
    "                    'd_model': param['d_head'],# + param['num_mem_tokens']-1,\n",
    "                    'd_inner': param['d_head'],\n",
    "                    'dropout': 0,\n",
    "                    'dropatt': 0,\n",
    "                    'tie_weight': True,\n",
    "                    'div_val': 1, # ???????\n",
    "                    'tie_projs': [False],\n",
    "                    'tgt_len': DEC_SEQ_LEN,\n",
    "                    'ext_len': 0, \n",
    "                    'cutoffs': [],\n",
    "                    'attn_type': 0,}\n",
    "\n",
    "model = MemTransformerLM(**param, **fixed_parameters)#.cuda()\n",
    "\n",
    "gen_train = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_train', batch_size=21)\n",
    "src, tgt, _, _ = next(gen_train)\n",
    "# src, tgt = src.cuda(), tgt.cuda()\n",
    "src, tgt = src.cpu().T, tgt.cpu().T\n",
    "\n",
    "mems = tuple()\n",
    "# model(src, tgt.contiguous(), *mems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8bd627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a54127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayd98/Desktop/MIPT/TXL/pytorch/mem_transformer.py:267: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1104.)\n",
      "  attn_score = attn_score.float().masked_fill(\n"
     ]
    }
   ],
   "source": [
    "out = model(src, src.contiguous())\n",
    "mem_tokens, loss, mems = out[0], out[1], out[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92978633",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_train', batch_size=BATCH_SIZE)\n",
    "gen_val = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_val', batch_size=VAL_SIZE)\n",
    "gen_test = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_test', batch_size=TEST_SIZE)\n",
    "\n",
    "\n",
    "print_file = f'logs/{TASK_NAME}_{TAG}_memory_logs.txt'\n",
    "t = time.time()\n",
    "with torch.cuda.device(0):\n",
    "    for init_num in range(NUM_INITS):\n",
    "        with open(print_file, 'a') as f:\n",
    "            f.write('\\n\\nInit number ' + str(init_num)+'\\n')\n",
    "        for i, param in enumerate(list(model_parameters)):\n",
    "            with open(print_file, 'a') as f:\n",
    "                f.write('\\n\\n' + str(param)+'\\n')\n",
    "            param['enc_depth'], param['enc_heads'] = param['depth,heads']\n",
    "            param['dec_depth'], param['dec_heads'] = param['depth,heads']\n",
    "            param.pop('depth,heads')\n",
    "\n",
    "            with open(print_file, 'a') as f:\n",
    "                f.write(f'{i / len(model_parameters) * 100}%')\n",
    "            model = TransformerXL\n",
    "            model = XTransformer(**param).cuda()\n",
    "\n",
    "            model_name = f\"{TASK_NAME}{INPUT_LEN}_dim{param['dim']}d{param['enc_depth']}h{param['enc_heads']}M{param['enc_num_memory_tokens']}l{param['enc_max_seq_len']}_{TAG}_v{init_num}\"\n",
    "\n",
    "            optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "            train_validate_model(model, \n",
    "                            train_generator=gen_train, \n",
    "                            val_generator=gen_val, \n",
    "                            optim=optim, \n",
    "                            model_name=model_name, \n",
    "                            config=param,\n",
    "                            num_batches=NUM_BATCHES,\n",
    "                            generate_every=GENERATE_EVERY,\n",
    "                            print_file=print_file,\n",
    "                            tag=TAG,\n",
    "                            overfit_stop=False)\n",
    "            test_model(model, gen_test, model_name, param, TASK_NAME, tag=TAG)\n",
    "            with open(print_file, 'a') as f:\n",
    "                f.write(f'\\nTotal time: {time.time() - t}\\n')\n",
    "            t = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
