{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c64d5c6",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08617d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "import torch.nn.functional as F\n",
    "from pytorch.experiment_utils.generate_data import *\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('pytorch')\n",
    "sys.path.append('pytorch/utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c807fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../evaluation64/tl24xl0mt24_l4h4_lr25e-5_rlrp-copy/20220114-184122\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97594c8b",
   "metadata": {},
   "source": [
    "#### reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "62bc7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '../../data'\n",
    "path = '/home/ayd98/Desktop/MIPT/Memory transformer/data24'\n",
    "tr_iter = data_loader('train', path=path, task_name='reverse', batch_size=32,\n",
    "                                        tgt_len=24, device='cpu')\n",
    "gen = next(tr_iter)\n",
    "data, target, l = next(gen)\n",
    "\n",
    "# # acc = 1\n",
    "# path = '../models/reverse_tl12xl0mt6.pt'\n",
    "\n",
    "# # acc = 0.8\n",
    "# path = '../models/reverse_tl12xl6mt0.pt'\n",
    "\n",
    "# path = '../models/reverse_tl24mt24.pt'\n",
    "\n",
    "# path = '../models/reverse_tl24xl24.pt'\n",
    "\n",
    "path = '../models/reverse_tl8xl0mt8.pt'\n",
    "\n",
    "model = torch.load(path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e4d205fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.num_mem_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653dc10c",
   "metadata": {},
   "source": [
    "#### copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0461ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy_tl24xl0mt24.pt\t   remote_logs_gpu8\t  reverse_tl24xl24.pt\n",
      "copy_tl24xl24mt0.pt\t   reverse_tl12xl0mt6.pt  reverse_tl8xl0mt8.pt\n",
      "copy_tl8xl0mt8.pt\t   reverse_tl12xl6mt0.pt  reverse_tl8xl8.pt\n",
      "copy_tl8xl8mt0_low_acc.pt  reverse_tl24mt24.pt\n"
     ]
    }
   ],
   "source": [
    "!ls ../models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19081628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/ayd98/Desktop/MIPT/Memory transformer/data24'\n",
    "# tr_iter = data_loader('train', path=path, task_name='copy', batch_size=32,\n",
    "#                                         tgt_len=24, device='cpu')\n",
    "# gen = next(tr_iter)\n",
    "# data, target, l = next(gen)\n",
    "\n",
    "# # # tl24mt24\n",
    "# # # acc ~1\n",
    "# # path = '../models/copy_tl24xl0mt24.pt'\n",
    "\n",
    "# # # tl24mt24\n",
    "# # # acc ~1\n",
    "# # path = '../models/copy_tl24xl24mt0.pt'\n",
    "\n",
    "\n",
    "# # acc ~1\n",
    "# path = '../models/copy_tl8xl0mt8.pt'\n",
    "\n",
    "\n",
    "# # # acc ~0.2\n",
    "# # path = '../models/copy_tl8xl8mt0_low_acc.pt'\n",
    "\n",
    "# model = torch.load(path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608833da",
   "metadata": {},
   "source": [
    "#### retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "afec4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path = '../../data'\n",
    "# path = '../../../x-transformers/data'\n",
    "# tr_iter = data_loader('train', path=path, task_name='retrieval', batch_size=32,\n",
    "#                                         tgt_len=10, device='cuda')\n",
    "# gen = next(tr_iter)\n",
    "# data, target, l = next(gen)\n",
    "\n",
    "# # path = '../evaluation/tl10xl0mt0_lr1e-4_rlrp-retrieval/20220113-160611/model.pt'\n",
    "\n",
    "# # #l1h1\n",
    "# # path = '../evaluation/tl10xl0mt0_l1h1_lr1e-4_rlrp-retrieval/20220113-151239/model.pt'\n",
    "\n",
    "# # #l2h2\n",
    "# # path = '../evaluation/tl10xl0mt0_l2h2_lr3e-4_rlrp_as1-retrieval/20220113-101945/model.pt'\n",
    "\n",
    "# #l6h4\n",
    "# path = '../evaluation/tl5xl5mt0_l6h4_lr25e-5_rlrp-retrieval/20220117-103246/model.pt'\n",
    "\n",
    "\n",
    "# model = torch.load(path).to(device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5777b715",
   "metadata": {},
   "source": [
    "forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0e2599d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_maps(model, data, target, mems=None, mem_tokens=model.num_mem_tokens):\n",
    "    self = model\n",
    "\n",
    "    if not mems: mems = model.init_mems(data.device)\n",
    "\n",
    "    tgt_len = target.size(0)\n",
    "\n",
    "\n",
    "    dec_inp = data\n",
    "\n",
    "    word_emb = self.word_emb(dec_inp)\n",
    "\n",
    "    mlen = mems[0].size(0) if mems is not None else 0\n",
    "\n",
    "    # Concat with mem_tokens\n",
    "    if mem_tokens is not None:\n",
    "        word_emb = torch.cat((mem_tokens, word_emb), dim=0)\n",
    "        if self.mem_at_end:\n",
    "            word_emb = torch.cat((word_emb, mem_tokens), dim=0)\n",
    "\n",
    "    qlen = word_emb.shape[0]\n",
    "    klen = mlen + qlen\n",
    "    if self.same_length:\n",
    "        all_ones = word_emb.new_ones(qlen, klen)\n",
    "        mask_len = klen - self.mem_len\n",
    "        if mask_len > 0:\n",
    "            mask_shift_len = qlen - mask_len\n",
    "        else:\n",
    "            mask_shift_len = qlen\n",
    "        dec_attn_mask = (torch.triu(all_ones, 1+mlen)\n",
    "                + torch.tril(all_ones, -mask_shift_len)).byte()[:, :, None] # -1\n",
    "    else:\n",
    "        dec_attn_mask = torch.triu(\n",
    "            word_emb.new_ones(qlen, klen), diagonal=1+mlen).byte()\n",
    "\n",
    "        if self.num_mem_tokens != 0:\n",
    "            dec_attn_mask[:self.num_mem_tokens, mlen:mlen+self.num_mem_tokens] = 0\n",
    "            dec_attn_mask[:self.num_mem_tokens, :mlen] = 1 - int(self.read_mem_from_cache)\n",
    "            if self.mem_at_end:\n",
    "                dec_attn_mask[-self.num_mem_tokens:, -self.num_mem_tokens:] = 0\n",
    "                dec_attn_mask[-self.num_mem_tokens:, :mlen] = 1 - int(self.read_mem_from_cache)\n",
    "        dec_attn_mask = dec_attn_mask[:,:,None]\n",
    "\n",
    "    hids = []\n",
    "    # if self.attn_type == 0: # default\n",
    "    pos_seq = torch.arange(klen-1, -1, -1.0, device=word_emb.device, \n",
    "                           dtype=word_emb.dtype)\n",
    "    if self.clamp_len > 0:\n",
    "        pos_seq.clamp_(max=self.clamp_len)\n",
    "    pos_emb = self.pos_emb(pos_seq)\n",
    "\n",
    "    core_out = self.drop(word_emb)\n",
    "    pos_emb = self.drop(pos_emb)\n",
    "\n",
    "    hids.append(core_out)\n",
    "    \n",
    "    attn_maps = [] \n",
    "    for i, layer in enumerate(self.layers):\n",
    "        print('got layer ', i)\n",
    "        mems_i = None if mems is None else mems[i]\n",
    "        core_out = layer(core_out, pos_emb, self.r_w_bias,\n",
    "                self.r_r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i)\n",
    "        hids.append(core_out)\n",
    "        \n",
    "        w, r, r_w_bias, r_r_bias, attn_mask, mems1 = core_out, pos_emb, self.r_w_bias, self.r_r_bias, dec_attn_mask, mems_i\n",
    "        \n",
    "        qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)\n",
    "\n",
    "        self1 = layer.dec_attn\n",
    "        if mems1 is not None:\n",
    "            cat = torch.cat([mems1, w], 0)\n",
    "            if self1.pre_lnorm:\n",
    "                w_heads = self1.qkv_net(self1.layer_norm(cat))\n",
    "            else:\n",
    "                w_heads = self1.qkv_net(cat)\n",
    "            r_head_k = self1.r_net(r)\n",
    "\n",
    "            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n",
    "            w_head_q = w_head_q[-qlen:]\n",
    "        else:\n",
    "            if self1.pre_lnorm:\n",
    "                w_heads = self1.qkv_net(self1.layer_norm(w))\n",
    "            else:\n",
    "                w_heads = self1.qkv_net(w)\n",
    "            r_head_k = self1.r_net(r)\n",
    "\n",
    "            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n",
    "\n",
    "        klen = w_head_k.size(0)\n",
    "\n",
    "        w_head_q = w_head_q.view(qlen, bsz, self1.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n",
    "        w_head_k = w_head_k.view(klen, bsz, self1.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n",
    "        w_head_v = w_head_v.view(klen, bsz, self1.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n",
    "\n",
    "        r_head_k = r_head_k.view(rlen, self1.n_head, self.d_head)                # qlen x n_head x d_head\n",
    "\n",
    "        #### compute attention score\n",
    "        rw_head_q = w_head_q + r_w_bias                                         # qlen x bsz x n_head x d_head\n",
    "        AC = torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n",
    "\n",
    "        rr_head_q = w_head_q + r_r_bias\n",
    "        BD = torch.einsum('ibnd,jnd->ijbn', (rr_head_q, r_head_k))              # qlen x klen x bsz x n_head\n",
    "        BD = self1._rel_shift(BD)\n",
    "\n",
    "        # [qlen x klen x bsz x n_head]\n",
    "        attn_score = AC + BD\n",
    "        attn_score.mul_(self1.scale)\n",
    "\n",
    "        #### compute attention probability\n",
    "        if attn_mask is not None and attn_mask.any().item():\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_score = attn_score.float().masked_fill(\n",
    "                    attn_mask[None,:,:,None].bool(), -float('inf')).type_as(attn_score)\n",
    "            elif attn_mask.dim() == 3:\n",
    "                attn_score = attn_score.float().masked_fill(\n",
    "                    attn_mask[:,:,:,None].bool(), -float('inf')).type_as(attn_score)\n",
    "        \n",
    "        attn_prob = F.softmax(attn_score, dim=1)\n",
    "#         attn_prob = self1.dropatt(attn_prob)\n",
    "        \n",
    "        attn_maps.append(attn_prob)\n",
    "        \n",
    "    return attn_maps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e441a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_maps = get_attn_maps(model, data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3fa07f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got layer  0\n",
      "got layer  1\n",
      "got layer  2\n",
      "got layer  3\n",
      "tensor(3.5487, grad_fn=<MeanBackward0>)\n",
      "got layer  0\n",
      "got layer  1\n",
      "got layer  2\n",
      "got layer  3\n",
      "tensor(3.5642, grad_fn=<MeanBackward0>)\n",
      "got layer  0\n",
      "got layer  1\n",
      "got layer  2\n",
      "got layer  3\n",
      "tensor(5.4558, grad_fn=<MeanBackward0>)\n",
      "got layer  0\n",
      "got layer  1\n",
      "got layer  2\n",
      "got layer  3\n",
      "tensor(1.6020e-05, grad_fn=<MeanBackward0>)\n",
      "got layer  0\n",
      "got layer  1\n",
      "got layer  2\n",
      "got layer  3\n",
      "tensor(2.3665e-05, grad_fn=<MeanBackward0>)\n",
      "got layer  0\n",
      "got layer  1\n",
      "got layer  2\n",
      "got layer  3\n",
      "tensor(2.3096e-05, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_chunks = 6\n",
    "\n",
    "mems = tuple()\n",
    "if model.mem_tokens is not None:\n",
    "    mem_tokens = model.mem_tokens.repeat(1, batch_size, 1)\n",
    "else:\n",
    "    mem_tokens = None\n",
    "\n",
    "data_segs = torch.chunk(data, n_chunks)\n",
    "target_segs = torch.chunk(target, n_chunks)\n",
    "chunk_attn_maps = []\n",
    "\n",
    "for i, (d, t) in enumerate(zip(data_segs, target_segs)):\n",
    "    \n",
    "    \n",
    "    attn_maps = get_attn_maps(model, d, t, mems, mem_tokens)\n",
    "    chunk_attn_maps.append(attn_maps)\n",
    "    \n",
    "    ret = model(d, t, *mems, mem_tokens=mem_tokens)\n",
    "    if model.num_mem_tokens == 0:\n",
    "        loss, mems = ret[0], ret[1:]\n",
    "    else:\n",
    "        mem_tokens, loss, mems = ret[0], ret[1], ret[2:]\n",
    "\n",
    "    \n",
    "\n",
    "    print(loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "abeba9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_attn_maps), len(chunk_attn_maps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "02d86a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 8)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mem_len, model.num_mem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb32264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'images/reverse_tl8mt8.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "edee57c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2880x12960 with 96 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontdict = {'fontsize': 25}\n",
    "fd_small = {'fontsize': 16}\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "batch_ind = 0\n",
    "ticks = data[:, batch_ind].chunk(n_chunks)\n",
    "\n",
    "ls = len(chunk_attn_maps[0])\n",
    "hs = chunk_attn_maps[0][0].shape[-1]\n",
    "n_chunks = len(chunk_attn_maps)\n",
    "fig, axs = plt.subplots(n_chunks*ls, hs, figsize=(40, 30 * n_chunks))\n",
    "\n",
    "for c, attn_maps in enumerate(chunk_attn_maps):\n",
    "    # if c == 1:\n",
    "    #     break\n",
    "#     ls = len(attn_maps)\n",
    "    hs = attn_maps[0].shape[-1]\n",
    "    for l, layer_maps in enumerate(attn_maps):\n",
    "        # if l == 1:\n",
    "        #     break\n",
    "        row = l + ls * c\n",
    "        for h in range(layer_maps.shape[-1]):\n",
    "            attn_map = layer_maps[:, :, batch_ind, h]\n",
    "\n",
    "            if ls == hs == 1:\n",
    "                axis = axs\n",
    "            elif ls == 1:\n",
    "                axis = axs[h]\n",
    "            elif hs == 1:\n",
    "                axis = axs[row]\n",
    "            else: \n",
    "                axis = axs[row, h]\n",
    "            axis.matshow(attn_map.cpu().detach().numpy(), cmap='Reds')\n",
    "            \n",
    "            axis.set_ylabel(\"Layer \"+str(l+1), fontdict=fontdict)\n",
    "        \n",
    "            # if l == len(attn_maps)-1:\n",
    "            axis.set_xlabel(\"Head \"+str(h+1), fontdict=fontdict)\n",
    "            \n",
    "            # if l == 0:\n",
    "            axis.set_title(f'Chunk {c+1}', fontdict=fontdict)\n",
    "            \n",
    "            \n",
    "            tokens = ['mem'] * model.num_mem_tokens + list(ticks[c].numpy()) + ['mem'] * model.num_mem_tokens\n",
    "            \n",
    "\n",
    "            axis.set_yticks(range(len(tokens)))\n",
    "            axis.set_yticklabels(tokens, fontdict=fd_small)\n",
    "            if c > 0:\n",
    "                tokens = ['mem'] * model.mem_len + tokens\n",
    "\n",
    "            axis.set_xticks(range(len(tokens)))\n",
    "            axis.set_xticklabels(tokens, rotation=90, fontdict=fd_small)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(name, facecolor='white', format='pdf')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "242b1539",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fontdict = {'fontsize': 13}\n",
    "\n",
    "# batch_ind = 0\n",
    "\n",
    "# for c, attn_maps in enumerate(chunk_attn_maps):\n",
    "#     ls = len(attn_maps)\n",
    "#     hs = attn_maps[0].shape[-1]\n",
    "#     fig, axs = plt.subplots(ls, hs, figsize=(20, 10))\n",
    "#     for l, layer_maps in enumerate(attn_maps):\n",
    "#         for h in range(layer_maps.shape[-1]):\n",
    "#             attn_map = layer_maps[:, :, batch_ind, h]\n",
    "\n",
    "#             if ls == hs == 1:\n",
    "#                 axis = axs\n",
    "#             elif ls == 1:\n",
    "#                 axis = axs[h]\n",
    "#             elif hs == 1:\n",
    "#                 axis = axs[l]\n",
    "#             else: \n",
    "#                 axis = axs[l, h]\n",
    "#             axis.matshow(attn_map.cpu().detach().numpy(), cmap='Reds')\n",
    "            \n",
    "#             axis.set_ylabel(\"Layer \"+str(l+1), fontdict=fontdict)\n",
    "        \n",
    "#             if l == len(attn_maps)-1:\n",
    "#                 axis.set_xlabel(\"Head \"+str(h+1), fontdict=fontdict)\n",
    "            \n",
    "#             if l == 0:\n",
    "#                 axis.set_title(f'Chunk {c+1}', fontdict=fontdict)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afaf032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4e1c1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model._forward(data,mem_tokens = mem_tokens)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "5a98adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fontdict = {'fontsize': 10}\n",
    "\n",
    "# batch_ind = 10\n",
    "# ls = len(attn_maps)\n",
    "# hs = attn_maps[0].shape[-1]\n",
    "# fig, axs = plt.subplots(ls, hs, figsize=(20, 10))\n",
    "# for l, layer_maps in enumerate(attn_maps):\n",
    "# #     start = cn * param['enc_max_seq_len']\n",
    "# #     end = start + param['enc_max_seq_len']\n",
    "# #     tokens_x = ['[MEM]'] * num_memory_tokens + src_tokens_list[::-1][cn]\n",
    "# #     tokens_y = tokens_x\n",
    "\n",
    "# #     if len(chunk_maps) == 1:\n",
    "# #         chunk_maps = [chunk_maps]\n",
    "#     for h in range(layer_maps.shape[-1]):\n",
    "#         attn_map = layer_maps[:, :, batch_ind, h]\n",
    "        \n",
    "#         if ls == hs == 1:\n",
    "#             axis = axs\n",
    "#         elif ls == 1:\n",
    "#             axis = axs[h]\n",
    "#         elif hs == 1:\n",
    "#             axis = axs[l]\n",
    "#         else: \n",
    "#             axis = axs[l, h]\n",
    "#         axis.matshow(attn_map.cpu().detach().numpy(), cmap='Reds')\n",
    "#         axis.set_xlabel(\"Head \"+str(h+1), fontdict=fontdict)\n",
    "#         axis.set_ylabel(\"Layer \"+str(l+1), fontdict=fontdict)\n",
    "\n",
    "# #         axs[row, i].set_xticks(range(len(tokens_x)))\n",
    "# #         axs[row, i].set_xticklabels(tokens_x)\n",
    "# #         axs[row, i].set_xticklabels(tokens_x,fontdict=fontdict, rotation=90)\n",
    "\n",
    "# #         axs[row, i].set_yticks(range(len(tokens_y)))\n",
    "# #         axs[row, i].set_yticklabels(tokens_y, fontdict=fontdict)\n",
    "\n",
    "# #         axs[row, i].set_ylabel(f'Layer {l+1}. Chunk {cn+1}.', fontdict = fontdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b7fc62ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attn_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0d283181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].dec_attn(core_out, pos_emb, self.r_w_bias,\n",
    "#             self.r_r_bias, attn_mask=dec_attn_mask, mems=mems_i)\n",
    "w, r, r_w_bias, r_r_bias, attn_mask, mems = core_out, pos_emb, self.r_w_bias, self.r_r_bias, dec_attn_mask, mems_i\n",
    "qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)\n",
    "\n",
    "self1 = model.layers[0].dec_attn\n",
    "if mems is not None:\n",
    "    cat = torch.cat([mems, w], 0)\n",
    "    if self1.pre_lnorm:\n",
    "        w_heads = self1.qkv_net(self1.layer_norm(cat))\n",
    "    else:\n",
    "        w_heads = self1.qkv_net(cat)\n",
    "    r_head_k = self1.r_net(r)\n",
    "\n",
    "    w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n",
    "    w_head_q = w_head_q[-qlen:]\n",
    "else:\n",
    "    if self1.pre_lnorm:\n",
    "        w_heads = self1.qkv_net(self1.layer_norm(w))\n",
    "    else:\n",
    "        w_heads = self1.qkv_net(w)\n",
    "    r_head_k = self1.r_net(r)\n",
    "\n",
    "    w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n",
    "\n",
    "klen = w_head_k.size(0)\n",
    "\n",
    "w_head_q = w_head_q.view(qlen, bsz, self1.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n",
    "w_head_k = w_head_k.view(klen, bsz, self1.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n",
    "w_head_v = w_head_v.view(klen, bsz, self1.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n",
    "\n",
    "r_head_k = r_head_k.view(rlen, self1.n_head, self.d_head)                # qlen x n_head x d_head\n",
    "\n",
    "#### compute attention score\n",
    "rw_head_q = w_head_q + r_w_bias                                         # qlen x bsz x n_head x d_head\n",
    "AC = torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n",
    "\n",
    "rr_head_q = w_head_q + r_r_bias\n",
    "BD = torch.einsum('ibnd,jnd->ijbn', (rr_head_q, r_head_k))              # qlen x klen x bsz x n_head\n",
    "BD = self1._rel_shift(BD)\n",
    "\n",
    "# [qlen x klen x bsz x n_head]\n",
    "attn_score = AC + BD\n",
    "attn_score.mul_(self1.scale)\n",
    "\n",
    "#### compute attention probability\n",
    "if attn_mask is not None and attn_mask.any().item():\n",
    "    if attn_mask.dim() == 2:\n",
    "        attn_score = attn_score.float().masked_fill(\n",
    "            attn_mask[None,:,:,None].bool(), -float('inf')).type_as(attn_score)\n",
    "    elif attn_mask.dim() == 3:\n",
    "        attn_score = attn_score.float().masked_fill(\n",
    "            attn_mask[:,:,:,None].bool(), -float('inf')).type_as(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5b75191d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 32, 2])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f49453a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden, new_mems = model._forward(data, mems=mems, mem_tokens=mem_tokens)\n",
    "\n",
    "# num_mem = model.num_mem_tokens\n",
    "# if model.num_mem_tokens > 0:\n",
    "#     if model.mem_at_end:\n",
    "#         pred_hid = hidden[-tgt_len - num_mem:-num_mem]\n",
    "#         # mem_tokens_read = hidden[-tgt_len - 2*num_mem:-tgt_len - num_mem]\n",
    "#         mem_tokens_write = hidden[-num_mem:]\n",
    "#     else:\n",
    "#         pred_hid = hidden[-tgt_len:]\n",
    "#         mem_tokens_write = hidden[-tgt_len-num_mem:-tgt_len]\n",
    "# else:\n",
    "#     pred_hid = hidden[-tgt_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b7098e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9532, -1.8821, -1.0881,  ...,  0.0000,  1.2782,  0.0061],\n",
       "         [-1.6859,  1.8584,  0.0000,  ...,  0.8348, -0.7798,  0.4036],\n",
       "         [-3.0759,  0.1327,  1.5251,  ...,  2.0039,  0.0731,  0.2712],\n",
       "         ...,\n",
       "         [-1.1547,  2.3262,  5.8595,  ...,  1.6864, -0.3181,  0.1501],\n",
       "         [-0.3798,  4.9749, -0.7932,  ...,  0.1019, -0.3646, -0.5206],\n",
       "         [-0.0496,  0.9676,  0.0915,  ...,  0.0000, -0.5638,  0.1200]],\n",
       "\n",
       "        [[ 1.8450,  1.1259,  1.9215,  ..., -1.9701,  5.0149, -0.8256],\n",
       "         [-3.9350,  4.0707,  0.3410,  ...,  2.7524,  1.5695,  0.3983],\n",
       "         [-5.4039,  1.2346,  1.3648,  ...,  2.1260,  2.7881,  0.4112],\n",
       "         ...,\n",
       "         [ 0.0000, -1.4881,  2.1511,  ...,  1.7566,  2.2029, -0.3557],\n",
       "         [-1.9413,  3.0423,  1.3793,  ...,  1.1176,  2.4531,  0.0000],\n",
       "         [-3.8715,  3.2237, -0.5814,  ...,  2.8435,  0.0000, -0.8100]],\n",
       "\n",
       "        [[-1.1954, -0.2763,  0.0000,  ..., -0.0099,  1.1094, -0.9892],\n",
       "         [ 0.0000, -1.0579, -1.4567,  ...,  2.0959,  0.3414,  0.3849],\n",
       "         [-1.2391, -0.2831,  1.1788,  ...,  2.5443,  0.0000,  0.7146],\n",
       "         ...,\n",
       "         [-0.4725, -1.0622,  4.0108,  ...,  0.0000,  0.0000,  0.0259],\n",
       "         [ 0.0000,  0.0000,  1.1924,  ...,  1.0535,  0.4411,  1.1024],\n",
       "         [ 0.1322,  2.5791,  0.0000,  ...,  0.2108, -0.1756, -0.2041]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1328,  3.1463, -0.6830,  ...,  0.3725,  5.4878,  0.0000],\n",
       "         [-0.3572,  5.2434,  0.6007,  ...,  0.3910,  0.0000,  0.0000],\n",
       "         [-4.1204,  0.0000,  0.4774,  ...,  2.9972,  2.5634, -0.6348],\n",
       "         ...,\n",
       "         [-2.5625,  0.0000,  0.5767,  ...,  2.8052,  5.1717, -0.1588],\n",
       "         [-0.2659,  0.1333,  1.3946,  ...,  0.0675,  4.4518, -0.2184],\n",
       "         [-3.7421,  3.6906,  1.7548,  ...,  2.1734,  0.0000,  0.9533]],\n",
       "\n",
       "        [[-0.5415, -2.7183, -1.8504,  ...,  1.5502,  1.4190,  0.0000],\n",
       "         [-0.1634,  3.0153, -1.1745,  ...,  0.4160,  3.2860, -0.5773],\n",
       "         [ 0.0000, -0.9236, -0.0700,  ...,  0.0000,  4.2719, -0.5766],\n",
       "         ...,\n",
       "         [ 0.0000, -0.2355,  0.0000,  ...,  0.0000, -0.3511,  0.2785],\n",
       "         [-1.4636,  1.7968, -1.1108,  ...,  1.0201,  4.1642, -0.5042],\n",
       "         [-1.4856, -0.3688,  5.2112,  ...,  2.1544, -1.0272,  0.9231]],\n",
       "\n",
       "        [[ 0.0802,  3.1053, -0.2149,  ..., -0.1384,  3.2898, -2.2721],\n",
       "         [ 0.0000,  3.9711,  2.4205,  ..., -0.3083,  0.0067,  0.2030],\n",
       "         [ 0.0000,  3.3939,  3.5696,  ...,  2.7409, -0.0203, -0.3400],\n",
       "         ...,\n",
       "         [ 0.0000, -0.1823, -0.1534,  ...,  1.8210,  4.4575, -0.4606],\n",
       "         [-0.0096, -0.2709,  4.4357,  ..., -0.2738,  3.4662,  0.0000],\n",
       "         [-4.9917,  3.9166, -0.0857,  ...,  3.4486,  4.4603,  0.2473]]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_hid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b5461",
   "metadata": {},
   "source": [
    "### start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "2cda73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 parse_test.py --path ../evaluation\n",
    "# !python3 parse_test.py --path ../evaluation64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcdd468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class dummy:\n",
    "    init=1\n",
    "\n",
    "\n",
    "args = dummy\n",
    "args.init = 'normal'\n",
    "args.init_range = 0.1\n",
    "args.init_std = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4d8a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(weight):\n",
    "    if args.init == 'uniform':\n",
    "        nn.init.uniform_(weight, -args.init_range, args.init_range)\n",
    "    elif args.init == 'normal':\n",
    "        nn.init.normal_(weight, 0.0, args.init_std)\n",
    "\n",
    "def init_bias(bias):\n",
    "    nn.init.constant_(bias, 0.0)\n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            init_weight(m.weight)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            init_bias(m.bias)\n",
    "    elif classname.find('AdaptiveEmbedding') != -1:\n",
    "        if hasattr(m, 'n_emb_projs'):\n",
    "            for i in range(m.n_emb_projs):\n",
    "                if getattr(m, f'emb_projs_{i}') is not None:\n",
    "                    nn.init.normal_(getattr(m, f'emb_projs_{i}'), 0.0, args.proj_init_std)\n",
    "    elif classname.find('Embedding') != -1:\n",
    "        if hasattr(m, 'weight'):\n",
    "            init_weight(m.weight)\n",
    "    elif classname.find('ProjectedAdaptiveLogSoftmax') != -1:\n",
    "        if hasattr(m, 'cluster_weight') and m.cluster_weight is not None:\n",
    "            init_weight(m.cluster_weight)\n",
    "        if hasattr(m, 'cluster_bias') and m.cluster_bias is not None:\n",
    "            init_bias(m.cluster_bias)\n",
    "        if hasattr(m, 'n_out_projs'):\n",
    "            for i in range(m.n_out_projs):\n",
    "                if getattr(m, f'out_projs_{i}') is not None:\n",
    "                    nn.init.normal_(getattr(m, f'out_projs_{i}'), 0.0, args.proj_init_std)\n",
    "    elif classname.find('LayerNorm') != -1:\n",
    "        if hasattr(m, 'weight'):\n",
    "            nn.init.normal_(m.weight, 1.0, args.init_std)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            init_bias(m.bias)\n",
    "    elif classname.find('TransformerLM') != -1:\n",
    "        if hasattr(m, 'r_emb'):\n",
    "            init_weight(m.r_emb)\n",
    "        if hasattr(m, 'r_w_bias'):\n",
    "            init_weight(m.r_w_bias)\n",
    "        if hasattr(m, 'r_r_bias'):\n",
    "            init_weight(m.r_r_bias)\n",
    "        if hasattr(m, 'r_bias'):\n",
    "            init_bias(m.r_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb3dd23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveEmbedding(\n",
       "  (emb_layers): ModuleList(\n",
       "    (0): Embedding(10, 128)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mem_transformer import MemTransformerLM\n",
    "\n",
    "model = MemTransformerLM(10, n_layer=1, n_head=2, d_model=128,\n",
    "        d_head=64, d_inner=128, dropout=0, dropatt=False,\n",
    "        tie_weight=True, d_embed=128, div_val=1,\n",
    "        tie_projs=False, pre_lnorm=False, tgt_len=24,\n",
    "        ext_len=0, mem_len=0, cutoffs=[100],\n",
    "        num_mem_tokens=0, mem_at_end=True, read_mem_from_cache=True, \n",
    "        same_length=False, attn_type=0,)\n",
    "#         clamp_len=args.clamp_len, sample_softmax=args.sample_softmax)\n",
    "model.apply(weights_init)\n",
    "model.word_emb.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72dd7bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_model = nn.DataParallel(model, device_ids=[0, 1], dim=1, output_device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ea62c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0985,  0.1691, -0.0038,  ..., -0.2408, -0.1691, -0.1152],\n",
       "        [ 0.1081,  0.3223,  0.1111,  ...,  0.1258,  0.3097,  0.2033],\n",
       "        [-0.0880, -0.0667, -0.0200,  ...,  0.1696, -0.2326, -0.3018],\n",
       "        ...,\n",
       "        [ 0.0272,  0.1531, -0.3182,  ..., -0.1996, -0.0177, -0.0040],\n",
       "        [-0.2096, -0.0216, -0.0216,  ...,  0.0793,  0.5224,  0.2413],\n",
       "        [ 0.1567,  0.2344, -0.1599,  ...,  0.0427,  0.1003, -0.1154]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_model.module.layers[0].dec_attn.qkv_net.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a91c4d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0005, grad_fn=<MeanBackward0>),\n",
       " tensor(0.1994, grad_fn=<StdBackward>),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0985,  0.1691, -0.0038,  ..., -0.2408, -0.1691, -0.1152],\n",
       "         [ 0.1081,  0.3223,  0.1111,  ...,  0.1258,  0.3097,  0.2033],\n",
       "         [-0.0880, -0.0667, -0.0200,  ...,  0.1696, -0.2326, -0.3018],\n",
       "         ...,\n",
       "         [ 0.0272,  0.1531, -0.3182,  ..., -0.1996, -0.0177, -0.0040],\n",
       "         [-0.2096, -0.0216, -0.0216,  ...,  0.0793,  0.5224,  0.2413],\n",
       "         [ 0.1567,  0.2344, -0.1599,  ...,  0.0427,  0.1003, -0.1154]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].dec_attn.qkv_net.weight.mean(), model.layers[0].dec_attn.qkv_net.weight.std(), model.layers[0].dec_attn.qkv_net.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffebb0a",
   "metadata": {},
   "source": [
    "### runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12834166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[0]\n",
      "Experiment dir : ../evaluation64/test-copy/20220113-165650\n",
      "====================================================================================================\n",
      "    - data : /home/admin/x-transformers/data24\n",
      "    - dataset : copy\n",
      "    - n_layer : 4\n",
      "    - n_head : 4\n",
      "    - d_head : 64\n",
      "    - d_embed : 128\n",
      "    - d_model : 128\n",
      "    - d_inner : 256\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.0001\n",
      "    - mom : 0.0\n",
      "    - scheduler : dev_perf\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 1e-06\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 250000\n",
      "    - batch_size : 32\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 8\n",
      "    - eval_tgt_len : 8\n",
      "    - ext_len : 0\n",
      "    - mem_len : 0\n",
      "    - num_mem_tokens : 8\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : False\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : ../evaluation64/test-copy/20220113-165650\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : 50\n",
      "    - sample_softmax : -1\n",
      "    - patience : 3\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [0]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : True\n",
      "    - read_mem_from_cache : True\n",
      "    - answer_size : 48\n",
      "    - tied : True\n",
      "    - ntokens : 12\n",
      "    - n_all_param : 924172\n",
      "    - n_nonemb_param : 921088\n",
      "====================================================================================================\n",
      "#params = 924172\n",
      "#non emb params = 921088\n",
      "0.014384031295776367\n",
      "0.012453317642211914\n",
      "0.012887716293334961\n",
      "0.014344930648803711\n",
      "0.011752128601074219\n",
      "0.01321554183959961\n",
      "0.012583494186401367\n",
      "0.012475728988647461\n",
      "0.012231111526489258\n",
      "0.006891727447509766\n",
      "0.006827116012573242\n",
      "0.006208896636962891\n",
      "0.006954193115234375\n",
      "0.006595134735107422\n",
      "0.006580352783203125\n",
      "0.0066378116607666016\n",
      "0.00912332534790039\n",
      "0.010039806365966797\n",
      "0.00857686996459961\n",
      "0.007897377014160156\n",
      "0.008298873901367188\n",
      "0.007946491241455078\n",
      "0.007507801055908203\n",
      "0.0076062679290771484\n",
      "0.006876468658447266\n",
      "0.00735926628112793\n",
      "0.006577968597412109\n",
      "0.009942054748535156\n",
      "0.010612249374389648\n",
      "0.012047767639160156\n",
      "0.010819673538208008\n",
      "0.011752128601074219\n",
      "0.007909059524536133\n",
      "0.009424448013305664\n",
      "0.007154703140258789\n",
      "0.007513761520385742\n",
      "0.007785320281982422\n",
      "0.008414268493652344\n",
      "0.007273435592651367\n",
      "0.0076787471771240234\n",
      "0.00907278060913086\n",
      "0.007871389389038086\n",
      "0.008666753768920898\n",
      "0.009960174560546875\n",
      "0.006955623626708984\n",
      "0.00707554817199707\n",
      "0.007343292236328125\n",
      "0.006342411041259766\n",
      "0.007645845413208008\n",
      "0.006912708282470703\n",
      "0.006774425506591797\n",
      "0.0062408447265625\n",
      "0.007000923156738281\n",
      "0.006800651550292969\n",
      "0.008404731750488281\n",
      "0.0075778961181640625\n",
      "0.006833076477050781\n",
      "0.0075283050537109375\n",
      "0.007891178131103516\n",
      "0.011593818664550781\n",
      "0.008516550064086914\n",
      "0.00861501693725586\n",
      "0.008172750473022461\n",
      "0.008142948150634766\n",
      "0.007479667663574219\n",
      "0.007289409637451172\n",
      "0.007960081100463867\n",
      "0.009833574295043945\n",
      "0.007556915283203125\n",
      "0.007311582565307617\n",
      "0.007402658462524414\n",
      "0.007065773010253906\n",
      "0.007712125778198242\n",
      "0.007958650588989258\n",
      "0.008958816528320312\n",
      "0.007994413375854492\n",
      "0.007502079010009766\n",
      "0.007443904876708984\n",
      "0.007901906967163086\n",
      "0.007964372634887695\n",
      "0.007802486419677734\n",
      "0.008428812026977539\n",
      "0.007879495620727539\n",
      "0.007824182510375977\n",
      "0.007477283477783203\n",
      "0.007163047790527344\n",
      "0.006787538528442383\n",
      "0.007277488708496094\n",
      "0.006856441497802734\n",
      "0.007147073745727539\n",
      "0.009575843811035156\n",
      "0.0076863765716552734\n",
      "0.007741689682006836\n",
      "0.007412433624267578\n",
      "0.0072481632232666016\n",
      "0.007315635681152344\n",
      "0.0077021121978759766\n",
      "0.007490634918212891\n",
      "0.007354736328125\n",
      "0.008064985275268555\n",
      "0.0072727203369140625\n",
      "0.007311582565307617\n",
      "0.007977485656738281\n",
      "0.007831096649169922\n",
      "0.007393836975097656\n",
      "0.007514238357543945\n",
      "0.008160114288330078\n",
      "0.007117271423339844\n",
      "0.0075206756591796875\n",
      "0.007386445999145508\n",
      "0.006765604019165039\n",
      "0.006825447082519531\n",
      "0.0075914859771728516\n",
      "0.006905794143676758\n",
      "0.0077381134033203125\n",
      "0.007377147674560547\n",
      "0.007157087326049805\n",
      "0.012591361999511719\n",
      "0.011174917221069336\n",
      "0.007397174835205078\n",
      "0.00699162483215332\n",
      "0.008392810821533203\n",
      "0.0072956085205078125\n",
      "0.00705409049987793\n",
      "0.007429361343383789\n",
      "0.0075190067291259766\n",
      "0.007685184478759766\n",
      "0.012047767639160156\n",
      "0.00927114486694336\n",
      "0.0096282958984375\n",
      "0.010035276412963867\n",
      "0.008961915969848633\n",
      "0.009725809097290039\n",
      "0.009340047836303711\n",
      "0.009518623352050781\n",
      "0.008864879608154297\n",
      "0.009889364242553711\n",
      "0.009165763854980469\n",
      "0.008765459060668945\n",
      "0.009263277053833008\n",
      "0.009521961212158203\n",
      "0.01118612289428711\n",
      "0.008510828018188477\n",
      "0.009306907653808594\n",
      "0.010620355606079102\n",
      "0.009228706359863281\n",
      "0.009160995483398438\n",
      "0.008433103561401367\n",
      "0.008987188339233398\n",
      "0.009385347366333008\n",
      "0.009361743927001953\n",
      "0.009268045425415039\n",
      "0.008764505386352539\n",
      "0.00801992416381836\n",
      "0.009241104125976562\n",
      "0.008797645568847656\n",
      "0.007673501968383789\n",
      "0.007662534713745117\n",
      "0.007595062255859375\n",
      "0.007449626922607422\n",
      "0.0077152252197265625\n",
      "0.008208036422729492\n",
      "0.008118867874145508\n",
      "0.007311582565307617\n",
      "0.00821542739868164\n",
      "0.007096529006958008\n",
      "0.007443666458129883\n",
      "0.0071103572845458984\n",
      "0.0076029300689697266\n",
      "0.0076868534088134766\n",
      "0.00721430778503418\n",
      "0.007594108581542969\n",
      "0.00804281234741211\n",
      "0.007253170013427734\n",
      "0.0075092315673828125\n",
      "0.008751630783081055\n",
      "0.007636547088623047\n",
      "0.007027864456176758\n",
      "0.007861614227294922\n",
      "0.010643959045410156\n",
      "0.00857686996459961\n",
      "0.007609128952026367\n",
      "0.0070950984954833984\n",
      "0.007517099380493164\n",
      "^C\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exiting from training early\n",
      "Traceback (most recent call last):\n",
      "  File \"train_synthetic.py\", line 787, in <module>\n",
      "    with open(os.path.join(args.work_dir, 'model.pt'), 'rb') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../evaluation64/test-copy/20220113-165650/model.pt'\n"
     ]
    }
   ],
   "source": [
    "!bash run_copy.sh train --work_dir ../evaluation64/test --lr 0.0001 --tgt_len 8 --eval_tgt_len 8 --mem_len 0 --num_mem_tokens 8 --device_ids 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f3e89ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[0, 1]\n",
      "Experiment dir : ../evaluation64/test-copy/20220113-165713\n",
      "====================================================================================================\n",
      "    - data : /home/admin/x-transformers/data24\n",
      "    - dataset : copy\n",
      "    - n_layer : 4\n",
      "    - n_head : 4\n",
      "    - d_head : 64\n",
      "    - d_embed : 128\n",
      "    - d_model : 128\n",
      "    - d_inner : 256\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.0001\n",
      "    - mom : 0.0\n",
      "    - scheduler : dev_perf\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 1e-06\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 250000\n",
      "    - batch_size : 32\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 8\n",
      "    - eval_tgt_len : 8\n",
      "    - ext_len : 0\n",
      "    - mem_len : 0\n",
      "    - num_mem_tokens : 8\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : False\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : ../evaluation64/test-copy/20220113-165713\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : 50\n",
      "    - sample_softmax : -1\n",
      "    - patience : 3\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [0, 1]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : True\n",
      "    - read_mem_from_cache : True\n",
      "    - answer_size : 48\n",
      "    - tied : True\n",
      "    - ntokens : 12\n",
      "    - n_all_param : 924172\n",
      "    - n_nonemb_param : 921088\n",
      "====================================================================================================\n",
      "#params = 924172\n",
      "#non emb params = 921088\n",
      "3.2790133953094482\n",
      "0.01885366439819336\n",
      "0.02485203742980957\n",
      "0.029021263122558594\n",
      "0.028493404388427734\n",
      "0.02154827117919922\n",
      "0.02093029022216797\n",
      "0.022150278091430664\n",
      "0.022985219955444336\n",
      "0.022953510284423828\n",
      "0.02167367935180664\n",
      "0.023720502853393555\n",
      "0.019898653030395508\n",
      "0.02072429656982422\n",
      "0.019051074981689453\n",
      "0.018941164016723633\n",
      "0.01720285415649414\n",
      "0.022680997848510742\n",
      "0.024146318435668945\n",
      "0.020323514938354492\n",
      "0.029302120208740234\n",
      "0.023421049118041992\n",
      "0.016626596450805664\n",
      "0.018725156784057617\n",
      "0.018270015716552734\n",
      "0.019165992736816406\n",
      "0.017883777618408203\n",
      "0.02266693115234375\n",
      "0.026524782180786133\n",
      "0.02335977554321289\n",
      "0.019403934478759766\n",
      "0.017150163650512695\n",
      "0.021712303161621094\n",
      "0.01961517333984375\n",
      "0.024809598922729492\n",
      "0.017901182174682617\n",
      "0.022401094436645508\n",
      "0.019350528717041016\n",
      "0.01757645606994629\n",
      "0.01729106903076172\n",
      "0.017031431198120117\n",
      "0.019245624542236328\n",
      "0.021739959716796875\n",
      "0.022495031356811523\n",
      "0.017797470092773438\n",
      "0.023150920867919922\n",
      "0.02121710777282715\n",
      "0.01730823516845703\n",
      "0.020402193069458008\n",
      "0.017247676849365234\n",
      "0.02131342887878418\n",
      "0.018478870391845703\n",
      "0.01939535140991211\n",
      "0.01787257194519043\n",
      "0.02226996421813965\n",
      "0.018719911575317383\n",
      "0.019004344940185547\n",
      "0.0186004638671875\n",
      "0.021903514862060547\n",
      "0.01832127571105957\n",
      "0.02080368995666504\n",
      "0.022149324417114258\n",
      "0.02135944366455078\n",
      "0.021604537963867188\n",
      "0.020091533660888672\n",
      "0.03457975387573242\n",
      "0.02422308921813965\n",
      "0.018651962280273438\n",
      "0.017481327056884766\n",
      "0.018099069595336914\n",
      "0.01999378204345703\n",
      "0.020300865173339844\n",
      "0.020338773727416992\n",
      "0.019640445709228516\n",
      "0.019786357879638672\n",
      "0.022562026977539062\n",
      "0.01822972297668457\n",
      "0.01764965057373047\n",
      "0.018152713775634766\n",
      "0.020981311798095703\n",
      "0.017780303955078125\n",
      "0.025499820709228516\n",
      "0.025815486907958984\n",
      "0.018973827362060547\n",
      "0.019080162048339844\n",
      "0.019044876098632812\n",
      "0.019080400466918945\n",
      "0.023343563079833984\n",
      "0.02293229103088379\n",
      "0.0186460018157959\n",
      "0.028928279876708984\n",
      "0.024660825729370117\n",
      "0.019249916076660156\n",
      "0.022109270095825195\n",
      "^C\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exiting from training early\n",
      "Traceback (most recent call last):\n",
      "  File \"train_synthetic.py\", line 787, in <module>\n",
      "    with open(os.path.join(args.work_dir, 'model.pt'), 'rb') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../evaluation64/test-copy/20220113-165713/model.pt'\n"
     ]
    }
   ],
   "source": [
    "!bash run_copy.sh train --work_dir ../evaluation64/test --lr 0.0001 --tgt_len 8 --eval_tgt_len 8 --mem_len 0 --num_mem_tokens 8 --device_ids 0 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "26f5ee26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[0, 1]\n",
      "Experiment dir : LM-TFM-wt103/20220113-164320\n",
      "Loading cached dataset...\n",
      "====================================================================================================\n",
      "    - data : ../../data/wt103\n",
      "    - dataset : wt103\n",
      "    - n_layer : 16\n",
      "    - n_head : 10\n",
      "    - d_head : 41\n",
      "    - d_embed : 410\n",
      "    - d_model : 410\n",
      "    - d_inner : 2100\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.00025\n",
      "    - mom : 0.0\n",
      "    - scheduler : cosine\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 0.0\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 200000\n",
      "    - batch_size : 2\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 150\n",
      "    - eval_tgt_len : 150\n",
      "    - ext_len : 0\n",
      "    - mem_len : 150\n",
      "    - num_mem_tokens : 0\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : True\n",
      "    - div_val : 4\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : LM-TFM-wt103/20220113-164320\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : -1\n",
      "    - sample_softmax : -1\n",
      "    - patience : 0\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [0, 1]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : False\n",
      "    - read_mem_from_cache : False\n",
      "    - tied : True\n",
      "    - n_token : 267735\n",
      "    - n_all_param : 56373328\n",
      "    - n_nonemb_param : 41066400\n",
      "====================================================================================================\n",
      "#params = 56373328\n",
      "#non emb params = 41066400\n",
      "Parameter containing:\n",
      "tensor([[-0.0032,  0.0393,  0.0215,  ..., -0.0290,  0.0196, -0.0049],\n",
      "        [ 0.0167,  0.0309,  0.0005,  ..., -0.0074,  0.0044, -0.0441],\n",
      "        [-0.0260, -0.0100,  0.0114,  ...,  0.0092, -0.0675, -0.0231],\n",
      "        ...,\n",
      "        [ 0.0051,  0.0074,  0.0194,  ...,  0.0219,  0.0110, -0.0211],\n",
      "        [ 0.0060, -0.0554,  0.0053,  ...,  0.0108,  0.0097,  0.0167],\n",
      "        [-0.0168,  0.0171,  0.0074,  ...,  0.0041, -0.0027, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "/home/admin/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Parameter containing:\n",
      "tensor([[-0.0033,  0.0392,  0.0215,  ..., -0.0289,  0.0198, -0.0047],\n",
      "        [ 0.0169,  0.0307,  0.0004,  ..., -0.0074,  0.0043, -0.0443],\n",
      "        [-0.0261, -0.0102,  0.0116,  ...,  0.0094, -0.0676, -0.0230],\n",
      "        ...,\n",
      "        [ 0.0048,  0.0077,  0.0192,  ...,  0.0216,  0.0113, -0.0208],\n",
      "        [ 0.0057, -0.0552,  0.0051,  ...,  0.0106,  0.0099,  0.0164],\n",
      "        [-0.0171,  0.0174,  0.0071,  ...,  0.0038, -0.0025, -0.0049]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0034,  0.0391,  0.0214,  ..., -0.0288,  0.0199, -0.0047],\n",
      "        [ 0.0170,  0.0305,  0.0004,  ..., -0.0073,  0.0042, -0.0443],\n",
      "        [-0.0262, -0.0103,  0.0116,  ...,  0.0095, -0.0677, -0.0229],\n",
      "        ...,\n",
      "        [ 0.0047,  0.0077,  0.0190,  ...,  0.0215,  0.0115, -0.0206],\n",
      "        [ 0.0056, -0.0550,  0.0049,  ...,  0.0104,  0.0101,  0.0163],\n",
      "        [-0.0172,  0.0176,  0.0070,  ...,  0.0036, -0.0023, -0.0046]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0034,  0.0390,  0.0214,  ..., -0.0287,  0.0200, -0.0046],\n",
      "        [ 0.0171,  0.0304,  0.0004,  ..., -0.0072,  0.0042, -0.0444],\n",
      "        [-0.0262, -0.0104,  0.0117,  ...,  0.0095, -0.0678, -0.0229],\n",
      "        ...,\n",
      "        [ 0.0045,  0.0076,  0.0188,  ...,  0.0214,  0.0116, -0.0204],\n",
      "        [ 0.0054, -0.0550,  0.0047,  ...,  0.0103,  0.0102,  0.0162],\n",
      "        [-0.0173,  0.0177,  0.0068,  ...,  0.0035, -0.0022, -0.0044]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0034,  0.0390,  0.0213,  ..., -0.0287,  0.0200, -0.0046],\n",
      "        [ 0.0171,  0.0303,  0.0004,  ..., -0.0072,  0.0042, -0.0445],\n",
      "        [-0.0262, -0.0105,  0.0117,  ...,  0.0096, -0.0679, -0.0229],\n",
      "        ...,\n",
      "        [ 0.0044,  0.0076,  0.0187,  ...,  0.0214,  0.0118, -0.0203],\n",
      "        [ 0.0053, -0.0550,  0.0046,  ...,  0.0103,  0.0103,  0.0162],\n",
      "        [-0.0173,  0.0178,  0.0067,  ...,  0.0034, -0.0021, -0.0042]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0034,  0.0389,  0.0213,  ..., -0.0286,  0.0201, -0.0046],\n",
      "        [ 0.0172,  0.0303,  0.0004,  ..., -0.0071,  0.0042, -0.0445],\n",
      "        [-0.0262, -0.0105,  0.0117,  ...,  0.0096, -0.0679, -0.0228],\n",
      "        ...,\n",
      "        [ 0.0043,  0.0077,  0.0185,  ...,  0.0213,  0.0119, -0.0201],\n",
      "        [ 0.0052, -0.0550,  0.0045,  ...,  0.0102,  0.0104,  0.0161],\n",
      "        [-0.0174,  0.0179,  0.0066,  ...,  0.0033, -0.0020, -0.0041]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0389,  0.0212,  ..., -0.0286,  0.0201, -0.0045],\n",
      "        [ 0.0172,  0.0302,  0.0004,  ..., -0.0071,  0.0042, -0.0445],\n",
      "        [-0.0263, -0.0106,  0.0117,  ...,  0.0096, -0.0680, -0.0228],\n",
      "        ...,\n",
      "        [ 0.0042,  0.0077,  0.0184,  ...,  0.0212,  0.0120, -0.0200],\n",
      "        [ 0.0052, -0.0550,  0.0044,  ...,  0.0101,  0.0105,  0.0160],\n",
      "        [-0.0174,  0.0179,  0.0066,  ...,  0.0032, -0.0019, -0.0040]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0389,  0.0212,  ..., -0.0286,  0.0202, -0.0045],\n",
      "        [ 0.0172,  0.0302,  0.0004,  ..., -0.0071,  0.0042, -0.0446],\n",
      "        [-0.0263, -0.0106,  0.0117,  ...,  0.0097, -0.0680, -0.0228],\n",
      "        ...,\n",
      "        [ 0.0042,  0.0077,  0.0184,  ...,  0.0212,  0.0121, -0.0200],\n",
      "        [ 0.0051, -0.0550,  0.0044,  ...,  0.0101,  0.0105,  0.0160],\n",
      "        [-0.0175,  0.0180,  0.0065,  ...,  0.0031, -0.0019, -0.0038]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0389,  0.0212,  ..., -0.0285,  0.0202, -0.0045],\n",
      "        [ 0.0173,  0.0301,  0.0004,  ..., -0.0071,  0.0042, -0.0446],\n",
      "        [-0.0263, -0.0106,  0.0118,  ...,  0.0097, -0.0680, -0.0228],\n",
      "        ...,\n",
      "        [ 0.0041,  0.0077,  0.0183,  ...,  0.0211,  0.0121, -0.0199],\n",
      "        [ 0.0050, -0.0550,  0.0043,  ...,  0.0100,  0.0106,  0.0160],\n",
      "        [-0.0175,  0.0180,  0.0064,  ...,  0.0031, -0.0018, -0.0037]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0388,  0.0212,  ..., -0.0285,  0.0202, -0.0045],\n",
      "        [ 0.0173,  0.0301,  0.0004,  ..., -0.0071,  0.0042, -0.0446],\n",
      "        [-0.0263, -0.0106,  0.0118,  ...,  0.0097, -0.0680, -0.0228],\n",
      "        ...,\n",
      "        [ 0.0041,  0.0078,  0.0182,  ...,  0.0211,  0.0122, -0.0198],\n",
      "        [ 0.0050, -0.0550,  0.0042,  ...,  0.0100,  0.0106,  0.0159],\n",
      "        [-0.0176,  0.0181,  0.0064,  ...,  0.0030, -0.0018, -0.0037]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0388,  0.0212,  ..., -0.0285,  0.0203, -0.0045],\n",
      "        [ 0.0173,  0.0301,  0.0004,  ..., -0.0070,  0.0042, -0.0446],\n",
      "        [-0.0263, -0.0107,  0.0118,  ...,  0.0097, -0.0681, -0.0228],\n",
      "        ...,\n",
      "        [ 0.0040,  0.0078,  0.0182,  ...,  0.0210,  0.0123, -0.0198],\n",
      "        [ 0.0049, -0.0550,  0.0042,  ...,  0.0100,  0.0107,  0.0159],\n",
      "        [-0.0176,  0.0181,  0.0063,  ...,  0.0030, -0.0017, -0.0036]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0388,  0.0212,  ..., -0.0285,  0.0203, -0.0045],\n",
      "        [ 0.0173,  0.0300,  0.0004,  ..., -0.0070,  0.0042, -0.0446],\n",
      "        [-0.0263, -0.0107,  0.0118,  ...,  0.0097, -0.0681, -0.0227],\n",
      "        ...,\n",
      "        [ 0.0040,  0.0078,  0.0181,  ...,  0.0210,  0.0123, -0.0197],\n",
      "        [ 0.0049, -0.0550,  0.0041,  ...,  0.0099,  0.0107,  0.0159],\n",
      "        [-0.0176,  0.0181,  0.0063,  ...,  0.0029, -0.0017, -0.0035]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0388,  0.0212,  ..., -0.0285,  0.0203, -0.0045],\n",
      "        [ 0.0174,  0.0300,  0.0004,  ..., -0.0070,  0.0042, -0.0446],\n",
      "        [-0.0263, -0.0107,  0.0118,  ...,  0.0097, -0.0681, -0.0227],\n",
      "        ...,\n",
      "        [ 0.0039,  0.0078,  0.0181,  ...,  0.0210,  0.0124, -0.0197],\n",
      "        [ 0.0049, -0.0550,  0.0041,  ...,  0.0099,  0.0107,  0.0158],\n",
      "        [-0.0176,  0.0182,  0.0063,  ...,  0.0029, -0.0017, -0.0034]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0388,  0.0211,  ..., -0.0285,  0.0203, -0.0045],\n",
      "        [ 0.0174,  0.0300,  0.0004,  ..., -0.0070,  0.0042, -0.0447],\n",
      "        [-0.0263, -0.0107,  0.0118,  ...,  0.0098, -0.0681, -0.0227],\n",
      "        ...,\n",
      "        [ 0.0039,  0.0079,  0.0180,  ...,  0.0209,  0.0124, -0.0196],\n",
      "        [ 0.0048, -0.0550,  0.0040,  ...,  0.0099,  0.0108,  0.0158],\n",
      "        [-0.0177,  0.0182,  0.0062,  ...,  0.0028, -0.0016, -0.0034]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0388,  0.0211,  ..., -0.0285,  0.0203, -0.0045],\n",
      "        [ 0.0174,  0.0300,  0.0004,  ..., -0.0070,  0.0042, -0.0447],\n",
      "        [-0.0263, -0.0107,  0.0118,  ...,  0.0098, -0.0681, -0.0227],\n",
      "        ...,\n",
      "        [ 0.0039,  0.0079,  0.0180,  ...,  0.0209,  0.0124, -0.0196],\n",
      "        [ 0.0048, -0.0550,  0.0040,  ...,  0.0098,  0.0108,  0.0158],\n",
      "        [-0.0177,  0.0182,  0.0062,  ...,  0.0028, -0.0016, -0.0033]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0388,  0.0211,  ..., -0.0285,  0.0203, -0.0045],\n",
      "        [ 0.0174,  0.0299,  0.0004,  ..., -0.0070,  0.0042, -0.0447],\n",
      "        [-0.0263, -0.0107,  0.0118,  ...,  0.0098, -0.0681, -0.0227],\n",
      "        ...,\n",
      "        [ 0.0039,  0.0079,  0.0180,  ...,  0.0209,  0.0125, -0.0196],\n",
      "        [ 0.0048, -0.0550,  0.0040,  ...,  0.0098,  0.0108,  0.0158],\n",
      "        [-0.0177,  0.0182,  0.0062,  ...,  0.0028, -0.0016, -0.0033]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0388,  0.0211,  ..., -0.0285,  0.0203, -0.0045],\n",
      "        [ 0.0174,  0.0299,  0.0004,  ..., -0.0070,  0.0042, -0.0447],\n",
      "        [-0.0263, -0.0107,  0.0118,  ...,  0.0098, -0.0681, -0.0227],\n",
      "        ...,\n",
      "        [ 0.0038,  0.0079,  0.0179,  ...,  0.0208,  0.0125, -0.0195],\n",
      "        [ 0.0048, -0.0550,  0.0040,  ...,  0.0098,  0.0108,  0.0158],\n",
      "        [-0.0177,  0.0182,  0.0061,  ...,  0.0028, -0.0016, -0.0032]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0387,  0.0211,  ..., -0.0284,  0.0204, -0.0044],\n",
      "        [ 0.0174,  0.0299,  0.0004,  ..., -0.0070,  0.0042, -0.0447],\n",
      "        [-0.0263, -0.0108,  0.0118,  ...,  0.0098, -0.0682, -0.0227],\n",
      "        ...,\n",
      "        [ 0.0038,  0.0079,  0.0179,  ...,  0.0208,  0.0125, -0.0195],\n",
      "        [ 0.0047, -0.0550,  0.0039,  ...,  0.0098,  0.0109,  0.0157],\n",
      "        [-0.0177,  0.0183,  0.0061,  ...,  0.0027, -0.0016, -0.0032]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0387,  0.0211,  ..., -0.0284,  0.0204, -0.0044],\n",
      "        [ 0.0174,  0.0299,  0.0004,  ..., -0.0070,  0.0042, -0.0447],\n",
      "        [-0.0263, -0.0108,  0.0118,  ...,  0.0098, -0.0682, -0.0227],\n",
      "        ...,\n",
      "        [ 0.0038,  0.0079,  0.0179,  ...,  0.0208,  0.0126, -0.0195],\n",
      "        [ 0.0047, -0.0550,  0.0039,  ...,  0.0098,  0.0109,  0.0157],\n",
      "        [-0.0178,  0.0183,  0.0061,  ...,  0.0027, -0.0015, -0.0032]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0387,  0.0211,  ..., -0.0284,  0.0204, -0.0044],\n",
      "        [ 0.0174,  0.0299,  0.0004,  ..., -0.0070,  0.0042, -0.0447],\n",
      "        [-0.0263, -0.0108,  0.0118,  ...,  0.0098, -0.0682, -0.0227],\n",
      "        ...,\n",
      "        [ 0.0038,  0.0080,  0.0179,  ...,  0.0208,  0.0126, -0.0195],\n",
      "        [ 0.0047, -0.0550,  0.0039,  ...,  0.0098,  0.0109,  0.0157],\n",
      "        [-0.0178,  0.0183,  0.0061,  ...,  0.0027, -0.0015, -0.0031]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0387,  0.0211,  ..., -0.0284,  0.0204, -0.0044],\n",
      "        [ 0.0174,  0.0299,  0.0004,  ..., -0.0070,  0.0042, -0.0447],\n",
      "        [-0.0263, -0.0108,  0.0118,  ...,  0.0098, -0.0682, -0.0227],\n",
      "        ...,\n",
      "        [ 0.0038,  0.0080,  0.0178,  ...,  0.0208,  0.0126, -0.0194],\n",
      "        [ 0.0047, -0.0550,  0.0039,  ...,  0.0097,  0.0109,  0.0157],\n",
      "        [-0.0178,  0.0183,  0.0061,  ...,  0.0027, -0.0015, -0.0031]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "^C\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "!bash run_wt103_base.sh train --gpu0_bsz -1 --batch_size 2 --device_ids 0 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b97efb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[0]\n",
      "Experiment dir : LM-TFM-wt103/20220113-164351\n",
      "Loading cached dataset...\n",
      "====================================================================================================\n",
      "    - data : ../../data/wt103\n",
      "    - dataset : wt103\n",
      "    - n_layer : 16\n",
      "    - n_head : 10\n",
      "    - d_head : 41\n",
      "    - d_embed : 410\n",
      "    - d_model : 410\n",
      "    - d_inner : 2100\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.00025\n",
      "    - mom : 0.0\n",
      "    - scheduler : cosine\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 0.0\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 200000\n",
      "    - batch_size : 2\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 150\n",
      "    - eval_tgt_len : 150\n",
      "    - ext_len : 0\n",
      "    - mem_len : 150\n",
      "    - num_mem_tokens : 0\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : True\n",
      "    - div_val : 4\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : LM-TFM-wt103/20220113-164351\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : -1\n",
      "    - sample_softmax : -1\n",
      "    - patience : 0\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [0]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : False\n",
      "    - read_mem_from_cache : False\n",
      "    - tied : True\n",
      "    - n_token : 267735\n",
      "    - n_all_param : 56373328\n",
      "    - n_nonemb_param : 41066400\n",
      "====================================================================================================\n",
      "#params = 56373328\n",
      "#non emb params = 41066400\n",
      "Parameter containing:\n",
      "tensor([[-0.0032,  0.0393,  0.0215,  ..., -0.0290,  0.0196, -0.0049],\n",
      "        [ 0.0167,  0.0309,  0.0005,  ..., -0.0074,  0.0044, -0.0441],\n",
      "        [-0.0260, -0.0100,  0.0114,  ...,  0.0092, -0.0675, -0.0231],\n",
      "        ...,\n",
      "        [ 0.0051,  0.0074,  0.0194,  ...,  0.0219,  0.0110, -0.0211],\n",
      "        [ 0.0060, -0.0554,  0.0053,  ...,  0.0108,  0.0097,  0.0167],\n",
      "        [-0.0168,  0.0171,  0.0074,  ...,  0.0041, -0.0027, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "/home/admin/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Parameter containing:\n",
      "tensor([[-0.0033,  0.0391,  0.0217,  ..., -0.0290,  0.0197, -0.0048],\n",
      "        [ 0.0166,  0.0307,  0.0004,  ..., -0.0075,  0.0045, -0.0443],\n",
      "        [-0.0261, -0.0101,  0.0116,  ...,  0.0093, -0.0674, -0.0232],\n",
      "        ...,\n",
      "        [ 0.0053,  0.0077,  0.0192,  ...,  0.0216,  0.0108, -0.0213],\n",
      "        [ 0.0057, -0.0557,  0.0051,  ...,  0.0106,  0.0100,  0.0169],\n",
      "        [-0.0171,  0.0174,  0.0071,  ...,  0.0038, -0.0025, -0.0054]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0034,  0.0390,  0.0218,  ..., -0.0289,  0.0197, -0.0049],\n",
      "        [ 0.0165,  0.0306,  0.0004,  ..., -0.0075,  0.0046, -0.0443],\n",
      "        [-0.0262, -0.0101,  0.0117,  ...,  0.0094, -0.0674, -0.0234],\n",
      "        ...,\n",
      "        [ 0.0055,  0.0078,  0.0190,  ...,  0.0215,  0.0106, -0.0214],\n",
      "        [ 0.0056, -0.0558,  0.0049,  ...,  0.0104,  0.0101,  0.0171],\n",
      "        [-0.0172,  0.0172,  0.0069,  ...,  0.0037, -0.0023, -0.0054]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0390,  0.0219,  ..., -0.0289,  0.0197, -0.0048],\n",
      "        [ 0.0165,  0.0305,  0.0004,  ..., -0.0075,  0.0046, -0.0444],\n",
      "        [-0.0263, -0.0101,  0.0117,  ...,  0.0094, -0.0674, -0.0235],\n",
      "        ...,\n",
      "        [ 0.0056,  0.0079,  0.0188,  ...,  0.0213,  0.0105, -0.0215],\n",
      "        [ 0.0055, -0.0559,  0.0048,  ...,  0.0104,  0.0102,  0.0172],\n",
      "        [-0.0173,  0.0170,  0.0068,  ...,  0.0035, -0.0022, -0.0054]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0389,  0.0220,  ..., -0.0288,  0.0197, -0.0048],\n",
      "        [ 0.0165,  0.0305,  0.0004,  ..., -0.0074,  0.0046, -0.0444],\n",
      "        [-0.0264, -0.0101,  0.0118,  ...,  0.0094, -0.0674, -0.0235],\n",
      "        ...,\n",
      "        [ 0.0058,  0.0080,  0.0187,  ...,  0.0212,  0.0104, -0.0215],\n",
      "        [ 0.0054, -0.0560,  0.0047,  ...,  0.0104,  0.0103,  0.0174],\n",
      "        [-0.0174,  0.0169,  0.0066,  ...,  0.0034, -0.0021, -0.0053]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0389,  0.0220,  ..., -0.0288,  0.0197, -0.0048],\n",
      "        [ 0.0165,  0.0304,  0.0004,  ..., -0.0074,  0.0046, -0.0444],\n",
      "        [-0.0264, -0.0101,  0.0118,  ...,  0.0094, -0.0674, -0.0236],\n",
      "        ...,\n",
      "        [ 0.0059,  0.0081,  0.0186,  ...,  0.0212,  0.0104, -0.0215],\n",
      "        [ 0.0053, -0.0561,  0.0046,  ...,  0.0104,  0.0104,  0.0175],\n",
      "        [-0.0175,  0.0167,  0.0065,  ...,  0.0034, -0.0020, -0.0053]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0388,  0.0221,  ..., -0.0287,  0.0198, -0.0048],\n",
      "        [ 0.0165,  0.0304,  0.0004,  ..., -0.0074,  0.0046, -0.0444],\n",
      "        [-0.0264, -0.0101,  0.0119,  ...,  0.0094, -0.0674, -0.0236],\n",
      "        ...,\n",
      "        [ 0.0059,  0.0081,  0.0185,  ...,  0.0211,  0.0103, -0.0216],\n",
      "        [ 0.0053, -0.0561,  0.0045,  ...,  0.0104,  0.0104,  0.0176],\n",
      "        [-0.0176,  0.0165,  0.0064,  ...,  0.0033, -0.0019, -0.0053]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0388,  0.0221,  ..., -0.0287,  0.0198, -0.0048],\n",
      "        [ 0.0165,  0.0304,  0.0004,  ..., -0.0074,  0.0046, -0.0445],\n",
      "        [-0.0265, -0.0101,  0.0119,  ...,  0.0094, -0.0674, -0.0237],\n",
      "        ...,\n",
      "        [ 0.0060,  0.0082,  0.0184,  ...,  0.0210,  0.0102, -0.0216],\n",
      "        [ 0.0052, -0.0562,  0.0045,  ...,  0.0104,  0.0105,  0.0176],\n",
      "        [-0.0177,  0.0163,  0.0063,  ...,  0.0032, -0.0017, -0.0053]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0388,  0.0221,  ..., -0.0287,  0.0198, -0.0048],\n",
      "        [ 0.0165,  0.0303,  0.0004,  ..., -0.0074,  0.0046, -0.0445],\n",
      "        [-0.0265, -0.0101,  0.0119,  ...,  0.0094, -0.0674, -0.0237],\n",
      "        ...,\n",
      "        [ 0.0061,  0.0082,  0.0183,  ...,  0.0210,  0.0102, -0.0216],\n",
      "        [ 0.0052, -0.0562,  0.0044,  ...,  0.0104,  0.0105,  0.0177],\n",
      "        [-0.0178,  0.0162,  0.0063,  ...,  0.0032, -0.0017, -0.0053]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0035,  0.0388,  0.0221,  ..., -0.0287,  0.0198, -0.0048],\n",
      "        [ 0.0165,  0.0303,  0.0004,  ..., -0.0074,  0.0046, -0.0445],\n",
      "        [-0.0265, -0.0101,  0.0119,  ...,  0.0094, -0.0674, -0.0237],\n",
      "        ...,\n",
      "        [ 0.0061,  0.0083,  0.0182,  ...,  0.0209,  0.0101, -0.0217],\n",
      "        [ 0.0051, -0.0563,  0.0044,  ...,  0.0104,  0.0106,  0.0178],\n",
      "        [-0.0179,  0.0160,  0.0062,  ...,  0.0032, -0.0016, -0.0052]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0388,  0.0222,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0165,  0.0303,  0.0004,  ..., -0.0074,  0.0046, -0.0445],\n",
      "        [-0.0265, -0.0101,  0.0119,  ...,  0.0094, -0.0674, -0.0238],\n",
      "        ...,\n",
      "        [ 0.0062,  0.0083,  0.0182,  ...,  0.0209,  0.0101, -0.0217],\n",
      "        [ 0.0051, -0.0563,  0.0043,  ...,  0.0104,  0.0106,  0.0178],\n",
      "        [-0.0179,  0.0159,  0.0061,  ...,  0.0032, -0.0015, -0.0052]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0222,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0165,  0.0303,  0.0004,  ..., -0.0074,  0.0046, -0.0445],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0238],\n",
      "        ...,\n",
      "        [ 0.0062,  0.0084,  0.0181,  ...,  0.0208,  0.0101, -0.0217],\n",
      "        [ 0.0051, -0.0564,  0.0043,  ...,  0.0103,  0.0106,  0.0178],\n",
      "        [-0.0180,  0.0158,  0.0061,  ...,  0.0031, -0.0014, -0.0052]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0222,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0165,  0.0303,  0.0004,  ..., -0.0074,  0.0046, -0.0445],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0238],\n",
      "        ...,\n",
      "        [ 0.0063,  0.0084,  0.0181,  ...,  0.0208,  0.0100, -0.0217],\n",
      "        [ 0.0050, -0.0564,  0.0043,  ...,  0.0103,  0.0107,  0.0179],\n",
      "        [-0.0180,  0.0157,  0.0060,  ...,  0.0031, -0.0014, -0.0052]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0222,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0165,  0.0302,  0.0004,  ..., -0.0074,  0.0046, -0.0445],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0238],\n",
      "        ...,\n",
      "        [ 0.0063,  0.0084,  0.0180,  ...,  0.0207,  0.0100, -0.0218],\n",
      "        [ 0.0050, -0.0564,  0.0042,  ...,  0.0103,  0.0107,  0.0179],\n",
      "        [-0.0181,  0.0156,  0.0060,  ...,  0.0031, -0.0013, -0.0052]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0222,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0165,  0.0302,  0.0004,  ..., -0.0074,  0.0046, -0.0445],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0238],\n",
      "        ...,\n",
      "        [ 0.0063,  0.0085,  0.0180,  ...,  0.0207,  0.0100, -0.0218],\n",
      "        [ 0.0050, -0.0564,  0.0042,  ...,  0.0103,  0.0107,  0.0180],\n",
      "        [-0.0181,  0.0155,  0.0059,  ...,  0.0031, -0.0013, -0.0052]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0222,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0074,  0.0046, -0.0445],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0238],\n",
      "        ...,\n",
      "        [ 0.0064,  0.0085,  0.0179,  ...,  0.0207,  0.0099, -0.0218],\n",
      "        [ 0.0050, -0.0565,  0.0042,  ...,  0.0103,  0.0107,  0.0180],\n",
      "        [-0.0181,  0.0154,  0.0059,  ...,  0.0030, -0.0012, -0.0052]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0223,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0074,  0.0046, -0.0445],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0238],\n",
      "        ...,\n",
      "        [ 0.0064,  0.0085,  0.0179,  ...,  0.0207,  0.0099, -0.0218],\n",
      "        [ 0.0050, -0.0565,  0.0041,  ...,  0.0103,  0.0108,  0.0180],\n",
      "        [-0.0182,  0.0154,  0.0059,  ...,  0.0030, -0.0012, -0.0052]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0223,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0074,  0.0046, -0.0445],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0064,  0.0085,  0.0179,  ...,  0.0206,  0.0099, -0.0218],\n",
      "        [ 0.0049, -0.0565,  0.0041,  ...,  0.0103,  0.0108,  0.0180],\n",
      "        [-0.0182,  0.0153,  0.0058,  ...,  0.0030, -0.0011, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0223,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0074,  0.0046, -0.0445],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0064,  0.0085,  0.0178,  ...,  0.0206,  0.0099, -0.0218],\n",
      "        [ 0.0049, -0.0565,  0.0041,  ...,  0.0103,  0.0108,  0.0181],\n",
      "        [-0.0182,  0.0152,  0.0058,  ...,  0.0030, -0.0011, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0223,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0073,  0.0046, -0.0445],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0065,  0.0085,  0.0178,  ...,  0.0206,  0.0099, -0.0219],\n",
      "        [ 0.0049, -0.0565,  0.0041,  ...,  0.0103,  0.0108,  0.0181],\n",
      "        [-0.0182,  0.0152,  0.0058,  ...,  0.0030, -0.0011, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0223,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0073,  0.0046, -0.0445],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0065,  0.0086,  0.0178,  ...,  0.0206,  0.0098, -0.0219],\n",
      "        [ 0.0049, -0.0565,  0.0041,  ...,  0.0103,  0.0108,  0.0181],\n",
      "        [-0.0183,  0.0151,  0.0058,  ...,  0.0030, -0.0010, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0223,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0073,  0.0046, -0.0445],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0065,  0.0086,  0.0178,  ...,  0.0206,  0.0098, -0.0219],\n",
      "        [ 0.0049, -0.0565,  0.0041,  ...,  0.0103,  0.0108,  0.0181],\n",
      "        [-0.0183,  0.0151,  0.0057,  ...,  0.0030, -0.0010, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0223,  ..., -0.0286,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0073,  0.0046, -0.0446],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0065,  0.0086,  0.0178,  ...,  0.0206,  0.0098, -0.0219],\n",
      "        [ 0.0049, -0.0566,  0.0040,  ...,  0.0103,  0.0109,  0.0181],\n",
      "        [-0.0183,  0.0150,  0.0057,  ...,  0.0030, -0.0010, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0223,  ..., -0.0285,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0073,  0.0046, -0.0446],\n",
      "        [-0.0266, -0.0101,  0.0120,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0065,  0.0086,  0.0177,  ...,  0.0205,  0.0098, -0.0219],\n",
      "        [ 0.0049, -0.0566,  0.0040,  ...,  0.0103,  0.0109,  0.0181],\n",
      "        [-0.0183,  0.0150,  0.0057,  ...,  0.0030, -0.0010, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0223,  ..., -0.0285,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0073,  0.0046, -0.0446],\n",
      "        [-0.0266, -0.0100,  0.0120,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0066,  0.0086,  0.0177,  ...,  0.0205,  0.0098, -0.0219],\n",
      "        [ 0.0049, -0.0566,  0.0040,  ...,  0.0103,  0.0109,  0.0182],\n",
      "        [-0.0183,  0.0150,  0.0057,  ...,  0.0030, -0.0009, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0223,  ..., -0.0285,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0073,  0.0046, -0.0446],\n",
      "        [-0.0266, -0.0100,  0.0120,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0066,  0.0086,  0.0177,  ...,  0.0205,  0.0098, -0.0219],\n",
      "        [ 0.0049, -0.0566,  0.0040,  ...,  0.0102,  0.0109,  0.0182],\n",
      "        [-0.0183,  0.0149,  0.0057,  ...,  0.0030, -0.0009, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0387,  0.0223,  ..., -0.0285,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0073,  0.0046, -0.0446],\n",
      "        [-0.0266, -0.0100,  0.0121,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0066,  0.0086,  0.0177,  ...,  0.0205,  0.0098, -0.0219],\n",
      "        [ 0.0048, -0.0566,  0.0040,  ...,  0.0102,  0.0109,  0.0182],\n",
      "        [-0.0184,  0.0149,  0.0057,  ...,  0.0030, -0.0009, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0386,  0.0223,  ..., -0.0285,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0302,  0.0004,  ..., -0.0073,  0.0046, -0.0446],\n",
      "        [-0.0267, -0.0100,  0.0121,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0066,  0.0086,  0.0177,  ...,  0.0205,  0.0098, -0.0219],\n",
      "        [ 0.0048, -0.0566,  0.0040,  ...,  0.0102,  0.0109,  0.0182],\n",
      "        [-0.0184,  0.0149,  0.0057,  ...,  0.0029, -0.0009, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0386,  0.0223,  ..., -0.0285,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0301,  0.0004,  ..., -0.0073,  0.0046, -0.0446],\n",
      "        [-0.0267, -0.0100,  0.0121,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0066,  0.0086,  0.0177,  ...,  0.0205,  0.0098, -0.0219],\n",
      "        [ 0.0048, -0.0566,  0.0040,  ...,  0.0102,  0.0109,  0.0182],\n",
      "        [-0.0184,  0.0148,  0.0056,  ...,  0.0029, -0.0009, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0386,  0.0223,  ..., -0.0285,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0301,  0.0004,  ..., -0.0073,  0.0046, -0.0446],\n",
      "        [-0.0267, -0.0100,  0.0121,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0066,  0.0086,  0.0177,  ...,  0.0205,  0.0098, -0.0219],\n",
      "        [ 0.0048, -0.0566,  0.0040,  ...,  0.0102,  0.0109,  0.0182],\n",
      "        [-0.0184,  0.0148,  0.0056,  ...,  0.0029, -0.0009, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0386,  0.0223,  ..., -0.0285,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0301,  0.0004,  ..., -0.0073,  0.0046, -0.0446],\n",
      "        [-0.0267, -0.0100,  0.0121,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0066,  0.0087,  0.0177,  ...,  0.0205,  0.0098, -0.0219],\n",
      "        [ 0.0048, -0.0566,  0.0040,  ...,  0.0102,  0.0109,  0.0182],\n",
      "        [-0.0184,  0.0148,  0.0056,  ...,  0.0029, -0.0008, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0386,  0.0223,  ..., -0.0285,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0301,  0.0004,  ..., -0.0073,  0.0046, -0.0446],\n",
      "        [-0.0267, -0.0100,  0.0121,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0066,  0.0087,  0.0177,  ...,  0.0205,  0.0097, -0.0219],\n",
      "        [ 0.0048, -0.0566,  0.0040,  ...,  0.0102,  0.0109,  0.0182],\n",
      "        [-0.0184,  0.0148,  0.0056,  ...,  0.0029, -0.0008, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0036,  0.0386,  0.0223,  ..., -0.0285,  0.0198, -0.0048],\n",
      "        [ 0.0164,  0.0301,  0.0004,  ..., -0.0073,  0.0046, -0.0446],\n",
      "        [-0.0267, -0.0100,  0.0121,  ...,  0.0094, -0.0674, -0.0239],\n",
      "        ...,\n",
      "        [ 0.0066,  0.0087,  0.0176,  ...,  0.0205,  0.0097, -0.0219],\n",
      "        [ 0.0048, -0.0566,  0.0040,  ...,  0.0102,  0.0109,  0.0182],\n",
      "        [-0.0184,  0.0147,  0.0056,  ...,  0.0029, -0.0008, -0.0051]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "^C\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exiting from training early\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 614, in <module>\n",
      "    with open(os.path.join(args.work_dir, 'model.pt'), 'rb') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'LM-TFM-wt103/20220113-164351/model.pt'\n"
     ]
    }
   ],
   "source": [
    "!bash run_wt103_base.sh train --gpu0_bsz -1 --batch_size 2 --device_ids 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20b721d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[0, 2]\n",
      "Experiment dir : ../evaluation/test-retrieval/20220113-174715\n",
      "====================================================================================================\n",
      "    - data : /home/admin/x-transformers/data\n",
      "    - dataset : retrieval\n",
      "    - n_layer : 4\n",
      "    - n_head : 2\n",
      "    - d_head : 32\n",
      "    - d_embed : 64\n",
      "    - d_model : 64\n",
      "    - d_inner : 128\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.0001\n",
      "    - mom : 0.0\n",
      "    - scheduler : dev_perf\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 1e-06\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 250000\n",
      "    - batch_size : 2\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 10\n",
      "    - eval_tgt_len : 10\n",
      "    - ext_len : 0\n",
      "    - mem_len : 0\n",
      "    - num_mem_tokens : 0\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : False\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : ../evaluation/test-retrieval/20220113-174715\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : 50\n",
      "    - sample_softmax : -1\n",
      "    - patience : 3\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [0, 2]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : True\n",
      "    - read_mem_from_cache : True\n",
      "    - answer_size : 1\n",
      "    - tied : True\n",
      "    - ntokens : 37\n",
      "    - n_all_param : 151781\n",
      "    - n_nonemb_param : 149248\n",
      "====================================================================================================\n",
      "#params = 151781\n",
      "#non emb params = 149248\n",
      "| epoch   1 step      200 |    200 batches | lr 0.0001 | ms/batch 67.06253 | loss 3.19397 | ppl    24.385\n",
      "| epoch   1 step      400 |    400 batches | lr 0.0001 | ms/batch 47.67098 | loss 2.65098 | ppl    14.168\n",
      "| epoch   1 step      600 |    600 batches | lr 0.0001 | ms/batch 48.80099 | loss 2.48583 | ppl    12.011\n",
      "| epoch   1 step      800 |    800 batches | lr 0.0001 | ms/batch 48.23248 | loss 2.41459 | ppl    11.185\n",
      "| epoch   1 step     1000 |   1000 batches | lr 0.0001 | ms/batch 47.73203 | loss 2.38022 | ppl    10.807\n",
      "| epoch   1 step     1200 |   1200 batches | lr 0.0001 | ms/batch 46.96503 | loss 2.32374 | ppl    10.214\n",
      "| epoch   1 step     1400 |   1400 batches | lr 0.0001 | ms/batch 47.79328 | loss 2.26550 | ppl     9.636\n",
      "| epoch   1 step     1600 |   1600 batches | lr 0.0001 | ms/batch 49.43492 | loss 2.20743 | ppl     9.092\n",
      "| epoch   1 step     1800 |   1800 batches | lr 0.0001 | ms/batch 48.31326 | loss 2.19296 | ppl     8.962\n",
      "| epoch   1 step     2000 |   2000 batches | lr 0.0001 | ms/batch 48.43036 | loss 2.12333 | ppl     8.359\n",
      "| epoch   1 step     2200 |   2200 batches | lr 0.0001 | ms/batch 47.94430 | loss 2.10240 | ppl     8.186\n",
      "| epoch   1 step     2400 |   2400 batches | lr 0.0001 | ms/batch 49.26441 | loss 2.06797 | ppl     7.909\n",
      "| epoch   1 step     2600 |   2600 batches | lr 0.0001 | ms/batch 48.57509 | loss 2.02951 | ppl     7.610\n",
      "| epoch   1 step     2800 |   2800 batches | lr 0.0001 | ms/batch 48.12732 | loss 1.99450 | ppl     7.349\n",
      "| epoch   1 step     3000 |   3000 batches | lr 0.0001 | ms/batch 47.46184 | loss 1.96434 | ppl     7.130\n",
      "| epoch   1 step     3200 |   3200 batches | lr 0.0001 | ms/batch 48.25170 | loss 1.92124 | ppl     6.829\n",
      "| epoch   1 step     3400 |   3400 batches | lr 0.0001 | ms/batch 49.13155 | loss 1.92152 | ppl     6.831\n",
      "| epoch   1 step     3600 |   3600 batches | lr 0.0001 | ms/batch 49.38801 | loss 1.89841 | ppl     6.675\n",
      "| epoch   1 step     3800 |   3800 batches | lr 0.0001 | ms/batch 47.30689 | loss 1.88138 | ppl     6.563\n",
      "| epoch   1 step     4000 |   4000 batches | lr 0.0001 | ms/batch 47.94352 | loss 1.85431 | ppl     6.387\n",
      "|\n",
      "Source: [36  7 29  8 20  9 15  0 15 10]\n",
      "Target: [ 7 29  8 20  9 15  0 15 10  0]\n",
      "Teacher forcing: acc:0.3\n",
      "Preds:  [34  7 29  8 20  9 15  0 15  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step     4000 | time: 198.18s | valid loss 1.80625 | valid ppl    6.0876 | valid acc 0.3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     4200 |   4200 batches | lr 0.0001 | ms/batch 54.35634 | loss 1.84076 | ppl     6.301\n",
      "| epoch   1 step     4400 |   4400 batches | lr 0.0001 | ms/batch 48.51163 | loss 1.81741 | ppl     6.156\n",
      "| epoch   1 step     4600 |   4600 batches | lr 0.0001 | ms/batch 48.79801 | loss 1.79429 | ppl     6.015\n",
      "| epoch   1 step     4800 |   4800 batches | lr 0.0001 | ms/batch 48.40742 | loss 1.76769 | ppl     5.857\n",
      "| epoch   1 step     5000 |   5000 batches | lr 0.0001 | ms/batch 49.06423 | loss 1.78206 | ppl     5.942\n",
      "| epoch   1 step     5200 |   5200 batches | lr 0.0001 | ms/batch 49.08030 | loss 1.76618 | ppl     5.848\n",
      "| epoch   1 step     5400 |   5400 batches | lr 0.0001 | ms/batch 47.94515 | loss 1.74620 | ppl     5.733\n",
      "| epoch   1 step     5600 |   5600 batches | lr 0.0001 | ms/batch 47.33218 | loss 1.72708 | ppl     5.624\n",
      "| epoch   1 step     5800 |   5800 batches | lr 0.0001 | ms/batch 50.38702 | loss 1.73884 | ppl     5.691\n",
      "| epoch   1 step     6000 |   6000 batches | lr 0.0001 | ms/batch 50.15918 | loss 1.72992 | ppl     5.640\n",
      "| epoch   1 step     6200 |   6200 batches | lr 0.0001 | ms/batch 49.60191 | loss 1.70268 | ppl     5.489\n",
      "| epoch   1 step     6400 |   6400 batches | lr 0.0001 | ms/batch 48.88404 | loss 1.68999 | ppl     5.419\n",
      "| epoch   1 step     6600 |   6600 batches | lr 0.0001 | ms/batch 48.17276 | loss 1.67622 | ppl     5.345\n",
      "| epoch   1 step     6800 |   6800 batches | lr 0.0001 | ms/batch 48.86685 | loss 1.67059 | ppl     5.315\n",
      "| epoch   1 step     7000 |   7000 batches | lr 0.0001 | ms/batch 48.45829 | loss 1.65739 | ppl     5.246\n",
      "| epoch   1 step     7200 |   7200 batches | lr 0.0001 | ms/batch 50.88583 | loss 1.67343 | ppl     5.330\n",
      "| epoch   1 step     7400 |   7400 batches | lr 0.0001 | ms/batch 47.62621 | loss 1.64788 | ppl     5.196\n",
      "| epoch   1 step     7600 |   7600 batches | lr 0.0001 | ms/batch 48.53717 | loss 1.66245 | ppl     5.272\n",
      "| epoch   1 step     7800 |   7800 batches | lr 0.0001 | ms/batch 48.77168 | loss 1.65479 | ppl     5.232\n",
      "| epoch   1 step     8000 |   8000 batches | lr 0.0001 | ms/batch 48.35628 | loss 1.63867 | ppl     5.148\n",
      "|\n",
      "Source: [17  8 32  6 26  1 13  4 17 10]\n",
      "Target: [ 8 32  6 26  1 13  4 17 10  8]\n",
      "Teacher forcing: acc:0.22\n",
      "Preds:  [17  8 32  6 26  1 13  4 17  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step     8000 | time: 196.32s | valid loss 1.62925 | valid ppl    5.1001 | valid acc 0.22\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     8200 |   8200 batches | lr 0.0001 | ms/batch 54.96695 | loss 1.63427 | ppl     5.126\n",
      "| epoch   1 step     8400 |   8400 batches | lr 0.0001 | ms/batch 48.74202 | loss 1.64327 | ppl     5.172\n",
      "| epoch   1 step     8600 |   8600 batches | lr 0.0001 | ms/batch 49.06601 | loss 1.63274 | ppl     5.118\n",
      "| epoch   1 step     8800 |   8800 batches | lr 0.0001 | ms/batch 48.11739 | loss 1.61757 | ppl     5.041\n",
      "| epoch   1 step     9000 |   9000 batches | lr 0.0001 | ms/batch 48.72381 | loss 1.61990 | ppl     5.053\n",
      "| epoch   1 step     9200 |   9200 batches | lr 0.0001 | ms/batch 47.61437 | loss 1.57430 | ppl     4.827\n",
      "| epoch   1 step     9400 |   9400 batches | lr 0.0001 | ms/batch 48.29152 | loss 1.63323 | ppl     5.120\n",
      "| epoch   1 step     9600 |   9600 batches | lr 0.0001 | ms/batch 48.37304 | loss 1.59976 | ppl     4.952\n",
      "| epoch   1 step     9800 |   9800 batches | lr 0.0001 | ms/batch 48.38312 | loss 1.60775 | ppl     4.992\n",
      "| epoch   1 step    10000 |  10000 batches | lr 0.0001 | ms/batch 49.59878 | loss 1.62048 | ppl     5.056\n",
      "| epoch   1 step    10200 |  10200 batches | lr 0.0001 | ms/batch 47.34444 | loss 1.61156 | ppl     5.011\n",
      "| epoch   1 step    10400 |  10400 batches | lr 0.0001 | ms/batch 48.37361 | loss 1.59351 | ppl     4.921\n",
      "| epoch   1 step    10600 |  10600 batches | lr 0.0001 | ms/batch 47.20553 | loss 1.60955 | ppl     5.001\n",
      "| epoch   1 step    10800 |  10800 batches | lr 0.0001 | ms/batch 47.17879 | loss 1.58018 | ppl     4.856\n",
      "| epoch   1 step    11000 |  11000 batches | lr 0.0001 | ms/batch 49.04266 | loss 1.57354 | ppl     4.824\n",
      "| epoch   1 step    11200 |  11200 batches | lr 0.0001 | ms/batch 48.28885 | loss 1.59569 | ppl     4.932\n",
      "| epoch   1 step    11400 |  11400 batches | lr 0.0001 | ms/batch 49.73373 | loss 1.56966 | ppl     4.805\n",
      "| epoch   1 step    11600 |  11600 batches | lr 0.0001 | ms/batch 49.59649 | loss 1.57895 | ppl     4.850\n",
      "| epoch   1 step    11800 |  11800 batches | lr 0.0001 | ms/batch 50.73557 | loss 1.56357 | ppl     4.776\n",
      "| epoch   1 step    12000 |  12000 batches | lr 0.0001 | ms/batch 49.65857 | loss 1.57131 | ppl     4.813\n",
      "|\n",
      "Source: [15  0 31  4 13  2 28  8 28 10]\n",
      "Target: [ 0 31  4 13  2 28  8 28 10  8]\n",
      "Teacher forcing: acc:0.31\n",
      "Preds:  [15  0 28  4 13  2 28  8 28  8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step    12000 | time: 195.81s | valid loss 1.46614 | valid ppl    4.3325 | valid acc 0.31\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    12200 |  12200 batches | lr 0.0001 | ms/batch 55.56697 | loss 1.56467 | ppl     4.781\n",
      "| epoch   1 step    12400 |  12400 batches | lr 0.0001 | ms/batch 49.30020 | loss 1.58998 | ppl     4.904\n",
      "| epoch   1 step    12600 |  12600 batches | lr 0.0001 | ms/batch 49.75185 | loss 1.58032 | ppl     4.857\n",
      "| epoch   1 step    12800 |  12800 batches | lr 0.0001 | ms/batch 48.34072 | loss 1.58875 | ppl     4.898\n",
      "| epoch   1 step    13000 |  13000 batches | lr 0.0001 | ms/batch 47.31021 | loss 1.57244 | ppl     4.818\n",
      "| epoch   1 step    13200 |  13200 batches | lr 0.0001 | ms/batch 47.27083 | loss 1.55019 | ppl     4.712\n",
      "| epoch   1 step    13400 |  13400 batches | lr 0.0001 | ms/batch 49.77978 | loss 1.56830 | ppl     4.798\n",
      "| epoch   1 step    13600 |  13600 batches | lr 0.0001 | ms/batch 49.31504 | loss 1.56708 | ppl     4.793\n",
      "| epoch   1 step    13800 |  13800 batches | lr 0.0001 | ms/batch 48.00427 | loss 1.56033 | ppl     4.760\n",
      "| epoch   1 step    14000 |  14000 batches | lr 0.0001 | ms/batch 49.45391 | loss 1.58621 | ppl     4.885\n",
      "| epoch   1 step    14200 |  14200 batches | lr 0.0001 | ms/batch 48.28727 | loss 1.56969 | ppl     4.805\n",
      "| epoch   1 step    14400 |  14400 batches | lr 0.0001 | ms/batch 48.77292 | loss 1.55356 | ppl     4.728\n",
      "| epoch   1 step    14600 |  14600 batches | lr 0.0001 | ms/batch 48.22274 | loss 1.56134 | ppl     4.765\n",
      "| epoch   1 step    14800 |  14800 batches | lr 0.0001 | ms/batch 49.92961 | loss 1.54642 | ppl     4.695\n",
      "| epoch   1 step    15000 |  15000 batches | lr 0.0001 | ms/batch 50.73759 | loss 1.53709 | ppl     4.651\n",
      "| epoch   1 step    15200 |  15200 batches | lr 0.0001 | ms/batch 51.40921 | loss 1.57234 | ppl     4.818\n",
      "| epoch   1 step    15400 |  15400 batches | lr 0.0001 | ms/batch 50.73386 | loss 1.55109 | ppl     4.717\n",
      "| epoch   1 step    15600 |  15600 batches | lr 0.0001 | ms/batch 52.64642 | loss 1.54844 | ppl     4.704\n",
      "| epoch   1 step    15800 |  15800 batches | lr 0.0001 | ms/batch 51.37078 | loss 1.54328 | ppl     4.680\n",
      "| epoch   1 step    16000 |  16000 batches | lr 0.0001 | ms/batch 52.57076 | loss 1.56612 | ppl     4.788\n",
      "|\n",
      "Source: [20  6 21  9 18  1 33  7 20 10]\n",
      "Target: [ 6 21  9 18  1 33  7 20 10  6]\n",
      "Teacher forcing: acc:0.2\n",
      "Preds:  [20  6 21  9 25  1 33  7 20  1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   4 at step    16000 | time: 199.87s | valid loss 1.51126 | valid ppl    4.5325 | valid acc 0.2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    16200 |  16200 batches | lr 0.0001 | ms/batch 57.41042 | loss 1.55305 | ppl     4.726\n",
      "| epoch   1 step    16400 |  16400 batches | lr 0.0001 | ms/batch 48.82969 | loss 1.55378 | ppl     4.729\n",
      "| epoch   1 step    16600 |  16600 batches | lr 0.0001 | ms/batch 50.00844 | loss 1.55017 | ppl     4.712\n",
      "| epoch   1 step    16800 |  16800 batches | lr 0.0001 | ms/batch 47.34094 | loss 1.57439 | ppl     4.828\n",
      "| epoch   1 step    17000 |  17000 batches | lr 0.0001 | ms/batch 47.81534 | loss 1.52774 | ppl     4.608\n",
      "| epoch   1 step    17200 |  17200 batches | lr 0.0001 | ms/batch 48.74467 | loss 1.54416 | ppl     4.684\n",
      "| epoch   1 step    17400 |  17400 batches | lr 0.0001 | ms/batch 48.89464 | loss 1.52650 | ppl     4.602\n",
      "| epoch   1 step    17600 |  17600 batches | lr 0.0001 | ms/batch 49.04033 | loss 1.53912 | ppl     4.660\n",
      "| epoch   1 step    17800 |  17800 batches | lr 0.0001 | ms/batch 48.26925 | loss 1.55205 | ppl     4.721\n",
      "| epoch   1 step    18000 |  18000 batches | lr 0.0001 | ms/batch 47.30057 | loss 1.49661 | ppl     4.467\n",
      "| epoch   1 step    18200 |  18200 batches | lr 0.0001 | ms/batch 48.88168 | loss 1.52025 | ppl     4.573\n",
      "| epoch   1 step    18400 |  18400 batches | lr 0.0001 | ms/batch 48.18093 | loss 1.57396 | ppl     4.826\n",
      "| epoch   1 step    18600 |  18600 batches | lr 0.0001 | ms/batch 49.42544 | loss 1.54656 | ppl     4.695\n",
      "| epoch   1 step    18800 |  18800 batches | lr 0.0001 | ms/batch 48.23120 | loss 1.55638 | ppl     4.742\n",
      "| epoch   1 step    19000 |  19000 batches | lr 0.0001 | ms/batch 47.14391 | loss 1.56021 | ppl     4.760\n",
      "| epoch   1 step    19200 |  19200 batches | lr 0.0001 | ms/batch 47.62865 | loss 1.56007 | ppl     4.759\n",
      "| epoch   1 step    19400 |  19400 batches | lr 0.0001 | ms/batch 49.60140 | loss 1.54145 | ppl     4.671\n",
      "| epoch   1 step    19600 |  19600 batches | lr 0.0001 | ms/batch 47.69976 | loss 1.53503 | ppl     4.641\n",
      "| epoch   1 step    19800 |  19800 batches | lr 0.0001 | ms/batch 47.73383 | loss 1.57074 | ppl     4.810\n",
      "| epoch   1 step    20000 |  20000 batches | lr 0.0001 | ms/batch 47.92998 | loss 1.53038 | ppl     4.620\n",
      "|\n",
      "Source: [20  8 26  6 21  1 34  4 26 10]\n",
      "Target: [ 8 26  6 21  1 34  4 26 10  6]\n",
      "Teacher forcing: acc:0.26\n",
      "Preds:  [20  8 26  6 21  1 34  4 26  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   5 at step    20000 | time: 195.17s | valid loss 1.46287 | valid ppl    4.3183 | valid acc 0.26\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    20200 |  20200 batches | lr 0.0001 | ms/batch 54.25724 | loss 1.52298 | ppl     4.586\n",
      "| epoch   1 step    20400 |  20400 batches | lr 0.0001 | ms/batch 47.47817 | loss 1.54109 | ppl     4.670\n",
      "| epoch   1 step    20600 |  20600 batches | lr 0.0001 | ms/batch 47.86546 | loss 1.50121 | ppl     4.487\n",
      "| epoch   1 step    20800 |  20800 batches | lr 0.0001 | ms/batch 47.30270 | loss 1.54990 | ppl     4.711\n",
      "| epoch   1 step    21000 |  21000 batches | lr 0.0001 | ms/batch 50.22389 | loss 1.56582 | ppl     4.787\n",
      "| epoch   1 step    21200 |  21200 batches | lr 0.0001 | ms/batch 48.43084 | loss 1.54608 | ppl     4.693\n",
      "| epoch   1 step    21400 |  21400 batches | lr 0.0001 | ms/batch 48.12924 | loss 1.52481 | ppl     4.594\n",
      "| epoch   1 step    21600 |  21600 batches | lr 0.0001 | ms/batch 48.82509 | loss 1.50687 | ppl     4.513\n",
      "| epoch   1 step    21800 |  21800 batches | lr 0.0001 | ms/batch 49.02773 | loss 1.51403 | ppl     4.545\n",
      "| epoch   1 step    22000 |  22000 batches | lr 0.0001 | ms/batch 48.27067 | loss 1.52455 | ppl     4.593\n",
      "| epoch   1 step    22200 |  22200 batches | lr 0.0001 | ms/batch 49.45909 | loss 1.53910 | ppl     4.660\n",
      "| epoch   1 step    22400 |  22400 batches | lr 0.0001 | ms/batch 48.83299 | loss 1.51526 | ppl     4.551\n",
      "| epoch   1 step    22600 |  22600 batches | lr 0.0001 | ms/batch 49.42712 | loss 1.55435 | ppl     4.732\n",
      "| epoch   1 step    22800 |  22800 batches | lr 0.0001 | ms/batch 48.79868 | loss 1.53596 | ppl     4.646\n",
      "| epoch   1 step    23000 |  23000 batches | lr 0.0001 | ms/batch 49.13548 | loss 1.51629 | ppl     4.555\n",
      "| epoch   1 step    23200 |  23200 batches | lr 0.0001 | ms/batch 47.69072 | loss 1.57083 | ppl     4.811\n",
      "| epoch   1 step    23400 |  23400 batches | lr 0.0001 | ms/batch 48.02186 | loss 1.52913 | ppl     4.614\n",
      "| epoch   1 step    23600 |  23600 batches | lr 0.0001 | ms/batch 47.99205 | loss 1.51216 | ppl     4.537\n",
      "| epoch   1 step    23800 |  23800 batches | lr 0.0001 | ms/batch 48.50861 | loss 1.53339 | ppl     4.634\n",
      "| epoch   1 step    24000 |  24000 batches | lr 0.0001 | ms/batch 49.12434 | loss 1.53763 | ppl     4.654\n",
      "|\n",
      "Source: [15  3 14  8 25  4 12  6 25 10]\n",
      "Target: [ 3 14  8 25  4 12  6 25 10  4]\n",
      "Teacher forcing: acc:0.27\n",
      "Preds:  [15  3 14  8 25  4 19  6 25  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   6 at step    24000 | time: 195.36s | valid loss 1.47721 | valid ppl    4.3807 | valid acc 0.27\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    24200 |  24200 batches | lr 0.0001 | ms/batch 54.72469 | loss 1.51864 | ppl     4.566\n",
      "| epoch   1 step    24400 |  24400 batches | lr 0.0001 | ms/batch 48.48342 | loss 1.52040 | ppl     4.574\n",
      "| epoch   1 step    24600 |  24600 batches | lr 0.0001 | ms/batch 47.68590 | loss 1.48011 | ppl     4.393\n",
      "| epoch   1 step    24800 |  24800 batches | lr 0.0001 | ms/batch 49.84938 | loss 1.54103 | ppl     4.669\n",
      "| epoch   1 step    25000 |  25000 batches | lr 0.0001 | ms/batch 48.13405 | loss 1.54504 | ppl     4.688\n",
      "| epoch   1 step    25200 |  25200 batches | lr 0.0001 | ms/batch 48.19666 | loss 1.49528 | ppl     4.461\n",
      "| epoch   1 step    25400 |  25400 batches | lr 0.0001 | ms/batch 49.39431 | loss 1.52986 | ppl     4.618\n",
      "| epoch   1 step    25600 |  25600 batches | lr 0.0001 | ms/batch 49.24834 | loss 1.53502 | ppl     4.641\n",
      "| epoch   1 step    25800 |  25800 batches | lr 0.0001 | ms/batch 47.44162 | loss 1.52710 | ppl     4.605\n",
      "| epoch   1 step    26000 |  26000 batches | lr 0.0001 | ms/batch 49.13969 | loss 1.51130 | ppl     4.533\n",
      "| epoch   1 step    26200 |  26200 batches | lr 0.0001 | ms/batch 48.64611 | loss 1.50972 | ppl     4.525\n",
      "| epoch   1 step    26400 |  26400 batches | lr 0.0001 | ms/batch 48.34011 | loss 1.52606 | ppl     4.600\n",
      "| epoch   1 step    26600 |  26600 batches | lr 0.0001 | ms/batch 50.16548 | loss 1.50088 | ppl     4.486\n",
      "| epoch   1 step    26800 |  26800 batches | lr 0.0001 | ms/batch 49.38393 | loss 1.50085 | ppl     4.485\n",
      "| epoch   1 step    27000 |  27000 batches | lr 0.0001 | ms/batch 48.60644 | loss 1.53930 | ppl     4.661\n",
      "| epoch   1 step    27200 |  27200 batches | lr 0.0001 | ms/batch 48.53894 | loss 1.56539 | ppl     4.785\n",
      "| epoch   1 step    27400 |  27400 batches | lr 0.0001 | ms/batch 49.02166 | loss 1.53104 | ppl     4.623\n",
      "| epoch   1 step    27600 |  27600 batches | lr 0.0001 | ms/batch 49.06948 | loss 1.53267 | ppl     4.631\n",
      "| epoch   1 step    27800 |  27800 batches | lr 0.0001 | ms/batch 48.74201 | loss 1.52463 | ppl     4.593\n",
      "| epoch   1 step    28000 |  28000 batches | lr 0.0001 | ms/batch 49.05406 | loss 1.55436 | ppl     4.732\n",
      "|\n",
      "Source: [20  9 12  2 23  6 21  3 23 10]\n",
      "Target: [ 9 12  2 23  6 21  3 23 10  6]\n",
      "Teacher forcing: acc:0.28\n",
      "Preds:  [20  9 12  2 23  6 21  3 23  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   7 at step    28000 | time: 196.33s | valid loss 1.47175 | valid ppl    4.3568 | valid acc 0.28\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    28200 |  28200 batches | lr 0.0001 | ms/batch 55.48084 | loss 1.49834 | ppl     4.474\n",
      "| epoch   1 step    28400 |  28400 batches | lr 0.0001 | ms/batch 50.15035 | loss 1.54389 | ppl     4.683\n",
      "| epoch   1 step    28600 |  28600 batches | lr 0.0001 | ms/batch 49.86275 | loss 1.49736 | ppl     4.470\n",
      "| epoch   1 step    28800 |  28800 batches | lr 0.0001 | ms/batch 47.82989 | loss 1.51988 | ppl     4.572\n",
      "| epoch   1 step    29000 |  29000 batches | lr 0.0001 | ms/batch 49.12754 | loss 1.51943 | ppl     4.570\n",
      "| epoch   1 step    29200 |  29200 batches | lr 0.0001 | ms/batch 47.63249 | loss 1.52022 | ppl     4.573\n",
      "| epoch   1 step    29400 |  29400 batches | lr 0.0001 | ms/batch 47.81472 | loss 1.52448 | ppl     4.593\n",
      "| epoch   1 step    29600 |  29600 batches | lr 0.0001 | ms/batch 49.49217 | loss 1.53506 | ppl     4.642\n",
      "| epoch   1 step    29800 |  29800 batches | lr 0.0001 | ms/batch 48.64987 | loss 1.52459 | ppl     4.593\n",
      "| epoch   1 step    30000 |  30000 batches | lr 0.0001 | ms/batch 48.20104 | loss 1.51708 | ppl     4.559\n",
      "| epoch   1 step    30200 |  30200 batches | lr 0.0001 | ms/batch 47.95468 | loss 1.52461 | ppl     4.593\n",
      "| epoch   1 step    30400 |  30400 batches | lr 0.0001 | ms/batch 49.59283 | loss 1.52019 | ppl     4.573\n",
      "| epoch   1 step    30600 |  30600 batches | lr 0.0001 | ms/batch 50.34138 | loss 1.51602 | ppl     4.554\n",
      "| epoch   1 step    30800 |  30800 batches | lr 0.0001 | ms/batch 47.95007 | loss 1.51118 | ppl     4.532\n",
      "| epoch   1 step    31000 |  31000 batches | lr 0.0001 | ms/batch 48.06972 | loss 1.50868 | ppl     4.521\n",
      "| epoch   1 step    31200 |  31200 batches | lr 0.0001 | ms/batch 48.55903 | loss 1.52055 | ppl     4.575\n",
      "| epoch   1 step    31400 |  31400 batches | lr 0.0001 | ms/batch 48.75961 | loss 1.52317 | ppl     4.587\n",
      "| epoch   1 step    31600 |  31600 batches | lr 0.0001 | ms/batch 48.21742 | loss 1.51355 | ppl     4.543\n",
      "| epoch   1 step    31800 |  31800 batches | lr 0.0001 | ms/batch 49.67359 | loss 1.51993 | ppl     4.572\n",
      "| epoch   1 step    32000 |  32000 batches | lr 0.0001 | ms/batch 52.19981 | loss 1.53832 | ppl     4.657\n",
      "|\n",
      "Source: [11  8 19  7 33  0 27  6 27 10]\n",
      "Target: [ 8 19  7 33  0 27  6 27 10  6]\n",
      "Teacher forcing: acc:0.25\n",
      "Preds:  [11  8 25  7 33  0 27  6 27  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   8 at step    32000 | time: 197.21s | valid loss 1.45171 | valid ppl    4.2704 | valid acc 0.25\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    32200 |  32200 batches | lr 0.0001 | ms/batch 57.34397 | loss 1.50058 | ppl     4.484\n",
      "| epoch   1 step    32400 |  32400 batches | lr 0.0001 | ms/batch 49.66681 | loss 1.50365 | ppl     4.498\n",
      "| epoch   1 step    32600 |  32600 batches | lr 0.0001 | ms/batch 51.58193 | loss 1.50139 | ppl     4.488\n",
      "| epoch   1 step    32800 |  32800 batches | lr 0.0001 | ms/batch 49.90198 | loss 1.49173 | ppl     4.445\n",
      "| epoch   1 step    33000 |  33000 batches | lr 0.0001 | ms/batch 49.89256 | loss 1.53780 | ppl     4.654\n",
      "| epoch   1 step    33200 |  33200 batches | lr 0.0001 | ms/batch 50.73840 | loss 1.51911 | ppl     4.568\n",
      "| epoch   1 step    33400 |  33400 batches | lr 0.0001 | ms/batch 52.34408 | loss 1.52251 | ppl     4.584\n",
      "| epoch   1 step    33600 |  33600 batches | lr 0.0001 | ms/batch 50.77350 | loss 1.54189 | ppl     4.673\n",
      "| epoch   1 step    33800 |  33800 batches | lr 0.0001 | ms/batch 50.39815 | loss 1.54449 | ppl     4.686\n",
      "| epoch   1 step    34000 |  34000 batches | lr 0.0001 | ms/batch 51.64171 | loss 1.51795 | ppl     4.563\n",
      "| epoch   1 step    34200 |  34200 batches | lr 0.0001 | ms/batch 51.55957 | loss 1.53650 | ppl     4.648\n",
      "| epoch   1 step    34400 |  34400 batches | lr 0.0001 | ms/batch 51.88366 | loss 1.53830 | ppl     4.657\n",
      "| epoch   1 step    34600 |  34600 batches | lr 0.0001 | ms/batch 51.72744 | loss 1.50783 | ppl     4.517\n",
      "| epoch   1 step    34800 |  34800 batches | lr 0.0001 | ms/batch 50.59394 | loss 1.56125 | ppl     4.765\n",
      "| epoch   1 step    35000 |  35000 batches | lr 0.0001 | ms/batch 51.37008 | loss 1.51833 | ppl     4.565\n",
      "| epoch   1 step    35200 |  35200 batches | lr 0.0001 | ms/batch 50.71188 | loss 1.51946 | ppl     4.570\n",
      "| epoch   1 step    35400 |  35400 batches | lr 0.0001 | ms/batch 49.96315 | loss 1.48690 | ppl     4.423\n",
      "| epoch   1 step    35600 |  35600 batches | lr 0.0001 | ms/batch 48.37199 | loss 1.53441 | ppl     4.639\n",
      "| epoch   1 step    35800 |  35800 batches | lr 0.0001 | ms/batch 48.53104 | loss 1.48085 | ppl     4.397\n",
      "| epoch   1 step    36000 |  36000 batches | lr 0.0001 | ms/batch 47.83676 | loss 1.48929 | ppl     4.434\n",
      "|\n",
      "Source: [25  3 31  1 24  2 12  0 25 10]\n",
      "Target: [ 3 31  1 24  2 12  0 25 10  3]\n",
      "Teacher forcing: acc:0.24\n",
      "Preds:  [25  3 26  1 35  2 33  0 25  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   9 at step    36000 | time: 203.31s | valid loss 1.49683 | valid ppl    4.4675 | valid acc 0.24\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    36200 |  36200 batches | lr 0.0001 | ms/batch 54.84846 | loss 1.54482 | ppl     4.687\n",
      "| epoch   1 step    36400 |  36400 batches | lr 0.0001 | ms/batch 49.60594 | loss 1.49240 | ppl     4.448\n",
      "| epoch   1 step    36600 |  36600 batches | lr 0.0001 | ms/batch 48.85145 | loss 1.53802 | ppl     4.655\n",
      "| epoch   1 step    36800 |  36800 batches | lr 0.0001 | ms/batch 48.61630 | loss 1.50357 | ppl     4.498\n",
      "| epoch   1 step    37000 |  37000 batches | lr 0.0001 | ms/batch 50.13471 | loss 1.52782 | ppl     4.608\n",
      "| epoch   1 step    37200 |  37200 batches | lr 0.0001 | ms/batch 50.03274 | loss 1.52248 | ppl     4.584\n",
      "| epoch   1 step    37400 |  37400 batches | lr 0.0001 | ms/batch 48.37809 | loss 1.48537 | ppl     4.417\n",
      "| epoch   1 step    37600 |  37600 batches | lr 0.0001 | ms/batch 49.15709 | loss 1.50940 | ppl     4.524\n",
      "| epoch   1 step    37800 |  37800 batches | lr 0.0001 | ms/batch 49.40266 | loss 1.51387 | ppl     4.544\n",
      "| epoch   1 step    38000 |  38000 batches | lr 0.0001 | ms/batch 48.62307 | loss 1.47550 | ppl     4.373\n",
      "| epoch   1 step    38200 |  38200 batches | lr 0.0001 | ms/batch 48.93953 | loss 1.54457 | ppl     4.686\n",
      "| epoch   1 step    38400 |  38400 batches | lr 0.0001 | ms/batch 49.98735 | loss 1.49261 | ppl     4.449\n",
      "| epoch   1 step    38600 |  38600 batches | lr 0.0001 | ms/batch 49.74925 | loss 1.50180 | ppl     4.490\n",
      "| epoch   1 step    38800 |  38800 batches | lr 0.0001 | ms/batch 49.68746 | loss 1.53214 | ppl     4.628\n",
      "| epoch   1 step    39000 |  39000 batches | lr 0.0001 | ms/batch 50.00923 | loss 1.54507 | ppl     4.688\n",
      "| epoch   1 step    39200 |  39200 batches | lr 0.0001 | ms/batch 49.76922 | loss 1.49337 | ppl     4.452\n",
      "| epoch   1 step    39400 |  39400 batches | lr 0.0001 | ms/batch 49.30936 | loss 1.49550 | ppl     4.462\n",
      "| epoch   1 step    39600 |  39600 batches | lr 0.0001 | ms/batch 50.37002 | loss 1.53384 | ppl     4.636\n",
      "| epoch   1 step    39800 |  39800 batches | lr 0.0001 | ms/batch 51.42254 | loss 1.54950 | ppl     4.709\n",
      "| epoch   1 step    40000 |  40000 batches | lr 0.0001 | ms/batch 49.62755 | loss 1.52006 | ppl     4.573\n",
      "|\n",
      "Source: [14  9 16  3 20  0 35  5 16 10]\n",
      "Target: [ 9 16  3 20  0 35  5 16 10  3]\n",
      "Teacher forcing: acc:0.21\n",
      "Preds:  [35  9 16  3 35  0 35  5 16  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  10 at step    40000 | time: 199.26s | valid loss 1.53271 | valid ppl    4.6307 | valid acc 0.21\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    40200 |  40200 batches | lr 0.0001 | ms/batch 55.55674 | loss 1.50333 | ppl     4.497\n",
      "| epoch   1 step    40400 |  40400 batches | lr 0.0001 | ms/batch 52.33525 | loss 1.52233 | ppl     4.583\n",
      "| epoch   1 step    40600 |  40600 batches | lr 0.0001 | ms/batch 51.15842 | loss 1.50111 | ppl     4.487\n",
      "| epoch   1 step    40800 |  40800 batches | lr 0.0001 | ms/batch 50.82425 | loss 1.50424 | ppl     4.501\n",
      "| epoch   1 step    41000 |  41000 batches | lr 0.0001 | ms/batch 51.68130 | loss 1.53287 | ppl     4.631\n",
      "| epoch   1 step    41200 |  41200 batches | lr 0.0001 | ms/batch 50.47821 | loss 1.50872 | ppl     4.521\n",
      "| epoch   1 step    41400 |  41400 batches | lr 0.0001 | ms/batch 51.78806 | loss 1.50736 | ppl     4.515\n",
      "| epoch   1 step    41600 |  41600 batches | lr 0.0001 | ms/batch 51.14552 | loss 1.48768 | ppl     4.427\n",
      "| epoch   1 step    41800 |  41800 batches | lr 0.0001 | ms/batch 50.02422 | loss 1.48353 | ppl     4.408\n",
      "| epoch   1 step    42000 |  42000 batches | lr 0.0001 | ms/batch 51.16303 | loss 1.48114 | ppl     4.398\n",
      "| epoch   1 step    42200 |  42200 batches | lr 0.0001 | ms/batch 50.62981 | loss 1.53302 | ppl     4.632\n",
      "| epoch   1 step    42400 |  42400 batches | lr 0.0001 | ms/batch 50.74169 | loss 1.49921 | ppl     4.478\n",
      "| epoch   1 step    42600 |  42600 batches | lr 0.0001 | ms/batch 48.47384 | loss 1.55144 | ppl     4.718\n",
      "| epoch   1 step    42800 |  42800 batches | lr 0.0001 | ms/batch 48.09519 | loss 1.49668 | ppl     4.467\n",
      "| epoch   1 step    43000 |  43000 batches | lr 0.0001 | ms/batch 48.47291 | loss 1.50557 | ppl     4.507\n",
      "| epoch   1 step    43200 |  43200 batches | lr 0.0001 | ms/batch 50.48872 | loss 1.53972 | ppl     4.663\n",
      "| epoch   1 step    43400 |  43400 batches | lr 0.0001 | ms/batch 48.88026 | loss 1.51203 | ppl     4.536\n",
      "| epoch   1 step    43600 |  43600 batches | lr 0.0001 | ms/batch 49.38380 | loss 1.51795 | ppl     4.563\n",
      "| epoch   1 step    43800 |  43800 batches | lr 0.0001 | ms/batch 48.41922 | loss 1.50493 | ppl     4.504\n",
      "| epoch   1 step    44000 |  44000 batches | lr 0.0001 | ms/batch 51.03723 | loss 1.48301 | ppl     4.406\n",
      "|\n",
      "Source: [11  7 29  9 23  2 14  0 23 10]\n",
      "Target: [ 7 29  9 23  2 14  0 23 10  2]\n",
      "Teacher forcing: acc:0.22\n",
      "Preds:  [11  7 29  9 25  2 14  0 35  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  11 at step    44000 | time: 202.22s | valid loss 1.46486 | valid ppl    4.3269 | valid acc 0.22\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    44200 |  44200 batches | lr 0.0001 | ms/batch 55.34699 | loss 1.51335 | ppl     4.542\n",
      "| epoch   1 step    44400 |  44400 batches | lr 0.0001 | ms/batch 49.90221 | loss 1.49022 | ppl     4.438\n",
      "| epoch   1 step    44600 |  44600 batches | lr 0.0001 | ms/batch 51.14974 | loss 1.49315 | ppl     4.451\n",
      "| epoch   1 step    44800 |  44800 batches | lr 0.0001 | ms/batch 51.00883 | loss 1.50336 | ppl     4.497\n",
      "| epoch   1 step    45000 |  45000 batches | lr 0.0001 | ms/batch 50.80408 | loss 1.47553 | ppl     4.373\n",
      "| epoch   1 step    45200 |  45200 batches | lr 0.0001 | ms/batch 50.58014 | loss 1.49901 | ppl     4.477\n",
      "| epoch   1 step    45400 |  45400 batches | lr 0.0001 | ms/batch 49.25587 | loss 1.48514 | ppl     4.416\n",
      "| epoch   1 step    45600 |  45600 batches | lr 0.0001 | ms/batch 48.78538 | loss 1.49772 | ppl     4.471\n",
      "| epoch   1 step    45800 |  45800 batches | lr 0.0001 | ms/batch 50.64684 | loss 1.49362 | ppl     4.453\n",
      "| epoch   1 step    46000 |  46000 batches | lr 0.0001 | ms/batch 50.03382 | loss 1.50892 | ppl     4.522\n",
      "| epoch   1 step    46200 |  46200 batches | lr 0.0001 | ms/batch 50.50876 | loss 1.49473 | ppl     4.458\n",
      "| epoch   1 step    46400 |  46400 batches | lr 0.0001 | ms/batch 49.27851 | loss 1.49622 | ppl     4.465\n",
      "| epoch   1 step    46600 |  46600 batches | lr 0.0001 | ms/batch 49.34126 | loss 1.52201 | ppl     4.581\n",
      "| epoch   1 step    46800 |  46800 batches | lr 0.0001 | ms/batch 50.85174 | loss 1.46620 | ppl     4.333\n",
      "| epoch   1 step    47000 |  47000 batches | lr 0.0001 | ms/batch 49.10566 | loss 1.56731 | ppl     4.794\n",
      "| epoch   1 step    47200 |  47200 batches | lr 0.0001 | ms/batch 51.13795 | loss 1.48824 | ppl     4.429\n",
      "| epoch   1 step    47400 |  47400 batches | lr 0.0001 | ms/batch 48.70012 | loss 1.52137 | ppl     4.578\n",
      "| epoch   1 step    47600 |  47600 batches | lr 0.0001 | ms/batch 49.35355 | loss 1.49470 | ppl     4.458\n",
      "| epoch   1 step    47800 |  47800 batches | lr 0.0001 | ms/batch 50.35430 | loss 1.50801 | ppl     4.518\n",
      "| epoch   1 step    48000 |  48000 batches | lr 0.0001 | ms/batch 48.96080 | loss 1.49206 | ppl     4.446\n",
      "|\n",
      "Source: [23  2 27  1 12  9 35  8 35 10]\n",
      "Target: [ 2 27  1 12  9 35  8 35 10  8]\n",
      "Teacher forcing: acc:0.2\n",
      "Preds:  [25  2 25  1 25  9 35  8 35  8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  12 at step    48000 | time: 201.05s | valid loss 1.46154 | valid ppl    4.3126 | valid acc 0.2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    48200 |  48200 batches | lr 5e-05 | ms/batch 54.86887 | loss 1.47595 | ppl     4.375\n",
      "| epoch   1 step    48400 |  48400 batches | lr 5e-05 | ms/batch 50.04306 | loss 1.50048 | ppl     4.484\n",
      "| epoch   1 step    48600 |  48600 batches | lr 5e-05 | ms/batch 48.42831 | loss 1.47828 | ppl     4.385\n",
      "| epoch   1 step    48800 |  48800 batches | lr 5e-05 | ms/batch 47.82849 | loss 1.44959 | ppl     4.261\n",
      "| epoch   1 step    49000 |  49000 batches | lr 5e-05 | ms/batch 50.21929 | loss 1.47878 | ppl     4.388\n",
      "| epoch   1 step    49200 |  49200 batches | lr 5e-05 | ms/batch 49.38166 | loss 1.49310 | ppl     4.451\n",
      "| epoch   1 step    49400 |  49400 batches | lr 5e-05 | ms/batch 49.80539 | loss 1.49411 | ppl     4.455\n",
      "| epoch   1 step    49600 |  49600 batches | lr 5e-05 | ms/batch 49.61230 | loss 1.47868 | ppl     4.387\n",
      "| epoch   1 step    49800 |  49800 batches | lr 5e-05 | ms/batch 49.51865 | loss 1.48306 | ppl     4.406\n",
      "| epoch   1 step    50000 |  50000 batches | lr 5e-05 | ms/batch 48.91962 | loss 1.49505 | ppl     4.460\n",
      "| epoch   1 step    50200 |  50200 batches | lr 5e-05 | ms/batch 48.81726 | loss 1.46268 | ppl     4.317\n",
      "| epoch   1 step    50400 |  50400 batches | lr 5e-05 | ms/batch 47.88932 | loss 1.48040 | ppl     4.395\n",
      "| epoch   1 step    50600 |  50600 batches | lr 5e-05 | ms/batch 48.96922 | loss 1.48332 | ppl     4.408\n",
      "| epoch   1 step    50800 |  50800 batches | lr 5e-05 | ms/batch 50.56885 | loss 1.49595 | ppl     4.464\n",
      "| epoch   1 step    51000 |  51000 batches | lr 5e-05 | ms/batch 50.33598 | loss 1.48127 | ppl     4.399\n",
      "| epoch   1 step    51200 |  51200 batches | lr 5e-05 | ms/batch 51.15322 | loss 1.49702 | ppl     4.468\n",
      "| epoch   1 step    51400 |  51400 batches | lr 5e-05 | ms/batch 49.75519 | loss 1.47496 | ppl     4.371\n",
      "| epoch   1 step    51600 |  51600 batches | lr 5e-05 | ms/batch 50.20723 | loss 1.50354 | ppl     4.498\n",
      "| epoch   1 step    51800 |  51800 batches | lr 5e-05 | ms/batch 50.65908 | loss 1.48315 | ppl     4.407\n",
      "| epoch   1 step    52000 |  52000 batches | lr 5e-05 | ms/batch 49.13082 | loss 1.46688 | ppl     4.336\n",
      "|\n",
      "Source: [16  7 32  1 34  5 19  9 19 10]\n",
      "Target: [ 7 32  1 34  5 19  9 19 10  9]\n",
      "Teacher forcing: acc:0.24\n",
      "Preds:  [ 0  7 35  1 25  5 25  9 25  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  13 at step    52000 | time: 199.14s | valid loss 1.40046 | valid ppl    4.0571 | valid acc 0.24\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    52200 |  52200 batches | lr 5e-05 | ms/batch 54.37046 | loss 1.47291 | ppl     4.362\n",
      "| epoch   1 step    52400 |  52400 batches | lr 5e-05 | ms/batch 49.33545 | loss 1.49335 | ppl     4.452\n",
      "| epoch   1 step    52600 |  52600 batches | lr 5e-05 | ms/batch 49.05352 | loss 1.47485 | ppl     4.370\n",
      "| epoch   1 step    52800 |  52800 batches | lr 5e-05 | ms/batch 50.09238 | loss 1.46441 | ppl     4.325\n",
      "| epoch   1 step    53000 |  53000 batches | lr 5e-05 | ms/batch 51.29411 | loss 1.44993 | ppl     4.263\n",
      "| epoch   1 step    53200 |  53200 batches | lr 5e-05 | ms/batch 49.41749 | loss 1.48211 | ppl     4.402\n",
      "| epoch   1 step    53400 |  53400 batches | lr 5e-05 | ms/batch 50.30822 | loss 1.50323 | ppl     4.496\n",
      "| epoch   1 step    53600 |  53600 batches | lr 5e-05 | ms/batch 50.39235 | loss 1.46061 | ppl     4.309\n",
      "| epoch   1 step    53800 |  53800 batches | lr 5e-05 | ms/batch 49.78079 | loss 1.47205 | ppl     4.358\n",
      "| epoch   1 step    54000 |  54000 batches | lr 5e-05 | ms/batch 48.20713 | loss 1.51242 | ppl     4.538\n",
      "| epoch   1 step    54200 |  54200 batches | lr 5e-05 | ms/batch 48.07070 | loss 1.49202 | ppl     4.446\n",
      "| epoch   1 step    54400 |  54400 batches | lr 5e-05 | ms/batch 48.15057 | loss 1.46247 | ppl     4.317\n",
      "| epoch   1 step    54600 |  54600 batches | lr 5e-05 | ms/batch 48.26782 | loss 1.48617 | ppl     4.420\n",
      "| epoch   1 step    54800 |  54800 batches | lr 5e-05 | ms/batch 48.77029 | loss 1.47755 | ppl     4.382\n",
      "| epoch   1 step    55000 |  55000 batches | lr 5e-05 | ms/batch 48.28305 | loss 1.48148 | ppl     4.399\n",
      "| epoch   1 step    55200 |  55200 batches | lr 5e-05 | ms/batch 47.90347 | loss 1.50056 | ppl     4.484\n",
      "| epoch   1 step    55400 |  55400 batches | lr 5e-05 | ms/batch 50.16081 | loss 1.49606 | ppl     4.464\n",
      "| epoch   1 step    55600 |  55600 batches | lr 5e-05 | ms/batch 50.09902 | loss 1.48371 | ppl     4.409\n",
      "| epoch   1 step    55800 |  55800 batches | lr 5e-05 | ms/batch 50.09315 | loss 1.49144 | ppl     4.443\n",
      "| epoch   1 step    56000 |  56000 batches | lr 5e-05 | ms/batch 49.46906 | loss 1.52148 | ppl     4.579\n",
      "|\n",
      "Source: [33  8 36  5 16  4 19  7 16 10]\n",
      "Target: [ 8 36  5 16  4 19  7 16 10  4]\n",
      "Teacher forcing: acc:0.21\n",
      "Preds:  [35  8 35  5 35  4 25  7 35  5]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  14 at step    56000 | time: 198.40s | valid loss 1.44177 | valid ppl    4.2282 | valid acc 0.21\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    56200 |  56200 batches | lr 5e-05 | ms/batch 55.14322 | loss 1.48175 | ppl     4.401\n",
      "| epoch   1 step    56400 |  56400 batches | lr 5e-05 | ms/batch 48.15152 | loss 1.49588 | ppl     4.463\n",
      "| epoch   1 step    56600 |  56600 batches | lr 5e-05 | ms/batch 49.25940 | loss 1.46738 | ppl     4.338\n",
      "| epoch   1 step    56800 |  56800 batches | lr 5e-05 | ms/batch 49.33539 | loss 1.47493 | ppl     4.371\n",
      "| epoch   1 step    57000 |  57000 batches | lr 5e-05 | ms/batch 48.60625 | loss 1.44335 | ppl     4.235\n",
      "| epoch   1 step    57200 |  57200 batches | lr 5e-05 | ms/batch 48.95719 | loss 1.47037 | ppl     4.351\n",
      "| epoch   1 step    57400 |  57400 batches | lr 5e-05 | ms/batch 49.76807 | loss 1.48629 | ppl     4.421\n",
      "| epoch   1 step    57600 |  57600 batches | lr 5e-05 | ms/batch 48.07238 | loss 1.48084 | ppl     4.397\n",
      "| epoch   1 step    57800 |  57800 batches | lr 5e-05 | ms/batch 48.77845 | loss 1.48298 | ppl     4.406\n",
      "| epoch   1 step    58000 |  58000 batches | lr 5e-05 | ms/batch 48.86009 | loss 1.48725 | ppl     4.425\n",
      "| epoch   1 step    58200 |  58200 batches | lr 5e-05 | ms/batch 50.12160 | loss 1.48780 | ppl     4.427\n",
      "| epoch   1 step    58400 |  58400 batches | lr 5e-05 | ms/batch 50.58211 | loss 1.49475 | ppl     4.458\n",
      "| epoch   1 step    58600 |  58600 batches | lr 5e-05 | ms/batch 49.60154 | loss 1.51025 | ppl     4.528\n",
      "| epoch   1 step    58800 |  58800 batches | lr 5e-05 | ms/batch 47.69295 | loss 1.47487 | ppl     4.370\n",
      "| epoch   1 step    59000 |  59000 batches | lr 5e-05 | ms/batch 47.59488 | loss 1.48477 | ppl     4.414\n",
      "| epoch   1 step    59200 |  59200 batches | lr 5e-05 | ms/batch 49.32587 | loss 1.47642 | ppl     4.377\n",
      "| epoch   1 step    59400 |  59400 batches | lr 5e-05 | ms/batch 47.89593 | loss 1.47674 | ppl     4.379\n",
      "| epoch   1 step    59600 |  59600 batches | lr 5e-05 | ms/batch 48.70656 | loss 1.45058 | ppl     4.266\n",
      "| epoch   1 step    59800 |  59800 batches | lr 5e-05 | ms/batch 48.15745 | loss 1.47701 | ppl     4.380\n",
      "| epoch   1 step    60000 |  60000 batches | lr 5e-05 | ms/batch 48.95240 | loss 1.51442 | ppl     4.547\n",
      "|\n",
      "Source: [33  3 28  8 24  0 21  2 21 10]\n",
      "Target: [ 3 28  8 24  0 21  2 21 10  2]\n",
      "Teacher forcing: acc:0.23\n",
      "Preds:  [33  3 28  8 35  0 35  2 35  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  15 at step    60000 | time: 196.70s | valid loss 1.45269 | valid ppl    4.2746 | valid acc 0.23\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    60200 |  60200 batches | lr 5e-05 | ms/batch 54.84620 | loss 1.48455 | ppl     4.413\n",
      "| epoch   1 step    60400 |  60400 batches | lr 5e-05 | ms/batch 50.36932 | loss 1.46242 | ppl     4.316\n",
      "| epoch   1 step    60600 |  60600 batches | lr 5e-05 | ms/batch 49.37376 | loss 1.53633 | ppl     4.648\n",
      "| epoch   1 step    60800 |  60800 batches | lr 5e-05 | ms/batch 48.87490 | loss 1.47321 | ppl     4.363\n",
      "| epoch   1 step    61000 |  61000 batches | lr 5e-05 | ms/batch 47.91299 | loss 1.48334 | ppl     4.408\n",
      "| epoch   1 step    61200 |  61200 batches | lr 5e-05 | ms/batch 48.24127 | loss 1.48329 | ppl     4.407\n",
      "| epoch   1 step    61400 |  61400 batches | lr 5e-05 | ms/batch 48.89259 | loss 1.49550 | ppl     4.462\n",
      "| epoch   1 step    61600 |  61600 batches | lr 5e-05 | ms/batch 50.58638 | loss 1.49062 | ppl     4.440\n",
      "| epoch   1 step    61800 |  61800 batches | lr 5e-05 | ms/batch 50.07174 | loss 1.46147 | ppl     4.312\n",
      "| epoch   1 step    62000 |  62000 batches | lr 5e-05 | ms/batch 49.86667 | loss 1.49266 | ppl     4.449\n",
      "| epoch   1 step    62200 |  62200 batches | lr 5e-05 | ms/batch 49.58527 | loss 1.49780 | ppl     4.472\n",
      "| epoch   1 step    62400 |  62400 batches | lr 5e-05 | ms/batch 49.68991 | loss 1.43670 | ppl     4.207\n",
      "| epoch   1 step    62600 |  62600 batches | lr 5e-05 | ms/batch 49.16689 | loss 1.48264 | ppl     4.405\n",
      "| epoch   1 step    62800 |  62800 batches | lr 5e-05 | ms/batch 49.19088 | loss 1.48936 | ppl     4.434\n",
      "| epoch   1 step    63000 |  63000 batches | lr 5e-05 | ms/batch 48.95018 | loss 1.47030 | ppl     4.351\n",
      "| epoch   1 step    63200 |  63200 batches | lr 5e-05 | ms/batch 47.98903 | loss 1.46457 | ppl     4.326\n",
      "| epoch   1 step    63400 |  63400 batches | lr 5e-05 | ms/batch 48.93603 | loss 1.46525 | ppl     4.329\n",
      "| epoch   1 step    63600 |  63600 batches | lr 5e-05 | ms/batch 48.50804 | loss 1.46668 | ppl     4.335\n",
      "| epoch   1 step    63800 |  63800 batches | lr 5e-05 | ms/batch 50.00434 | loss 1.49346 | ppl     4.452\n",
      "| epoch   1 step    64000 |  64000 batches | lr 5e-05 | ms/batch 50.33619 | loss 1.48471 | ppl     4.414\n",
      "|\n",
      "Source: [16  6 15  0 35  2 22  7 35 10]\n",
      "Target: [ 6 15  0 35  2 22  7 35 10  2]\n",
      "Teacher forcing: acc:0.31\n",
      "Preds:  [16  6 25  0 35  2 25  7 35  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  16 at step    64000 | time: 198.35s | valid loss 1.37749 | valid ppl    3.9650 | valid acc 0.31\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    64200 |  64200 batches | lr 5e-05 | ms/batch 55.94441 | loss 1.47586 | ppl     4.375\n",
      "| epoch   1 step    64400 |  64400 batches | lr 5e-05 | ms/batch 50.69323 | loss 1.46902 | ppl     4.345\n",
      "| epoch   1 step    64600 |  64600 batches | lr 5e-05 | ms/batch 50.39735 | loss 1.45628 | ppl     4.290\n",
      "| epoch   1 step    64800 |  64800 batches | lr 5e-05 | ms/batch 50.55006 | loss 1.48777 | ppl     4.427\n",
      "| epoch   1 step    65000 |  65000 batches | lr 5e-05 | ms/batch 48.94781 | loss 1.46073 | ppl     4.309\n",
      "| epoch   1 step    65200 |  65200 batches | lr 5e-05 | ms/batch 49.37682 | loss 1.46445 | ppl     4.325\n",
      "| epoch   1 step    65400 |  65400 batches | lr 5e-05 | ms/batch 48.07259 | loss 1.45665 | ppl     4.292\n",
      "| epoch   1 step    65600 |  65600 batches | lr 5e-05 | ms/batch 46.71400 | loss 1.47796 | ppl     4.384\n",
      "| epoch   1 step    65800 |  65800 batches | lr 5e-05 | ms/batch 47.88938 | loss 1.49600 | ppl     4.464\n",
      "| epoch   1 step    66000 |  66000 batches | lr 5e-05 | ms/batch 49.36754 | loss 1.49936 | ppl     4.479\n",
      "| epoch   1 step    66200 |  66200 batches | lr 5e-05 | ms/batch 50.65108 | loss 1.47690 | ppl     4.379\n",
      "| epoch   1 step    66400 |  66400 batches | lr 5e-05 | ms/batch 49.65649 | loss 1.50562 | ppl     4.507\n",
      "| epoch   1 step    66600 |  66600 batches | lr 5e-05 | ms/batch 48.03667 | loss 1.44866 | ppl     4.257\n",
      "| epoch   1 step    66800 |  66800 batches | lr 5e-05 | ms/batch 49.44465 | loss 1.47299 | ppl     4.362\n",
      "| epoch   1 step    67000 |  67000 batches | lr 5e-05 | ms/batch 47.83683 | loss 1.47827 | ppl     4.385\n",
      "| epoch   1 step    67200 |  67200 batches | lr 5e-05 | ms/batch 47.95364 | loss 1.46716 | ppl     4.337\n",
      "| epoch   1 step    67400 |  67400 batches | lr 5e-05 | ms/batch 49.10901 | loss 1.49128 | ppl     4.443\n",
      "| epoch   1 step    67600 |  67600 batches | lr 5e-05 | ms/batch 48.85528 | loss 1.43712 | ppl     4.209\n",
      "| epoch   1 step    67800 |  67800 batches | lr 5e-05 | ms/batch 48.88166 | loss 1.49537 | ppl     4.461\n",
      "| epoch   1 step    68000 |  68000 batches | lr 5e-05 | ms/batch 46.89397 | loss 1.44524 | ppl     4.243\n",
      "|\n",
      "Source: [22  7 21  8 25  9 11  0 22 10]\n",
      "Target: [ 7 21  8 25  9 11  0 22 10  7]\n",
      "Teacher forcing: acc:0.34\n",
      "Preds:  [22  7 35  8 25  9 35  0 22  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  17 at step    68000 | time: 197.13s | valid loss 1.39099 | valid ppl    4.0188 | valid acc 0.34\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    68200 |  68200 batches | lr 5e-05 | ms/batch 55.34861 | loss 1.45529 | ppl     4.286\n",
      "| epoch   1 step    68400 |  68400 batches | lr 5e-05 | ms/batch 48.59815 | loss 1.48324 | ppl     4.407\n",
      "| epoch   1 step    68600 |  68600 batches | lr 5e-05 | ms/batch 48.57068 | loss 1.48178 | ppl     4.401\n",
      "| epoch   1 step    68800 |  68800 batches | lr 5e-05 | ms/batch 50.13273 | loss 1.50745 | ppl     4.515\n",
      "| epoch   1 step    69000 |  69000 batches | lr 5e-05 | ms/batch 48.37582 | loss 1.46354 | ppl     4.321\n",
      "| epoch   1 step    69200 |  69200 batches | lr 5e-05 | ms/batch 48.22575 | loss 1.48711 | ppl     4.424\n",
      "| epoch   1 step    69400 |  69400 batches | lr 5e-05 | ms/batch 48.42379 | loss 1.49434 | ppl     4.456\n",
      "| epoch   1 step    69600 |  69600 batches | lr 5e-05 | ms/batch 50.03994 | loss 1.48923 | ppl     4.434\n",
      "| epoch   1 step    69800 |  69800 batches | lr 5e-05 | ms/batch 50.08480 | loss 1.49815 | ppl     4.473\n",
      "| epoch   1 step    70000 |  70000 batches | lr 5e-05 | ms/batch 50.62042 | loss 1.47728 | ppl     4.381\n",
      "| epoch   1 step    70200 |  70200 batches | lr 5e-05 | ms/batch 49.09919 | loss 1.46264 | ppl     4.317\n",
      "| epoch   1 step    70400 |  70400 batches | lr 5e-05 | ms/batch 50.72100 | loss 1.46787 | ppl     4.340\n",
      "| epoch   1 step    70600 |  70600 batches | lr 5e-05 | ms/batch 48.41805 | loss 1.47500 | ppl     4.371\n",
      "| epoch   1 step    70800 |  70800 batches | lr 5e-05 | ms/batch 49.90685 | loss 1.46068 | ppl     4.309\n",
      "| epoch   1 step    71000 |  71000 batches | lr 5e-05 | ms/batch 47.79293 | loss 1.48817 | ppl     4.429\n",
      "| epoch   1 step    71200 |  71200 batches | lr 5e-05 | ms/batch 49.25150 | loss 1.44209 | ppl     4.230\n",
      "| epoch   1 step    71400 |  71400 batches | lr 5e-05 | ms/batch 52.05827 | loss 1.48337 | ppl     4.408\n",
      "| epoch   1 step    71600 |  71600 batches | lr 5e-05 | ms/batch 51.46656 | loss 1.42552 | ppl     4.160\n",
      "| epoch   1 step    71800 |  71800 batches | lr 5e-05 | ms/batch 52.02762 | loss 1.45448 | ppl     4.282\n",
      "| epoch   1 step    72000 |  72000 batches | lr 5e-05 | ms/batch 50.76524 | loss 1.50592 | ppl     4.508\n",
      "|\n",
      "Source: [16  6 29  3 19  1 20  9 20 10]\n",
      "Target: [ 6 29  3 19  1 20  9 20 10  9]\n",
      "Teacher forcing: acc:0.2\n",
      "Preds:  [1 6 1 3 1 1 1 9 1 6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  18 at step    72000 | time: 199.90s | valid loss 1.45637 | valid ppl    4.2904 | valid acc 0.2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    72200 |  72200 batches | lr 5e-05 | ms/batch 60.01239 | loss 1.48840 | ppl     4.430\n",
      "| epoch   1 step    72400 |  72400 batches | lr 5e-05 | ms/batch 52.22415 | loss 1.44950 | ppl     4.261\n",
      "| epoch   1 step    72600 |  72600 batches | lr 5e-05 | ms/batch 50.39537 | loss 1.51004 | ppl     4.527\n",
      "| epoch   1 step    72800 |  72800 batches | lr 5e-05 | ms/batch 51.16660 | loss 1.47582 | ppl     4.375\n",
      "| epoch   1 step    73000 |  73000 batches | lr 5e-05 | ms/batch 50.95953 | loss 1.49334 | ppl     4.452\n",
      "| epoch   1 step    73200 |  73200 batches | lr 5e-05 | ms/batch 50.33317 | loss 1.48557 | ppl     4.417\n",
      "| epoch   1 step    73400 |  73400 batches | lr 5e-05 | ms/batch 50.89467 | loss 1.47549 | ppl     4.373\n",
      "| epoch   1 step    73600 |  73600 batches | lr 5e-05 | ms/batch 50.57351 | loss 1.47686 | ppl     4.379\n",
      "| epoch   1 step    73800 |  73800 batches | lr 5e-05 | ms/batch 52.26902 | loss 1.46790 | ppl     4.340\n",
      "| epoch   1 step    74000 |  74000 batches | lr 5e-05 | ms/batch 53.48218 | loss 1.45802 | ppl     4.297\n",
      "| epoch   1 step    74200 |  74200 batches | lr 5e-05 | ms/batch 50.84564 | loss 1.47824 | ppl     4.385\n",
      "| epoch   1 step    74400 |  74400 batches | lr 5e-05 | ms/batch 48.90612 | loss 1.48444 | ppl     4.412\n",
      "| epoch   1 step    74600 |  74600 batches | lr 5e-05 | ms/batch 49.64285 | loss 1.47913 | ppl     4.389\n",
      "| epoch   1 step    74800 |  74800 batches | lr 5e-05 | ms/batch 48.61855 | loss 1.48063 | ppl     4.396\n",
      "| epoch   1 step    75000 |  75000 batches | lr 5e-05 | ms/batch 50.60961 | loss 1.48722 | ppl     4.425\n",
      "| epoch   1 step    75200 |  75200 batches | lr 5e-05 | ms/batch 49.48884 | loss 1.45727 | ppl     4.294\n",
      "| epoch   1 step    75400 |  75400 batches | lr 5e-05 | ms/batch 48.98128 | loss 1.47539 | ppl     4.373\n",
      "| epoch   1 step    75600 |  75600 batches | lr 5e-05 | ms/batch 47.16729 | loss 1.47737 | ppl     4.381\n",
      "| epoch   1 step    75800 |  75800 batches | lr 5e-05 | ms/batch 50.90534 | loss 1.47632 | ppl     4.377\n",
      "| epoch   1 step    76000 |  76000 batches | lr 5e-05 | ms/batch 49.34220 | loss 1.46991 | ppl     4.349\n",
      "|\n",
      "Source: [23  3 27  6 31  5 12  8 31 10]\n",
      "Target: [ 3 27  6 31  5 12  8 31 10  5]\n",
      "Teacher forcing: acc:0.17\n",
      "Preds:  [35  3 15  6 15  5 35  8 15  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  19 at step    76000 | time: 203.34s | valid loss 1.42992 | valid ppl    4.1783 | valid acc 0.17\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    76200 |  76200 batches | lr 5e-05 | ms/batch 56.47284 | loss 1.45811 | ppl     4.298\n",
      "| epoch   1 step    76400 |  76400 batches | lr 5e-05 | ms/batch 51.16222 | loss 1.47479 | ppl     4.370\n",
      "| epoch   1 step    76600 |  76600 batches | lr 5e-05 | ms/batch 51.15390 | loss 1.47902 | ppl     4.389\n",
      "| epoch   1 step    76800 |  76800 batches | lr 5e-05 | ms/batch 50.30733 | loss 1.48600 | ppl     4.419\n",
      "| epoch   1 step    77000 |  77000 batches | lr 5e-05 | ms/batch 48.91449 | loss 1.45222 | ppl     4.273\n",
      "| epoch   1 step    77200 |  77200 batches | lr 5e-05 | ms/batch 49.23238 | loss 1.46388 | ppl     4.323\n",
      "| epoch   1 step    77400 |  77400 batches | lr 5e-05 | ms/batch 48.58885 | loss 1.47671 | ppl     4.378\n",
      "| epoch   1 step    77600 |  77600 batches | lr 5e-05 | ms/batch 49.00416 | loss 1.48180 | ppl     4.401\n",
      "| epoch   1 step    77800 |  77800 batches | lr 5e-05 | ms/batch 50.30978 | loss 1.44551 | ppl     4.244\n",
      "| epoch   1 step    78000 |  78000 batches | lr 5e-05 | ms/batch 50.89524 | loss 1.48851 | ppl     4.431\n",
      "| epoch   1 step    78200 |  78200 batches | lr 5e-05 | ms/batch 53.03398 | loss 1.42875 | ppl     4.173\n",
      "| epoch   1 step    78400 |  78400 batches | lr 5e-05 | ms/batch 50.71902 | loss 1.49412 | ppl     4.455\n",
      "| epoch   1 step    78600 |  78600 batches | lr 5e-05 | ms/batch 50.03186 | loss 1.46448 | ppl     4.325\n",
      "| epoch   1 step    78800 |  78800 batches | lr 5e-05 | ms/batch 48.16210 | loss 1.47082 | ppl     4.353\n",
      "| epoch   1 step    79000 |  79000 batches | lr 5e-05 | ms/batch 49.16655 | loss 1.45137 | ppl     4.269\n",
      "| epoch   1 step    79200 |  79200 batches | lr 5e-05 | ms/batch 50.00201 | loss 1.49526 | ppl     4.461\n",
      "| epoch   1 step    79400 |  79400 batches | lr 5e-05 | ms/batch 50.41048 | loss 1.48406 | ppl     4.411\n",
      "| epoch   1 step    79600 |  79600 batches | lr 5e-05 | ms/batch 51.64405 | loss 1.47848 | ppl     4.386\n",
      "| epoch   1 step    79800 |  79800 batches | lr 5e-05 | ms/batch 48.73010 | loss 1.47357 | ppl     4.365\n",
      "| epoch   1 step    80000 |  80000 batches | lr 5e-05 | ms/batch 48.34961 | loss 1.41928 | ppl     4.134\n",
      "|\n",
      "Source: [22  6 16  2 34  4 35  8 22 10]\n",
      "Target: [ 6 16  2 34  4 35  8 22 10  6]\n",
      "Teacher forcing: acc:0.28\n",
      "Preds:  [22  6 16  2 25  4 35  8 22  8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  20 at step    80000 | time: 201.31s | valid loss 1.40138 | valid ppl    4.0608 | valid acc 0.28\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    80200 |  80200 batches | lr 2.5e-05 | ms/batch 56.11896 | loss 1.48773 | ppl     4.427\n",
      "| epoch   1 step    80400 |  80400 batches | lr 2.5e-05 | ms/batch 50.75946 | loss 1.47572 | ppl     4.374\n",
      "| epoch   1 step    80600 |  80600 batches | lr 2.5e-05 | ms/batch 50.64871 | loss 1.45126 | ppl     4.268\n",
      "| epoch   1 step    80800 |  80800 batches | lr 2.5e-05 | ms/batch 48.86152 | loss 1.45927 | ppl     4.303\n",
      "| epoch   1 step    81000 |  81000 batches | lr 2.5e-05 | ms/batch 49.25231 | loss 1.47617 | ppl     4.376\n",
      "| epoch   1 step    81200 |  81200 batches | lr 2.5e-05 | ms/batch 49.97489 | loss 1.44596 | ppl     4.246\n",
      "| epoch   1 step    81400 |  81400 batches | lr 2.5e-05 | ms/batch 50.41427 | loss 1.48209 | ppl     4.402\n",
      "| epoch   1 step    81600 |  81600 batches | lr 2.5e-05 | ms/batch 49.83832 | loss 1.49395 | ppl     4.455\n",
      "| epoch   1 step    81800 |  81800 batches | lr 2.5e-05 | ms/batch 50.38075 | loss 1.48556 | ppl     4.417\n",
      "| epoch   1 step    82000 |  82000 batches | lr 2.5e-05 | ms/batch 49.68183 | loss 1.47306 | ppl     4.363\n",
      "| epoch   1 step    82200 |  82200 batches | lr 2.5e-05 | ms/batch 49.11759 | loss 1.43787 | ppl     4.212\n",
      "| epoch   1 step    82400 |  82400 batches | lr 2.5e-05 | ms/batch 50.35960 | loss 1.45732 | ppl     4.294\n",
      "| epoch   1 step    82600 |  82600 batches | lr 2.5e-05 | ms/batch 49.25137 | loss 1.47364 | ppl     4.365\n",
      "| epoch   1 step    82800 |  82800 batches | lr 2.5e-05 | ms/batch 50.04346 | loss 1.44766 | ppl     4.253\n",
      "| epoch   1 step    83000 |  83000 batches | lr 2.5e-05 | ms/batch 50.86313 | loss 1.47768 | ppl     4.383\n",
      "| epoch   1 step    83200 |  83200 batches | lr 2.5e-05 | ms/batch 50.94282 | loss 1.46648 | ppl     4.334\n",
      "| epoch   1 step    83400 |  83400 batches | lr 2.5e-05 | ms/batch 49.36791 | loss 1.45941 | ppl     4.303\n",
      "| epoch   1 step    83600 |  83600 batches | lr 2.5e-05 | ms/batch 51.47036 | loss 1.48085 | ppl     4.397\n",
      "| epoch   1 step    83800 |  83800 batches | lr 2.5e-05 | ms/batch 50.09551 | loss 1.45810 | ppl     4.298\n",
      "| epoch   1 step    84000 |  84000 batches | lr 2.5e-05 | ms/batch 51.88454 | loss 1.48734 | ppl     4.425\n",
      "|\n",
      "Source: [20  9 15  2 23  5 21  4 23 10]\n",
      "Target: [ 9 15  2 23  5 21  4 23 10  5]\n",
      "Teacher forcing: acc:0.24\n",
      "Preds:  [20  9 15  9 32  5 32  4 32  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  21 at step    84000 | time: 201.87s | valid loss 1.41250 | valid ppl    4.1062 | valid acc 0.24\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    84200 |  84200 batches | lr 2.5e-05 | ms/batch 58.70779 | loss 1.44194 | ppl     4.229\n",
      "| epoch   1 step    84400 |  84400 batches | lr 2.5e-05 | ms/batch 49.30500 | loss 1.44832 | ppl     4.256\n",
      "| epoch   1 step    84600 |  84600 batches | lr 2.5e-05 | ms/batch 50.98842 | loss 1.44586 | ppl     4.245\n",
      "| epoch   1 step    84800 |  84800 batches | lr 2.5e-05 | ms/batch 49.93370 | loss 1.45255 | ppl     4.274\n",
      "| epoch   1 step    85000 |  85000 batches | lr 2.5e-05 | ms/batch 49.60456 | loss 1.48041 | ppl     4.395\n",
      "| epoch   1 step    85200 |  85200 batches | lr 2.5e-05 | ms/batch 49.49891 | loss 1.46105 | ppl     4.310\n",
      "| epoch   1 step    85400 |  85400 batches | lr 2.5e-05 | ms/batch 48.58571 | loss 1.43340 | ppl     4.193\n",
      "| epoch   1 step    85600 |  85600 batches | lr 2.5e-05 | ms/batch 48.86713 | loss 1.46891 | ppl     4.345\n",
      "| epoch   1 step    85800 |  85800 batches | lr 2.5e-05 | ms/batch 49.65090 | loss 1.44695 | ppl     4.250\n",
      "| epoch   1 step    86000 |  86000 batches | lr 2.5e-05 | ms/batch 48.88003 | loss 1.44170 | ppl     4.228\n",
      "| epoch   1 step    86200 |  86200 batches | lr 2.5e-05 | ms/batch 50.69674 | loss 1.47263 | ppl     4.361\n",
      "| epoch   1 step    86400 |  86400 batches | lr 2.5e-05 | ms/batch 48.42089 | loss 1.43766 | ppl     4.211\n",
      "| epoch   1 step    86600 |  86600 batches | lr 2.5e-05 | ms/batch 46.93218 | loss 1.45208 | ppl     4.272\n",
      "| epoch   1 step    86800 |  86800 batches | lr 2.5e-05 | ms/batch 49.82112 | loss 1.47071 | ppl     4.352\n",
      "| epoch   1 step    87000 |  87000 batches | lr 2.5e-05 | ms/batch 48.35523 | loss 1.46601 | ppl     4.332\n",
      "| epoch   1 step    87200 |  87200 batches | lr 2.5e-05 | ms/batch 48.72037 | loss 1.45977 | ppl     4.305\n",
      "| epoch   1 step    87400 |  87400 batches | lr 2.5e-05 | ms/batch 49.48842 | loss 1.46118 | ppl     4.311\n",
      "| epoch   1 step    87600 |  87600 batches | lr 2.5e-05 | ms/batch 49.50138 | loss 1.46784 | ppl     4.340\n",
      "| epoch   1 step    87800 |  87800 batches | lr 2.5e-05 | ms/batch 47.97001 | loss 1.45115 | ppl     4.268\n",
      "| epoch   1 step    88000 |  88000 batches | lr 2.5e-05 | ms/batch 48.16359 | loss 1.44028 | ppl     4.222\n",
      "|\n",
      "Source: [24  4 28  1 15  3 25  7 25 10]\n",
      "Target: [ 4 28  1 15  3 25  7 25 10  7]\n",
      "Teacher forcing: acc:0.22\n",
      "Preds:  [35  4 28  1 15  3 25  7 25  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  22 at step    88000 | time: 198.40s | valid loss 1.47445 | valid ppl    4.3686 | valid acc 0.22\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    88200 |  88200 batches | lr 2.5e-05 | ms/batch 55.60302 | loss 1.46305 | ppl     4.319\n",
      "| epoch   1 step    88400 |  88400 batches | lr 2.5e-05 | ms/batch 48.67696 | loss 1.47197 | ppl     4.358\n",
      "| epoch   1 step    88600 |  88600 batches | lr 2.5e-05 | ms/batch 48.60014 | loss 1.45636 | ppl     4.290\n",
      "| epoch   1 step    88800 |  88800 batches | lr 2.5e-05 | ms/batch 47.21492 | loss 1.48805 | ppl     4.428\n",
      "| epoch   1 step    89000 |  89000 batches | lr 2.5e-05 | ms/batch 49.54206 | loss 1.45191 | ppl     4.271\n",
      "| epoch   1 step    89200 |  89200 batches | lr 2.5e-05 | ms/batch 49.70271 | loss 1.46304 | ppl     4.319\n",
      "| epoch   1 step    89400 |  89400 batches | lr 2.5e-05 | ms/batch 50.29227 | loss 1.43729 | ppl     4.209\n",
      "| epoch   1 step    89600 |  89600 batches | lr 2.5e-05 | ms/batch 48.70259 | loss 1.48280 | ppl     4.405\n",
      "| epoch   1 step    89800 |  89800 batches | lr 2.5e-05 | ms/batch 48.40334 | loss 1.47904 | ppl     4.389\n",
      "| epoch   1 step    90000 |  90000 batches | lr 2.5e-05 | ms/batch 48.50702 | loss 1.46006 | ppl     4.306\n",
      "| epoch   1 step    90200 |  90200 batches | lr 2.5e-05 | ms/batch 48.92201 | loss 1.44860 | ppl     4.257\n",
      "| epoch   1 step    90400 |  90400 batches | lr 2.5e-05 | ms/batch 48.28494 | loss 1.47467 | ppl     4.370\n",
      "| epoch   1 step    90600 |  90600 batches | lr 2.5e-05 | ms/batch 49.73501 | loss 1.46024 | ppl     4.307\n",
      "| epoch   1 step    90800 |  90800 batches | lr 2.5e-05 | ms/batch 49.44940 | loss 1.46948 | ppl     4.347\n",
      "| epoch   1 step    91000 |  91000 batches | lr 2.5e-05 | ms/batch 50.17457 | loss 1.46305 | ppl     4.319\n",
      "| epoch   1 step    91200 |  91200 batches | lr 2.5e-05 | ms/batch 50.02421 | loss 1.47262 | ppl     4.361\n",
      "| epoch   1 step    91400 |  91400 batches | lr 2.5e-05 | ms/batch 50.16392 | loss 1.45952 | ppl     4.304\n",
      "| epoch   1 step    91600 |  91600 batches | lr 2.5e-05 | ms/batch 49.61385 | loss 1.46006 | ppl     4.306\n",
      "| epoch   1 step    91800 |  91800 batches | lr 2.5e-05 | ms/batch 49.41110 | loss 1.45637 | ppl     4.290\n",
      "| epoch   1 step    92000 |  92000 batches | lr 2.5e-05 | ms/batch 49.22894 | loss 1.47560 | ppl     4.374\n",
      "|\n",
      "Source: [33  2 31  7 15  1 24  6 31 10]\n",
      "Target: [ 2 31  7 15  1 24  6 31 10  7]\n",
      "Teacher forcing: acc:0.23\n",
      "Preds:  [35  2 25  7 15  1 35  6 25  1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  23 at step    92000 | time: 198.07s | valid loss 1.45947 | valid ppl    4.3037 | valid acc 0.23\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    92200 |  92200 batches | lr 2.5e-05 | ms/batch 55.49269 | loss 1.46168 | ppl     4.313\n",
      "| epoch   1 step    92400 |  92400 batches | lr 2.5e-05 | ms/batch 49.39247 | loss 1.49079 | ppl     4.441\n",
      "| epoch   1 step    92600 |  92600 batches | lr 2.5e-05 | ms/batch 49.88074 | loss 1.48065 | ppl     4.396\n",
      "| epoch   1 step    92800 |  92800 batches | lr 2.5e-05 | ms/batch 47.74863 | loss 1.44227 | ppl     4.230\n",
      "| epoch   1 step    93000 |  93000 batches | lr 2.5e-05 | ms/batch 48.47198 | loss 1.44455 | ppl     4.240\n",
      "| epoch   1 step    93200 |  93200 batches | lr 2.5e-05 | ms/batch 48.29608 | loss 1.46734 | ppl     4.338\n",
      "| epoch   1 step    93400 |  93400 batches | lr 2.5e-05 | ms/batch 49.32162 | loss 1.45534 | ppl     4.286\n",
      "| epoch   1 step    93600 |  93600 batches | lr 2.5e-05 | ms/batch 47.62534 | loss 1.44060 | ppl     4.223\n",
      "| epoch   1 step    93800 |  93800 batches | lr 2.5e-05 | ms/batch 49.95502 | loss 1.43782 | ppl     4.212\n",
      "| epoch   1 step    94000 |  94000 batches | lr 2.5e-05 | ms/batch 48.49899 | loss 1.45948 | ppl     4.304\n",
      "| epoch   1 step    94200 |  94200 batches | lr 2.5e-05 | ms/batch 48.62146 | loss 1.47979 | ppl     4.392\n",
      "| epoch   1 step    94400 |  94400 batches | lr 2.5e-05 | ms/batch 48.68402 | loss 1.43954 | ppl     4.219\n",
      "| epoch   1 step    94600 |  94600 batches | lr 2.5e-05 | ms/batch 48.75317 | loss 1.44257 | ppl     4.232\n",
      "| epoch   1 step    94800 |  94800 batches | lr 2.5e-05 | ms/batch 48.99184 | loss 1.43232 | ppl     4.188\n",
      "| epoch   1 step    95000 |  95000 batches | lr 2.5e-05 | ms/batch 49.82700 | loss 1.49870 | ppl     4.476\n",
      "| epoch   1 step    95200 |  95200 batches | lr 2.5e-05 | ms/batch 48.74343 | loss 1.45281 | ppl     4.275\n",
      "| epoch   1 step    95400 |  95400 batches | lr 2.5e-05 | ms/batch 48.70211 | loss 1.49448 | ppl     4.457\n",
      "| epoch   1 step    95600 |  95600 batches | lr 2.5e-05 | ms/batch 50.25580 | loss 1.46638 | ppl     4.334\n",
      "| epoch   1 step    95800 |  95800 batches | lr 2.5e-05 | ms/batch 53.30537 | loss 1.46107 | ppl     4.311\n",
      "| epoch   1 step    96000 |  96000 batches | lr 2.5e-05 | ms/batch 51.88095 | loss 1.48150 | ppl     4.400\n",
      "|\n",
      "Source: [34  8 32  1 21  7 18  2 18 10]\n",
      "Target: [ 8 32  1 21  7 18  2 18 10  2]\n",
      "Teacher forcing: acc:0.3\n",
      "Preds:  [25  8 32  1 35  7 18  2 18  1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  24 at step    96000 | time: 198.61s | valid loss 1.39624 | valid ppl    4.0400 | valid acc 0.3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    96200 |  96200 batches | lr 1.25e-05 | ms/batch 59.62652 | loss 1.47815 | ppl     4.385\n",
      "| epoch   1 step    96400 |  96400 batches | lr 1.25e-05 | ms/batch 53.13127 | loss 1.46517 | ppl     4.328\n",
      "| epoch   1 step    96600 |  96600 batches | lr 1.25e-05 | ms/batch 50.09792 | loss 1.47494 | ppl     4.371\n",
      "| epoch   1 step    96800 |  96800 batches | lr 1.25e-05 | ms/batch 50.49541 | loss 1.43458 | ppl     4.198\n",
      "| epoch   1 step    97000 |  97000 batches | lr 1.25e-05 | ms/batch 49.04529 | loss 1.48319 | ppl     4.407\n",
      "| epoch   1 step    97200 |  97200 batches | lr 1.25e-05 | ms/batch 49.16054 | loss 1.45596 | ppl     4.289\n",
      "| epoch   1 step    97400 |  97400 batches | lr 1.25e-05 | ms/batch 50.28021 | loss 1.44323 | ppl     4.234\n",
      "| epoch   1 step    97600 |  97600 batches | lr 1.25e-05 | ms/batch 48.41515 | loss 1.45480 | ppl     4.284\n",
      "| epoch   1 step    97800 |  97800 batches | lr 1.25e-05 | ms/batch 47.90895 | loss 1.47233 | ppl     4.359\n",
      "| epoch   1 step    98000 |  98000 batches | lr 1.25e-05 | ms/batch 48.20467 | loss 1.44218 | ppl     4.230\n",
      "| epoch   1 step    98200 |  98200 batches | lr 1.25e-05 | ms/batch 49.40751 | loss 1.45449 | ppl     4.282\n",
      "| epoch   1 step    98400 |  98400 batches | lr 1.25e-05 | ms/batch 49.93291 | loss 1.48837 | ppl     4.430\n",
      "| epoch   1 step    98600 |  98600 batches | lr 1.25e-05 | ms/batch 49.65988 | loss 1.48292 | ppl     4.406\n",
      "| epoch   1 step    98800 |  98800 batches | lr 1.25e-05 | ms/batch 51.68432 | loss 1.44597 | ppl     4.246\n",
      "| epoch   1 step    99000 |  99000 batches | lr 1.25e-05 | ms/batch 50.76645 | loss 1.44611 | ppl     4.247\n",
      "| epoch   1 step    99200 |  99200 batches | lr 1.25e-05 | ms/batch 51.09493 | loss 1.45917 | ppl     4.302\n",
      "| epoch   1 step    99400 |  99400 batches | lr 1.25e-05 | ms/batch 49.79707 | loss 1.45341 | ppl     4.278\n",
      "| epoch   1 step    99600 |  99600 batches | lr 1.25e-05 | ms/batch 48.75751 | loss 1.44141 | ppl     4.227\n",
      "| epoch   1 step    99800 |  99800 batches | lr 1.25e-05 | ms/batch 49.60679 | loss 1.44221 | ppl     4.230\n",
      "| epoch   1 step   100000 | 100000 batches | lr 1.25e-05 | ms/batch 49.10925 | loss 1.44928 | ppl     4.260\n",
      "|\n",
      "Source: [19  5 15  3 36  9 35  6 15 10]\n",
      "Target: [ 5 15  3 36  9 35  6 15 10  3]\n",
      "Teacher forcing: acc:0.25\n",
      "Preds:  [19  5 25  3 25  9 35  6 25  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  25 at step   100000 | time: 201.22s | valid loss 1.40354 | valid ppl    4.0696 | valid acc 0.25\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   100200 | 100200 batches | lr 1.25e-05 | ms/batch 56.55560 | loss 1.47111 | ppl     4.354\n",
      "| epoch   1 step   100400 | 100400 batches | lr 1.25e-05 | ms/batch 49.47762 | loss 1.43990 | ppl     4.220\n",
      "| epoch   1 step   100600 | 100600 batches | lr 1.25e-05 | ms/batch 48.08841 | loss 1.43770 | ppl     4.211\n",
      "| epoch   1 step   100800 | 100800 batches | lr 1.25e-05 | ms/batch 49.43975 | loss 1.44636 | ppl     4.248\n",
      "| epoch   1 step   101000 | 101000 batches | lr 1.25e-05 | ms/batch 48.93506 | loss 1.47629 | ppl     4.377\n",
      "| epoch   1 step   101200 | 101200 batches | lr 1.25e-05 | ms/batch 49.86061 | loss 1.45360 | ppl     4.279\n",
      "| epoch   1 step   101400 | 101400 batches | lr 1.25e-05 | ms/batch 48.91324 | loss 1.44873 | ppl     4.258\n",
      "| epoch   1 step   101600 | 101600 batches | lr 1.25e-05 | ms/batch 49.98352 | loss 1.45393 | ppl     4.280\n",
      "| epoch   1 step   101800 | 101800 batches | lr 1.25e-05 | ms/batch 49.86147 | loss 1.44979 | ppl     4.262\n",
      "| epoch   1 step   102000 | 102000 batches | lr 1.25e-05 | ms/batch 50.34582 | loss 1.46581 | ppl     4.331\n",
      "| epoch   1 step   102200 | 102200 batches | lr 1.25e-05 | ms/batch 48.42165 | loss 1.45686 | ppl     4.292\n",
      "| epoch   1 step   102400 | 102400 batches | lr 1.25e-05 | ms/batch 49.75628 | loss 1.44646 | ppl     4.248\n",
      "| epoch   1 step   102600 | 102600 batches | lr 1.25e-05 | ms/batch 48.68984 | loss 1.43019 | ppl     4.180\n",
      "| epoch   1 step   102800 | 102800 batches | lr 1.25e-05 | ms/batch 50.09331 | loss 1.48557 | ppl     4.417\n",
      "| epoch   1 step   103000 | 103000 batches | lr 1.25e-05 | ms/batch 50.12133 | loss 1.46397 | ppl     4.323\n",
      "| epoch   1 step   103200 | 103200 batches | lr 1.25e-05 | ms/batch 50.16466 | loss 1.44731 | ppl     4.252\n",
      "| epoch   1 step   103400 | 103400 batches | lr 1.25e-05 | ms/batch 49.60701 | loss 1.46012 | ppl     4.306\n",
      "| epoch   1 step   103600 | 103600 batches | lr 1.25e-05 | ms/batch 49.93191 | loss 1.46147 | ppl     4.312\n",
      "| epoch   1 step   103800 | 103800 batches | lr 1.25e-05 | ms/batch 49.89462 | loss 1.45525 | ppl     4.286\n",
      "| epoch   1 step   104000 | 104000 batches | lr 1.25e-05 | ms/batch 48.76966 | loss 1.45980 | ppl     4.305\n",
      "|\n",
      "Source: [16  4 32  7 35  8 31  0 16 10]\n",
      "Target: [ 4 32  7 35  8 31  0 16 10  4]\n",
      "Teacher forcing: acc:0.25\n",
      "Preds:  [ 1  4 32  7 35  8 35  0 16  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  26 at step   104000 | time: 199.29s | valid loss 1.40846 | valid ppl    4.0897 | valid acc 0.25\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   104200 | 104200 batches | lr 1.25e-05 | ms/batch 56.14698 | loss 1.45523 | ppl     4.285\n",
      "| epoch   1 step   104400 | 104400 batches | lr 1.25e-05 | ms/batch 49.10329 | loss 1.44259 | ppl     4.232\n",
      "| epoch   1 step   104600 | 104600 batches | lr 1.25e-05 | ms/batch 48.53749 | loss 1.46731 | ppl     4.338\n",
      "| epoch   1 step   104800 | 104800 batches | lr 1.25e-05 | ms/batch 49.29774 | loss 1.44419 | ppl     4.238\n",
      "| epoch   1 step   105000 | 105000 batches | lr 1.25e-05 | ms/batch 49.21011 | loss 1.46932 | ppl     4.346\n",
      "| epoch   1 step   105200 | 105200 batches | lr 1.25e-05 | ms/batch 50.36090 | loss 1.43257 | ppl     4.189\n",
      "| epoch   1 step   105400 | 105400 batches | lr 1.25e-05 | ms/batch 49.39069 | loss 1.44435 | ppl     4.239\n",
      "| epoch   1 step   105600 | 105600 batches | lr 1.25e-05 | ms/batch 51.53932 | loss 1.44512 | ppl     4.242\n",
      "| epoch   1 step   105800 | 105800 batches | lr 1.25e-05 | ms/batch 48.42489 | loss 1.47125 | ppl     4.355\n",
      "| epoch   1 step   106000 | 106000 batches | lr 1.25e-05 | ms/batch 48.68450 | loss 1.44420 | ppl     4.238\n",
      "| epoch   1 step   106200 | 106200 batches | lr 1.25e-05 | ms/batch 49.00953 | loss 1.47332 | ppl     4.364\n",
      "| epoch   1 step   106400 | 106400 batches | lr 1.25e-05 | ms/batch 50.70432 | loss 1.47066 | ppl     4.352\n",
      "| epoch   1 step   106600 | 106600 batches | lr 1.25e-05 | ms/batch 49.28990 | loss 1.45665 | ppl     4.292\n",
      "| epoch   1 step   106800 | 106800 batches | lr 1.25e-05 | ms/batch 48.98210 | loss 1.48274 | ppl     4.405\n",
      "| epoch   1 step   107000 | 107000 batches | lr 1.25e-05 | ms/batch 49.10050 | loss 1.47705 | ppl     4.380\n",
      "| epoch   1 step   107200 | 107200 batches | lr 1.25e-05 | ms/batch 50.17440 | loss 1.48160 | ppl     4.400\n",
      "| epoch   1 step   107400 | 107400 batches | lr 1.25e-05 | ms/batch 49.85161 | loss 1.45089 | ppl     4.267\n",
      "| epoch   1 step   107600 | 107600 batches | lr 1.25e-05 | ms/batch 47.48244 | loss 1.45906 | ppl     4.302\n",
      "| epoch   1 step   107800 | 107800 batches | lr 1.25e-05 | ms/batch 48.78169 | loss 1.44224 | ppl     4.230\n",
      "| epoch   1 step   108000 | 108000 batches | lr 1.25e-05 | ms/batch 47.74207 | loss 1.47704 | ppl     4.380\n",
      "|\n",
      "Source: [33  4 11  8 27  9 18  7 11 10]\n",
      "Target: [ 4 11  8 27  9 18  7 11 10  8]\n",
      "Teacher forcing: acc:0.3\n",
      "Preds:  [33  4 35  8 27  9 18  7 35  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  27 at step   108000 | time: 198.46s | valid loss 1.40642 | valid ppl    4.0813 | valid acc 0.3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   108200 | 108200 batches | lr 1.25e-05 | ms/batch 55.28195 | loss 1.46222 | ppl     4.316\n",
      "| epoch   1 step   108400 | 108400 batches | lr 1.25e-05 | ms/batch 49.29702 | loss 1.44708 | ppl     4.251\n",
      "| epoch   1 step   108600 | 108600 batches | lr 1.25e-05 | ms/batch 49.36427 | loss 1.43556 | ppl     4.202\n",
      "| epoch   1 step   108800 | 108800 batches | lr 1.25e-05 | ms/batch 51.51887 | loss 1.45837 | ppl     4.299\n",
      "| epoch   1 step   109000 | 109000 batches | lr 1.25e-05 | ms/batch 49.20509 | loss 1.45920 | ppl     4.303\n",
      "| epoch   1 step   109200 | 109200 batches | lr 1.25e-05 | ms/batch 49.56923 | loss 1.46352 | ppl     4.321\n",
      "| epoch   1 step   109400 | 109400 batches | lr 1.25e-05 | ms/batch 49.65747 | loss 1.47597 | ppl     4.375\n",
      "| epoch   1 step   109600 | 109600 batches | lr 1.25e-05 | ms/batch 50.18824 | loss 1.46082 | ppl     4.309\n",
      "| epoch   1 step   109800 | 109800 batches | lr 1.25e-05 | ms/batch 49.59923 | loss 1.45232 | ppl     4.273\n",
      "| epoch   1 step   110000 | 110000 batches | lr 1.25e-05 | ms/batch 50.47887 | loss 1.45573 | ppl     4.288\n",
      "| epoch   1 step   110200 | 110200 batches | lr 1.25e-05 | ms/batch 49.46362 | loss 1.43936 | ppl     4.218\n",
      "| epoch   1 step   110400 | 110400 batches | lr 1.25e-05 | ms/batch 50.33256 | loss 1.49122 | ppl     4.443\n",
      "| epoch   1 step   110600 | 110600 batches | lr 1.25e-05 | ms/batch 50.37030 | loss 1.44625 | ppl     4.247\n",
      "| epoch   1 step   110800 | 110800 batches | lr 1.25e-05 | ms/batch 52.58150 | loss 1.44749 | ppl     4.252\n",
      "| epoch   1 step   111000 | 111000 batches | lr 1.25e-05 | ms/batch 47.91754 | loss 1.44488 | ppl     4.241\n",
      "| epoch   1 step   111200 | 111200 batches | lr 1.25e-05 | ms/batch 50.31087 | loss 1.43290 | ppl     4.191\n",
      "| epoch   1 step   111400 | 111400 batches | lr 1.25e-05 | ms/batch 50.95096 | loss 1.46941 | ppl     4.347\n",
      "| epoch   1 step   111600 | 111600 batches | lr 1.25e-05 | ms/batch 49.71097 | loss 1.48068 | ppl     4.396\n",
      "| epoch   1 step   111800 | 111800 batches | lr 1.25e-05 | ms/batch 50.58890 | loss 1.45048 | ppl     4.265\n",
      "| epoch   1 step   112000 | 112000 batches | lr 1.25e-05 | ms/batch 50.91144 | loss 1.47018 | ppl     4.350\n",
      "|\n",
      "Source: [11  1 13  7 30  5 36  4 13 10]\n",
      "Target: [ 1 13  7 30  5 36  4 13 10  7]\n",
      "Teacher forcing: acc:0.24\n",
      "Preds:  [35  1 13  7 35  5 35  4 13  1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  28 at step   112000 | time: 201.39s | valid loss 1.41228 | valid ppl    4.1053 | valid acc 0.24\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   112200 | 112200 batches | lr 6.25e-06 | ms/batch 57.05114 | loss 1.46327 | ppl     4.320\n",
      "| epoch   1 step   112400 | 112400 batches | lr 6.25e-06 | ms/batch 51.58063 | loss 1.45842 | ppl     4.299\n",
      "| epoch   1 step   112600 | 112600 batches | lr 6.25e-06 | ms/batch 49.52487 | loss 1.44095 | ppl     4.225\n",
      "| epoch   1 step   112800 | 112800 batches | lr 6.25e-06 | ms/batch 48.69505 | loss 1.45972 | ppl     4.305\n",
      "| epoch   1 step   113000 | 113000 batches | lr 6.25e-06 | ms/batch 50.11115 | loss 1.47390 | ppl     4.366\n",
      "| epoch   1 step   113200 | 113200 batches | lr 6.25e-06 | ms/batch 50.58065 | loss 1.46836 | ppl     4.342\n",
      "| epoch   1 step   113400 | 113400 batches | lr 6.25e-06 | ms/batch 48.86408 | loss 1.45110 | ppl     4.268\n",
      "| epoch   1 step   113600 | 113600 batches | lr 6.25e-06 | ms/batch 49.31797 | loss 1.46537 | ppl     4.329\n",
      "| epoch   1 step   113800 | 113800 batches | lr 6.25e-06 | ms/batch 49.65076 | loss 1.44285 | ppl     4.233\n",
      "| epoch   1 step   114000 | 114000 batches | lr 6.25e-06 | ms/batch 51.51149 | loss 1.47116 | ppl     4.354\n",
      "| epoch   1 step   114200 | 114200 batches | lr 6.25e-06 | ms/batch 48.66837 | loss 1.43313 | ppl     4.192\n",
      "| epoch   1 step   114400 | 114400 batches | lr 6.25e-06 | ms/batch 48.82648 | loss 1.46469 | ppl     4.326\n",
      "| epoch   1 step   114600 | 114600 batches | lr 6.25e-06 | ms/batch 48.92991 | loss 1.45882 | ppl     4.301\n",
      "| epoch   1 step   114800 | 114800 batches | lr 6.25e-06 | ms/batch 48.04440 | loss 1.45263 | ppl     4.274\n",
      "| epoch   1 step   115000 | 115000 batches | lr 6.25e-06 | ms/batch 48.60262 | loss 1.45163 | ppl     4.270\n",
      "| epoch   1 step   115200 | 115200 batches | lr 6.25e-06 | ms/batch 50.33772 | loss 1.45699 | ppl     4.293\n",
      "| epoch   1 step   115400 | 115400 batches | lr 6.25e-06 | ms/batch 49.34060 | loss 1.43986 | ppl     4.220\n",
      "| epoch   1 step   115600 | 115600 batches | lr 6.25e-06 | ms/batch 48.16451 | loss 1.42930 | ppl     4.176\n",
      "| epoch   1 step   115800 | 115800 batches | lr 6.25e-06 | ms/batch 48.95831 | loss 1.41723 | ppl     4.126\n",
      "| epoch   1 step   116000 | 116000 batches | lr 6.25e-06 | ms/batch 46.33549 | loss 1.45567 | ppl     4.287\n",
      "|\n",
      "Source: [19  8 25  0 33  9 14  4 33 10]\n",
      "Target: [ 8 25  0 33  9 14  4 33 10  9]\n",
      "Teacher forcing: acc:0.35\n",
      "Preds:  [ 1  8  5  0 25  9 35  4 25  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  29 at step   116000 | time: 198.48s | valid loss 1.39282 | valid ppl    4.0262 | valid acc 0.35\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   116200 | 116200 batches | lr 6.25e-06 | ms/batch 51.13148 | loss 1.46056 | ppl     4.308\n",
      "| epoch   1 step   116400 | 116400 batches | lr 6.25e-06 | ms/batch 45.27222 | loss 1.46143 | ppl     4.312\n",
      "| epoch   1 step   116600 | 116600 batches | lr 6.25e-06 | ms/batch 45.09362 | loss 1.47050 | ppl     4.351\n",
      "| epoch   1 step   116800 | 116800 batches | lr 6.25e-06 | ms/batch 46.15714 | loss 1.49500 | ppl     4.459\n",
      "| epoch   1 step   117000 | 117000 batches | lr 6.25e-06 | ms/batch 45.56191 | loss 1.42214 | ppl     4.146\n",
      "| epoch   1 step   117200 | 117200 batches | lr 6.25e-06 | ms/batch 45.53063 | loss 1.43861 | ppl     4.215\n",
      "| epoch   1 step   117400 | 117400 batches | lr 6.25e-06 | ms/batch 47.13202 | loss 1.47468 | ppl     4.370\n",
      "| epoch   1 step   117600 | 117600 batches | lr 6.25e-06 | ms/batch 45.71434 | loss 1.42672 | ppl     4.165\n",
      "| epoch   1 step   117800 | 117800 batches | lr 6.25e-06 | ms/batch 45.40503 | loss 1.45895 | ppl     4.301\n",
      "| epoch   1 step   118000 | 118000 batches | lr 6.25e-06 | ms/batch 44.32364 | loss 1.44886 | ppl     4.258\n",
      "| epoch   1 step   118200 | 118200 batches | lr 6.25e-06 | ms/batch 45.43392 | loss 1.44439 | ppl     4.239\n",
      "| epoch   1 step   118400 | 118400 batches | lr 6.25e-06 | ms/batch 45.25005 | loss 1.45351 | ppl     4.278\n",
      "| epoch   1 step   118600 | 118600 batches | lr 6.25e-06 | ms/batch 46.32707 | loss 1.46557 | ppl     4.330\n",
      "| epoch   1 step   118800 | 118800 batches | lr 6.25e-06 | ms/batch 45.70950 | loss 1.46217 | ppl     4.315\n",
      "| epoch   1 step   119000 | 119000 batches | lr 6.25e-06 | ms/batch 45.04471 | loss 1.45532 | ppl     4.286\n",
      "| epoch   1 step   119200 | 119200 batches | lr 6.25e-06 | ms/batch 44.87185 | loss 1.42102 | ppl     4.141\n",
      "| epoch   1 step   119400 | 119400 batches | lr 6.25e-06 | ms/batch 46.75360 | loss 1.43060 | ppl     4.181\n",
      "| epoch   1 step   119600 | 119600 batches | lr 6.25e-06 | ms/batch 45.20717 | loss 1.46722 | ppl     4.337\n",
      "| epoch   1 step   119800 | 119800 batches | lr 6.25e-06 | ms/batch 45.42758 | loss 1.43401 | ppl     4.195\n",
      "| epoch   1 step   120000 | 120000 batches | lr 6.25e-06 | ms/batch 43.98606 | loss 1.45297 | ppl     4.276\n",
      "|\n",
      "Source: [22  7 20  5 36  1 15  2 15 10]\n",
      "Target: [ 7 20  5 36  1 15  2 15 10  2]\n",
      "Teacher forcing: acc:0.26\n",
      "Preds:  [22  7 22  5 22  1 15  2 15  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  30 at step   120000 | time: 183.12s | valid loss 1.39471 | valid ppl    4.0338 | valid acc 0.26\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   120200 | 120200 batches | lr 6.25e-06 | ms/batch 51.47354 | loss 1.44752 | ppl     4.253\n",
      "| epoch   1 step   120400 | 120400 batches | lr 6.25e-06 | ms/batch 45.19449 | loss 1.42884 | ppl     4.174\n",
      "| epoch   1 step   120600 | 120600 batches | lr 6.25e-06 | ms/batch 44.68376 | loss 1.42351 | ppl     4.152\n",
      "| epoch   1 step   120800 | 120800 batches | lr 6.25e-06 | ms/batch 45.20367 | loss 1.44822 | ppl     4.256\n",
      "| epoch   1 step   121000 | 121000 batches | lr 6.25e-06 | ms/batch 46.12641 | loss 1.44891 | ppl     4.258\n",
      "| epoch   1 step   121200 | 121200 batches | lr 6.25e-06 | ms/batch 45.72967 | loss 1.44461 | ppl     4.240\n",
      "| epoch   1 step   121400 | 121400 batches | lr 6.25e-06 | ms/batch 45.92938 | loss 1.46569 | ppl     4.331\n",
      "| epoch   1 step   121600 | 121600 batches | lr 6.25e-06 | ms/batch 44.74366 | loss 1.44694 | ppl     4.250\n",
      "| epoch   1 step   121800 | 121800 batches | lr 6.25e-06 | ms/batch 44.19801 | loss 1.46138 | ppl     4.312\n",
      "| epoch   1 step   122000 | 122000 batches | lr 6.25e-06 | ms/batch 46.81017 | loss 1.46532 | ppl     4.329\n",
      "| epoch   1 step   122200 | 122200 batches | lr 6.25e-06 | ms/batch 45.95623 | loss 1.45305 | ppl     4.276\n",
      "| epoch   1 step   122400 | 122400 batches | lr 6.25e-06 | ms/batch 45.44323 | loss 1.44729 | ppl     4.252\n",
      "| epoch   1 step   122600 | 122600 batches | lr 6.25e-06 | ms/batch 46.69462 | loss 1.43674 | ppl     4.207\n",
      "| epoch   1 step   122800 | 122800 batches | lr 6.25e-06 | ms/batch 45.41527 | loss 1.44106 | ppl     4.225\n",
      "| epoch   1 step   123000 | 123000 batches | lr 6.25e-06 | ms/batch 45.83058 | loss 1.45285 | ppl     4.275\n",
      "| epoch   1 step   123200 | 123200 batches | lr 6.25e-06 | ms/batch 44.05239 | loss 1.45858 | ppl     4.300\n",
      "| epoch   1 step   123400 | 123400 batches | lr 6.25e-06 | ms/batch 44.95196 | loss 1.43802 | ppl     4.212\n",
      "| epoch   1 step   123600 | 123600 batches | lr 6.25e-06 | ms/batch 45.32686 | loss 1.45115 | ppl     4.268\n",
      "| epoch   1 step   123800 | 123800 batches | lr 6.25e-06 | ms/batch 46.11875 | loss 1.45792 | ppl     4.297\n",
      "| epoch   1 step   124000 | 124000 batches | lr 6.25e-06 | ms/batch 45.87131 | loss 1.45912 | ppl     4.302\n",
      "|\n",
      "Source: [33  2 24  5 21  7 15  9 21 10]\n",
      "Target: [ 2 24  5 21  7 15  9 21 10  7]\n",
      "Teacher forcing: acc:0.25\n",
      "Preds:  [35  2 35  5 35  7 15  9 35  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  31 at step   124000 | time: 183.24s | valid loss 1.42552 | valid ppl    4.1600 | valid acc 0.25\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   124200 | 124200 batches | lr 6.25e-06 | ms/batch 52.16912 | loss 1.45820 | ppl     4.298\n",
      "| epoch   1 step   124400 | 124400 batches | lr 6.25e-06 | ms/batch 45.98209 | loss 1.44491 | ppl     4.241\n",
      "| epoch   1 step   124600 | 124600 batches | lr 6.25e-06 | ms/batch 43.74830 | loss 1.46958 | ppl     4.347\n",
      "| epoch   1 step   124800 | 124800 batches | lr 6.25e-06 | ms/batch 43.89111 | loss 1.43743 | ppl     4.210\n",
      "| epoch   1 step   125000 | 125000 batches | lr 6.25e-06 | ms/batch 45.24175 | loss 1.44975 | ppl     4.262\n",
      "| epoch   1 step   125200 | 125200 batches | lr 6.25e-06 | ms/batch 45.54961 | loss 1.41357 | ppl     4.111\n",
      "| epoch   1 step   125400 | 125400 batches | lr 6.25e-06 | ms/batch 46.36756 | loss 1.46036 | ppl     4.307\n",
      "| epoch   1 step   125600 | 125600 batches | lr 6.25e-06 | ms/batch 44.95855 | loss 1.43813 | ppl     4.213\n",
      "| epoch   1 step   125800 | 125800 batches | lr 6.25e-06 | ms/batch 44.75440 | loss 1.46378 | ppl     4.322\n",
      "| epoch   1 step   126000 | 126000 batches | lr 6.25e-06 | ms/batch 45.82580 | loss 1.44869 | ppl     4.258\n",
      "| epoch   1 step   126200 | 126200 batches | lr 6.25e-06 | ms/batch 45.00190 | loss 1.41999 | ppl     4.137\n",
      "| epoch   1 step   126400 | 126400 batches | lr 6.25e-06 | ms/batch 44.67168 | loss 1.43873 | ppl     4.215\n",
      "| epoch   1 step   126600 | 126600 batches | lr 6.25e-06 | ms/batch 45.80002 | loss 1.43498 | ppl     4.200\n",
      "| epoch   1 step   126800 | 126800 batches | lr 6.25e-06 | ms/batch 46.46140 | loss 1.44993 | ppl     4.263\n",
      "| epoch   1 step   127000 | 127000 batches | lr 6.25e-06 | ms/batch 45.37089 | loss 1.44931 | ppl     4.260\n",
      "| epoch   1 step   127200 | 127200 batches | lr 6.25e-06 | ms/batch 44.44198 | loss 1.42496 | ppl     4.158\n",
      "| epoch   1 step   127400 | 127400 batches | lr 6.25e-06 | ms/batch 44.24054 | loss 1.46448 | ppl     4.325\n",
      "| epoch   1 step   127600 | 127600 batches | lr 6.25e-06 | ms/batch 44.44582 | loss 1.46221 | ppl     4.315\n",
      "| epoch   1 step   127800 | 127800 batches | lr 6.25e-06 | ms/batch 44.60556 | loss 1.45573 | ppl     4.288\n",
      "| epoch   1 step   128000 | 128000 batches | lr 6.25e-06 | ms/batch 45.27384 | loss 1.46470 | ppl     4.326\n",
      "|\n",
      "Source: [17  0 21  7 18  2 35  8 18 10]\n",
      "Target: [ 0 21  7 18  2 35  8 18 10  2]\n",
      "Teacher forcing: acc:0.29\n",
      "Preds:  [ 5  0 26  7 18  2 35  8 18  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  32 at step   128000 | time: 181.69s | valid loss 1.40959 | valid ppl    4.0943 | valid acc 0.29\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   128200 | 128200 batches | lr 3.13e-06 | ms/batch 51.50391 | loss 1.42886 | ppl     4.174\n",
      "| epoch   1 step   128400 | 128400 batches | lr 3.13e-06 | ms/batch 46.25000 | loss 1.42892 | ppl     4.174\n",
      "| epoch   1 step   128600 | 128600 batches | lr 3.13e-06 | ms/batch 45.65667 | loss 1.46901 | ppl     4.345\n",
      "| epoch   1 step   128800 | 128800 batches | lr 3.13e-06 | ms/batch 45.93715 | loss 1.45468 | ppl     4.283\n",
      "| epoch   1 step   129000 | 129000 batches | lr 3.13e-06 | ms/batch 45.72885 | loss 1.45454 | ppl     4.283\n",
      "| epoch   1 step   129200 | 129200 batches | lr 3.13e-06 | ms/batch 45.86775 | loss 1.45238 | ppl     4.273\n",
      "| epoch   1 step   129400 | 129400 batches | lr 3.13e-06 | ms/batch 45.12709 | loss 1.46910 | ppl     4.345\n",
      "| epoch   1 step   129600 | 129600 batches | lr 3.13e-06 | ms/batch 45.29193 | loss 1.44687 | ppl     4.250\n",
      "| epoch   1 step   129800 | 129800 batches | lr 3.13e-06 | ms/batch 45.79764 | loss 1.43647 | ppl     4.206\n",
      "| epoch   1 step   130000 | 130000 batches | lr 3.13e-06 | ms/batch 45.26506 | loss 1.45155 | ppl     4.270\n",
      "| epoch   1 step   130200 | 130200 batches | lr 3.13e-06 | ms/batch 44.96283 | loss 1.45595 | ppl     4.289\n",
      "| epoch   1 step   130400 | 130400 batches | lr 3.13e-06 | ms/batch 44.96268 | loss 1.43759 | ppl     4.211\n",
      "| epoch   1 step   130600 | 130600 batches | lr 3.13e-06 | ms/batch 45.21871 | loss 1.45978 | ppl     4.305\n",
      "| epoch   1 step   130800 | 130800 batches | lr 3.13e-06 | ms/batch 45.36109 | loss 1.43115 | ppl     4.184\n",
      "| epoch   1 step   131000 | 131000 batches | lr 3.13e-06 | ms/batch 45.32159 | loss 1.43536 | ppl     4.201\n",
      "| epoch   1 step   131200 | 131200 batches | lr 3.13e-06 | ms/batch 45.18764 | loss 1.43926 | ppl     4.218\n",
      "| epoch   1 step   131400 | 131400 batches | lr 3.13e-06 | ms/batch 45.57098 | loss 1.42695 | ppl     4.166\n",
      "| epoch   1 step   131600 | 131600 batches | lr 3.13e-06 | ms/batch 46.17015 | loss 1.45639 | ppl     4.290\n",
      "| epoch   1 step   131800 | 131800 batches | lr 3.13e-06 | ms/batch 45.50004 | loss 1.44021 | ppl     4.222\n",
      "| epoch   1 step   132000 | 132000 batches | lr 3.13e-06 | ms/batch 46.25664 | loss 1.45888 | ppl     4.301\n",
      "|\n",
      "Source: [35  3 27  8 31  6 22  4 27 10]\n",
      "Target: [ 3 27  8 31  6 22  4 27 10  8]\n",
      "Teacher forcing: acc:0.27\n",
      "Preds:  [ 2  3  0  8 35  6 22  4 35  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  33 at step   132000 | time: 183.42s | valid loss 1.40142 | valid ppl    4.0609 | valid acc 0.27\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   132200 | 132200 batches | lr 3.13e-06 | ms/batch 52.15369 | loss 1.45687 | ppl     4.292\n",
      "| epoch   1 step   132400 | 132400 batches | lr 3.13e-06 | ms/batch 45.31786 | loss 1.46124 | ppl     4.311\n",
      "| epoch   1 step   132600 | 132600 batches | lr 3.13e-06 | ms/batch 45.42134 | loss 1.44094 | ppl     4.225\n",
      "| epoch   1 step   132800 | 132800 batches | lr 3.13e-06 | ms/batch 44.89685 | loss 1.45077 | ppl     4.266\n",
      "| epoch   1 step   133000 | 133000 batches | lr 3.13e-06 | ms/batch 45.84024 | loss 1.44838 | ppl     4.256\n",
      "| epoch   1 step   133200 | 133200 batches | lr 3.13e-06 | ms/batch 44.55859 | loss 1.45031 | ppl     4.264\n",
      "| epoch   1 step   133400 | 133400 batches | lr 3.13e-06 | ms/batch 44.44510 | loss 1.44817 | ppl     4.255\n",
      "| epoch   1 step   133600 | 133600 batches | lr 3.13e-06 | ms/batch 44.92660 | loss 1.46587 | ppl     4.331\n",
      "| epoch   1 step   133800 | 133800 batches | lr 3.13e-06 | ms/batch 44.22128 | loss 1.44074 | ppl     4.224\n",
      "| epoch   1 step   134000 | 134000 batches | lr 3.13e-06 | ms/batch 45.76033 | loss 1.44291 | ppl     4.233\n",
      "| epoch   1 step   134200 | 134200 batches | lr 3.13e-06 | ms/batch 45.50233 | loss 1.44810 | ppl     4.255\n",
      "| epoch   1 step   134400 | 134400 batches | lr 3.13e-06 | ms/batch 46.88065 | loss 1.45015 | ppl     4.264\n",
      "| epoch   1 step   134600 | 134600 batches | lr 3.13e-06 | ms/batch 46.30882 | loss 1.46524 | ppl     4.329\n",
      "| epoch   1 step   134800 | 134800 batches | lr 3.13e-06 | ms/batch 45.34118 | loss 1.43385 | ppl     4.195\n",
      "| epoch   1 step   135000 | 135000 batches | lr 3.13e-06 | ms/batch 45.57538 | loss 1.45014 | ppl     4.264\n",
      "| epoch   1 step   135200 | 135200 batches | lr 3.13e-06 | ms/batch 46.22212 | loss 1.45792 | ppl     4.297\n",
      "| epoch   1 step   135400 | 135400 batches | lr 3.13e-06 | ms/batch 45.03267 | loss 1.49049 | ppl     4.439\n",
      "| epoch   1 step   135600 | 135600 batches | lr 3.13e-06 | ms/batch 45.14309 | loss 1.45744 | ppl     4.295\n",
      "| epoch   1 step   135800 | 135800 batches | lr 3.13e-06 | ms/batch 45.77239 | loss 1.44620 | ppl     4.247\n",
      "| epoch   1 step   136000 | 136000 batches | lr 3.13e-06 | ms/batch 44.86951 | loss 1.43401 | ppl     4.195\n",
      "|\n",
      "Source: [31  1 32  2 19  4 35  8 31 10]\n",
      "Target: [ 1 32  2 19  4 35  8 31 10  1]\n",
      "Teacher forcing: acc:0.2\n",
      "Preds:  [25  1 32  2  1  4 35  8 25  8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  34 at step   136000 | time: 182.84s | valid loss 1.42554 | valid ppl    4.1601 | valid acc 0.2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   136200 | 136200 batches | lr 3.13e-06 | ms/batch 52.00328 | loss 1.47532 | ppl     4.372\n",
      "| epoch   1 step   136400 | 136400 batches | lr 3.13e-06 | ms/batch 45.92703 | loss 1.46487 | ppl     4.327\n",
      "| epoch   1 step   136600 | 136600 batches | lr 3.13e-06 | ms/batch 45.33715 | loss 1.44941 | ppl     4.261\n",
      "| epoch   1 step   136800 | 136800 batches | lr 3.13e-06 | ms/batch 46.27934 | loss 1.45374 | ppl     4.279\n",
      "| epoch   1 step   137000 | 137000 batches | lr 3.13e-06 | ms/batch 44.93430 | loss 1.44509 | ppl     4.242\n",
      "| epoch   1 step   137200 | 137200 batches | lr 3.13e-06 | ms/batch 44.77335 | loss 1.44779 | ppl     4.254\n",
      "| epoch   1 step   137400 | 137400 batches | lr 3.13e-06 | ms/batch 45.13192 | loss 1.44080 | ppl     4.224\n",
      "| epoch   1 step   137600 | 137600 batches | lr 3.13e-06 | ms/batch 44.75970 | loss 1.44099 | ppl     4.225\n",
      "| epoch   1 step   137800 | 137800 batches | lr 3.13e-06 | ms/batch 44.42886 | loss 1.42079 | ppl     4.140\n",
      "| epoch   1 step   138000 | 138000 batches | lr 3.13e-06 | ms/batch 46.66964 | loss 1.42744 | ppl     4.168\n",
      "| epoch   1 step   138200 | 138200 batches | lr 3.13e-06 | ms/batch 46.56868 | loss 1.44276 | ppl     4.232\n",
      "| epoch   1 step   138400 | 138400 batches | lr 3.13e-06 | ms/batch 45.76225 | loss 1.44318 | ppl     4.234\n",
      "| epoch   1 step   138600 | 138600 batches | lr 3.13e-06 | ms/batch 46.13138 | loss 1.45590 | ppl     4.288\n",
      "| epoch   1 step   138800 | 138800 batches | lr 3.13e-06 | ms/batch 45.28226 | loss 1.42563 | ppl     4.160\n",
      "| epoch   1 step   139000 | 139000 batches | lr 3.13e-06 | ms/batch 45.42724 | loss 1.44181 | ppl     4.228\n",
      "| epoch   1 step   139200 | 139200 batches | lr 3.13e-06 | ms/batch 45.35326 | loss 1.42898 | ppl     4.174\n",
      "| epoch   1 step   139400 | 139400 batches | lr 3.13e-06 | ms/batch 44.46006 | loss 1.44528 | ppl     4.243\n",
      "| epoch   1 step   139600 | 139600 batches | lr 3.13e-06 | ms/batch 44.69434 | loss 1.46042 | ppl     4.308\n",
      "| epoch   1 step   139800 | 139800 batches | lr 3.13e-06 | ms/batch 45.31111 | loss 1.45955 | ppl     4.304\n",
      "| epoch   1 step   140000 | 140000 batches | lr 3.13e-06 | ms/batch 46.26415 | loss 1.43076 | ppl     4.182\n",
      "|\n",
      "Source: [17  0 19  6 23  3 13  5 13 10]\n",
      "Target: [ 0 19  6 23  3 13  5 13 10  5]\n",
      "Teacher forcing: acc:0.27\n",
      "Preds:  [ 5  0  5  6 25  3 13  5 13  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  35 at step   140000 | time: 183.09s | valid loss 1.40476 | valid ppl    4.0746 | valid acc 0.27\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   140200 | 140200 batches | lr 3.13e-06 | ms/batch 52.17283 | loss 1.43039 | ppl     4.180\n",
      "| epoch   1 step   140400 | 140400 batches | lr 3.13e-06 | ms/batch 45.45080 | loss 1.45926 | ppl     4.303\n",
      "| epoch   1 step   140600 | 140600 batches | lr 3.13e-06 | ms/batch 44.67038 | loss 1.45981 | ppl     4.305\n",
      "| epoch   1 step   140800 | 140800 batches | lr 3.13e-06 | ms/batch 45.02864 | loss 1.43230 | ppl     4.188\n",
      "| epoch   1 step   141000 | 141000 batches | lr 3.13e-06 | ms/batch 45.51604 | loss 1.46146 | ppl     4.312\n",
      "| epoch   1 step   141200 | 141200 batches | lr 3.13e-06 | ms/batch 43.89174 | loss 1.44541 | ppl     4.244\n",
      "| epoch   1 step   141400 | 141400 batches | lr 3.13e-06 | ms/batch 46.26206 | loss 1.46769 | ppl     4.339\n",
      "| epoch   1 step   141600 | 141600 batches | lr 3.13e-06 | ms/batch 45.75079 | loss 1.44377 | ppl     4.237\n",
      "| epoch   1 step   141800 | 141800 batches | lr 3.13e-06 | ms/batch 45.13254 | loss 1.42685 | ppl     4.166\n",
      "| epoch   1 step   142000 | 142000 batches | lr 3.13e-06 | ms/batch 46.10961 | loss 1.45452 | ppl     4.282\n",
      "| epoch   1 step   142200 | 142200 batches | lr 3.13e-06 | ms/batch 44.34658 | loss 1.46401 | ppl     4.323\n",
      "| epoch   1 step   142400 | 142400 batches | lr 3.13e-06 | ms/batch 44.99774 | loss 1.44528 | ppl     4.243\n",
      "| epoch   1 step   142600 | 142600 batches | lr 3.13e-06 | ms/batch 44.89572 | loss 1.44595 | ppl     4.246\n",
      "| epoch   1 step   142800 | 142800 batches | lr 3.13e-06 | ms/batch 46.33646 | loss 1.47205 | ppl     4.358\n",
      "| epoch   1 step   143000 | 143000 batches | lr 3.13e-06 | ms/batch 45.92051 | loss 1.44043 | ppl     4.223\n",
      "| epoch   1 step   143200 | 143200 batches | lr 3.13e-06 | ms/batch 45.81821 | loss 1.46821 | ppl     4.341\n",
      "| epoch   1 step   143400 | 143400 batches | lr 3.13e-06 | ms/batch 46.64332 | loss 1.43351 | ppl     4.193\n",
      "| epoch   1 step   143600 | 143600 batches | lr 3.13e-06 | ms/batch 45.24323 | loss 1.44586 | ppl     4.246\n",
      "| epoch   1 step   143800 | 143800 batches | lr 3.13e-06 | ms/batch 46.75993 | loss 1.45929 | ppl     4.303\n",
      "| epoch   1 step   144000 | 144000 batches | lr 3.13e-06 | ms/batch 46.90780 | loss 1.44500 | ppl     4.242\n",
      "|\n",
      "Source: [17  5 21  1 35  7 24  3 17 10]\n",
      "Target: [ 5 21  1 35  7 24  3 17 10  5]\n",
      "Teacher forcing: acc:0.29\n",
      "Preds:  [ 5  5 26  1 35  7 25  3 25  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  36 at step   144000 | time: 183.70s | valid loss 1.40819 | valid ppl    4.0886 | valid acc 0.29\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   144200 | 144200 batches | lr 1.56e-06 | ms/batch 52.86672 | loss 1.43303 | ppl     4.191\n",
      "| epoch   1 step   144400 | 144400 batches | lr 1.56e-06 | ms/batch 46.26072 | loss 1.45675 | ppl     4.292\n",
      "| epoch   1 step   144600 | 144600 batches | lr 1.56e-06 | ms/batch 45.69115 | loss 1.46225 | ppl     4.316\n",
      "| epoch   1 step   144800 | 144800 batches | lr 1.56e-06 | ms/batch 45.60779 | loss 1.43120 | ppl     4.184\n",
      "| epoch   1 step   145000 | 145000 batches | lr 1.56e-06 | ms/batch 45.51068 | loss 1.46640 | ppl     4.334\n",
      "| epoch   1 step   145200 | 145200 batches | lr 1.56e-06 | ms/batch 44.55562 | loss 1.46217 | ppl     4.315\n",
      "| epoch   1 step   145400 | 145400 batches | lr 1.56e-06 | ms/batch 45.29486 | loss 1.48336 | ppl     4.408\n",
      "| epoch   1 step   145600 | 145600 batches | lr 1.56e-06 | ms/batch 44.11960 | loss 1.44472 | ppl     4.241\n",
      "| epoch   1 step   145800 | 145800 batches | lr 1.56e-06 | ms/batch 45.38271 | loss 1.45306 | ppl     4.276\n",
      "| epoch   1 step   146000 | 146000 batches | lr 1.56e-06 | ms/batch 45.63451 | loss 1.45091 | ppl     4.267\n",
      "| epoch   1 step   146200 | 146200 batches | lr 1.56e-06 | ms/batch 46.61595 | loss 1.43366 | ppl     4.194\n",
      "| epoch   1 step   146400 | 146400 batches | lr 1.56e-06 | ms/batch 45.89685 | loss 1.44616 | ppl     4.247\n",
      "| epoch   1 step   146600 | 146600 batches | lr 1.56e-06 | ms/batch 45.15915 | loss 1.43097 | ppl     4.183\n",
      "| epoch   1 step   146800 | 146800 batches | lr 1.56e-06 | ms/batch 43.77070 | loss 1.44634 | ppl     4.248\n",
      "| epoch   1 step   147000 | 147000 batches | lr 1.56e-06 | ms/batch 45.45077 | loss 1.46541 | ppl     4.329\n",
      "| epoch   1 step   147200 | 147200 batches | lr 1.56e-06 | ms/batch 45.89100 | loss 1.43647 | ppl     4.206\n",
      "| epoch   1 step   147400 | 147400 batches | lr 1.56e-06 | ms/batch 45.63484 | loss 1.44911 | ppl     4.259\n",
      "| epoch   1 step   147600 | 147600 batches | lr 1.56e-06 | ms/batch 45.06463 | loss 1.48265 | ppl     4.405\n",
      "| epoch   1 step   147800 | 147800 batches | lr 1.56e-06 | ms/batch 45.04567 | loss 1.46222 | ppl     4.316\n",
      "| epoch   1 step   148000 | 148000 batches | lr 1.56e-06 | ms/batch 45.77489 | loss 1.47188 | ppl     4.357\n",
      "|\n",
      "Source: [29  8 27  2 26  7 17  9 29 10]\n",
      "Target: [ 8 27  2 26  7 17  9 29 10  8]\n",
      "Teacher forcing: acc:0.22\n",
      "Preds:  [ 1  8 25  2 26  7 25  9  1  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  37 at step   148000 | time: 182.96s | valid loss 1.42581 | valid ppl    4.1612 | valid acc 0.22\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   148200 | 148200 batches | lr 1.56e-06 | ms/batch 52.78281 | loss 1.46100 | ppl     4.310\n",
      "| epoch   1 step   148400 | 148400 batches | lr 1.56e-06 | ms/batch 45.04087 | loss 1.46113 | ppl     4.311\n",
      "| epoch   1 step   148600 | 148600 batches | lr 1.56e-06 | ms/batch 45.92231 | loss 1.41650 | ppl     4.123\n",
      "| epoch   1 step   148800 | 148800 batches | lr 1.56e-06 | ms/batch 46.06778 | loss 1.44258 | ppl     4.232\n",
      "| epoch   1 step   149000 | 149000 batches | lr 1.56e-06 | ms/batch 45.58752 | loss 1.43667 | ppl     4.207\n",
      "| epoch   1 step   149200 | 149200 batches | lr 1.56e-06 | ms/batch 44.75696 | loss 1.44860 | ppl     4.257\n",
      "| epoch   1 step   149400 | 149400 batches | lr 1.56e-06 | ms/batch 45.35418 | loss 1.43582 | ppl     4.203\n",
      "| epoch   1 step   149600 | 149600 batches | lr 1.56e-06 | ms/batch 46.04068 | loss 1.45890 | ppl     4.301\n",
      "| epoch   1 step   149800 | 149800 batches | lr 1.56e-06 | ms/batch 44.59079 | loss 1.44821 | ppl     4.255\n",
      "| epoch   1 step   150000 | 150000 batches | lr 1.56e-06 | ms/batch 45.20463 | loss 1.44820 | ppl     4.255\n",
      "| epoch   1 step   150200 | 150200 batches | lr 1.56e-06 | ms/batch 45.26112 | loss 1.42597 | ppl     4.162\n",
      "| epoch   1 step   150400 | 150400 batches | lr 1.56e-06 | ms/batch 45.81276 | loss 1.45408 | ppl     4.281\n",
      "| epoch   1 step   150600 | 150600 batches | lr 1.56e-06 | ms/batch 44.58325 | loss 1.44562 | ppl     4.245\n",
      "| epoch   1 step   150800 | 150800 batches | lr 1.56e-06 | ms/batch 44.41631 | loss 1.43406 | ppl     4.196\n",
      "| epoch   1 step   151000 | 151000 batches | lr 1.56e-06 | ms/batch 45.32500 | loss 1.46376 | ppl     4.322\n",
      "| epoch   1 step   151200 | 151200 batches | lr 1.56e-06 | ms/batch 44.35085 | loss 1.44615 | ppl     4.247\n",
      "| epoch   1 step   151400 | 151400 batches | lr 1.56e-06 | ms/batch 45.00543 | loss 1.43921 | ppl     4.217\n",
      "| epoch   1 step   151600 | 151600 batches | lr 1.56e-06 | ms/batch 45.78361 | loss 1.45500 | ppl     4.284\n",
      "| epoch   1 step   151800 | 151800 batches | lr 1.56e-06 | ms/batch 45.88966 | loss 1.43599 | ppl     4.204\n",
      "| epoch   1 step   152000 | 152000 batches | lr 1.56e-06 | ms/batch 44.29812 | loss 1.46055 | ppl     4.308\n",
      "|\n",
      "Source: [31  2 25  5 13  8 24  4 31 10]\n",
      "Target: [ 2 25  5 13  8 24  4 31 10  2]\n",
      "Teacher forcing: acc:0.27\n",
      "Preds:  [25  2 25  5 25  8 25  4 25  5]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  38 at step   152000 | time: 182.40s | valid loss 1.41523 | valid ppl    4.1174 | valid acc 0.27\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   152200 | 152200 batches | lr 1.56e-06 | ms/batch 50.85585 | loss 1.44946 | ppl     4.261\n",
      "| epoch   1 step   152400 | 152400 batches | lr 1.56e-06 | ms/batch 45.63803 | loss 1.45349 | ppl     4.278\n",
      "| epoch   1 step   152600 | 152600 batches | lr 1.56e-06 | ms/batch 45.17474 | loss 1.46553 | ppl     4.330\n",
      "| epoch   1 step   152800 | 152800 batches | lr 1.56e-06 | ms/batch 45.71481 | loss 1.42631 | ppl     4.163\n",
      "| epoch   1 step   153000 | 153000 batches | lr 1.56e-06 | ms/batch 45.72314 | loss 1.48127 | ppl     4.399\n",
      "| epoch   1 step   153200 | 153200 batches | lr 1.56e-06 | ms/batch 47.16944 | loss 1.45009 | ppl     4.263\n",
      "| epoch   1 step   153400 | 153400 batches | lr 1.56e-06 | ms/batch 45.28121 | loss 1.45823 | ppl     4.298\n",
      "| epoch   1 step   153600 | 153600 batches | lr 1.56e-06 | ms/batch 45.40519 | loss 1.46919 | ppl     4.346\n",
      "| epoch   1 step   153800 | 153800 batches | lr 1.56e-06 | ms/batch 47.00084 | loss 1.44806 | ppl     4.255\n",
      "| epoch   1 step   154000 | 154000 batches | lr 1.56e-06 | ms/batch 45.23211 | loss 1.45438 | ppl     4.282\n",
      "| epoch   1 step   154200 | 154200 batches | lr 1.56e-06 | ms/batch 45.26405 | loss 1.45476 | ppl     4.283\n",
      "| epoch   1 step   154400 | 154400 batches | lr 1.56e-06 | ms/batch 44.98206 | loss 1.44880 | ppl     4.258\n",
      "| epoch   1 step   154600 | 154600 batches | lr 1.56e-06 | ms/batch 46.23543 | loss 1.43577 | ppl     4.203\n",
      "| epoch   1 step   154800 | 154800 batches | lr 1.56e-06 | ms/batch 45.11412 | loss 1.47212 | ppl     4.358\n",
      "| epoch   1 step   155000 | 155000 batches | lr 1.56e-06 | ms/batch 46.46020 | loss 1.43780 | ppl     4.211\n",
      "| epoch   1 step   155200 | 155200 batches | lr 1.56e-06 | ms/batch 45.81776 | loss 1.44228 | ppl     4.230\n",
      "| epoch   1 step   155400 | 155400 batches | lr 1.56e-06 | ms/batch 45.52632 | loss 1.45564 | ppl     4.287\n",
      "| epoch   1 step   155600 | 155600 batches | lr 1.56e-06 | ms/batch 46.88432 | loss 1.45001 | ppl     4.263\n",
      "| epoch   1 step   155800 | 155800 batches | lr 1.56e-06 | ms/batch 46.71233 | loss 1.45204 | ppl     4.272\n",
      "| epoch   1 step   156000 | 156000 batches | lr 1.56e-06 | ms/batch 46.53133 | loss 1.46334 | ppl     4.320\n",
      "|\n",
      "Source: [32  3 30  8 17  7 27  6 17 10]\n",
      "Target: [ 3 30  8 17  7 27  6 17 10  7]\n",
      "Teacher forcing: acc:0.21\n",
      "Preds:  [ 1  3 35  8 25  7 35  6 25  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  39 at step   156000 | time: 184.60s | valid loss 1.41866 | valid ppl    4.1316 | valid acc 0.21\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   156200 | 156200 batches | lr 1.56e-06 | ms/batch 52.78661 | loss 1.46389 | ppl     4.323\n",
      "| epoch   1 step   156400 | 156400 batches | lr 1.56e-06 | ms/batch 46.06018 | loss 1.45120 | ppl     4.268\n",
      "| epoch   1 step   156600 | 156600 batches | lr 1.56e-06 | ms/batch 46.38302 | loss 1.47436 | ppl     4.368\n",
      "| epoch   1 step   156800 | 156800 batches | lr 1.56e-06 | ms/batch 46.88718 | loss 1.45434 | ppl     4.282\n",
      "| epoch   1 step   157000 | 157000 batches | lr 1.56e-06 | ms/batch 46.97529 | loss 1.48470 | ppl     4.414\n",
      "| epoch   1 step   157200 | 157200 batches | lr 1.56e-06 | ms/batch 47.28223 | loss 1.47739 | ppl     4.382\n",
      "| epoch   1 step   157400 | 157400 batches | lr 1.56e-06 | ms/batch 47.32699 | loss 1.46080 | ppl     4.309\n",
      "| epoch   1 step   157600 | 157600 batches | lr 1.56e-06 | ms/batch 47.79646 | loss 1.47919 | ppl     4.389\n",
      "| epoch   1 step   157800 | 157800 batches | lr 1.56e-06 | ms/batch 47.72891 | loss 1.44259 | ppl     4.232\n",
      "| epoch   1 step   158000 | 158000 batches | lr 1.56e-06 | ms/batch 46.49262 | loss 1.44042 | ppl     4.222\n",
      "| epoch   1 step   158200 | 158200 batches | lr 1.56e-06 | ms/batch 45.92649 | loss 1.45425 | ppl     4.281\n",
      "| epoch   1 step   158400 | 158400 batches | lr 1.56e-06 | ms/batch 47.15747 | loss 1.45205 | ppl     4.272\n",
      "| epoch   1 step   158600 | 158600 batches | lr 1.56e-06 | ms/batch 46.87497 | loss 1.46654 | ppl     4.334\n",
      "| epoch   1 step   158800 | 158800 batches | lr 1.56e-06 | ms/batch 47.20677 | loss 1.43892 | ppl     4.216\n",
      "| epoch   1 step   159000 | 159000 batches | lr 1.56e-06 | ms/batch 47.07195 | loss 1.46515 | ppl     4.328\n",
      "| epoch   1 step   159200 | 159200 batches | lr 1.56e-06 | ms/batch 47.64042 | loss 1.47014 | ppl     4.350\n",
      "| epoch   1 step   159400 | 159400 batches | lr 1.56e-06 | ms/batch 47.20651 | loss 1.46163 | ppl     4.313\n",
      "| epoch   1 step   159600 | 159600 batches | lr 1.56e-06 | ms/batch 48.04886 | loss 1.43750 | ppl     4.210\n",
      "| epoch   1 step   159800 | 159800 batches | lr 1.56e-06 | ms/batch 47.35031 | loss 1.45505 | ppl     4.285\n",
      "| epoch   1 step   160000 | 160000 batches | lr 1.56e-06 | ms/batch 46.21571 | loss 1.45378 | ppl     4.279\n",
      "|\n",
      "Source: [19  3 15  7 24  9 20  2 20 10]\n",
      "Target: [ 3 15  7 24  9 20  2 20 10  2]\n",
      "Teacher forcing: acc:0.29\n",
      "Preds:  [ 1  3  1  7 25  9 26  2 26  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  40 at step   160000 | time: 189.19s | valid loss 1.40998 | valid ppl    4.0959 | valid acc 0.29\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   160200 | 160200 batches | lr 1e-06 | ms/batch 52.25637 | loss 1.46251 | ppl     4.317\n",
      "| epoch   1 step   160400 | 160400 batches | lr 1e-06 | ms/batch 46.91514 | loss 1.47249 | ppl     4.360\n",
      "| epoch   1 step   160600 | 160600 batches | lr 1e-06 | ms/batch 45.71040 | loss 1.45949 | ppl     4.304\n",
      "| epoch   1 step   160800 | 160800 batches | lr 1e-06 | ms/batch 46.10128 | loss 1.43621 | ppl     4.205\n",
      "| epoch   1 step   161000 | 161000 batches | lr 1e-06 | ms/batch 45.66628 | loss 1.42751 | ppl     4.168\n",
      "| epoch   1 step   161200 | 161200 batches | lr 1e-06 | ms/batch 44.99242 | loss 1.43958 | ppl     4.219\n",
      "| epoch   1 step   161400 | 161400 batches | lr 1e-06 | ms/batch 45.13192 | loss 1.46381 | ppl     4.322\n",
      "| epoch   1 step   161600 | 161600 batches | lr 1e-06 | ms/batch 44.55098 | loss 1.45026 | ppl     4.264\n",
      "| epoch   1 step   161800 | 161800 batches | lr 1e-06 | ms/batch 46.15380 | loss 1.45661 | ppl     4.291\n",
      "| epoch   1 step   162000 | 162000 batches | lr 1e-06 | ms/batch 45.07679 | loss 1.47157 | ppl     4.356\n",
      "| epoch   1 step   162200 | 162200 batches | lr 1e-06 | ms/batch 46.26847 | loss 1.43959 | ppl     4.219\n",
      "| epoch   1 step   162400 | 162400 batches | lr 1e-06 | ms/batch 45.12855 | loss 1.48633 | ppl     4.421\n",
      "| epoch   1 step   162600 | 162600 batches | lr 1e-06 | ms/batch 45.57274 | loss 1.43017 | ppl     4.179\n",
      "| epoch   1 step   162800 | 162800 batches | lr 1e-06 | ms/batch 45.10639 | loss 1.44222 | ppl     4.230\n",
      "| epoch   1 step   163000 | 163000 batches | lr 1e-06 | ms/batch 46.10702 | loss 1.44252 | ppl     4.231\n",
      "| epoch   1 step   163200 | 163200 batches | lr 1e-06 | ms/batch 45.52873 | loss 1.45312 | ppl     4.276\n",
      "| epoch   1 step   163400 | 163400 batches | lr 1e-06 | ms/batch 45.50367 | loss 1.44036 | ppl     4.222\n",
      "| epoch   1 step   163600 | 163600 batches | lr 1e-06 | ms/batch 46.47808 | loss 1.46176 | ppl     4.314\n",
      "| epoch   1 step   163800 | 163800 batches | lr 1e-06 | ms/batch 46.41189 | loss 1.45796 | ppl     4.297\n",
      "| epoch   1 step   164000 | 164000 batches | lr 1e-06 | ms/batch 46.19262 | loss 1.41991 | ppl     4.137\n",
      "|\n",
      "Source: [12  6 11  1 30  8 16  2 16 10]\n",
      "Target: [ 6 11  1 30  8 16  2 16 10  2]\n",
      "Teacher forcing: acc:0.25\n",
      "Preds:  [35  6 35  1 35  8 35  2 35  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  41 at step   164000 | time: 184.19s | valid loss 1.41193 | valid ppl    4.1039 | valid acc 0.25\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   164200 | 164200 batches | lr 1e-06 | ms/batch 52.82393 | loss 1.43938 | ppl     4.218\n",
      "| epoch   1 step   164400 | 164400 batches | lr 1e-06 | ms/batch 45.75633 | loss 1.44794 | ppl     4.254\n",
      "| epoch   1 step   164600 | 164600 batches | lr 1e-06 | ms/batch 45.61574 | loss 1.46782 | ppl     4.340\n",
      "| epoch   1 step   164800 | 164800 batches | lr 1e-06 | ms/batch 45.78428 | loss 1.45955 | ppl     4.304\n",
      "| epoch   1 step   165000 | 165000 batches | lr 1e-06 | ms/batch 45.74823 | loss 1.43817 | ppl     4.213\n",
      "| epoch   1 step   165200 | 165200 batches | lr 1e-06 | ms/batch 46.12708 | loss 1.44947 | ppl     4.261\n",
      "| epoch   1 step   165400 | 165400 batches | lr 1e-06 | ms/batch 46.31055 | loss 1.43317 | ppl     4.192\n",
      "| epoch   1 step   165600 | 165600 batches | lr 1e-06 | ms/batch 46.54101 | loss 1.43444 | ppl     4.197\n",
      "| epoch   1 step   165800 | 165800 batches | lr 1e-06 | ms/batch 46.60059 | loss 1.43612 | ppl     4.204\n",
      "| epoch   1 step   166000 | 166000 batches | lr 1e-06 | ms/batch 46.18529 | loss 1.45025 | ppl     4.264\n",
      "| epoch   1 step   166200 | 166200 batches | lr 1e-06 | ms/batch 45.98025 | loss 1.45499 | ppl     4.284\n",
      "| epoch   1 step   166400 | 166400 batches | lr 1e-06 | ms/batch 46.16700 | loss 1.44258 | ppl     4.232\n",
      "| epoch   1 step   166600 | 166600 batches | lr 1e-06 | ms/batch 46.05161 | loss 1.45126 | ppl     4.268\n",
      "| epoch   1 step   166800 | 166800 batches | lr 1e-06 | ms/batch 46.36786 | loss 1.48429 | ppl     4.412\n",
      "| epoch   1 step   167000 | 167000 batches | lr 1e-06 | ms/batch 47.18196 | loss 1.44576 | ppl     4.245\n",
      "| epoch   1 step   167200 | 167200 batches | lr 1e-06 | ms/batch 45.91741 | loss 1.47707 | ppl     4.380\n",
      "| epoch   1 step   167400 | 167400 batches | lr 1e-06 | ms/batch 46.33791 | loss 1.46806 | ppl     4.341\n",
      "| epoch   1 step   167600 | 167600 batches | lr 1e-06 | ms/batch 47.61805 | loss 1.43254 | ppl     4.189\n",
      "| epoch   1 step   167800 | 167800 batches | lr 1e-06 | ms/batch 46.78599 | loss 1.48953 | ppl     4.435\n",
      "| epoch   1 step   168000 | 168000 batches | lr 1e-06 | ms/batch 44.80096 | loss 1.46024 | ppl     4.307\n",
      "|\n",
      "Source: [29  1 20  6 18  0 35  4 29 10]\n",
      "Target: [ 1 20  6 18  0 35  4 29 10  1]\n",
      "Teacher forcing: acc:0.24\n",
      "Preds:  [ 1  1  1  6 18  0 35  4  1  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  42 at step   168000 | time: 186.22s | valid loss 1.42120 | valid ppl    4.1421 | valid acc 0.24\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   168200 | 168200 batches | lr 1e-06 | ms/batch 51.09522 | loss 1.44524 | ppl     4.243\n",
      "| epoch   1 step   168400 | 168400 batches | lr 1e-06 | ms/batch 44.87845 | loss 1.46065 | ppl     4.309\n",
      "| epoch   1 step   168600 | 168600 batches | lr 1e-06 | ms/batch 45.60079 | loss 1.45398 | ppl     4.280\n",
      "| epoch   1 step   168800 | 168800 batches | lr 1e-06 | ms/batch 45.12143 | loss 1.46174 | ppl     4.313\n",
      "| epoch   1 step   169000 | 169000 batches | lr 1e-06 | ms/batch 45.34580 | loss 1.47019 | ppl     4.350\n",
      "| epoch   1 step   169200 | 169200 batches | lr 1e-06 | ms/batch 45.91507 | loss 1.43573 | ppl     4.203\n",
      "| epoch   1 step   169400 | 169400 batches | lr 1e-06 | ms/batch 44.41455 | loss 1.46861 | ppl     4.343\n",
      "| epoch   1 step   169600 | 169600 batches | lr 1e-06 | ms/batch 43.81221 | loss 1.45064 | ppl     4.266\n",
      "| epoch   1 step   169800 | 169800 batches | lr 1e-06 | ms/batch 45.03577 | loss 1.44884 | ppl     4.258\n",
      "| epoch   1 step   170000 | 170000 batches | lr 1e-06 | ms/batch 45.63623 | loss 1.45030 | ppl     4.264\n",
      "| epoch   1 step   170200 | 170200 batches | lr 1e-06 | ms/batch 45.13339 | loss 1.47601 | ppl     4.375\n",
      "| epoch   1 step   170400 | 170400 batches | lr 1e-06 | ms/batch 45.44051 | loss 1.44248 | ppl     4.231\n",
      "| epoch   1 step   170600 | 170600 batches | lr 1e-06 | ms/batch 45.52836 | loss 1.45474 | ppl     4.283\n",
      "| epoch   1 step   170800 | 170800 batches | lr 1e-06 | ms/batch 45.64351 | loss 1.44347 | ppl     4.235\n",
      "| epoch   1 step   171000 | 171000 batches | lr 1e-06 | ms/batch 44.58874 | loss 1.43027 | ppl     4.180\n",
      "| epoch   1 step   171200 | 171200 batches | lr 1e-06 | ms/batch 45.68038 | loss 1.42942 | ppl     4.176\n",
      "| epoch   1 step   171400 | 171400 batches | lr 1e-06 | ms/batch 45.62315 | loss 1.44373 | ppl     4.236\n",
      "| epoch   1 step   171600 | 171600 batches | lr 1e-06 | ms/batch 45.59800 | loss 1.43028 | ppl     4.180\n",
      "| epoch   1 step   171800 | 171800 batches | lr 1e-06 | ms/batch 44.67465 | loss 1.45523 | ppl     4.285\n",
      "| epoch   1 step   172000 | 172000 batches | lr 1e-06 | ms/batch 46.09658 | loss 1.45996 | ppl     4.306\n",
      "|\n",
      "Source: [16  9 22  7 12  2 30  5 16 10]\n",
      "Target: [ 9 22  7 12  2 30  5 16 10  9]\n",
      "Teacher forcing: acc:0.27\n",
      "Preds:  [ 1  9  1  7 35  2 35  5  1  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  43 at step   172000 | time: 182.13s | valid loss 1.41530 | valid ppl    4.1177 | valid acc 0.27\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   172200 | 172200 batches | lr 1e-06 | ms/batch 50.39978 | loss 1.43742 | ppl     4.210\n",
      "| epoch   1 step   172400 | 172400 batches | lr 1e-06 | ms/batch 43.91985 | loss 1.45094 | ppl     4.267\n",
      "| epoch   1 step   172600 | 172600 batches | lr 1e-06 | ms/batch 44.72324 | loss 1.45304 | ppl     4.276\n",
      "| epoch   1 step   172800 | 172800 batches | lr 1e-06 | ms/batch 45.03399 | loss 1.44883 | ppl     4.258\n",
      "| epoch   1 step   173000 | 173000 batches | lr 1e-06 | ms/batch 45.67293 | loss 1.47023 | ppl     4.350\n",
      "| epoch   1 step   173200 | 173200 batches | lr 1e-06 | ms/batch 44.75939 | loss 1.44195 | ppl     4.229\n",
      "| epoch   1 step   173400 | 173400 batches | lr 1e-06 | ms/batch 46.48283 | loss 1.44010 | ppl     4.221\n",
      "| epoch   1 step   173600 | 173600 batches | lr 1e-06 | ms/batch 45.81971 | loss 1.45440 | ppl     4.282\n",
      "| epoch   1 step   173800 | 173800 batches | lr 1e-06 | ms/batch 46.06771 | loss 1.44729 | ppl     4.252\n",
      "| epoch   1 step   174000 | 174000 batches | lr 1e-06 | ms/batch 45.81063 | loss 1.43081 | ppl     4.182\n",
      "| epoch   1 step   174200 | 174200 batches | lr 1e-06 | ms/batch 44.55042 | loss 1.46570 | ppl     4.331\n",
      "| epoch   1 step   174400 | 174400 batches | lr 1e-06 | ms/batch 45.08943 | loss 1.44392 | ppl     4.237\n",
      "| epoch   1 step   174600 | 174600 batches | lr 1e-06 | ms/batch 43.96535 | loss 1.46669 | ppl     4.335\n",
      "| epoch   1 step   174800 | 174800 batches | lr 1e-06 | ms/batch 45.99624 | loss 1.45249 | ppl     4.274\n",
      "| epoch   1 step   175000 | 175000 batches | lr 1e-06 | ms/batch 45.75797 | loss 1.46829 | ppl     4.342\n",
      "| epoch   1 step   175200 | 175200 batches | lr 1e-06 | ms/batch 45.20013 | loss 1.42687 | ppl     4.166\n",
      "| epoch   1 step   175400 | 175400 batches | lr 1e-06 | ms/batch 46.11062 | loss 1.44484 | ppl     4.241\n",
      "| epoch   1 step   175600 | 175600 batches | lr 1e-06 | ms/batch 45.49279 | loss 1.44414 | ppl     4.238\n",
      "| epoch   1 step   175800 | 175800 batches | lr 1e-06 | ms/batch 43.97476 | loss 1.44933 | ppl     4.260\n",
      "| epoch   1 step   176000 | 176000 batches | lr 1e-06 | ms/batch 45.08693 | loss 1.46140 | ppl     4.312\n",
      "|\n",
      "Source: [20  7 36  9 11  4 17  6 20 10]\n",
      "Target: [ 7 36  9 11  4 17  6 20 10  7]\n",
      "Teacher forcing: acc:0.22\n",
      "Preds:  [20  7 36  9 32  4 26  6 20  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  44 at step   176000 | time: 181.96s | valid loss 1.42470 | valid ppl    4.1566 | valid acc 0.22\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   176200 | 176200 batches | lr 1e-06 | ms/batch 51.81640 | loss 1.44483 | ppl     4.241\n",
      "| epoch   1 step   176400 | 176400 batches | lr 1e-06 | ms/batch 44.68002 | loss 1.42281 | ppl     4.149\n",
      "| epoch   1 step   176600 | 176600 batches | lr 1e-06 | ms/batch 45.49035 | loss 1.42645 | ppl     4.164\n",
      "| epoch   1 step   176800 | 176800 batches | lr 1e-06 | ms/batch 44.45475 | loss 1.44565 | ppl     4.245\n",
      "| epoch   1 step   177000 | 177000 batches | lr 1e-06 | ms/batch 44.83841 | loss 1.44413 | ppl     4.238\n",
      "| epoch   1 step   177200 | 177200 batches | lr 1e-06 | ms/batch 46.22992 | loss 1.42785 | ppl     4.170\n",
      "| epoch   1 step   177400 | 177400 batches | lr 1e-06 | ms/batch 46.17055 | loss 1.41775 | ppl     4.128\n",
      "| epoch   1 step   177600 | 177600 batches | lr 1e-06 | ms/batch 46.28765 | loss 1.44703 | ppl     4.250\n",
      "| epoch   1 step   177800 | 177800 batches | lr 1e-06 | ms/batch 46.24694 | loss 1.47079 | ppl     4.353\n",
      "| epoch   1 step   178000 | 178000 batches | lr 1e-06 | ms/batch 47.47350 | loss 1.42754 | ppl     4.168\n",
      "| epoch   1 step   178200 | 178200 batches | lr 1e-06 | ms/batch 45.19918 | loss 1.42536 | ppl     4.159\n",
      "| epoch   1 step   178400 | 178400 batches | lr 1e-06 | ms/batch 44.85756 | loss 1.46083 | ppl     4.310\n",
      "| epoch   1 step   178600 | 178600 batches | lr 1e-06 | ms/batch 46.35663 | loss 1.45375 | ppl     4.279\n",
      "| epoch   1 step   178800 | 178800 batches | lr 1e-06 | ms/batch 46.56973 | loss 1.45706 | ppl     4.293\n",
      "| epoch   1 step   179000 | 179000 batches | lr 1e-06 | ms/batch 46.67142 | loss 1.43326 | ppl     4.192\n",
      "| epoch   1 step   179200 | 179200 batches | lr 1e-06 | ms/batch 45.90089 | loss 1.47829 | ppl     4.385\n",
      "| epoch   1 step   179400 | 179400 batches | lr 1e-06 | ms/batch 45.07984 | loss 1.47321 | ppl     4.363\n",
      "| epoch   1 step   179600 | 179600 batches | lr 1e-06 | ms/batch 44.93549 | loss 1.44922 | ppl     4.260\n",
      "| epoch   1 step   179800 | 179800 batches | lr 1e-06 | ms/batch 45.61994 | loss 1.44624 | ppl     4.247\n",
      "| epoch   1 step   180000 | 180000 batches | lr 1e-06 | ms/batch 44.09893 | loss 1.42952 | ppl     4.177\n",
      "|\n",
      "Source: [18  6 14  4 15  2 22  8 18 10]\n",
      "Target: [ 6 14  4 15  2 22  8 18 10  6]\n",
      "Teacher forcing: acc:0.28\n",
      "Preds:  [18  6 14  4 15  2 22  8 18  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  45 at step   180000 | time: 183.85s | valid loss 1.39325 | valid ppl    4.0279 | valid acc 0.28\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   180200 | 180200 batches | lr 1e-06 | ms/batch 51.65972 | loss 1.45079 | ppl     4.267\n",
      "| epoch   1 step   180400 | 180400 batches | lr 1e-06 | ms/batch 45.79248 | loss 1.45162 | ppl     4.270\n",
      "| epoch   1 step   180600 | 180600 batches | lr 1e-06 | ms/batch 46.51069 | loss 1.46372 | ppl     4.322\n",
      "| epoch   1 step   180800 | 180800 batches | lr 1e-06 | ms/batch 45.39609 | loss 1.43277 | ppl     4.190\n",
      "| epoch   1 step   181000 | 181000 batches | lr 1e-06 | ms/batch 45.96542 | loss 1.44540 | ppl     4.244\n",
      "| epoch   1 step   181200 | 181200 batches | lr 1e-06 | ms/batch 46.90845 | loss 1.44730 | ppl     4.252\n",
      "| epoch   1 step   181400 | 181400 batches | lr 1e-06 | ms/batch 47.45895 | loss 1.42292 | ppl     4.149\n",
      "| epoch   1 step   181600 | 181600 batches | lr 1e-06 | ms/batch 47.22441 | loss 1.45688 | ppl     4.293\n",
      "| epoch   1 step   181800 | 181800 batches | lr 1e-06 | ms/batch 46.67114 | loss 1.46246 | ppl     4.317\n",
      "| epoch   1 step   182000 | 182000 batches | lr 1e-06 | ms/batch 45.89399 | loss 1.46041 | ppl     4.308\n",
      "| epoch   1 step   182200 | 182200 batches | lr 1e-06 | ms/batch 46.57899 | loss 1.44012 | ppl     4.221\n",
      "| epoch   1 step   182400 | 182400 batches | lr 1e-06 | ms/batch 46.55364 | loss 1.45245 | ppl     4.274\n",
      "| epoch   1 step   182600 | 182600 batches | lr 1e-06 | ms/batch 46.48127 | loss 1.44254 | ppl     4.231\n",
      "| epoch   1 step   182800 | 182800 batches | lr 1e-06 | ms/batch 47.04666 | loss 1.46232 | ppl     4.316\n",
      "| epoch   1 step   183000 | 183000 batches | lr 1e-06 | ms/batch 46.17175 | loss 1.46607 | ppl     4.332\n",
      "| epoch   1 step   183200 | 183200 batches | lr 1e-06 | ms/batch 46.43116 | loss 1.45354 | ppl     4.278\n",
      "| epoch   1 step   183400 | 183400 batches | lr 1e-06 | ms/batch 46.96187 | loss 1.45502 | ppl     4.285\n",
      "| epoch   1 step   183600 | 183600 batches | lr 1e-06 | ms/batch 45.75810 | loss 1.45134 | ppl     4.269\n",
      "| epoch   1 step   183800 | 183800 batches | lr 1e-06 | ms/batch 45.66196 | loss 1.45547 | ppl     4.287\n",
      "| epoch   1 step   184000 | 184000 batches | lr 1e-06 | ms/batch 47.08516 | loss 1.45986 | ppl     4.305\n",
      "|\n",
      "Source: [33  2 17  5 29  1 26  9 29 10]\n",
      "Target: [ 2 17  5 29  1 26  9 29 10  1]\n",
      "Teacher forcing: acc:0.27\n",
      "Preds:  [35  2 25  5 29  1 25  9 29  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  46 at step   184000 | time: 186.98s | valid loss 1.39806 | valid ppl    4.0473 | valid acc 0.27\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   184200 | 184200 batches | lr 1e-06 | ms/batch 53.47286 | loss 1.43349 | ppl     4.193\n",
      "| epoch   1 step   184400 | 184400 batches | lr 1e-06 | ms/batch 47.31891 | loss 1.45692 | ppl     4.293\n",
      "| epoch   1 step   184600 | 184600 batches | lr 1e-06 | ms/batch 47.10257 | loss 1.46964 | ppl     4.348\n",
      "| epoch   1 step   184800 | 184800 batches | lr 1e-06 | ms/batch 46.58254 | loss 1.46258 | ppl     4.317\n",
      "| epoch   1 step   185000 | 185000 batches | lr 1e-06 | ms/batch 45.12034 | loss 1.42614 | ppl     4.163\n",
      "| epoch   1 step   185200 | 185200 batches | lr 1e-06 | ms/batch 45.68918 | loss 1.43349 | ppl     4.193\n",
      "| epoch   1 step   185400 | 185400 batches | lr 1e-06 | ms/batch 45.06193 | loss 1.43479 | ppl     4.199\n",
      "| epoch   1 step   185600 | 185600 batches | lr 1e-06 | ms/batch 46.38179 | loss 1.46450 | ppl     4.325\n",
      "| epoch   1 step   185800 | 185800 batches | lr 1e-06 | ms/batch 46.13543 | loss 1.43026 | ppl     4.180\n",
      "| epoch   1 step   186000 | 186000 batches | lr 1e-06 | ms/batch 46.14622 | loss 1.44764 | ppl     4.253\n",
      "| epoch   1 step   186200 | 186200 batches | lr 1e-06 | ms/batch 45.48301 | loss 1.45033 | ppl     4.265\n",
      "| epoch   1 step   186400 | 186400 batches | lr 1e-06 | ms/batch 45.83509 | loss 1.44028 | ppl     4.222\n",
      "| epoch   1 step   186600 | 186600 batches | lr 1e-06 | ms/batch 46.79392 | loss 1.45805 | ppl     4.298\n",
      "| epoch   1 step   186800 | 186800 batches | lr 1e-06 | ms/batch 45.66882 | loss 1.46351 | ppl     4.321\n",
      "| epoch   1 step   187000 | 187000 batches | lr 1e-06 | ms/batch 46.58521 | loss 1.42741 | ppl     4.168\n",
      "| epoch   1 step   187200 | 187200 batches | lr 1e-06 | ms/batch 45.07151 | loss 1.42199 | ppl     4.145\n",
      "| epoch   1 step   187400 | 187400 batches | lr 1e-06 | ms/batch 45.58195 | loss 1.44579 | ppl     4.245\n",
      "| epoch   1 step   187600 | 187600 batches | lr 1e-06 | ms/batch 45.96552 | loss 1.44114 | ppl     4.226\n",
      "| epoch   1 step   187800 | 187800 batches | lr 1e-06 | ms/batch 45.38524 | loss 1.43023 | ppl     4.180\n",
      "| epoch   1 step   188000 | 188000 batches | lr 1e-06 | ms/batch 45.66369 | loss 1.43263 | ppl     4.190\n",
      "|\n",
      "Source: [25  2 15  3 34  7 30  6 30 10]\n",
      "Target: [ 2 15  3 34  7 30  6 30 10  6]\n",
      "Teacher forcing: acc:0.27\n",
      "Preds:  [ 5  2  5  3 25  7 25  6 25  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  47 at step   188000 | time: 185.32s | valid loss 1.41744 | valid ppl    4.1265 | valid acc 0.27\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   188200 | 188200 batches | lr 1e-06 | ms/batch 51.53038 | loss 1.42550 | ppl     4.160\n",
      "| epoch   1 step   188400 | 188400 batches | lr 1e-06 | ms/batch 46.84041 | loss 1.45016 | ppl     4.264\n",
      "| epoch   1 step   188600 | 188600 batches | lr 1e-06 | ms/batch 46.66549 | loss 1.46508 | ppl     4.328\n",
      "| epoch   1 step   188800 | 188800 batches | lr 1e-06 | ms/batch 45.80567 | loss 1.46410 | ppl     4.324\n",
      "| epoch   1 step   189000 | 189000 batches | lr 1e-06 | ms/batch 46.52077 | loss 1.43953 | ppl     4.219\n",
      "| epoch   1 step   189200 | 189200 batches | lr 1e-06 | ms/batch 46.99725 | loss 1.42222 | ppl     4.146\n",
      "| epoch   1 step   189400 | 189400 batches | lr 1e-06 | ms/batch 45.84486 | loss 1.45302 | ppl     4.276\n",
      "| epoch   1 step   189600 | 189600 batches | lr 1e-06 | ms/batch 46.05444 | loss 1.44606 | ppl     4.246\n",
      "| epoch   1 step   189800 | 189800 batches | lr 1e-06 | ms/batch 45.40013 | loss 1.46163 | ppl     4.313\n",
      "| epoch   1 step   190000 | 190000 batches | lr 1e-06 | ms/batch 46.39616 | loss 1.43279 | ppl     4.190\n",
      "| epoch   1 step   190200 | 190200 batches | lr 1e-06 | ms/batch 47.30270 | loss 1.43444 | ppl     4.197\n",
      "| epoch   1 step   190400 | 190400 batches | lr 1e-06 | ms/batch 46.17644 | loss 1.41618 | ppl     4.121\n",
      "| epoch   1 step   190600 | 190600 batches | lr 1e-06 | ms/batch 45.86830 | loss 1.47361 | ppl     4.365\n",
      "| epoch   1 step   190800 | 190800 batches | lr 1e-06 | ms/batch 45.68640 | loss 1.45435 | ppl     4.282\n",
      "| epoch   1 step   191000 | 191000 batches | lr 1e-06 | ms/batch 45.70811 | loss 1.45647 | ppl     4.291\n",
      "| epoch   1 step   191200 | 191200 batches | lr 1e-06 | ms/batch 45.59511 | loss 1.43919 | ppl     4.217\n",
      "| epoch   1 step   191400 | 191400 batches | lr 1e-06 | ms/batch 44.95568 | loss 1.46607 | ppl     4.332\n",
      "| epoch   1 step   191600 | 191600 batches | lr 1e-06 | ms/batch 45.44377 | loss 1.44528 | ppl     4.243\n",
      "| epoch   1 step   191800 | 191800 batches | lr 1e-06 | ms/batch 45.74018 | loss 1.43051 | ppl     4.181\n",
      "| epoch   1 step   192000 | 192000 batches | lr 1e-06 | ms/batch 45.73954 | loss 1.45534 | ppl     4.286\n",
      "|\n",
      "Source: [18  5 11  3 34  2 33  9 11 10]\n",
      "Target: [ 5 11  3 34  2 33  9 11 10  3]\n",
      "Teacher forcing: acc:0.22\n",
      "Preds:  [18  5 25  3 25  2 33  9 25  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  48 at step   192000 | time: 185.28s | valid loss 1.41691 | valid ppl    4.1244 | valid acc 0.22\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   192200 | 192200 batches | lr 1e-06 | ms/batch 52.39585 | loss 1.44455 | ppl     4.240\n",
      "| epoch   1 step   192400 | 192400 batches | lr 1e-06 | ms/batch 44.61620 | loss 1.47555 | ppl     4.373\n",
      "| epoch   1 step   192600 | 192600 batches | lr 1e-06 | ms/batch 45.69049 | loss 1.46671 | ppl     4.335\n",
      "| epoch   1 step   192800 | 192800 batches | lr 1e-06 | ms/batch 44.80875 | loss 1.44675 | ppl     4.249\n",
      "| epoch   1 step   193000 | 193000 batches | lr 1e-06 | ms/batch 45.00640 | loss 1.45192 | ppl     4.271\n",
      "| epoch   1 step   193200 | 193200 batches | lr 1e-06 | ms/batch 44.92212 | loss 1.44251 | ppl     4.231\n",
      "| epoch   1 step   193400 | 193400 batches | lr 1e-06 | ms/batch 44.98224 | loss 1.46175 | ppl     4.314\n",
      "| epoch   1 step   193600 | 193600 batches | lr 1e-06 | ms/batch 45.97712 | loss 1.41191 | ppl     4.104\n",
      "| epoch   1 step   193800 | 193800 batches | lr 1e-06 | ms/batch 46.10394 | loss 1.45092 | ppl     4.267\n",
      "| epoch   1 step   194000 | 194000 batches | lr 1e-06 | ms/batch 45.88914 | loss 1.47248 | ppl     4.360\n",
      "| epoch   1 step   194200 | 194200 batches | lr 1e-06 | ms/batch 45.52279 | loss 1.43157 | ppl     4.185\n",
      "| epoch   1 step   194400 | 194400 batches | lr 1e-06 | ms/batch 45.55612 | loss 1.43312 | ppl     4.192\n",
      "| epoch   1 step   194600 | 194600 batches | lr 1e-06 | ms/batch 44.95673 | loss 1.44486 | ppl     4.241\n",
      "| epoch   1 step   194800 | 194800 batches | lr 1e-06 | ms/batch 45.06305 | loss 1.46917 | ppl     4.346\n",
      "| epoch   1 step   195000 | 195000 batches | lr 1e-06 | ms/batch 46.02057 | loss 1.42442 | ppl     4.155\n",
      "| epoch   1 step   195200 | 195200 batches | lr 1e-06 | ms/batch 44.98821 | loss 1.47724 | ppl     4.381\n",
      "| epoch   1 step   195400 | 195400 batches | lr 1e-06 | ms/batch 44.59770 | loss 1.48852 | ppl     4.431\n",
      "| epoch   1 step   195600 | 195600 batches | lr 1e-06 | ms/batch 44.82711 | loss 1.43974 | ppl     4.220\n",
      "| epoch   1 step   195800 | 195800 batches | lr 1e-06 | ms/batch 45.20835 | loss 1.45305 | ppl     4.276\n",
      "| epoch   1 step   196000 | 196000 batches | lr 1e-06 | ms/batch 46.09581 | loss 1.47777 | ppl     4.383\n",
      "|\n",
      "Source: [33  2 13  4 27  9 14  5 27 10]\n",
      "Target: [ 2 13  4 27  9 14  5 27 10  9]\n",
      "Teacher forcing: acc:0.16\n",
      "Preds:  [35  2 26  4 35  9 35  5 35  5]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  49 at step   196000 | time: 182.69s | valid loss 1.42478 | valid ppl    4.1569 | valid acc 0.16\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   196200 | 196200 batches | lr 1e-06 | ms/batch 51.90015 | loss 1.44916 | ppl     4.260\n",
      "| epoch   1 step   196400 | 196400 batches | lr 1e-06 | ms/batch 45.26849 | loss 1.45422 | ppl     4.281\n",
      "| epoch   1 step   196600 | 196600 batches | lr 1e-06 | ms/batch 45.43089 | loss 1.41395 | ppl     4.112\n",
      "| epoch   1 step   196800 | 196800 batches | lr 1e-06 | ms/batch 43.84591 | loss 1.45357 | ppl     4.278\n",
      "| epoch   1 step   197000 | 197000 batches | lr 1e-06 | ms/batch 44.51977 | loss 1.47252 | ppl     4.360\n",
      "| epoch   1 step   197200 | 197200 batches | lr 1e-06 | ms/batch 45.59159 | loss 1.46627 | ppl     4.333\n",
      "| epoch   1 step   197400 | 197400 batches | lr 1e-06 | ms/batch 44.76239 | loss 1.43486 | ppl     4.199\n",
      "| epoch   1 step   197600 | 197600 batches | lr 1e-06 | ms/batch 44.22069 | loss 1.43236 | ppl     4.189\n",
      "| epoch   1 step   197800 | 197800 batches | lr 1e-06 | ms/batch 45.64564 | loss 1.47536 | ppl     4.373\n",
      "| epoch   1 step   198000 | 198000 batches | lr 1e-06 | ms/batch 45.96620 | loss 1.43204 | ppl     4.187\n",
      "| epoch   1 step   198200 | 198200 batches | lr 1e-06 | ms/batch 45.47475 | loss 1.48204 | ppl     4.402\n",
      "| epoch   1 step   198400 | 198400 batches | lr 1e-06 | ms/batch 46.31919 | loss 1.44239 | ppl     4.231\n",
      "| epoch   1 step   198600 | 198600 batches | lr 1e-06 | ms/batch 45.55788 | loss 1.46241 | ppl     4.316\n",
      "| epoch   1 step   198800 | 198800 batches | lr 1e-06 | ms/batch 46.56184 | loss 1.44004 | ppl     4.221\n",
      "| epoch   1 step   199000 | 199000 batches | lr 1e-06 | ms/batch 46.48145 | loss 1.44719 | ppl     4.251\n",
      "| epoch   1 step   199200 | 199200 batches | lr 1e-06 | ms/batch 44.53370 | loss 1.42735 | ppl     4.168\n",
      "| epoch   1 step   199400 | 199400 batches | lr 1e-06 | ms/batch 45.43934 | loss 1.44867 | ppl     4.257\n",
      "| epoch   1 step   199600 | 199600 batches | lr 1e-06 | ms/batch 45.28693 | loss 1.44387 | ppl     4.237\n",
      "| epoch   1 step   199800 | 199800 batches | lr 1e-06 | ms/batch 44.55700 | loss 1.45778 | ppl     4.296\n",
      "| epoch   1 step   200000 | 200000 batches | lr 1e-06 | ms/batch 46.16087 | loss 1.43980 | ppl     4.220\n",
      "|\n",
      "Source: [34  4 16  7 27  5 18  6 18 10]\n",
      "Target: [ 4 16  7 27  5 18  6 18 10  6]\n",
      "Teacher forcing: acc:0.25\n",
      "Preds:  [25  4 25  7 25  5 25  6 25  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  50 at step   200000 | time: 182.68s | valid loss 1.41496 | valid ppl    4.1163 | valid acc 0.25\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   200200 | 200200 batches | lr 1e-06 | ms/batch 52.72070 | loss 1.45403 | ppl     4.280\n",
      "| epoch   1 step   200400 | 200400 batches | lr 1e-06 | ms/batch 45.72792 | loss 1.45370 | ppl     4.279\n",
      "| epoch   1 step   200600 | 200600 batches | lr 1e-06 | ms/batch 45.26270 | loss 1.45366 | ppl     4.279\n",
      "| epoch   1 step   200800 | 200800 batches | lr 1e-06 | ms/batch 45.29838 | loss 1.43528 | ppl     4.201\n",
      "| epoch   1 step   201000 | 201000 batches | lr 1e-06 | ms/batch 45.21504 | loss 1.45408 | ppl     4.281\n",
      "| epoch   1 step   201200 | 201200 batches | lr 1e-06 | ms/batch 44.32688 | loss 1.44289 | ppl     4.233\n",
      "| epoch   1 step   201400 | 201400 batches | lr 1e-06 | ms/batch 44.99528 | loss 1.43010 | ppl     4.179\n",
      "| epoch   1 step   201600 | 201600 batches | lr 1e-06 | ms/batch 45.73639 | loss 1.48346 | ppl     4.408\n",
      "| epoch   1 step   201800 | 201800 batches | lr 1e-06 | ms/batch 45.21118 | loss 1.44050 | ppl     4.223\n",
      "| epoch   1 step   202000 | 202000 batches | lr 1e-06 | ms/batch 45.11784 | loss 1.46184 | ppl     4.314\n",
      "| epoch   1 step   202200 | 202200 batches | lr 1e-06 | ms/batch 44.57582 | loss 1.41876 | ppl     4.132\n",
      "| epoch   1 step   202400 | 202400 batches | lr 1e-06 | ms/batch 44.54186 | loss 1.45240 | ppl     4.273\n",
      "| epoch   1 step   202600 | 202600 batches | lr 1e-06 | ms/batch 46.89863 | loss 1.45113 | ppl     4.268\n",
      "| epoch   1 step   202800 | 202800 batches | lr 1e-06 | ms/batch 45.49708 | loss 1.45902 | ppl     4.302\n",
      "| epoch   1 step   203000 | 203000 batches | lr 1e-06 | ms/batch 45.06736 | loss 1.43765 | ppl     4.211\n",
      "| epoch   1 step   203200 | 203200 batches | lr 1e-06 | ms/batch 46.61041 | loss 1.45113 | ppl     4.268\n",
      "| epoch   1 step   203400 | 203400 batches | lr 1e-06 | ms/batch 44.45327 | loss 1.44619 | ppl     4.247\n",
      "| epoch   1 step   203600 | 203600 batches | lr 1e-06 | ms/batch 44.70041 | loss 1.44946 | ppl     4.261\n",
      "| epoch   1 step   203800 | 203800 batches | lr 1e-06 | ms/batch 45.57707 | loss 1.42472 | ppl     4.157\n",
      "| epoch   1 step   204000 | 204000 batches | lr 1e-06 | ms/batch 47.06015 | loss 1.46225 | ppl     4.316\n",
      "|\n",
      "Source: [20  5 35  3 23  8 29  1 29 10]\n",
      "Target: [ 5 35  3 23  8 29  1 29 10  1]\n",
      "Teacher forcing: acc:0.27\n",
      "Preds:  [20  5 35  3 35  8  1  1  1  8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  51 at step   204000 | time: 183.02s | valid loss 1.39902 | valid ppl    4.0512 | valid acc 0.27\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   204200 | 204200 batches | lr 1e-06 | ms/batch 53.75650 | loss 1.44315 | ppl     4.234\n",
      "| epoch   1 step   204400 | 204400 batches | lr 1e-06 | ms/batch 47.17013 | loss 1.45018 | ppl     4.264\n",
      "| epoch   1 step   204600 | 204600 batches | lr 1e-06 | ms/batch 49.22364 | loss 1.44309 | ppl     4.234\n",
      "| epoch   1 step   204800 | 204800 batches | lr 1e-06 | ms/batch 45.32639 | loss 1.43614 | ppl     4.204\n",
      "| epoch   1 step   205000 | 205000 batches | lr 1e-06 | ms/batch 47.48147 | loss 1.45265 | ppl     4.274\n",
      "| epoch   1 step   205200 | 205200 batches | lr 1e-06 | ms/batch 45.69391 | loss 1.46307 | ppl     4.319\n",
      "| epoch   1 step   205400 | 205400 batches | lr 1e-06 | ms/batch 45.90690 | loss 1.45104 | ppl     4.268\n",
      "| epoch   1 step   205600 | 205600 batches | lr 1e-06 | ms/batch 45.15256 | loss 1.44406 | ppl     4.238\n",
      "| epoch   1 step   205800 | 205800 batches | lr 1e-06 | ms/batch 45.98474 | loss 1.48079 | ppl     4.396\n",
      "| epoch   1 step   206000 | 206000 batches | lr 1e-06 | ms/batch 45.30986 | loss 1.45606 | ppl     4.289\n",
      "| epoch   1 step   206200 | 206200 batches | lr 1e-06 | ms/batch 45.79375 | loss 1.43830 | ppl     4.214\n",
      "| epoch   1 step   206400 | 206400 batches | lr 1e-06 | ms/batch 45.21233 | loss 1.44936 | ppl     4.260\n",
      "| epoch   1 step   206600 | 206600 batches | lr 1e-06 | ms/batch 45.75899 | loss 1.44129 | ppl     4.226\n",
      "| epoch   1 step   206800 | 206800 batches | lr 1e-06 | ms/batch 45.11594 | loss 1.39567 | ppl     4.038\n",
      "| epoch   1 step   207000 | 207000 batches | lr 1e-06 | ms/batch 46.50420 | loss 1.45848 | ppl     4.299\n",
      "| epoch   1 step   207200 | 207200 batches | lr 1e-06 | ms/batch 45.57810 | loss 1.44423 | ppl     4.239\n",
      "| epoch   1 step   207400 | 207400 batches | lr 1e-06 | ms/batch 45.93331 | loss 1.43731 | ppl     4.209\n",
      "| epoch   1 step   207600 | 207600 batches | lr 1e-06 | ms/batch 46.83082 | loss 1.45279 | ppl     4.275\n",
      "| epoch   1 step   207800 | 207800 batches | lr 1e-06 | ms/batch 45.96827 | loss 1.43982 | ppl     4.220\n",
      "| epoch   1 step   208000 | 208000 batches | lr 1e-06 | ms/batch 46.10790 | loss 1.40754 | ppl     4.086\n",
      "|\n",
      "Source: [25  0 33  7 18  8 21  2 33 10]\n",
      "Target: [ 0 33  7 18  8 21  2 33 10  7]\n",
      "Teacher forcing: acc:0.29\n",
      "Preds:  [ 5  0 25  7 25  8 25  2 25  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  52 at step   208000 | time: 185.88s | valid loss 1.40664 | valid ppl    4.0822 | valid acc 0.29\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   208200 | 208200 batches | lr 1e-06 | ms/batch 51.47109 | loss 1.43874 | ppl     4.215\n",
      "| epoch   1 step   208400 | 208400 batches | lr 1e-06 | ms/batch 45.88766 | loss 1.43143 | ppl     4.185\n",
      "| epoch   1 step   208600 | 208600 batches | lr 1e-06 | ms/batch 46.02978 | loss 1.45223 | ppl     4.273\n",
      "| epoch   1 step   208800 | 208800 batches | lr 1e-06 | ms/batch 47.30567 | loss 1.46230 | ppl     4.316\n",
      "| epoch   1 step   209000 | 209000 batches | lr 1e-06 | ms/batch 45.99904 | loss 1.45950 | ppl     4.304\n",
      "| epoch   1 step   209200 | 209200 batches | lr 1e-06 | ms/batch 46.00975 | loss 1.44819 | ppl     4.255\n",
      "| epoch   1 step   209400 | 209400 batches | lr 1e-06 | ms/batch 47.22723 | loss 1.44133 | ppl     4.226\n",
      "| epoch   1 step   209600 | 209600 batches | lr 1e-06 | ms/batch 46.37822 | loss 1.42772 | ppl     4.169\n",
      "| epoch   1 step   209800 | 209800 batches | lr 1e-06 | ms/batch 47.45025 | loss 1.45522 | ppl     4.285\n",
      "| epoch   1 step   210000 | 210000 batches | lr 1e-06 | ms/batch 47.03420 | loss 1.47066 | ppl     4.352\n",
      "| epoch   1 step   210200 | 210200 batches | lr 1e-06 | ms/batch 46.16896 | loss 1.46309 | ppl     4.319\n",
      "| epoch   1 step   210400 | 210400 batches | lr 1e-06 | ms/batch 45.62127 | loss 1.46644 | ppl     4.334\n",
      "| epoch   1 step   210600 | 210600 batches | lr 1e-06 | ms/batch 45.27340 | loss 1.46074 | ppl     4.309\n",
      "| epoch   1 step   210800 | 210800 batches | lr 1e-06 | ms/batch 46.74030 | loss 1.44492 | ppl     4.241\n",
      "| epoch   1 step   211000 | 211000 batches | lr 1e-06 | ms/batch 44.98684 | loss 1.43168 | ppl     4.186\n",
      "| epoch   1 step   211200 | 211200 batches | lr 1e-06 | ms/batch 45.48688 | loss 1.45365 | ppl     4.279\n",
      "| epoch   1 step   211400 | 211400 batches | lr 1e-06 | ms/batch 44.99972 | loss 1.44102 | ppl     4.225\n",
      "| epoch   1 step   211600 | 211600 batches | lr 1e-06 | ms/batch 45.29162 | loss 1.45588 | ppl     4.288\n",
      "| epoch   1 step   211800 | 211800 batches | lr 1e-06 | ms/batch 45.26622 | loss 1.43476 | ppl     4.199\n",
      "| epoch   1 step   212000 | 212000 batches | lr 1e-06 | ms/batch 44.59184 | loss 1.45069 | ppl     4.266\n",
      "|\n",
      "Source: [34  6 29  1 31  2 13  0 13 10]\n",
      "Target: [ 6 29  1 31  2 13  0 13 10  0]\n",
      "Teacher forcing: acc:0.25\n",
      "Preds:  [25  6  1  1 25  2  1  0  1  1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  53 at step   212000 | time: 185.11s | valid loss 1.41505 | valid ppl    4.1167 | valid acc 0.25\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   212200 | 212200 batches | lr 1e-06 | ms/batch 52.34309 | loss 1.44471 | ppl     4.241\n",
      "| epoch   1 step   212400 | 212400 batches | lr 1e-06 | ms/batch 45.26393 | loss 1.46960 | ppl     4.348\n",
      "| epoch   1 step   212600 | 212600 batches | lr 1e-06 | ms/batch 45.95870 | loss 1.46088 | ppl     4.310\n",
      "| epoch   1 step   212800 | 212800 batches | lr 1e-06 | ms/batch 45.95008 | loss 1.46286 | ppl     4.318\n",
      "| epoch   1 step   213000 | 213000 batches | lr 1e-06 | ms/batch 45.00223 | loss 1.43594 | ppl     4.204\n",
      "| epoch   1 step   213200 | 213200 batches | lr 1e-06 | ms/batch 44.47186 | loss 1.43694 | ppl     4.208\n",
      "| epoch   1 step   213400 | 213400 batches | lr 1e-06 | ms/batch 45.42387 | loss 1.45414 | ppl     4.281\n",
      "| epoch   1 step   213600 | 213600 batches | lr 1e-06 | ms/batch 45.21525 | loss 1.44827 | ppl     4.256\n",
      "| epoch   1 step   213800 | 213800 batches | lr 1e-06 | ms/batch 45.76963 | loss 1.46130 | ppl     4.312\n",
      "| epoch   1 step   214000 | 214000 batches | lr 1e-06 | ms/batch 45.23185 | loss 1.46347 | ppl     4.321\n",
      "| epoch   1 step   214200 | 214200 batches | lr 1e-06 | ms/batch 45.89466 | loss 1.43958 | ppl     4.219\n",
      "| epoch   1 step   214400 | 214400 batches | lr 1e-06 | ms/batch 45.21702 | loss 1.44926 | ppl     4.260\n",
      "| epoch   1 step   214600 | 214600 batches | lr 1e-06 | ms/batch 45.08881 | loss 1.44464 | ppl     4.240\n",
      "| epoch   1 step   214800 | 214800 batches | lr 1e-06 | ms/batch 46.54289 | loss 1.42444 | ppl     4.156\n",
      "| epoch   1 step   215000 | 215000 batches | lr 1e-06 | ms/batch 45.22831 | loss 1.45696 | ppl     4.293\n",
      "| epoch   1 step   215200 | 215200 batches | lr 1e-06 | ms/batch 45.87780 | loss 1.47647 | ppl     4.377\n",
      "| epoch   1 step   215400 | 215400 batches | lr 1e-06 | ms/batch 45.12298 | loss 1.45929 | ppl     4.303\n",
      "| epoch   1 step   215600 | 215600 batches | lr 1e-06 | ms/batch 45.21046 | loss 1.46314 | ppl     4.320\n",
      "| epoch   1 step   215800 | 215800 batches | lr 1e-06 | ms/batch 46.03277 | loss 1.44432 | ppl     4.239\n",
      "| epoch   1 step   216000 | 216000 batches | lr 1e-06 | ms/batch 47.42610 | loss 1.45129 | ppl     4.269\n",
      "|\n",
      "Source: [32  6 26  1 21  8 18  2 26 10]\n",
      "Target: [ 6 26  1 21  8 18  2 26 10  1]\n",
      "Teacher forcing: acc:0.19\n",
      "Preds:  [ 1  6 35  1 35  8 25  2 26  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  54 at step   216000 | time: 183.56s | valid loss 1.43491 | valid ppl    4.1993 | valid acc 0.19\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   216200 | 216200 batches | lr 1e-06 | ms/batch 52.00602 | loss 1.43813 | ppl     4.213\n",
      "| epoch   1 step   216400 | 216400 batches | lr 1e-06 | ms/batch 45.91987 | loss 1.44395 | ppl     4.237\n",
      "| epoch   1 step   216600 | 216600 batches | lr 1e-06 | ms/batch 45.24647 | loss 1.44099 | ppl     4.225\n",
      "| epoch   1 step   216800 | 216800 batches | lr 1e-06 | ms/batch 45.39804 | loss 1.44386 | ppl     4.237\n",
      "| epoch   1 step   217000 | 217000 batches | lr 1e-06 | ms/batch 45.93508 | loss 1.45771 | ppl     4.296\n",
      "| epoch   1 step   217200 | 217200 batches | lr 1e-06 | ms/batch 47.59878 | loss 1.44689 | ppl     4.250\n",
      "| epoch   1 step   217400 | 217400 batches | lr 1e-06 | ms/batch 46.45646 | loss 1.46222 | ppl     4.316\n",
      "| epoch   1 step   217600 | 217600 batches | lr 1e-06 | ms/batch 46.43818 | loss 1.42560 | ppl     4.160\n",
      "| epoch   1 step   217800 | 217800 batches | lr 1e-06 | ms/batch 46.15775 | loss 1.45490 | ppl     4.284\n",
      "| epoch   1 step   218000 | 218000 batches | lr 1e-06 | ms/batch 45.83676 | loss 1.45955 | ppl     4.304\n",
      "| epoch   1 step   218200 | 218200 batches | lr 1e-06 | ms/batch 44.72058 | loss 1.46178 | ppl     4.314\n",
      "| epoch   1 step   218400 | 218400 batches | lr 1e-06 | ms/batch 45.50502 | loss 1.43463 | ppl     4.198\n",
      "| epoch   1 step   218600 | 218600 batches | lr 1e-06 | ms/batch 45.90685 | loss 1.43433 | ppl     4.197\n",
      "| epoch   1 step   218800 | 218800 batches | lr 1e-06 | ms/batch 46.32029 | loss 1.44830 | ppl     4.256\n",
      "| epoch   1 step   219000 | 219000 batches | lr 1e-06 | ms/batch 44.76740 | loss 1.45038 | ppl     4.265\n",
      "| epoch   1 step   219200 | 219200 batches | lr 1e-06 | ms/batch 46.68733 | loss 1.41843 | ppl     4.131\n",
      "| epoch   1 step   219400 | 219400 batches | lr 1e-06 | ms/batch 47.95195 | loss 1.43976 | ppl     4.220\n",
      "| epoch   1 step   219600 | 219600 batches | lr 1e-06 | ms/batch 49.21745 | loss 1.45108 | ppl     4.268\n",
      "| epoch   1 step   219800 | 219800 batches | lr 1e-06 | ms/batch 50.23388 | loss 1.44175 | ppl     4.228\n",
      "| epoch   1 step   220000 | 220000 batches | lr 1e-06 | ms/batch 48.53640 | loss 1.46925 | ppl     4.346\n",
      "|\n",
      "Source: [30  5 32  2 28  6 27  9 27 10]\n",
      "Target: [ 5 32  2 28  6 27  9 27 10  9]\n",
      "Teacher forcing: acc:0.28\n",
      "Preds:  [25  5 32  2 28  6 35  9 35  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  55 at step   220000 | time: 187.48s | valid loss 1.40875 | valid ppl    4.0909 | valid acc 0.28\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   220200 | 220200 batches | lr 1e-06 | ms/batch 54.15469 | loss 1.49443 | ppl     4.457\n",
      "| epoch   1 step   220400 | 220400 batches | lr 1e-06 | ms/batch 48.39889 | loss 1.44999 | ppl     4.263\n",
      "| epoch   1 step   220600 | 220600 batches | lr 1e-06 | ms/batch 46.23687 | loss 1.45919 | ppl     4.302\n",
      "| epoch   1 step   220800 | 220800 batches | lr 1e-06 | ms/batch 45.36414 | loss 1.46136 | ppl     4.312\n",
      "| epoch   1 step   221000 | 221000 batches | lr 1e-06 | ms/batch 46.31689 | loss 1.43510 | ppl     4.200\n",
      "| epoch   1 step   221200 | 221200 batches | lr 1e-06 | ms/batch 44.70289 | loss 1.46150 | ppl     4.312\n",
      "| epoch   1 step   221400 | 221400 batches | lr 1e-06 | ms/batch 45.64733 | loss 1.43601 | ppl     4.204\n",
      "| epoch   1 step   221600 | 221600 batches | lr 1e-06 | ms/batch 47.28600 | loss 1.38967 | ppl     4.014\n",
      "| epoch   1 step   221800 | 221800 batches | lr 1e-06 | ms/batch 46.14611 | loss 1.44811 | ppl     4.255\n",
      "| epoch   1 step   222000 | 222000 batches | lr 1e-06 | ms/batch 46.53500 | loss 1.47404 | ppl     4.367\n",
      "| epoch   1 step   222200 | 222200 batches | lr 1e-06 | ms/batch 46.78289 | loss 1.43803 | ppl     4.212\n",
      "| epoch   1 step   222400 | 222400 batches | lr 1e-06 | ms/batch 45.85345 | loss 1.44587 | ppl     4.246\n",
      "| epoch   1 step   222600 | 222600 batches | lr 1e-06 | ms/batch 45.76886 | loss 1.43757 | ppl     4.210\n",
      "| epoch   1 step   222800 | 222800 batches | lr 1e-06 | ms/batch 46.32669 | loss 1.46392 | ppl     4.323\n",
      "| epoch   1 step   223000 | 223000 batches | lr 1e-06 | ms/batch 45.79772 | loss 1.45335 | ppl     4.277\n",
      "| epoch   1 step   223200 | 223200 batches | lr 1e-06 | ms/batch 45.71050 | loss 1.45101 | ppl     4.267\n",
      "| epoch   1 step   223400 | 223400 batches | lr 1e-06 | ms/batch 46.35234 | loss 1.43556 | ppl     4.202\n",
      "| epoch   1 step   223600 | 223600 batches | lr 1e-06 | ms/batch 45.52823 | loss 1.42584 | ppl     4.161\n",
      "| epoch   1 step   223800 | 223800 batches | lr 1e-06 | ms/batch 46.08759 | loss 1.43354 | ppl     4.194\n",
      "| epoch   1 step   224000 | 224000 batches | lr 1e-06 | ms/batch 45.46131 | loss 1.42047 | ppl     4.139\n",
      "|\n",
      "Source: [11  0 34  2 35  9 12  8 34 10]\n",
      "Target: [ 0 34  2 35  9 12  8 34 10  2]\n",
      "Teacher forcing: acc:0.19\n",
      "Preds:  [35  0 25  2 35  9 35  8 25  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  56 at step   224000 | time: 186.03s | valid loss 1.42220 | valid ppl    4.1462 | valid acc 0.19\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   224200 | 224200 batches | lr 1e-06 | ms/batch 50.33210 | loss 1.43885 | ppl     4.216\n",
      "| epoch   1 step   224400 | 224400 batches | lr 1e-06 | ms/batch 45.36385 | loss 1.46017 | ppl     4.307\n",
      "| epoch   1 step   224600 | 224600 batches | lr 1e-06 | ms/batch 45.08975 | loss 1.43376 | ppl     4.194\n",
      "| epoch   1 step   224800 | 224800 batches | lr 1e-06 | ms/batch 46.62745 | loss 1.44209 | ppl     4.230\n",
      "| epoch   1 step   225000 | 225000 batches | lr 1e-06 | ms/batch 45.79332 | loss 1.43408 | ppl     4.196\n",
      "| epoch   1 step   225200 | 225200 batches | lr 1e-06 | ms/batch 45.83178 | loss 1.41990 | ppl     4.137\n",
      "| epoch   1 step   225400 | 225400 batches | lr 1e-06 | ms/batch 45.26551 | loss 1.42050 | ppl     4.139\n",
      "| epoch   1 step   225600 | 225600 batches | lr 1e-06 | ms/batch 45.75319 | loss 1.47703 | ppl     4.380\n",
      "| epoch   1 step   225800 | 225800 batches | lr 1e-06 | ms/batch 44.08529 | loss 1.45548 | ppl     4.287\n",
      "| epoch   1 step   226000 | 226000 batches | lr 1e-06 | ms/batch 45.35356 | loss 1.41341 | ppl     4.110\n",
      "| epoch   1 step   226200 | 226200 batches | lr 1e-06 | ms/batch 44.81376 | loss 1.42856 | ppl     4.173\n",
      "| epoch   1 step   226400 | 226400 batches | lr 1e-06 | ms/batch 46.62363 | loss 1.42163 | ppl     4.144\n",
      "| epoch   1 step   226600 | 226600 batches | lr 1e-06 | ms/batch 44.84730 | loss 1.44169 | ppl     4.228\n",
      "| epoch   1 step   226800 | 226800 batches | lr 1e-06 | ms/batch 44.49507 | loss 1.46731 | ppl     4.338\n",
      "| epoch   1 step   227000 | 227000 batches | lr 1e-06 | ms/batch 45.23214 | loss 1.45411 | ppl     4.281\n",
      "| epoch   1 step   227200 | 227200 batches | lr 1e-06 | ms/batch 45.33446 | loss 1.41368 | ppl     4.111\n",
      "| epoch   1 step   227400 | 227400 batches | lr 1e-06 | ms/batch 45.84574 | loss 1.43638 | ppl     4.205\n",
      "| epoch   1 step   227600 | 227600 batches | lr 1e-06 | ms/batch 45.35666 | loss 1.45105 | ppl     4.268\n",
      "| epoch   1 step   227800 | 227800 batches | lr 1e-06 | ms/batch 44.22501 | loss 1.44372 | ppl     4.236\n",
      "| epoch   1 step   228000 | 228000 batches | lr 1e-06 | ms/batch 44.86800 | loss 1.42048 | ppl     4.139\n",
      "|\n",
      "Source: [14  7 18  5 12  3 21  1 18 10]\n",
      "Target: [ 7 18  5 12  3 21  1 18 10  5]\n",
      "Teacher forcing: acc:0.33\n",
      "Preds:  [14  7 18  5 35  3 35  1 18  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  57 at step   228000 | time: 182.25s | valid loss 1.39311 | valid ppl    4.0273 | valid acc 0.33\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   228200 | 228200 batches | lr 1e-06 | ms/batch 51.54703 | loss 1.44461 | ppl     4.240\n",
      "| epoch   1 step   228400 | 228400 batches | lr 1e-06 | ms/batch 45.32716 | loss 1.44487 | ppl     4.241\n",
      "| epoch   1 step   228600 | 228600 batches | lr 1e-06 | ms/batch 45.65605 | loss 1.44901 | ppl     4.259\n",
      "| epoch   1 step   228800 | 228800 batches | lr 1e-06 | ms/batch 44.53893 | loss 1.45508 | ppl     4.285\n",
      "| epoch   1 step   229000 | 229000 batches | lr 1e-06 | ms/batch 46.44068 | loss 1.47613 | ppl     4.376\n",
      "| epoch   1 step   229200 | 229200 batches | lr 1e-06 | ms/batch 45.24259 | loss 1.42388 | ppl     4.153\n",
      "| epoch   1 step   229400 | 229400 batches | lr 1e-06 | ms/batch 44.96009 | loss 1.43797 | ppl     4.212\n",
      "| epoch   1 step   229600 | 229600 batches | lr 1e-06 | ms/batch 44.03226 | loss 1.45843 | ppl     4.299\n",
      "| epoch   1 step   229800 | 229800 batches | lr 1e-06 | ms/batch 45.46871 | loss 1.46092 | ppl     4.310\n",
      "| epoch   1 step   230000 | 230000 batches | lr 1e-06 | ms/batch 45.22465 | loss 1.45339 | ppl     4.278\n",
      "| epoch   1 step   230200 | 230200 batches | lr 1e-06 | ms/batch 47.03535 | loss 1.45235 | ppl     4.273\n",
      "| epoch   1 step   230400 | 230400 batches | lr 1e-06 | ms/batch 45.76431 | loss 1.44540 | ppl     4.244\n",
      "| epoch   1 step   230600 | 230600 batches | lr 1e-06 | ms/batch 46.16801 | loss 1.47415 | ppl     4.367\n",
      "| epoch   1 step   230800 | 230800 batches | lr 1e-06 | ms/batch 45.38337 | loss 1.43383 | ppl     4.195\n",
      "| epoch   1 step   231000 | 231000 batches | lr 1e-06 | ms/batch 45.28789 | loss 1.48664 | ppl     4.422\n",
      "| epoch   1 step   231200 | 231200 batches | lr 1e-06 | ms/batch 45.23322 | loss 1.45360 | ppl     4.278\n",
      "| epoch   1 step   231400 | 231400 batches | lr 1e-06 | ms/batch 45.76261 | loss 1.45856 | ppl     4.300\n",
      "| epoch   1 step   231600 | 231600 batches | lr 1e-06 | ms/batch 45.84182 | loss 1.46142 | ppl     4.312\n",
      "| epoch   1 step   231800 | 231800 batches | lr 1e-06 | ms/batch 45.71191 | loss 1.43954 | ppl     4.219\n",
      "| epoch   1 step   232000 | 232000 batches | lr 1e-06 | ms/batch 45.14928 | loss 1.43185 | ppl     4.186\n",
      "|\n",
      "Source: [32  9 18  6 26  1 20  4 32 10]\n",
      "Target: [ 9 18  6 26  1 20  4 32 10  9]\n",
      "Teacher forcing: acc:0.23\n",
      "Preds:  [ 1  9 18  6 25  1 32  4 32  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  58 at step   232000 | time: 183.15s | valid loss 1.40842 | valid ppl    4.0895 | valid acc 0.23\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   232200 | 232200 batches | lr 1e-06 | ms/batch 51.39029 | loss 1.44915 | ppl     4.259\n",
      "| epoch   1 step   232400 | 232400 batches | lr 1e-06 | ms/batch 45.17171 | loss 1.43177 | ppl     4.186\n",
      "| epoch   1 step   232600 | 232600 batches | lr 1e-06 | ms/batch 45.98552 | loss 1.43234 | ppl     4.188\n",
      "| epoch   1 step   232800 | 232800 batches | lr 1e-06 | ms/batch 45.11410 | loss 1.44459 | ppl     4.240\n",
      "| epoch   1 step   233000 | 233000 batches | lr 1e-06 | ms/batch 45.85137 | loss 1.45857 | ppl     4.300\n",
      "| epoch   1 step   233200 | 233200 batches | lr 1e-06 | ms/batch 45.00557 | loss 1.48429 | ppl     4.412\n",
      "| epoch   1 step   233400 | 233400 batches | lr 1e-06 | ms/batch 45.95768 | loss 1.44516 | ppl     4.243\n",
      "| epoch   1 step   233600 | 233600 batches | lr 1e-06 | ms/batch 46.33888 | loss 1.44706 | ppl     4.251\n",
      "| epoch   1 step   233800 | 233800 batches | lr 1e-06 | ms/batch 45.98841 | loss 1.43885 | ppl     4.216\n",
      "| epoch   1 step   234000 | 234000 batches | lr 1e-06 | ms/batch 44.45812 | loss 1.43445 | ppl     4.197\n",
      "| epoch   1 step   234200 | 234200 batches | lr 1e-06 | ms/batch 46.49786 | loss 1.44765 | ppl     4.253\n",
      "| epoch   1 step   234400 | 234400 batches | lr 1e-06 | ms/batch 45.64111 | loss 1.44391 | ppl     4.237\n",
      "| epoch   1 step   234600 | 234600 batches | lr 1e-06 | ms/batch 45.63951 | loss 1.46268 | ppl     4.317\n",
      "| epoch   1 step   234800 | 234800 batches | lr 1e-06 | ms/batch 45.16397 | loss 1.46221 | ppl     4.315\n",
      "| epoch   1 step   235000 | 235000 batches | lr 1e-06 | ms/batch 46.05781 | loss 1.44285 | ppl     4.233\n",
      "| epoch   1 step   235200 | 235200 batches | lr 1e-06 | ms/batch 45.57477 | loss 1.44908 | ppl     4.259\n",
      "| epoch   1 step   235400 | 235400 batches | lr 1e-06 | ms/batch 45.56893 | loss 1.45062 | ppl     4.266\n",
      "| epoch   1 step   235600 | 235600 batches | lr 1e-06 | ms/batch 45.55629 | loss 1.46101 | ppl     4.310\n",
      "| epoch   1 step   235800 | 235800 batches | lr 1e-06 | ms/batch 46.13057 | loss 1.45442 | ppl     4.282\n",
      "| epoch   1 step   236000 | 236000 batches | lr 1e-06 | ms/batch 45.59800 | loss 1.44078 | ppl     4.224\n",
      "|\n",
      "Source: [18  9 34  0 36  3 32  4 36 10]\n",
      "Target: [ 9 34  0 36  3 32  4 36 10  3]\n",
      "Teacher forcing: acc:0.32\n",
      "Preds:  [18  9 25  0 25  3 32  4 35  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  59 at step   236000 | time: 183.84s | valid loss 1.39452 | valid ppl    4.0330 | valid acc 0.32\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   236200 | 236200 batches | lr 1e-06 | ms/batch 53.14377 | loss 1.46184 | ppl     4.314\n",
      "| epoch   1 step   236400 | 236400 batches | lr 1e-06 | ms/batch 45.25701 | loss 1.41526 | ppl     4.118\n",
      "| epoch   1 step   236600 | 236600 batches | lr 1e-06 | ms/batch 46.34487 | loss 1.45110 | ppl     4.268\n",
      "| epoch   1 step   236800 | 236800 batches | lr 1e-06 | ms/batch 46.84209 | loss 1.46058 | ppl     4.308\n",
      "| epoch   1 step   237000 | 237000 batches | lr 1e-06 | ms/batch 46.59724 | loss 1.43711 | ppl     4.209\n",
      "| epoch   1 step   237200 | 237200 batches | lr 1e-06 | ms/batch 46.54426 | loss 1.43963 | ppl     4.219\n",
      "| epoch   1 step   237400 | 237400 batches | lr 1e-06 | ms/batch 47.19356 | loss 1.46132 | ppl     4.312\n",
      "| epoch   1 step   237600 | 237600 batches | lr 1e-06 | ms/batch 45.70714 | loss 1.43916 | ppl     4.217\n",
      "| epoch   1 step   237800 | 237800 batches | lr 1e-06 | ms/batch 45.02954 | loss 1.43420 | ppl     4.196\n",
      "| epoch   1 step   238000 | 238000 batches | lr 1e-06 | ms/batch 45.34325 | loss 1.44423 | ppl     4.239\n",
      "| epoch   1 step   238200 | 238200 batches | lr 1e-06 | ms/batch 44.97023 | loss 1.45554 | ppl     4.287\n",
      "| epoch   1 step   238400 | 238400 batches | lr 1e-06 | ms/batch 46.51548 | loss 1.43850 | ppl     4.214\n",
      "| epoch   1 step   238600 | 238600 batches | lr 1e-06 | ms/batch 45.86577 | loss 1.45310 | ppl     4.276\n",
      "| epoch   1 step   238800 | 238800 batches | lr 1e-06 | ms/batch 45.02802 | loss 1.43707 | ppl     4.208\n",
      "| epoch   1 step   239000 | 239000 batches | lr 1e-06 | ms/batch 45.45774 | loss 1.44673 | ppl     4.249\n",
      "| epoch   1 step   239200 | 239200 batches | lr 1e-06 | ms/batch 45.08513 | loss 1.43574 | ppl     4.203\n",
      "| epoch   1 step   239400 | 239400 batches | lr 1e-06 | ms/batch 45.33198 | loss 1.44571 | ppl     4.245\n",
      "| epoch   1 step   239600 | 239600 batches | lr 1e-06 | ms/batch 45.94459 | loss 1.42571 | ppl     4.161\n",
      "| epoch   1 step   239800 | 239800 batches | lr 1e-06 | ms/batch 44.91641 | loss 1.44311 | ppl     4.234\n",
      "| epoch   1 step   240000 | 240000 batches | lr 1e-06 | ms/batch 44.96477 | loss 1.45852 | ppl     4.300\n",
      "|\n",
      "Source: [25  9 19  5 30  2 11  7 30 10]\n",
      "Target: [ 9 19  5 30  2 11  7 30 10  2]\n",
      "Teacher forcing: acc:0.2\n",
      "Preds:  [ 5  9  5  5  5  2 25  7 25  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  60 at step   240000 | time: 184.42s | valid loss 1.40914 | valid ppl    4.0924 | valid acc 0.2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   240200 | 240200 batches | lr 1e-06 | ms/batch 52.09685 | loss 1.45583 | ppl     4.288\n",
      "| epoch   1 step   240400 | 240400 batches | lr 1e-06 | ms/batch 44.51310 | loss 1.45429 | ppl     4.281\n",
      "| epoch   1 step   240600 | 240600 batches | lr 1e-06 | ms/batch 45.92142 | loss 1.44790 | ppl     4.254\n",
      "| epoch   1 step   240800 | 240800 batches | lr 1e-06 | ms/batch 45.17564 | loss 1.44708 | ppl     4.251\n",
      "| epoch   1 step   241000 | 241000 batches | lr 1e-06 | ms/batch 45.13379 | loss 1.44601 | ppl     4.246\n",
      "| epoch   1 step   241200 | 241200 batches | lr 1e-06 | ms/batch 44.79225 | loss 1.46089 | ppl     4.310\n",
      "| epoch   1 step   241400 | 241400 batches | lr 1e-06 | ms/batch 46.55230 | loss 1.43113 | ppl     4.183\n",
      "| epoch   1 step   241600 | 241600 batches | lr 1e-06 | ms/batch 45.23187 | loss 1.44277 | ppl     4.232\n",
      "| epoch   1 step   241800 | 241800 batches | lr 1e-06 | ms/batch 44.39456 | loss 1.42595 | ppl     4.162\n",
      "| epoch   1 step   242000 | 242000 batches | lr 1e-06 | ms/batch 44.70695 | loss 1.44331 | ppl     4.235\n",
      "| epoch   1 step   242200 | 242200 batches | lr 1e-06 | ms/batch 45.23947 | loss 1.44924 | ppl     4.260\n",
      "| epoch   1 step   242400 | 242400 batches | lr 1e-06 | ms/batch 45.72889 | loss 1.47997 | ppl     4.393\n",
      "| epoch   1 step   242600 | 242600 batches | lr 1e-06 | ms/batch 45.69564 | loss 1.44117 | ppl     4.226\n",
      "| epoch   1 step   242800 | 242800 batches | lr 1e-06 | ms/batch 46.29049 | loss 1.45952 | ppl     4.304\n",
      "| epoch   1 step   243000 | 243000 batches | lr 1e-06 | ms/batch 46.14330 | loss 1.46612 | ppl     4.332\n",
      "| epoch   1 step   243200 | 243200 batches | lr 1e-06 | ms/batch 47.27742 | loss 1.45349 | ppl     4.278\n",
      "| epoch   1 step   243400 | 243400 batches | lr 1e-06 | ms/batch 46.31645 | loss 1.44426 | ppl     4.239\n",
      "| epoch   1 step   243600 | 243600 batches | lr 1e-06 | ms/batch 46.92579 | loss 1.44923 | ppl     4.260\n",
      "| epoch   1 step   243800 | 243800 batches | lr 1e-06 | ms/batch 45.35816 | loss 1.46320 | ppl     4.320\n",
      "| epoch   1 step   244000 | 244000 batches | lr 1e-06 | ms/batch 47.11410 | loss 1.45524 | ppl     4.286\n",
      "|\n",
      "Source: [24  1 22  7 33  2 17  4 17 10]\n",
      "Target: [ 1 22  7 33  2 17  4 17 10  4]\n",
      "Teacher forcing: acc:0.23\n",
      "Preds:  [35  1 22  7 35  2 25  4 25  1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  61 at step   244000 | time: 184.05s | valid loss 1.40730 | valid ppl    4.0849 | valid acc 0.23\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   244200 | 244200 batches | lr 1e-06 | ms/batch 51.44120 | loss 1.46066 | ppl     4.309\n",
      "| epoch   1 step   244400 | 244400 batches | lr 1e-06 | ms/batch 46.08252 | loss 1.44268 | ppl     4.232\n",
      "| epoch   1 step   244600 | 244600 batches | lr 1e-06 | ms/batch 46.77591 | loss 1.44439 | ppl     4.239\n",
      "| epoch   1 step   244800 | 244800 batches | lr 1e-06 | ms/batch 47.61139 | loss 1.41994 | ppl     4.137\n",
      "| epoch   1 step   245000 | 245000 batches | lr 1e-06 | ms/batch 45.50455 | loss 1.43159 | ppl     4.185\n",
      "| epoch   1 step   245200 | 245200 batches | lr 1e-06 | ms/batch 46.47380 | loss 1.47044 | ppl     4.351\n",
      "| epoch   1 step   245400 | 245400 batches | lr 1e-06 | ms/batch 45.50385 | loss 1.45547 | ppl     4.287\n",
      "| epoch   1 step   245600 | 245600 batches | lr 1e-06 | ms/batch 46.46915 | loss 1.43206 | ppl     4.187\n",
      "| epoch   1 step   245800 | 245800 batches | lr 1e-06 | ms/batch 47.61681 | loss 1.47271 | ppl     4.361\n",
      "| epoch   1 step   246000 | 246000 batches | lr 1e-06 | ms/batch 48.09788 | loss 1.43927 | ppl     4.218\n",
      "| epoch   1 step   246200 | 246200 batches | lr 1e-06 | ms/batch 46.29147 | loss 1.46018 | ppl     4.307\n",
      "| epoch   1 step   246400 | 246400 batches | lr 1e-06 | ms/batch 47.52618 | loss 1.42657 | ppl     4.164\n",
      "| epoch   1 step   246600 | 246600 batches | lr 1e-06 | ms/batch 46.84348 | loss 1.45155 | ppl     4.270\n",
      "| epoch   1 step   246800 | 246800 batches | lr 1e-06 | ms/batch 46.91062 | loss 1.42855 | ppl     4.173\n",
      "| epoch   1 step   247000 | 247000 batches | lr 1e-06 | ms/batch 47.23042 | loss 1.46460 | ppl     4.326\n",
      "| epoch   1 step   247200 | 247200 batches | lr 1e-06 | ms/batch 47.04149 | loss 1.44512 | ppl     4.242\n",
      "| epoch   1 step   247400 | 247400 batches | lr 1e-06 | ms/batch 46.14129 | loss 1.45237 | ppl     4.273\n",
      "| epoch   1 step   247600 | 247600 batches | lr 1e-06 | ms/batch 45.99219 | loss 1.45469 | ppl     4.283\n",
      "| epoch   1 step   247800 | 247800 batches | lr 1e-06 | ms/batch 46.95269 | loss 1.44741 | ppl     4.252\n",
      "| epoch   1 step   248000 | 248000 batches | lr 1e-06 | ms/batch 46.05658 | loss 1.41887 | ppl     4.132\n",
      "|\n",
      "Source: [15  3 35  6 29  2 19  1 29 10]\n",
      "Target: [ 3 35  6 29  2 19  1 29 10  2]\n",
      "Teacher forcing: acc:0.18\n",
      "Preds:  [1 3 1 6 1 2 1 1 1 1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  62 at step   248000 | time: 187.75s | valid loss 1.41877 | valid ppl    4.1320 | valid acc 0.18\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   248200 | 248200 batches | lr 1e-06 | ms/batch 53.65090 | loss 1.47178 | ppl     4.357\n",
      "| epoch   1 step   248400 | 248400 batches | lr 1e-06 | ms/batch 47.10239 | loss 1.41229 | ppl     4.105\n",
      "| epoch   1 step   248600 | 248600 batches | lr 1e-06 | ms/batch 46.06607 | loss 1.45091 | ppl     4.267\n",
      "| epoch   1 step   248800 | 248800 batches | lr 1e-06 | ms/batch 47.79810 | loss 1.45117 | ppl     4.268\n",
      "| epoch   1 step   249000 | 249000 batches | lr 1e-06 | ms/batch 47.58829 | loss 1.44178 | ppl     4.228\n",
      "| epoch   1 step   249200 | 249200 batches | lr 1e-06 | ms/batch 47.58025 | loss 1.45662 | ppl     4.291\n",
      "| epoch   1 step   249400 | 249400 batches | lr 1e-06 | ms/batch 46.32982 | loss 1.43231 | ppl     4.188\n",
      "| epoch   1 step   249600 | 249600 batches | lr 1e-06 | ms/batch 46.97209 | loss 1.46462 | ppl     4.326\n",
      "| epoch   1 step   249800 | 249800 batches | lr 1e-06 | ms/batch 45.66720 | loss 1.47475 | ppl     4.370\n",
      "| epoch   1 step   250000 | 250000 batches | lr 1e-06 | ms/batch 46.15401 | loss 1.44488 | ppl     4.241\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "|\n",
      "Source: [32  9 20  1 36  6 13  3 20 10]\n",
      "Target: [ 9 20  1 36  6 13  3 20 10  1]\n",
      "Teacher forcing: acc:0.22\n",
      "Preds:  [35  9 35  1 35  6 13  3 35  1]\n",
      "\n",
      "====================================================================================================\n",
      "| End of training | test loss 1.45485 | test ppl   4.28384\n",
      " | test acc 0.22\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "!bash run_retrieval.sh train --work_dir ../evaluation/test --answer_size 1 --batch_size 2 --lr 0.0001 --tgt_len 10 --eval_tgt_len 10 --mem_len 0 --num_mem_tokens 0 --device_ids 0 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f50c599a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[0]\n",
      "Experiment dir : ../evaluation/test-retrieval/20220113-163700\n",
      "====================================================================================================\n",
      "    - data : /home/admin/x-transformers/data\n",
      "    - dataset : retrieval\n",
      "    - n_layer : 4\n",
      "    - n_head : 2\n",
      "    - d_head : 32\n",
      "    - d_embed : 64\n",
      "    - d_model : 64\n",
      "    - d_inner : 128\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.0001\n",
      "    - mom : 0.0\n",
      "    - scheduler : dev_perf\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 1e-06\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 250000\n",
      "    - batch_size : 32\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 10\n",
      "    - eval_tgt_len : 10\n",
      "    - ext_len : 0\n",
      "    - mem_len : 0\n",
      "    - num_mem_tokens : 0\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : False\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : ../evaluation/test-retrieval/20220113-163700\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : 50\n",
      "    - sample_softmax : -1\n",
      "    - patience : 3\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [0]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : True\n",
      "    - read_mem_from_cache : True\n",
      "    - answer_size : 1\n",
      "    - tied : True\n",
      "    - ntokens : 37\n",
      "    - n_all_param : 151781\n",
      "    - n_nonemb_param : 149248\n",
      "====================================================================================================\n",
      "#params = 151781\n",
      "#non emb params = 149248\n",
      "Parameter containing:\n",
      "tensor([[ 0.0051, -0.0199, -0.0028,  ..., -0.0145, -0.0046, -0.0098],\n",
      "        [ 0.0441,  0.0065, -0.0031,  ...,  0.0073, -0.0056,  0.0065],\n",
      "        [-0.0034,  0.0034,  0.0186,  ..., -0.0244,  0.0066, -0.0084],\n",
      "        ...,\n",
      "        [-0.0007,  0.0431,  0.0100,  ...,  0.0295, -0.0307, -0.0120],\n",
      "        [ 0.0066, -0.0079,  0.0158,  ...,  0.0218, -0.0011,  0.0069],\n",
      "        [ 0.0229, -0.0019, -0.0566,  ..., -0.0439, -0.0239, -0.0159]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0052, -0.0200, -0.0028,  ..., -0.0146, -0.0046, -0.0097],\n",
      "        [ 0.0441,  0.0065, -0.0031,  ...,  0.0074, -0.0056,  0.0065],\n",
      "        [-0.0034,  0.0034,  0.0186,  ..., -0.0243,  0.0066, -0.0085],\n",
      "        ...,\n",
      "        [-0.0006,  0.0432,  0.0101,  ...,  0.0294, -0.0308, -0.0121],\n",
      "        [ 0.0067, -0.0080,  0.0159,  ...,  0.0217, -0.0012,  0.0068],\n",
      "        [ 0.0230, -0.0018, -0.0565,  ..., -0.0440, -0.0240, -0.0158]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0052, -0.0200, -0.0028,  ..., -0.0146, -0.0046, -0.0097],\n",
      "        [ 0.0441,  0.0065, -0.0031,  ...,  0.0074, -0.0056,  0.0065],\n",
      "        [-0.0034,  0.0034,  0.0186,  ..., -0.0243,  0.0066, -0.0085],\n",
      "        ...,\n",
      "        [-0.0006,  0.0433,  0.0102,  ...,  0.0295, -0.0308, -0.0122],\n",
      "        [ 0.0067, -0.0081,  0.0158,  ...,  0.0216, -0.0013,  0.0067],\n",
      "        [ 0.0231, -0.0018, -0.0565,  ..., -0.0440, -0.0241, -0.0157]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0052, -0.0200, -0.0028,  ..., -0.0146, -0.0046, -0.0098],\n",
      "        [ 0.0441,  0.0065, -0.0031,  ...,  0.0074, -0.0056,  0.0065],\n",
      "        [-0.0034,  0.0035,  0.0185,  ..., -0.0242,  0.0066, -0.0085],\n",
      "        ...,\n",
      "        [-0.0007,  0.0434,  0.0101,  ...,  0.0295, -0.0308, -0.0123],\n",
      "        [ 0.0066, -0.0082,  0.0158,  ...,  0.0216, -0.0014,  0.0068],\n",
      "        [ 0.0231, -0.0017, -0.0564,  ..., -0.0440, -0.0242, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0051, -0.0199, -0.0028,  ..., -0.0146, -0.0046, -0.0098],\n",
      "        [ 0.0441,  0.0065, -0.0031,  ...,  0.0073, -0.0056,  0.0065],\n",
      "        [-0.0034,  0.0035,  0.0185,  ..., -0.0242,  0.0066, -0.0086],\n",
      "        ...,\n",
      "        [-0.0008,  0.0435,  0.0101,  ...,  0.0296, -0.0307, -0.0124],\n",
      "        [ 0.0066, -0.0083,  0.0159,  ...,  0.0215, -0.0015,  0.0068],\n",
      "        [ 0.0231, -0.0017, -0.0563,  ..., -0.0441, -0.0243, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0051, -0.0199, -0.0028,  ..., -0.0145, -0.0046, -0.0098],\n",
      "        [ 0.0442,  0.0065, -0.0031,  ...,  0.0073, -0.0056,  0.0065],\n",
      "        [-0.0034,  0.0035,  0.0185,  ..., -0.0242,  0.0066, -0.0086],\n",
      "        ...,\n",
      "        [-0.0009,  0.0434,  0.0100,  ...,  0.0297, -0.0306, -0.0125],\n",
      "        [ 0.0066, -0.0083,  0.0159,  ...,  0.0214, -0.0016,  0.0069],\n",
      "        [ 0.0231, -0.0018, -0.0562,  ..., -0.0441, -0.0243, -0.0155]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0051, -0.0199, -0.0028,  ..., -0.0145, -0.0046, -0.0098],\n",
      "        [ 0.0442,  0.0065, -0.0031,  ...,  0.0073, -0.0056,  0.0065],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0241,  0.0066, -0.0086],\n",
      "        ...,\n",
      "        [-0.0009,  0.0434,  0.0100,  ...,  0.0298, -0.0306, -0.0126],\n",
      "        [ 0.0066, -0.0084,  0.0159,  ...,  0.0213, -0.0017,  0.0069],\n",
      "        [ 0.0231, -0.0018, -0.0562,  ..., -0.0441, -0.0244, -0.0155]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0051, -0.0199, -0.0029,  ..., -0.0145, -0.0046, -0.0099],\n",
      "        [ 0.0442,  0.0065, -0.0031,  ...,  0.0073, -0.0056,  0.0065],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0241,  0.0066, -0.0086],\n",
      "        ...,\n",
      "        [-0.0010,  0.0435,  0.0099,  ...,  0.0298, -0.0305, -0.0126],\n",
      "        [ 0.0066, -0.0085,  0.0159,  ...,  0.0212, -0.0018,  0.0070],\n",
      "        [ 0.0230, -0.0018, -0.0561,  ..., -0.0442, -0.0245, -0.0155]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0050, -0.0199, -0.0029,  ..., -0.0144, -0.0046, -0.0099],\n",
      "        [ 0.0442,  0.0065, -0.0031,  ...,  0.0073, -0.0056,  0.0065],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0066, -0.0086],\n",
      "        ...,\n",
      "        [-0.0011,  0.0435,  0.0099,  ...,  0.0299, -0.0304, -0.0126],\n",
      "        [ 0.0065, -0.0086,  0.0159,  ...,  0.0211, -0.0019,  0.0070],\n",
      "        [ 0.0230, -0.0019, -0.0560,  ..., -0.0442, -0.0246, -0.0155]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0050, -0.0199, -0.0029,  ..., -0.0144, -0.0046, -0.0099],\n",
      "        [ 0.0442,  0.0065, -0.0031,  ...,  0.0073, -0.0056,  0.0065],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0066, -0.0087],\n",
      "        ...,\n",
      "        [-0.0012,  0.0435,  0.0099,  ...,  0.0300, -0.0303, -0.0126],\n",
      "        [ 0.0065, -0.0087,  0.0159,  ...,  0.0211, -0.0019,  0.0071],\n",
      "        [ 0.0230, -0.0019, -0.0559,  ..., -0.0443, -0.0246, -0.0154]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0050, -0.0199, -0.0029,  ..., -0.0143, -0.0046, -0.0100],\n",
      "        [ 0.0442,  0.0065, -0.0031,  ...,  0.0072, -0.0056,  0.0066],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0066, -0.0087],\n",
      "        ...,\n",
      "        [-0.0013,  0.0435,  0.0099,  ...,  0.0301, -0.0303, -0.0127],\n",
      "        [ 0.0065, -0.0087,  0.0159,  ...,  0.0210, -0.0019,  0.0071],\n",
      "        [ 0.0230, -0.0020, -0.0558,  ..., -0.0443, -0.0247, -0.0154]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0049, -0.0199, -0.0029,  ..., -0.0143, -0.0046, -0.0100],\n",
      "        [ 0.0442,  0.0065, -0.0031,  ...,  0.0073, -0.0056,  0.0066],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0066, -0.0087],\n",
      "        ...,\n",
      "        [-0.0013,  0.0435,  0.0098,  ...,  0.0301, -0.0302, -0.0127],\n",
      "        [ 0.0064, -0.0088,  0.0158,  ...,  0.0210, -0.0019,  0.0072],\n",
      "        [ 0.0230, -0.0020, -0.0557,  ..., -0.0443, -0.0248, -0.0154]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0049, -0.0199, -0.0029,  ..., -0.0142, -0.0046, -0.0100],\n",
      "        [ 0.0442,  0.0065, -0.0031,  ...,  0.0073, -0.0056,  0.0066],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0241,  0.0066, -0.0087],\n",
      "        ...,\n",
      "        [-0.0014,  0.0436,  0.0098,  ...,  0.0302, -0.0301, -0.0127],\n",
      "        [ 0.0064, -0.0088,  0.0157,  ...,  0.0210, -0.0019,  0.0072],\n",
      "        [ 0.0230, -0.0020, -0.0556,  ..., -0.0444, -0.0249, -0.0153]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0048, -0.0199, -0.0029,  ..., -0.0142, -0.0046, -0.0101],\n",
      "        [ 0.0442,  0.0065, -0.0031,  ...,  0.0073, -0.0056,  0.0066],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0066, -0.0087],\n",
      "        ...,\n",
      "        [-0.0015,  0.0436,  0.0098,  ...,  0.0303, -0.0300, -0.0127],\n",
      "        [ 0.0063, -0.0089,  0.0157,  ...,  0.0210, -0.0019,  0.0073],\n",
      "        [ 0.0230, -0.0021, -0.0555,  ..., -0.0444, -0.0250, -0.0153]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0048, -0.0198, -0.0029,  ..., -0.0141, -0.0046, -0.0101],\n",
      "        [ 0.0443,  0.0065, -0.0031,  ...,  0.0072, -0.0056,  0.0066],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0066, -0.0087],\n",
      "        ...,\n",
      "        [-0.0016,  0.0436,  0.0098,  ...,  0.0303, -0.0300, -0.0127],\n",
      "        [ 0.0063, -0.0089,  0.0157,  ...,  0.0210, -0.0019,  0.0073],\n",
      "        [ 0.0229, -0.0021, -0.0554,  ..., -0.0445, -0.0251, -0.0153]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0047, -0.0198, -0.0029,  ..., -0.0141, -0.0046, -0.0101],\n",
      "        [ 0.0443,  0.0064, -0.0031,  ...,  0.0072, -0.0056,  0.0066],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0066, -0.0087],\n",
      "        ...,\n",
      "        [-0.0016,  0.0436,  0.0097,  ...,  0.0304, -0.0299, -0.0128],\n",
      "        [ 0.0062, -0.0090,  0.0157,  ...,  0.0210, -0.0019,  0.0074],\n",
      "        [ 0.0229, -0.0022, -0.0553,  ..., -0.0445, -0.0252, -0.0152]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0047, -0.0198, -0.0029,  ..., -0.0141, -0.0046, -0.0102],\n",
      "        [ 0.0443,  0.0064, -0.0031,  ...,  0.0072, -0.0056,  0.0066],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0066, -0.0087],\n",
      "        ...,\n",
      "        [-0.0017,  0.0436,  0.0097,  ...,  0.0304, -0.0299, -0.0128],\n",
      "        [ 0.0062, -0.0090,  0.0156,  ...,  0.0210, -0.0019,  0.0074],\n",
      "        [ 0.0229, -0.0022, -0.0553,  ..., -0.0446, -0.0253, -0.0152]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0047, -0.0198, -0.0029,  ..., -0.0140, -0.0046, -0.0102],\n",
      "        [ 0.0443,  0.0064, -0.0031,  ...,  0.0072, -0.0056,  0.0067],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0017,  0.0436,  0.0097,  ...,  0.0305, -0.0298, -0.0128],\n",
      "        [ 0.0062, -0.0090,  0.0156,  ...,  0.0210, -0.0019,  0.0075],\n",
      "        [ 0.0229, -0.0023, -0.0552,  ..., -0.0446, -0.0253, -0.0152]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0046, -0.0198, -0.0029,  ..., -0.0140, -0.0046, -0.0102],\n",
      "        [ 0.0444,  0.0064, -0.0031,  ...,  0.0071, -0.0056,  0.0067],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0018,  0.0436,  0.0097,  ...,  0.0306, -0.0297, -0.0128],\n",
      "        [ 0.0062, -0.0091,  0.0156,  ...,  0.0209, -0.0019,  0.0075],\n",
      "        [ 0.0229, -0.0023, -0.0551,  ..., -0.0447, -0.0254, -0.0151]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0046, -0.0198, -0.0029,  ..., -0.0139, -0.0046, -0.0103],\n",
      "        [ 0.0444,  0.0064, -0.0030,  ...,  0.0071, -0.0056,  0.0067],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0019,  0.0436,  0.0096,  ...,  0.0306, -0.0297, -0.0128],\n",
      "        [ 0.0062, -0.0091,  0.0155,  ...,  0.0209, -0.0019,  0.0076],\n",
      "        [ 0.0229, -0.0023, -0.0550,  ..., -0.0447, -0.0255, -0.0151]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0045, -0.0198, -0.0029,  ..., -0.0139, -0.0046, -0.0103],\n",
      "        [ 0.0445,  0.0064, -0.0030,  ...,  0.0070, -0.0056,  0.0068],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0020,  0.0436,  0.0096,  ...,  0.0307, -0.0296, -0.0128],\n",
      "        [ 0.0062, -0.0092,  0.0155,  ...,  0.0209, -0.0019,  0.0076],\n",
      "        [ 0.0229, -0.0023, -0.0549,  ..., -0.0448, -0.0256, -0.0150]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0045, -0.0198, -0.0029,  ..., -0.0138, -0.0046, -0.0104],\n",
      "        [ 0.0445,  0.0064, -0.0030,  ...,  0.0070, -0.0056,  0.0068],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0020,  0.0436,  0.0095,  ...,  0.0308, -0.0295, -0.0128],\n",
      "        [ 0.0062, -0.0092,  0.0155,  ...,  0.0209, -0.0019,  0.0077],\n",
      "        [ 0.0229, -0.0024, -0.0548,  ..., -0.0448, -0.0257, -0.0150]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0044, -0.0197, -0.0029,  ..., -0.0138, -0.0046, -0.0104],\n",
      "        [ 0.0446,  0.0064, -0.0030,  ...,  0.0070, -0.0057,  0.0068],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0021,  0.0436,  0.0095,  ...,  0.0309, -0.0294, -0.0128],\n",
      "        [ 0.0062, -0.0092,  0.0154,  ...,  0.0209, -0.0019,  0.0077],\n",
      "        [ 0.0229, -0.0024, -0.0547,  ..., -0.0449, -0.0258, -0.0150]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0044, -0.0197, -0.0029,  ..., -0.0137, -0.0046, -0.0104],\n",
      "        [ 0.0446,  0.0064, -0.0030,  ...,  0.0069, -0.0057,  0.0069],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0022,  0.0436,  0.0095,  ...,  0.0309, -0.0293, -0.0128],\n",
      "        [ 0.0062, -0.0093,  0.0154,  ...,  0.0208, -0.0019,  0.0078],\n",
      "        [ 0.0230, -0.0025, -0.0546,  ..., -0.0449, -0.0258, -0.0149]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0044, -0.0197, -0.0030,  ..., -0.0137, -0.0045, -0.0105],\n",
      "        [ 0.0447,  0.0063, -0.0030,  ...,  0.0068, -0.0057,  0.0069],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0023,  0.0436,  0.0095,  ...,  0.0310, -0.0292, -0.0129],\n",
      "        [ 0.0062, -0.0093,  0.0154,  ...,  0.0207, -0.0019,  0.0078],\n",
      "        [ 0.0230, -0.0025, -0.0545,  ..., -0.0450, -0.0259, -0.0149]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0043, -0.0197, -0.0030,  ..., -0.0136, -0.0045, -0.0105],\n",
      "        [ 0.0447,  0.0063, -0.0030,  ...,  0.0068, -0.0057,  0.0070],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0023,  0.0437,  0.0094,  ...,  0.0311, -0.0291, -0.0129],\n",
      "        [ 0.0062, -0.0093,  0.0153,  ...,  0.0207, -0.0019,  0.0079],\n",
      "        [ 0.0231, -0.0025, -0.0545,  ..., -0.0450, -0.0260, -0.0148]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0042, -0.0197, -0.0030,  ..., -0.0136, -0.0045, -0.0106],\n",
      "        [ 0.0448,  0.0063, -0.0030,  ...,  0.0067, -0.0057,  0.0070],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0024,  0.0437,  0.0094,  ...,  0.0312, -0.0291, -0.0129],\n",
      "        [ 0.0062, -0.0094,  0.0153,  ...,  0.0206, -0.0019,  0.0080],\n",
      "        [ 0.0231, -0.0025, -0.0544,  ..., -0.0451, -0.0261, -0.0148]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0042, -0.0197, -0.0030,  ..., -0.0135, -0.0045, -0.0106],\n",
      "        [ 0.0449,  0.0063, -0.0030,  ...,  0.0066, -0.0057,  0.0071],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0239,  0.0066, -0.0087],\n",
      "        ...,\n",
      "        [-0.0025,  0.0437,  0.0093,  ...,  0.0313, -0.0290, -0.0130],\n",
      "        [ 0.0062, -0.0094,  0.0153,  ...,  0.0205, -0.0019,  0.0080],\n",
      "        [ 0.0232, -0.0025, -0.0543,  ..., -0.0452, -0.0262, -0.0147]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0041, -0.0197, -0.0030,  ..., -0.0134, -0.0045, -0.0107],\n",
      "        [ 0.0449,  0.0063, -0.0030,  ...,  0.0066, -0.0057,  0.0071],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0239,  0.0066, -0.0087],\n",
      "        ...,\n",
      "        [-0.0026,  0.0438,  0.0093,  ...,  0.0314, -0.0289, -0.0130],\n",
      "        [ 0.0063, -0.0095,  0.0153,  ...,  0.0205, -0.0019,  0.0080],\n",
      "        [ 0.0232, -0.0025, -0.0542,  ..., -0.0453, -0.0263, -0.0147]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0041, -0.0196, -0.0030,  ..., -0.0134, -0.0045, -0.0107],\n",
      "        [ 0.0450,  0.0063, -0.0030,  ...,  0.0065, -0.0058,  0.0072],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0066, -0.0087],\n",
      "        ...,\n",
      "        [-0.0027,  0.0438,  0.0092,  ...,  0.0315, -0.0288, -0.0130],\n",
      "        [ 0.0063, -0.0095,  0.0152,  ...,  0.0204, -0.0019,  0.0080],\n",
      "        [ 0.0233, -0.0025, -0.0541,  ..., -0.0453, -0.0264, -0.0146]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0040, -0.0196, -0.0030,  ..., -0.0134, -0.0045, -0.0107],\n",
      "        [ 0.0451,  0.0062, -0.0030,  ...,  0.0064, -0.0058,  0.0072],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0065, -0.0087],\n",
      "        ...,\n",
      "        [-0.0027,  0.0438,  0.0092,  ...,  0.0316, -0.0287, -0.0130],\n",
      "        [ 0.0063, -0.0095,  0.0152,  ...,  0.0203, -0.0020,  0.0081],\n",
      "        [ 0.0234, -0.0026, -0.0540,  ..., -0.0454, -0.0264, -0.0146]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0040, -0.0196, -0.0030,  ..., -0.0133, -0.0045, -0.0108],\n",
      "        [ 0.0451,  0.0062, -0.0030,  ...,  0.0064, -0.0058,  0.0073],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0065, -0.0087],\n",
      "        ...,\n",
      "        [-0.0028,  0.0438,  0.0091,  ...,  0.0317, -0.0286, -0.0130],\n",
      "        [ 0.0063, -0.0096,  0.0151,  ...,  0.0202, -0.0020,  0.0081],\n",
      "        [ 0.0234, -0.0026, -0.0539,  ..., -0.0454, -0.0265, -0.0145]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0039, -0.0196, -0.0030,  ..., -0.0133, -0.0044, -0.0108],\n",
      "        [ 0.0452,  0.0062, -0.0030,  ...,  0.0063, -0.0058,  0.0073],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0065, -0.0087],\n",
      "        ...,\n",
      "        [-0.0029,  0.0438,  0.0091,  ...,  0.0318, -0.0285, -0.0130],\n",
      "        [ 0.0063, -0.0096,  0.0151,  ...,  0.0202, -0.0020,  0.0081],\n",
      "        [ 0.0235, -0.0026, -0.0538,  ..., -0.0455, -0.0266, -0.0144]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0039, -0.0196, -0.0030,  ..., -0.0132, -0.0044, -0.0108],\n",
      "        [ 0.0453,  0.0062, -0.0030,  ...,  0.0062, -0.0058,  0.0074],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0065, -0.0087],\n",
      "        ...,\n",
      "        [-0.0030,  0.0438,  0.0091,  ...,  0.0319, -0.0284, -0.0131],\n",
      "        [ 0.0063, -0.0096,  0.0151,  ...,  0.0201, -0.0020,  0.0081],\n",
      "        [ 0.0235, -0.0026, -0.0538,  ..., -0.0455, -0.0267, -0.0144]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0038, -0.0196, -0.0030,  ..., -0.0132, -0.0044, -0.0109],\n",
      "        [ 0.0453,  0.0062, -0.0030,  ...,  0.0062, -0.0059,  0.0075],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0240,  0.0065, -0.0087],\n",
      "        ...,\n",
      "        [-0.0031,  0.0439,  0.0090,  ...,  0.0319, -0.0284, -0.0131],\n",
      "        [ 0.0063, -0.0096,  0.0151,  ...,  0.0200, -0.0020,  0.0081],\n",
      "        [ 0.0236, -0.0026, -0.0537,  ..., -0.0456, -0.0268, -0.0143]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0038, -0.0196, -0.0030,  ..., -0.0132, -0.0044, -0.0109],\n",
      "        [ 0.0454,  0.0061, -0.0030,  ...,  0.0061, -0.0059,  0.0075],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0240,  0.0065, -0.0087],\n",
      "        ...,\n",
      "        [-0.0032,  0.0439,  0.0090,  ...,  0.0320, -0.0283, -0.0131],\n",
      "        [ 0.0063, -0.0097,  0.0151,  ...,  0.0199, -0.0020,  0.0081],\n",
      "        [ 0.0236, -0.0026, -0.0536,  ..., -0.0457, -0.0269, -0.0142]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0038, -0.0196, -0.0030,  ..., -0.0131, -0.0044, -0.0109],\n",
      "        [ 0.0455,  0.0061, -0.0030,  ...,  0.0060, -0.0059,  0.0076],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0240,  0.0065, -0.0087],\n",
      "        ...,\n",
      "        [-0.0032,  0.0439,  0.0090,  ...,  0.0321, -0.0282, -0.0131],\n",
      "        [ 0.0063, -0.0097,  0.0150,  ...,  0.0199, -0.0020,  0.0082],\n",
      "        [ 0.0237, -0.0026, -0.0535,  ..., -0.0457, -0.0270, -0.0142]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0037, -0.0196, -0.0030,  ..., -0.0131, -0.0044, -0.0109],\n",
      "        [ 0.0455,  0.0061, -0.0031,  ...,  0.0060, -0.0059,  0.0076],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0240,  0.0065, -0.0088],\n",
      "        ...,\n",
      "        [-0.0033,  0.0439,  0.0089,  ...,  0.0322, -0.0281, -0.0131],\n",
      "        [ 0.0063, -0.0097,  0.0150,  ...,  0.0199, -0.0020,  0.0082],\n",
      "        [ 0.0237, -0.0026, -0.0534,  ..., -0.0458, -0.0271, -0.0141]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0037, -0.0195, -0.0030,  ..., -0.0130, -0.0044, -0.0110],\n",
      "        [ 0.0456,  0.0061, -0.0031,  ...,  0.0059, -0.0060,  0.0077],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0240,  0.0065, -0.0088],\n",
      "        ...,\n",
      "        [-0.0034,  0.0439,  0.0089,  ...,  0.0323, -0.0280, -0.0131],\n",
      "        [ 0.0063, -0.0098,  0.0150,  ...,  0.0198, -0.0020,  0.0082],\n",
      "        [ 0.0238, -0.0027, -0.0533,  ..., -0.0459, -0.0272, -0.0141]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0036, -0.0195, -0.0030,  ..., -0.0130, -0.0044, -0.0110],\n",
      "        [ 0.0457,  0.0060, -0.0031,  ...,  0.0058, -0.0060,  0.0078],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0240,  0.0065, -0.0088],\n",
      "        ...,\n",
      "        [-0.0035,  0.0439,  0.0089,  ...,  0.0323, -0.0280, -0.0131],\n",
      "        [ 0.0062, -0.0098,  0.0149,  ...,  0.0198, -0.0020,  0.0083],\n",
      "        [ 0.0238, -0.0027, -0.0532,  ..., -0.0460, -0.0273, -0.0140]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0036, -0.0195, -0.0030,  ..., -0.0130, -0.0044, -0.0110],\n",
      "        [ 0.0458,  0.0060, -0.0031,  ...,  0.0057, -0.0060,  0.0079],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0240,  0.0065, -0.0088],\n",
      "        ...,\n",
      "        [-0.0035,  0.0439,  0.0089,  ...,  0.0324, -0.0279, -0.0131],\n",
      "        [ 0.0062, -0.0099,  0.0149,  ...,  0.0198, -0.0021,  0.0083],\n",
      "        [ 0.0239, -0.0027, -0.0531,  ..., -0.0460, -0.0274, -0.0140]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0036, -0.0195, -0.0030,  ..., -0.0130, -0.0043, -0.0111],\n",
      "        [ 0.0459,  0.0060, -0.0031,  ...,  0.0056, -0.0061,  0.0079],\n",
      "        [-0.0036,  0.0036,  0.0185,  ..., -0.0240,  0.0065, -0.0088],\n",
      "        ...,\n",
      "        [-0.0036,  0.0439,  0.0089,  ...,  0.0325, -0.0278, -0.0131],\n",
      "        [ 0.0061, -0.0099,  0.0148,  ...,  0.0197, -0.0021,  0.0083],\n",
      "        [ 0.0239, -0.0028, -0.0530,  ..., -0.0461, -0.0275, -0.0139]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0035, -0.0195, -0.0030,  ..., -0.0129, -0.0043, -0.0111],\n",
      "        [ 0.0460,  0.0059, -0.0031,  ...,  0.0055, -0.0061,  0.0080],\n",
      "        [-0.0036,  0.0036,  0.0185,  ..., -0.0240,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0037,  0.0439,  0.0089,  ...,  0.0325, -0.0278, -0.0131],\n",
      "        [ 0.0061, -0.0100,  0.0147,  ...,  0.0197, -0.0022,  0.0083],\n",
      "        [ 0.0240, -0.0028, -0.0529,  ..., -0.0462, -0.0276, -0.0139]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0035, -0.0195, -0.0030,  ..., -0.0129, -0.0043, -0.0111],\n",
      "        [ 0.0461,  0.0059, -0.0031,  ...,  0.0054, -0.0062,  0.0081],\n",
      "        [-0.0036,  0.0036,  0.0185,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0037,  0.0439,  0.0088,  ...,  0.0326, -0.0277, -0.0131],\n",
      "        [ 0.0061, -0.0100,  0.0147,  ...,  0.0197, -0.0022,  0.0083],\n",
      "        [ 0.0240, -0.0028, -0.0528,  ..., -0.0463, -0.0277, -0.0138]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0034, -0.0195, -0.0029,  ..., -0.0128, -0.0043, -0.0112],\n",
      "        [ 0.0462,  0.0059, -0.0032,  ...,  0.0053, -0.0062,  0.0082],\n",
      "        [-0.0037,  0.0036,  0.0185,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0038,  0.0439,  0.0088,  ...,  0.0327, -0.0276, -0.0131],\n",
      "        [ 0.0060, -0.0101,  0.0147,  ...,  0.0197, -0.0022,  0.0084],\n",
      "        [ 0.0241, -0.0028, -0.0527,  ..., -0.0464, -0.0278, -0.0137]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0034, -0.0195, -0.0029,  ..., -0.0128, -0.0043, -0.0112],\n",
      "        [ 0.0463,  0.0058, -0.0032,  ...,  0.0052, -0.0063,  0.0083],\n",
      "        [-0.0037,  0.0036,  0.0186,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0039,  0.0439,  0.0088,  ...,  0.0327, -0.0276, -0.0131],\n",
      "        [ 0.0060, -0.0101,  0.0147,  ...,  0.0196, -0.0023,  0.0084],\n",
      "        [ 0.0241, -0.0029, -0.0527,  ..., -0.0464, -0.0279, -0.0137]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0033, -0.0194, -0.0029,  ..., -0.0127, -0.0042, -0.0113],\n",
      "        [ 0.0464,  0.0058, -0.0032,  ...,  0.0051, -0.0063,  0.0083],\n",
      "        [-0.0037,  0.0036,  0.0186,  ..., -0.0239,  0.0066, -0.0088],\n",
      "        ...,\n",
      "        [-0.0039,  0.0439,  0.0087,  ...,  0.0328, -0.0275, -0.0131],\n",
      "        [ 0.0060, -0.0102,  0.0147,  ...,  0.0196, -0.0023,  0.0084],\n",
      "        [ 0.0242, -0.0029, -0.0526,  ..., -0.0465, -0.0280, -0.0136]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0032, -0.0194, -0.0029,  ..., -0.0126, -0.0042, -0.0113],\n",
      "        [ 0.0465,  0.0058, -0.0032,  ...,  0.0050, -0.0064,  0.0084],\n",
      "        [-0.0037,  0.0036,  0.0186,  ..., -0.0239,  0.0066, -0.0089],\n",
      "        ...,\n",
      "        [-0.0040,  0.0439,  0.0087,  ...,  0.0329, -0.0274, -0.0131],\n",
      "        [ 0.0060, -0.0103,  0.0147,  ...,  0.0196, -0.0024,  0.0084],\n",
      "        [ 0.0242, -0.0029, -0.0525,  ..., -0.0466, -0.0281, -0.0136]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0032, -0.0194, -0.0029,  ..., -0.0126, -0.0042, -0.0114],\n",
      "        [ 0.0466,  0.0057, -0.0032,  ...,  0.0049, -0.0064,  0.0085],\n",
      "        [-0.0037,  0.0036,  0.0186,  ..., -0.0239,  0.0066, -0.0089],\n",
      "        ...,\n",
      "        [-0.0041,  0.0439,  0.0086,  ...,  0.0329, -0.0273, -0.0132],\n",
      "        [ 0.0059, -0.0103,  0.0147,  ...,  0.0195, -0.0024,  0.0084],\n",
      "        [ 0.0243, -0.0030, -0.0524,  ..., -0.0467, -0.0282, -0.0135]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0031, -0.0194, -0.0029,  ..., -0.0125, -0.0041, -0.0115],\n",
      "        [ 0.0467,  0.0057, -0.0033,  ...,  0.0048, -0.0065,  0.0086],\n",
      "        [-0.0037,  0.0036,  0.0186,  ..., -0.0239,  0.0066, -0.0089],\n",
      "        ...,\n",
      "        [-0.0041,  0.0440,  0.0086,  ...,  0.0330, -0.0272, -0.0132],\n",
      "        [ 0.0059, -0.0104,  0.0147,  ...,  0.0195, -0.0025,  0.0084],\n",
      "        [ 0.0243, -0.0031, -0.0523,  ..., -0.0468, -0.0283, -0.0135]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0030, -0.0194, -0.0029,  ..., -0.0124, -0.0041, -0.0115],\n",
      "        [ 0.0468,  0.0056, -0.0033,  ...,  0.0047, -0.0065,  0.0087],\n",
      "        [-0.0037,  0.0036,  0.0186,  ..., -0.0239,  0.0065, -0.0089],\n",
      "        ...,\n",
      "        [-0.0042,  0.0440,  0.0085,  ...,  0.0330, -0.0272, -0.0132],\n",
      "        [ 0.0058, -0.0105,  0.0147,  ...,  0.0195, -0.0025,  0.0084],\n",
      "        [ 0.0243, -0.0031, -0.0522,  ..., -0.0468, -0.0284, -0.0135]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0030, -0.0193, -0.0029,  ..., -0.0124, -0.0041, -0.0116],\n",
      "        [ 0.0469,  0.0056, -0.0033,  ...,  0.0046, -0.0066,  0.0088],\n",
      "        [-0.0037,  0.0036,  0.0186,  ..., -0.0239,  0.0065, -0.0089],\n",
      "        ...,\n",
      "        [-0.0043,  0.0440,  0.0085,  ...,  0.0331, -0.0271, -0.0132],\n",
      "        [ 0.0058, -0.0105,  0.0147,  ...,  0.0195, -0.0026,  0.0084],\n",
      "        [ 0.0243, -0.0032, -0.0522,  ..., -0.0469, -0.0285, -0.0134]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0029, -0.0193, -0.0028,  ..., -0.0123, -0.0040, -0.0116],\n",
      "        [ 0.0470,  0.0055, -0.0033,  ...,  0.0045, -0.0067,  0.0089],\n",
      "        [-0.0037,  0.0036,  0.0186,  ..., -0.0239,  0.0065, -0.0089],\n",
      "        ...,\n",
      "        [-0.0044,  0.0440,  0.0084,  ...,  0.0331, -0.0270, -0.0132],\n",
      "        [ 0.0057, -0.0106,  0.0147,  ...,  0.0195, -0.0026,  0.0085],\n",
      "        [ 0.0243, -0.0032, -0.0521,  ..., -0.0470, -0.0286, -0.0134]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0029, -0.0193, -0.0028,  ..., -0.0123, -0.0040, -0.0117],\n",
      "        [ 0.0471,  0.0055, -0.0034,  ...,  0.0044, -0.0067,  0.0089],\n",
      "        [-0.0037,  0.0036,  0.0186,  ..., -0.0240,  0.0065, -0.0089],\n",
      "        ...,\n",
      "        [-0.0044,  0.0440,  0.0084,  ...,  0.0332, -0.0269, -0.0132],\n",
      "        [ 0.0057, -0.0107,  0.0147,  ...,  0.0194, -0.0027,  0.0084],\n",
      "        [ 0.0243, -0.0032, -0.0520,  ..., -0.0471, -0.0287, -0.0134]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0028, -0.0193, -0.0028,  ..., -0.0122, -0.0040, -0.0117],\n",
      "        [ 0.0472,  0.0055, -0.0034,  ...,  0.0043, -0.0068,  0.0090],\n",
      "        [-0.0037,  0.0036,  0.0186,  ..., -0.0240,  0.0065, -0.0089],\n",
      "        ...,\n",
      "        [-0.0045,  0.0440,  0.0083,  ...,  0.0332, -0.0268, -0.0131],\n",
      "        [ 0.0056, -0.0107,  0.0147,  ...,  0.0194, -0.0027,  0.0085],\n",
      "        [ 0.0243, -0.0032, -0.0520,  ..., -0.0471, -0.0287, -0.0133]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0028, -0.0193, -0.0028,  ..., -0.0122, -0.0040, -0.0117],\n",
      "        [ 0.0473,  0.0054, -0.0034,  ...,  0.0042, -0.0068,  0.0091],\n",
      "        [-0.0037,  0.0036,  0.0186,  ..., -0.0240,  0.0065, -0.0089],\n",
      "        ...,\n",
      "        [-0.0046,  0.0440,  0.0082,  ...,  0.0333, -0.0268, -0.0131],\n",
      "        [ 0.0055, -0.0108,  0.0147,  ...,  0.0194, -0.0028,  0.0084],\n",
      "        [ 0.0243, -0.0033, -0.0519,  ..., -0.0472, -0.0288, -0.0133]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0027, -0.0192, -0.0028,  ..., -0.0121, -0.0039, -0.0118],\n",
      "        [ 0.0474,  0.0054, -0.0035,  ...,  0.0041, -0.0069,  0.0092],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0241,  0.0064, -0.0088],\n",
      "        ...,\n",
      "        [-0.0047,  0.0441,  0.0082,  ...,  0.0333, -0.0267, -0.0132],\n",
      "        [ 0.0055, -0.0109,  0.0147,  ...,  0.0193, -0.0028,  0.0084],\n",
      "        [ 0.0243, -0.0033, -0.0518,  ..., -0.0472, -0.0289, -0.0133]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0027, -0.0192, -0.0028,  ..., -0.0120, -0.0039, -0.0118],\n",
      "        [ 0.0475,  0.0053, -0.0035,  ...,  0.0040, -0.0070,  0.0093],\n",
      "        [-0.0036,  0.0035,  0.0185,  ..., -0.0241,  0.0064, -0.0088],\n",
      "        ...,\n",
      "        [-0.0047,  0.0441,  0.0081,  ...,  0.0334, -0.0266, -0.0132],\n",
      "        [ 0.0054, -0.0110,  0.0147,  ...,  0.0192, -0.0029,  0.0084],\n",
      "        [ 0.0244, -0.0033, -0.0517,  ..., -0.0473, -0.0290, -0.0133]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0026, -0.0192, -0.0028,  ..., -0.0120, -0.0039, -0.0119],\n",
      "        [ 0.0476,  0.0053, -0.0035,  ...,  0.0040, -0.0070,  0.0094],\n",
      "        [-0.0035,  0.0035,  0.0185,  ..., -0.0242,  0.0063, -0.0087],\n",
      "        ...,\n",
      "        [-0.0048,  0.0441,  0.0080,  ...,  0.0335, -0.0265, -0.0132],\n",
      "        [ 0.0054, -0.0111,  0.0147,  ...,  0.0192, -0.0029,  0.0084],\n",
      "        [ 0.0244, -0.0033, -0.0517,  ..., -0.0474, -0.0291, -0.0132]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0026, -0.0192, -0.0028,  ..., -0.0120, -0.0038, -0.0119],\n",
      "        [ 0.0476,  0.0052, -0.0035,  ...,  0.0039, -0.0071,  0.0094],\n",
      "        [-0.0034,  0.0035,  0.0185,  ..., -0.0243,  0.0063, -0.0086],\n",
      "        ...,\n",
      "        [-0.0049,  0.0441,  0.0079,  ...,  0.0335, -0.0264, -0.0132],\n",
      "        [ 0.0053, -0.0111,  0.0147,  ...,  0.0191, -0.0029,  0.0085],\n",
      "        [ 0.0244, -0.0033, -0.0516,  ..., -0.0474, -0.0291, -0.0132]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0026, -0.0192, -0.0028,  ..., -0.0119, -0.0038, -0.0119],\n",
      "        [ 0.0477,  0.0052, -0.0036,  ...,  0.0038, -0.0071,  0.0095],\n",
      "        [-0.0033,  0.0034,  0.0185,  ..., -0.0244,  0.0062, -0.0086],\n",
      "        ...,\n",
      "        [-0.0049,  0.0442,  0.0079,  ...,  0.0336, -0.0263, -0.0133],\n",
      "        [ 0.0052, -0.0112,  0.0147,  ...,  0.0191, -0.0030,  0.0085],\n",
      "        [ 0.0244, -0.0033, -0.0515,  ..., -0.0475, -0.0292, -0.0131]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0026, -0.0192, -0.0028,  ..., -0.0119, -0.0038, -0.0119],\n",
      "        [ 0.0478,  0.0052, -0.0036,  ...,  0.0037, -0.0072,  0.0096],\n",
      "        [-0.0032,  0.0034,  0.0184,  ..., -0.0244,  0.0062, -0.0085],\n",
      "        ...,\n",
      "        [-0.0050,  0.0442,  0.0078,  ...,  0.0337, -0.0262, -0.0133],\n",
      "        [ 0.0052, -0.0112,  0.0147,  ...,  0.0190, -0.0030,  0.0086],\n",
      "        [ 0.0244, -0.0033, -0.0515,  ..., -0.0475, -0.0292, -0.0131]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0026, -0.0192, -0.0028,  ..., -0.0119, -0.0038, -0.0119],\n",
      "        [ 0.0479,  0.0051, -0.0037,  ...,  0.0036, -0.0073,  0.0097],\n",
      "        [-0.0032,  0.0034,  0.0184,  ..., -0.0245,  0.0061, -0.0084],\n",
      "        ...,\n",
      "        [-0.0050,  0.0442,  0.0078,  ...,  0.0337, -0.0262, -0.0133],\n",
      "        [ 0.0051, -0.0112,  0.0147,  ...,  0.0189, -0.0030,  0.0086],\n",
      "        [ 0.0244, -0.0033, -0.0515,  ..., -0.0475, -0.0293, -0.0130]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0026, -0.0192, -0.0028,  ..., -0.0119, -0.0039, -0.0119],\n",
      "        [ 0.0480,  0.0050, -0.0037,  ...,  0.0035, -0.0074,  0.0098],\n",
      "        [-0.0031,  0.0033,  0.0184,  ..., -0.0246,  0.0060, -0.0084],\n",
      "        ...,\n",
      "        [-0.0051,  0.0442,  0.0077,  ...,  0.0338, -0.0261, -0.0133],\n",
      "        [ 0.0051, -0.0113,  0.0147,  ...,  0.0189, -0.0030,  0.0086],\n",
      "        [ 0.0244, -0.0033, -0.0515,  ..., -0.0476, -0.0293, -0.0130]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0027, -0.0192, -0.0028,  ..., -0.0120, -0.0039, -0.0119],\n",
      "        [ 0.0481,  0.0050, -0.0037,  ...,  0.0034, -0.0074,  0.0099],\n",
      "        [-0.0030,  0.0033,  0.0183,  ..., -0.0247,  0.0060, -0.0083],\n",
      "        ...,\n",
      "        [-0.0052,  0.0442,  0.0077,  ...,  0.0338, -0.0260, -0.0133],\n",
      "        [ 0.0051, -0.0113,  0.0147,  ...,  0.0188, -0.0031,  0.0087],\n",
      "        [ 0.0244, -0.0033, -0.0514,  ..., -0.0476, -0.0294, -0.0129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0027, -0.0192, -0.0028,  ..., -0.0120, -0.0039, -0.0119],\n",
      "        [ 0.0482,  0.0049, -0.0038,  ...,  0.0033, -0.0075,  0.0100],\n",
      "        [-0.0029,  0.0032,  0.0183,  ..., -0.0248,  0.0059, -0.0082],\n",
      "        ...,\n",
      "        [-0.0052,  0.0442,  0.0077,  ...,  0.0339, -0.0259, -0.0133],\n",
      "        [ 0.0051, -0.0113,  0.0147,  ...,  0.0187, -0.0031,  0.0087],\n",
      "        [ 0.0245, -0.0033, -0.0514,  ..., -0.0476, -0.0295, -0.0129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0027, -0.0193, -0.0028,  ..., -0.0120, -0.0039, -0.0118],\n",
      "        [ 0.0484,  0.0049, -0.0039,  ...,  0.0032, -0.0076,  0.0101],\n",
      "        [-0.0028,  0.0032,  0.0183,  ..., -0.0249,  0.0058, -0.0081],\n",
      "        ...,\n",
      "        [-0.0053,  0.0442,  0.0076,  ...,  0.0339, -0.0259, -0.0133],\n",
      "        [ 0.0051, -0.0113,  0.0148,  ...,  0.0187, -0.0032,  0.0088],\n",
      "        [ 0.0245, -0.0033, -0.0514,  ..., -0.0476, -0.0295, -0.0129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0027, -0.0193, -0.0029,  ..., -0.0120, -0.0039, -0.0118],\n",
      "        [ 0.0485,  0.0048, -0.0039,  ...,  0.0031, -0.0077,  0.0102],\n",
      "        [-0.0027,  0.0031,  0.0182,  ..., -0.0250,  0.0058, -0.0080],\n",
      "        ...,\n",
      "        [-0.0054,  0.0442,  0.0075,  ...,  0.0340, -0.0258, -0.0134],\n",
      "        [ 0.0052, -0.0113,  0.0148,  ...,  0.0187, -0.0032,  0.0088],\n",
      "        [ 0.0245, -0.0033, -0.0514,  ..., -0.0477, -0.0296, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0028, -0.0193, -0.0029,  ..., -0.0120, -0.0039, -0.0118],\n",
      "        [ 0.0486,  0.0047, -0.0040,  ...,  0.0030, -0.0078,  0.0103],\n",
      "        [-0.0027,  0.0031,  0.0182,  ..., -0.0251,  0.0057, -0.0080],\n",
      "        ...,\n",
      "        [-0.0054,  0.0442,  0.0075,  ...,  0.0340, -0.0257, -0.0134],\n",
      "        [ 0.0052, -0.0113,  0.0148,  ...,  0.0187, -0.0033,  0.0089],\n",
      "        [ 0.0244, -0.0033, -0.0513,  ..., -0.0477, -0.0296, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0028, -0.0193, -0.0029,  ..., -0.0121, -0.0040, -0.0118],\n",
      "        [ 0.0487,  0.0047, -0.0040,  ...,  0.0029, -0.0079,  0.0104],\n",
      "        [-0.0026,  0.0030,  0.0182,  ..., -0.0252,  0.0056, -0.0079],\n",
      "        ...,\n",
      "        [-0.0055,  0.0442,  0.0074,  ...,  0.0341, -0.0257, -0.0134],\n",
      "        [ 0.0052, -0.0113,  0.0148,  ...,  0.0186, -0.0033,  0.0090],\n",
      "        [ 0.0244, -0.0033, -0.0513,  ..., -0.0477, -0.0296, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0028, -0.0193, -0.0029,  ..., -0.0121, -0.0040, -0.0118],\n",
      "        [ 0.0488,  0.0046, -0.0041,  ...,  0.0028, -0.0080,  0.0105],\n",
      "        [-0.0025,  0.0030,  0.0181,  ..., -0.0253,  0.0056, -0.0078],\n",
      "        ...,\n",
      "        [-0.0055,  0.0442,  0.0074,  ...,  0.0341, -0.0256, -0.0134],\n",
      "        [ 0.0052, -0.0114,  0.0148,  ...,  0.0186, -0.0034,  0.0090],\n",
      "        [ 0.0244, -0.0033, -0.0513,  ..., -0.0477, -0.0297, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0028, -0.0193, -0.0029,  ..., -0.0121, -0.0040, -0.0118],\n",
      "        [ 0.0489,  0.0045, -0.0042,  ...,  0.0026, -0.0081,  0.0106],\n",
      "        [-0.0024,  0.0029,  0.0181,  ..., -0.0254,  0.0055, -0.0077],\n",
      "        ...,\n",
      "        [-0.0056,  0.0443,  0.0073,  ...,  0.0342, -0.0255, -0.0134],\n",
      "        [ 0.0052, -0.0114,  0.0149,  ...,  0.0186, -0.0035,  0.0091],\n",
      "        [ 0.0244, -0.0033, -0.0513,  ..., -0.0477, -0.0297, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0029, -0.0193, -0.0029,  ..., -0.0121, -0.0040, -0.0118],\n",
      "        [ 0.0491,  0.0044, -0.0043,  ...,  0.0025, -0.0082,  0.0108],\n",
      "        [-0.0022,  0.0029,  0.0180,  ..., -0.0255,  0.0054, -0.0076],\n",
      "        ...,\n",
      "        [-0.0057,  0.0443,  0.0072,  ...,  0.0343, -0.0255, -0.0134],\n",
      "        [ 0.0052, -0.0114,  0.0149,  ...,  0.0186, -0.0035,  0.0092],\n",
      "        [ 0.0244, -0.0033, -0.0512,  ..., -0.0477, -0.0297, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0029, -0.0194, -0.0029,  ..., -0.0121, -0.0040, -0.0118],\n",
      "        [ 0.0492,  0.0044, -0.0043,  ...,  0.0024, -0.0083,  0.0109],\n",
      "        [-0.0021,  0.0028,  0.0180,  ..., -0.0256,  0.0053, -0.0075],\n",
      "        ...,\n",
      "        [-0.0057,  0.0443,  0.0072,  ...,  0.0343, -0.0254, -0.0134],\n",
      "        [ 0.0052, -0.0115,  0.0149,  ...,  0.0185, -0.0036,  0.0092],\n",
      "        [ 0.0244, -0.0033, -0.0512,  ..., -0.0477, -0.0298, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0029, -0.0194, -0.0030,  ..., -0.0121, -0.0040, -0.0117],\n",
      "        [ 0.0493,  0.0043, -0.0044,  ...,  0.0023, -0.0084,  0.0110],\n",
      "        [-0.0020,  0.0028,  0.0179,  ..., -0.0257,  0.0052, -0.0074],\n",
      "        ...,\n",
      "        [-0.0058,  0.0443,  0.0071,  ...,  0.0344, -0.0253, -0.0134],\n",
      "        [ 0.0052, -0.0115,  0.0150,  ...,  0.0185, -0.0036,  0.0093],\n",
      "        [ 0.0244, -0.0033, -0.0512,  ..., -0.0477, -0.0298, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0029, -0.0194, -0.0030,  ..., -0.0122, -0.0040, -0.0117],\n",
      "        [ 0.0494,  0.0042, -0.0045,  ...,  0.0022, -0.0085,  0.0111],\n",
      "        [-0.0019,  0.0027,  0.0179,  ..., -0.0258,  0.0052, -0.0073],\n",
      "        ...,\n",
      "        [-0.0058,  0.0443,  0.0071,  ...,  0.0344, -0.0253, -0.0133],\n",
      "        [ 0.0052, -0.0116,  0.0150,  ...,  0.0185, -0.0037,  0.0094],\n",
      "        [ 0.0244, -0.0033, -0.0511,  ..., -0.0477, -0.0298, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0030, -0.0194, -0.0030,  ..., -0.0122, -0.0041, -0.0117],\n",
      "        [ 0.0495,  0.0042, -0.0045,  ...,  0.0021, -0.0085,  0.0112],\n",
      "        [-0.0018,  0.0027,  0.0178,  ..., -0.0259,  0.0051, -0.0073],\n",
      "        ...,\n",
      "        [-0.0059,  0.0443,  0.0070,  ...,  0.0345, -0.0252, -0.0133],\n",
      "        [ 0.0052, -0.0116,  0.0151,  ...,  0.0184, -0.0038,  0.0094],\n",
      "        [ 0.0243, -0.0033, -0.0511,  ..., -0.0477, -0.0299, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0030, -0.0194, -0.0030,  ..., -0.0122, -0.0041, -0.0117],\n",
      "        [ 0.0495,  0.0041, -0.0046,  ...,  0.0020, -0.0086,  0.0112],\n",
      "        [-0.0017,  0.0026,  0.0177,  ..., -0.0260,  0.0050, -0.0072],\n",
      "        ...,\n",
      "        [-0.0059,  0.0444,  0.0069,  ...,  0.0346, -0.0252, -0.0133],\n",
      "        [ 0.0052, -0.0117,  0.0151,  ...,  0.0184, -0.0038,  0.0095],\n",
      "        [ 0.0243, -0.0033, -0.0511,  ..., -0.0477, -0.0299, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0030, -0.0194, -0.0030,  ..., -0.0122, -0.0041, -0.0117],\n",
      "        [ 0.0496,  0.0041, -0.0046,  ...,  0.0019, -0.0087,  0.0113],\n",
      "        [-0.0016,  0.0026,  0.0177,  ..., -0.0261,  0.0049, -0.0071],\n",
      "        ...,\n",
      "        [-0.0060,  0.0444,  0.0069,  ...,  0.0346, -0.0251, -0.0133],\n",
      "        [ 0.0052, -0.0117,  0.0151,  ...,  0.0184, -0.0039,  0.0096],\n",
      "        [ 0.0243, -0.0033, -0.0511,  ..., -0.0478, -0.0299, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0031, -0.0194, -0.0030,  ..., -0.0122, -0.0041, -0.0116],\n",
      "        [ 0.0497,  0.0040, -0.0047,  ...,  0.0019, -0.0087,  0.0114],\n",
      "        [-0.0015,  0.0025,  0.0176,  ..., -0.0262,  0.0048, -0.0070],\n",
      "        ...,\n",
      "        [-0.0061,  0.0444,  0.0068,  ...,  0.0347, -0.0250, -0.0133],\n",
      "        [ 0.0052, -0.0118,  0.0152,  ...,  0.0184, -0.0039,  0.0097],\n",
      "        [ 0.0243, -0.0034, -0.0510,  ..., -0.0478, -0.0299, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0031, -0.0195, -0.0030,  ..., -0.0123, -0.0041, -0.0116],\n",
      "        [ 0.0497,  0.0040, -0.0048,  ...,  0.0018, -0.0088,  0.0115],\n",
      "        [-0.0014,  0.0024,  0.0176,  ..., -0.0263,  0.0047, -0.0069],\n",
      "        ...,\n",
      "        [-0.0061,  0.0444,  0.0068,  ...,  0.0347, -0.0250, -0.0133],\n",
      "        [ 0.0052, -0.0118,  0.0152,  ...,  0.0184, -0.0039,  0.0097],\n",
      "        [ 0.0243, -0.0034, -0.0510,  ..., -0.0478, -0.0300, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0031, -0.0195, -0.0031,  ..., -0.0123, -0.0042, -0.0116],\n",
      "        [ 0.0498,  0.0039, -0.0048,  ...,  0.0017, -0.0089,  0.0115],\n",
      "        [-0.0013,  0.0024,  0.0175,  ..., -0.0264,  0.0046, -0.0068],\n",
      "        ...,\n",
      "        [-0.0062,  0.0444,  0.0067,  ...,  0.0348, -0.0250, -0.0133],\n",
      "        [ 0.0052, -0.0119,  0.0152,  ...,  0.0184, -0.0040,  0.0097],\n",
      "        [ 0.0243, -0.0034, -0.0510,  ..., -0.0479, -0.0300, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0031, -0.0195, -0.0031,  ..., -0.0123, -0.0042, -0.0115],\n",
      "        [ 0.0499,  0.0038, -0.0049,  ...,  0.0016, -0.0090,  0.0116],\n",
      "        [-0.0012,  0.0023,  0.0174,  ..., -0.0265,  0.0046, -0.0067],\n",
      "        ...,\n",
      "        [-0.0062,  0.0445,  0.0067,  ...,  0.0349, -0.0249, -0.0133],\n",
      "        [ 0.0052, -0.0119,  0.0153,  ...,  0.0184, -0.0040,  0.0098],\n",
      "        [ 0.0243, -0.0034, -0.0510,  ..., -0.0479, -0.0300, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0032, -0.0195, -0.0031,  ..., -0.0124, -0.0042, -0.0115],\n",
      "        [ 0.0500,  0.0038, -0.0049,  ...,  0.0015, -0.0090,  0.0117],\n",
      "        [-0.0011,  0.0022,  0.0174,  ..., -0.0266,  0.0045, -0.0066],\n",
      "        ...,\n",
      "        [-0.0062,  0.0445,  0.0066,  ...,  0.0349, -0.0249, -0.0133],\n",
      "        [ 0.0052, -0.0119,  0.0153,  ...,  0.0184, -0.0041,  0.0098],\n",
      "        [ 0.0243, -0.0034, -0.0509,  ..., -0.0479, -0.0300, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0032, -0.0196, -0.0031,  ..., -0.0124, -0.0042, -0.0114],\n",
      "        [ 0.0501,  0.0037, -0.0050,  ...,  0.0014, -0.0091,  0.0118],\n",
      "        [-0.0010,  0.0022,  0.0173,  ..., -0.0267,  0.0044, -0.0065],\n",
      "        ...,\n",
      "        [-0.0063,  0.0445,  0.0066,  ...,  0.0350, -0.0248, -0.0133],\n",
      "        [ 0.0052, -0.0120,  0.0153,  ...,  0.0184, -0.0041,  0.0099],\n",
      "        [ 0.0242, -0.0034, -0.0509,  ..., -0.0480, -0.0301, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0033, -0.0196, -0.0031,  ..., -0.0124, -0.0043, -0.0114],\n",
      "        [ 0.0501,  0.0037, -0.0051,  ...,  0.0014, -0.0091,  0.0119],\n",
      "        [-0.0009,  0.0021,  0.0173,  ..., -0.0268,  0.0043, -0.0064],\n",
      "        ...,\n",
      "        [-0.0063,  0.0445,  0.0065,  ...,  0.0350, -0.0248, -0.0133],\n",
      "        [ 0.0052, -0.0120,  0.0154,  ...,  0.0184, -0.0041,  0.0099],\n",
      "        [ 0.0242, -0.0034, -0.0509,  ..., -0.0480, -0.0301, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0033, -0.0196, -0.0032,  ..., -0.0125, -0.0043, -0.0114],\n",
      "        [ 0.0502,  0.0036, -0.0051,  ...,  0.0013, -0.0092,  0.0119],\n",
      "        [-0.0008,  0.0021,  0.0172,  ..., -0.0269,  0.0043, -0.0063],\n",
      "        ...,\n",
      "        [-0.0064,  0.0444,  0.0065,  ...,  0.0351, -0.0247, -0.0133],\n",
      "        [ 0.0052, -0.0120,  0.0153,  ...,  0.0184, -0.0042,  0.0100],\n",
      "        [ 0.0242, -0.0033, -0.0509,  ..., -0.0480, -0.0301, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0033, -0.0197, -0.0032,  ..., -0.0125, -0.0043, -0.0113],\n",
      "        [ 0.0503,  0.0035, -0.0052,  ...,  0.0012, -0.0093,  0.0120],\n",
      "        [-0.0008,  0.0020,  0.0171,  ..., -0.0270,  0.0042, -0.0062],\n",
      "        ...,\n",
      "        [-0.0064,  0.0444,  0.0065,  ...,  0.0351, -0.0247, -0.0133],\n",
      "        [ 0.0051, -0.0121,  0.0153,  ...,  0.0184, -0.0042,  0.0100],\n",
      "        [ 0.0241, -0.0033, -0.0509,  ..., -0.0480, -0.0301, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0034, -0.0197, -0.0032,  ..., -0.0125, -0.0043, -0.0113],\n",
      "        [ 0.0504,  0.0035, -0.0052,  ...,  0.0012, -0.0093,  0.0121],\n",
      "        [-0.0007,  0.0019,  0.0171,  ..., -0.0271,  0.0041, -0.0062],\n",
      "        ...,\n",
      "        [-0.0065,  0.0444,  0.0064,  ...,  0.0351, -0.0247, -0.0133],\n",
      "        [ 0.0051, -0.0121,  0.0153,  ...,  0.0184, -0.0042,  0.0100],\n",
      "        [ 0.0241, -0.0033, -0.0510,  ..., -0.0480, -0.0301, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0034, -0.0197, -0.0032,  ..., -0.0126, -0.0044, -0.0112],\n",
      "        [ 0.0504,  0.0034, -0.0053,  ...,  0.0011, -0.0094,  0.0122],\n",
      "        [-0.0006,  0.0019,  0.0170,  ..., -0.0272,  0.0040, -0.0061],\n",
      "        ...,\n",
      "        [-0.0065,  0.0444,  0.0064,  ...,  0.0352, -0.0247, -0.0132],\n",
      "        [ 0.0051, -0.0122,  0.0152,  ...,  0.0184, -0.0042,  0.0101],\n",
      "        [ 0.0240, -0.0033, -0.0510,  ..., -0.0480, -0.0301, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0035, -0.0197, -0.0033,  ..., -0.0126, -0.0044, -0.0112],\n",
      "        [ 0.0505,  0.0033, -0.0054,  ...,  0.0010, -0.0095,  0.0122],\n",
      "        [-0.0005,  0.0018,  0.0170,  ..., -0.0272,  0.0040, -0.0060],\n",
      "        ...,\n",
      "        [-0.0066,  0.0444,  0.0064,  ...,  0.0352, -0.0246, -0.0132],\n",
      "        [ 0.0050, -0.0122,  0.0152,  ...,  0.0184, -0.0043,  0.0101],\n",
      "        [ 0.0240, -0.0033, -0.0510,  ..., -0.0480, -0.0302, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0035, -0.0198, -0.0033,  ..., -0.0127, -0.0044, -0.0111],\n",
      "        [ 0.0507,  0.0033, -0.0055,  ...,  0.0009, -0.0096,  0.0123],\n",
      "        [-0.0004,  0.0017,  0.0169,  ..., -0.0273,  0.0039, -0.0059],\n",
      "        ...,\n",
      "        [-0.0066,  0.0443,  0.0064,  ...,  0.0352, -0.0246, -0.0132],\n",
      "        [ 0.0050, -0.0122,  0.0151,  ...,  0.0184, -0.0043,  0.0101],\n",
      "        [ 0.0239, -0.0033, -0.0511,  ..., -0.0481, -0.0302, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0036, -0.0198, -0.0033,  ..., -0.0127, -0.0045, -0.0111],\n",
      "        [ 0.0508,  0.0032, -0.0056,  ...,  0.0008, -0.0097,  0.0124],\n",
      "        [-0.0003,  0.0017,  0.0168,  ..., -0.0274,  0.0038, -0.0058],\n",
      "        ...,\n",
      "        [-0.0067,  0.0442,  0.0064,  ...,  0.0353, -0.0246, -0.0131],\n",
      "        [ 0.0050, -0.0123,  0.0151,  ...,  0.0183, -0.0043,  0.0101],\n",
      "        [ 0.0239, -0.0032, -0.0511,  ..., -0.0481, -0.0302, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0036, -0.0199, -0.0033,  ..., -0.0128, -0.0045, -0.0110],\n",
      "        [ 0.0509,  0.0031, -0.0056,  ...,  0.0007, -0.0098,  0.0126],\n",
      "        [-0.0001,  0.0016,  0.0167,  ..., -0.0275,  0.0037, -0.0057],\n",
      "        ...,\n",
      "        [-0.0067,  0.0442,  0.0064,  ...,  0.0353, -0.0246, -0.0131],\n",
      "        [ 0.0049, -0.0123,  0.0150,  ...,  0.0183, -0.0043,  0.0102],\n",
      "        [ 0.0239, -0.0032, -0.0511,  ..., -0.0481, -0.0303, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.6556e-03, -1.9916e-02, -3.3705e-03,  ..., -1.2860e-02,\n",
      "         -4.5765e-03, -1.0945e-02],\n",
      "        [ 5.0986e-02,  3.0109e-03, -5.7310e-03,  ...,  6.1335e-04,\n",
      "         -9.8474e-03,  1.2657e-02],\n",
      "        [-2.8770e-05,  1.5330e-03,  1.6673e-02,  ..., -2.7639e-02,\n",
      "          3.6317e-03, -5.5461e-03],\n",
      "        ...,\n",
      "        [-6.7250e-03,  4.4113e-02,  6.3978e-03,  ...,  3.5353e-02,\n",
      "         -2.4541e-02, -1.3060e-02],\n",
      "        [ 4.8912e-03, -1.2289e-02,  1.4954e-02,  ...,  1.8310e-02,\n",
      "         -4.3456e-03,  1.0248e-02],\n",
      "        [ 2.3840e-02, -3.1945e-03, -5.1117e-02,  ..., -4.8124e-02,\n",
      "         -3.0294e-02, -1.2825e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 3.6833e-03, -1.9946e-02, -3.3883e-03,  ..., -1.2901e-02,\n",
      "         -4.6022e-03, -1.0902e-02],\n",
      "        [ 5.1089e-02,  2.9287e-03, -5.8104e-03,  ...,  5.1478e-04,\n",
      "         -9.9408e-03,  1.2761e-02],\n",
      "        [ 7.3830e-05,  1.4623e-03,  1.6606e-02,  ..., -2.7740e-02,\n",
      "          3.5440e-03, -5.4419e-03],\n",
      "        ...,\n",
      "        [-6.7378e-03,  4.4062e-02,  6.3948e-03,  ...,  3.5377e-02,\n",
      "         -2.4539e-02, -1.3049e-02],\n",
      "        [ 4.8408e-03, -1.2305e-02,  1.4879e-02,  ...,  1.8298e-02,\n",
      "         -4.3425e-03,  1.0292e-02],\n",
      "        [ 2.3801e-02, -3.1620e-03, -5.1131e-02,  ..., -4.8140e-02,\n",
      "         -3.0321e-02, -1.2834e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0037, -0.0200, -0.0034,  ..., -0.0129, -0.0046, -0.0108],\n",
      "        [ 0.0512,  0.0028, -0.0059,  ...,  0.0004, -0.0100,  0.0129],\n",
      "        [ 0.0002,  0.0014,  0.0165,  ..., -0.0278,  0.0035, -0.0053],\n",
      "        ...,\n",
      "        [-0.0068,  0.0440,  0.0064,  ...,  0.0354, -0.0245, -0.0131],\n",
      "        [ 0.0048, -0.0123,  0.0148,  ...,  0.0183, -0.0043,  0.0104],\n",
      "        [ 0.0238, -0.0031, -0.0511,  ..., -0.0482, -0.0303, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0038, -0.0200, -0.0035,  ..., -0.0130, -0.0047, -0.0108],\n",
      "        [ 0.0513,  0.0028, -0.0060,  ...,  0.0003, -0.0101,  0.0130],\n",
      "        [ 0.0003,  0.0013,  0.0165,  ..., -0.0279,  0.0034, -0.0052],\n",
      "        ...,\n",
      "        [-0.0068,  0.0440,  0.0064,  ...,  0.0354, -0.0246, -0.0131],\n",
      "        [ 0.0047, -0.0123,  0.0147,  ...,  0.0183, -0.0043,  0.0104],\n",
      "        [ 0.0237, -0.0031, -0.0511,  ..., -0.0482, -0.0304, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0039, -0.0201, -0.0035,  ..., -0.0131, -0.0048, -0.0107],\n",
      "        [ 0.0514,  0.0027, -0.0061,  ...,  0.0002, -0.0102,  0.0131],\n",
      "        [ 0.0004,  0.0012,  0.0164,  ..., -0.0280,  0.0033, -0.0051],\n",
      "        ...,\n",
      "        [-0.0068,  0.0439,  0.0065,  ...,  0.0354, -0.0246, -0.0131],\n",
      "        [ 0.0047, -0.0123,  0.0146,  ...,  0.0183, -0.0043,  0.0104],\n",
      "        [ 0.0237, -0.0031, -0.0511,  ..., -0.0482, -0.0304, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0040, -0.0202, -0.0036,  ..., -0.0132, -0.0048, -0.0106],\n",
      "        [ 0.0515,  0.0025, -0.0062,  ...,  0.0001, -0.0104,  0.0132],\n",
      "        [ 0.0005,  0.0011,  0.0163,  ..., -0.0281,  0.0032, -0.0050],\n",
      "        ...,\n",
      "        [-0.0068,  0.0439,  0.0065,  ...,  0.0355, -0.0246, -0.0131],\n",
      "        [ 0.0047, -0.0123,  0.0145,  ...,  0.0182, -0.0043,  0.0105],\n",
      "        [ 0.0236, -0.0030, -0.0511,  ..., -0.0482, -0.0304, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 4.0372e-03, -2.0235e-02, -3.6121e-03,  ..., -1.3251e-02,\n",
      "         -4.8891e-03, -1.0529e-02],\n",
      "        [ 5.1663e-02,  2.4413e-03, -6.2836e-03,  ...,  1.2565e-06,\n",
      "         -1.0460e-02,  1.3347e-02],\n",
      "        [ 6.1154e-04,  1.0780e-03,  1.6235e-02,  ..., -2.8229e-02,\n",
      "          3.1026e-03, -4.9057e-03],\n",
      "        ...,\n",
      "        [-6.8207e-03,  4.3811e-02,  6.5390e-03,  ...,  3.5507e-02,\n",
      "         -2.4644e-02, -1.3156e-02],\n",
      "        [ 4.6514e-03, -1.2334e-02,  1.4479e-02,  ...,  1.8197e-02,\n",
      "         -4.3118e-03,  1.0575e-02],\n",
      "        [ 2.3570e-02, -2.9851e-03, -5.1114e-02,  ..., -4.8246e-02,\n",
      "         -3.0464e-02, -1.2798e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0041, -0.0203, -0.0037,  ..., -0.0133, -0.0050, -0.0104],\n",
      "        [ 0.0518,  0.0023, -0.0064,  ..., -0.0001, -0.0106,  0.0135],\n",
      "        [ 0.0007,  0.0010,  0.0162,  ..., -0.0283,  0.0030, -0.0048],\n",
      "        ...,\n",
      "        [-0.0068,  0.0438,  0.0066,  ...,  0.0355, -0.0247, -0.0132],\n",
      "        [ 0.0046, -0.0123,  0.0144,  ...,  0.0182, -0.0043,  0.0106],\n",
      "        [ 0.0235, -0.0030, -0.0511,  ..., -0.0483, -0.0305, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0042, -0.0204, -0.0037,  ..., -0.0134, -0.0050, -0.0104],\n",
      "        [ 0.0519,  0.0022, -0.0065,  ..., -0.0002, -0.0107,  0.0136],\n",
      "        [ 0.0008,  0.0009,  0.0161,  ..., -0.0284,  0.0029, -0.0047],\n",
      "        ...,\n",
      "        [-0.0069,  0.0437,  0.0066,  ...,  0.0355, -0.0247, -0.0132],\n",
      "        [ 0.0046, -0.0123,  0.0144,  ...,  0.0181, -0.0044,  0.0107],\n",
      "        [ 0.0235, -0.0029, -0.0511,  ..., -0.0483, -0.0305, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0043, -0.0204, -0.0038,  ..., -0.0135, -0.0051, -0.0103],\n",
      "        [ 0.0521,  0.0021, -0.0066,  ..., -0.0004, -0.0108,  0.0137],\n",
      "        [ 0.0010,  0.0008,  0.0160,  ..., -0.0285,  0.0028, -0.0046],\n",
      "        ...,\n",
      "        [-0.0069,  0.0437,  0.0067,  ...,  0.0356, -0.0247, -0.0132],\n",
      "        [ 0.0046, -0.0124,  0.0143,  ...,  0.0181, -0.0044,  0.0108],\n",
      "        [ 0.0234, -0.0029, -0.0511,  ..., -0.0483, -0.0306, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0044, -0.0205, -0.0038,  ..., -0.0136, -0.0052, -0.0102],\n",
      "        [ 0.0522,  0.0020, -0.0067,  ..., -0.0005, -0.0109,  0.0138],\n",
      "        [ 0.0011,  0.0007,  0.0159,  ..., -0.0287,  0.0028, -0.0045],\n",
      "        ...,\n",
      "        [-0.0069,  0.0437,  0.0067,  ...,  0.0356, -0.0248, -0.0132],\n",
      "        [ 0.0046, -0.0124,  0.0143,  ...,  0.0181, -0.0044,  0.0109],\n",
      "        [ 0.0234, -0.0028, -0.0510,  ..., -0.0483, -0.0306, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0045, -0.0206, -0.0039,  ..., -0.0137, -0.0052, -0.0101],\n",
      "        [ 0.0523,  0.0019, -0.0068,  ..., -0.0006, -0.0110,  0.0140],\n",
      "        [ 0.0012,  0.0007,  0.0158,  ..., -0.0288,  0.0027, -0.0044],\n",
      "        ...,\n",
      "        [-0.0069,  0.0436,  0.0068,  ...,  0.0356, -0.0248, -0.0132],\n",
      "        [ 0.0046, -0.0124,  0.0142,  ...,  0.0181, -0.0044,  0.0109],\n",
      "        [ 0.0233, -0.0028, -0.0510,  ..., -0.0484, -0.0306, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0046, -0.0206, -0.0040,  ..., -0.0138, -0.0053, -0.0101],\n",
      "        [ 0.0525,  0.0018, -0.0069,  ..., -0.0007, -0.0111,  0.0141],\n",
      "        [ 0.0013,  0.0006,  0.0157,  ..., -0.0289,  0.0026, -0.0043],\n",
      "        ...,\n",
      "        [-0.0070,  0.0436,  0.0069,  ...,  0.0356, -0.0248, -0.0132],\n",
      "        [ 0.0046, -0.0124,  0.0141,  ...,  0.0180, -0.0045,  0.0110],\n",
      "        [ 0.0233, -0.0028, -0.0510,  ..., -0.0484, -0.0306, -0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0047, -0.0207, -0.0040,  ..., -0.0138, -0.0054, -0.0100],\n",
      "        [ 0.0526,  0.0017, -0.0070,  ..., -0.0008, -0.0113,  0.0142],\n",
      "        [ 0.0015,  0.0005,  0.0156,  ..., -0.0290,  0.0025, -0.0042],\n",
      "        ...,\n",
      "        [-0.0070,  0.0435,  0.0069,  ...,  0.0356, -0.0248, -0.0132],\n",
      "        [ 0.0046, -0.0124,  0.0141,  ...,  0.0180, -0.0046,  0.0111],\n",
      "        [ 0.0232, -0.0027, -0.0510,  ..., -0.0484, -0.0306, -0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0048, -0.0208, -0.0041,  ..., -0.0139, -0.0054, -0.0099],\n",
      "        [ 0.0527,  0.0016, -0.0071,  ..., -0.0010, -0.0114,  0.0143],\n",
      "        [ 0.0016,  0.0004,  0.0155,  ..., -0.0291,  0.0024, -0.0041],\n",
      "        ...,\n",
      "        [-0.0070,  0.0435,  0.0070,  ...,  0.0356, -0.0248, -0.0132],\n",
      "        [ 0.0047, -0.0124,  0.0140,  ...,  0.0180, -0.0046,  0.0112],\n",
      "        [ 0.0231, -0.0027, -0.0509,  ..., -0.0485, -0.0307, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0048, -0.0208, -0.0041,  ..., -0.0140, -0.0055, -0.0098],\n",
      "        [ 0.0528,  0.0015, -0.0072,  ..., -0.0011, -0.0114,  0.0144],\n",
      "        [ 0.0017,  0.0003,  0.0154,  ..., -0.0293,  0.0023, -0.0040],\n",
      "        ...,\n",
      "        [-0.0070,  0.0435,  0.0071,  ...,  0.0356, -0.0248, -0.0133],\n",
      "        [ 0.0047, -0.0124,  0.0140,  ...,  0.0180, -0.0047,  0.0113],\n",
      "        [ 0.0231, -0.0027, -0.0509,  ..., -0.0485, -0.0307, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0049, -0.0209, -0.0042,  ..., -0.0141, -0.0055, -0.0098],\n",
      "        [ 0.0529,  0.0014, -0.0073,  ..., -0.0012, -0.0115,  0.0145],\n",
      "        [ 0.0018,  0.0002,  0.0153,  ..., -0.0294,  0.0022, -0.0039],\n",
      "        ...,\n",
      "        [-0.0070,  0.0435,  0.0071,  ...,  0.0356, -0.0248, -0.0133],\n",
      "        [ 0.0047, -0.0124,  0.0139,  ...,  0.0180, -0.0048,  0.0114],\n",
      "        [ 0.0230, -0.0026, -0.0509,  ..., -0.0485, -0.0307, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 5.0039e-03, -2.0963e-02, -4.2658e-03,  ..., -1.4154e-02,\n",
      "         -5.6121e-03, -9.6825e-03],\n",
      "        [ 5.3067e-02,  1.2791e-03, -7.4244e-03,  ..., -1.3214e-03,\n",
      "         -1.1647e-02,  1.4611e-02],\n",
      "        [ 1.9546e-03,  9.3169e-05,  1.5234e-02,  ..., -2.9513e-02,\n",
      "          2.0902e-03, -3.8348e-03],\n",
      "        ...,\n",
      "        [-7.0516e-03,  4.3494e-02,  7.1865e-03,  ...,  3.5554e-02,\n",
      "         -2.4788e-02, -1.3322e-02],\n",
      "        [ 4.7486e-03, -1.2414e-02,  1.3906e-02,  ...,  1.7952e-02,\n",
      "         -4.8580e-03,  1.1464e-02],\n",
      "        [ 2.2973e-02, -2.5810e-03, -5.0917e-02,  ..., -4.8534e-02,\n",
      "         -3.0716e-02, -1.2498e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 5.0897e-03, -2.1035e-02, -4.3304e-03,  ..., -1.4240e-02,\n",
      "         -5.6784e-03, -9.6003e-03],\n",
      "        [ 5.3175e-02,  1.1754e-03, -7.5208e-03,  ..., -1.4379e-03,\n",
      "         -1.1737e-02,  1.4708e-02],\n",
      "        [ 2.0658e-03,  4.5346e-07,  1.5145e-02,  ..., -2.9624e-02,\n",
      "          2.0104e-03, -3.7489e-03],\n",
      "        ...,\n",
      "        [-7.0586e-03,  4.3499e-02,  7.2579e-03,  ...,  3.5557e-02,\n",
      "         -2.4777e-02, -1.3319e-02],\n",
      "        [ 4.7500e-03, -1.2404e-02,  1.3893e-02,  ...,  1.7939e-02,\n",
      "         -4.9316e-03,  1.1555e-02],\n",
      "        [ 2.2913e-02, -2.5302e-03, -5.0904e-02,  ..., -4.8564e-02,\n",
      "         -3.0737e-02, -1.2472e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0052, -0.0211, -0.0044,  ..., -0.0143, -0.0057, -0.0095],\n",
      "        [ 0.0533,  0.0011, -0.0076,  ..., -0.0015, -0.0118,  0.0148],\n",
      "        [ 0.0022, -0.0001,  0.0151,  ..., -0.0297,  0.0019, -0.0037],\n",
      "        ...,\n",
      "        [-0.0071,  0.0435,  0.0073,  ...,  0.0356, -0.0248, -0.0133],\n",
      "        [ 0.0048, -0.0124,  0.0139,  ...,  0.0179, -0.0050,  0.0116],\n",
      "        [ 0.0229, -0.0025, -0.0509,  ..., -0.0486, -0.0308, -0.0124]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0053, -0.0212, -0.0045,  ..., -0.0144, -0.0058, -0.0094],\n",
      "        [ 0.0534,  0.0010, -0.0077,  ..., -0.0017, -0.0119,  0.0149],\n",
      "        [ 0.0023, -0.0002,  0.0150,  ..., -0.0299,  0.0018, -0.0036],\n",
      "        ...,\n",
      "        [-0.0071,  0.0435,  0.0074,  ...,  0.0356, -0.0248, -0.0133],\n",
      "        [ 0.0048, -0.0124,  0.0138,  ...,  0.0179, -0.0050,  0.0117],\n",
      "        [ 0.0228, -0.0025, -0.0508,  ..., -0.0486, -0.0308, -0.0124]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0054, -0.0213, -0.0045,  ..., -0.0145, -0.0059, -0.0093],\n",
      "        [ 0.0535,  0.0009, -0.0078,  ..., -0.0018, -0.0120,  0.0150],\n",
      "        [ 0.0025, -0.0003,  0.0149,  ..., -0.0300,  0.0017, -0.0035],\n",
      "        ...,\n",
      "        [-0.0071,  0.0435,  0.0075,  ...,  0.0355, -0.0248, -0.0133],\n",
      "        [ 0.0048, -0.0124,  0.0138,  ...,  0.0178, -0.0051,  0.0118],\n",
      "        [ 0.0227, -0.0025, -0.0508,  ..., -0.0487, -0.0308, -0.0124]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0055, -0.0214, -0.0046,  ..., -0.0146, -0.0060, -0.0092],\n",
      "        [ 0.0536,  0.0008, -0.0079,  ..., -0.0019, -0.0121,  0.0151],\n",
      "        [ 0.0026, -0.0004,  0.0148,  ..., -0.0301,  0.0016, -0.0033],\n",
      "        ...,\n",
      "        [-0.0071,  0.0435,  0.0076,  ...,  0.0355, -0.0248, -0.0132],\n",
      "        [ 0.0048, -0.0125,  0.0138,  ...,  0.0178, -0.0051,  0.0119],\n",
      "        [ 0.0227, -0.0025, -0.0508,  ..., -0.0487, -0.0308, -0.0123]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0056, -0.0214, -0.0047,  ..., -0.0147, -0.0060, -0.0091],\n",
      "        [ 0.0538,  0.0007, -0.0080,  ..., -0.0020, -0.0122,  0.0152],\n",
      "        [ 0.0027, -0.0005,  0.0147,  ..., -0.0302,  0.0015, -0.0032],\n",
      "        ...,\n",
      "        [-0.0071,  0.0435,  0.0076,  ...,  0.0355, -0.0248, -0.0132],\n",
      "        [ 0.0048, -0.0125,  0.0138,  ...,  0.0177, -0.0052,  0.0120],\n",
      "        [ 0.0227, -0.0025, -0.0507,  ..., -0.0487, -0.0308, -0.0123]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0057, -0.0215, -0.0048,  ..., -0.0148, -0.0061, -0.0090],\n",
      "        [ 0.0539,  0.0006, -0.0081,  ..., -0.0022, -0.0123,  0.0153],\n",
      "        [ 0.0029, -0.0006,  0.0146,  ..., -0.0304,  0.0014, -0.0031],\n",
      "        ...,\n",
      "        [-0.0072,  0.0435,  0.0077,  ...,  0.0355, -0.0248, -0.0132],\n",
      "        [ 0.0048, -0.0126,  0.0138,  ...,  0.0177, -0.0052,  0.0120],\n",
      "        [ 0.0226, -0.0025, -0.0507,  ..., -0.0488, -0.0308, -0.0123]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0058, -0.0216, -0.0049,  ..., -0.0149, -0.0062, -0.0089],\n",
      "        [ 0.0540,  0.0005, -0.0082,  ..., -0.0023, -0.0124,  0.0155],\n",
      "        [ 0.0030, -0.0007,  0.0145,  ..., -0.0305,  0.0013, -0.0030],\n",
      "        ...,\n",
      "        [-0.0072,  0.0435,  0.0077,  ...,  0.0355, -0.0248, -0.0132],\n",
      "        [ 0.0048, -0.0126,  0.0138,  ...,  0.0176, -0.0052,  0.0121],\n",
      "        [ 0.0226, -0.0025, -0.0506,  ..., -0.0488, -0.0309, -0.0122]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0059, -0.0217, -0.0050,  ..., -0.0150, -0.0063, -0.0088],\n",
      "        [ 0.0541,  0.0004, -0.0083,  ..., -0.0024, -0.0125,  0.0155],\n",
      "        [ 0.0032, -0.0008,  0.0144,  ..., -0.0306,  0.0012, -0.0028],\n",
      "        ...,\n",
      "        [-0.0072,  0.0435,  0.0078,  ...,  0.0355, -0.0248, -0.0132],\n",
      "        [ 0.0048, -0.0126,  0.0138,  ...,  0.0176, -0.0053,  0.0122],\n",
      "        [ 0.0226, -0.0025, -0.0506,  ..., -0.0488, -0.0309, -0.0122]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0060, -0.0218, -0.0050,  ..., -0.0151, -0.0064, -0.0087],\n",
      "        [ 0.0542,  0.0003, -0.0083,  ..., -0.0025, -0.0126,  0.0156],\n",
      "        [ 0.0033, -0.0009,  0.0143,  ..., -0.0307,  0.0011, -0.0027],\n",
      "        ...,\n",
      "        [-0.0072,  0.0435,  0.0078,  ...,  0.0355, -0.0247, -0.0132],\n",
      "        [ 0.0048, -0.0127,  0.0139,  ...,  0.0176, -0.0053,  0.0123],\n",
      "        [ 0.0225, -0.0025, -0.0505,  ..., -0.0488, -0.0309, -0.0122]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0062, -0.0219, -0.0051,  ..., -0.0152, -0.0065, -0.0086],\n",
      "        [ 0.0543,  0.0002, -0.0084,  ..., -0.0025, -0.0127,  0.0157],\n",
      "        [ 0.0034, -0.0010,  0.0142,  ..., -0.0308,  0.0010, -0.0026],\n",
      "        ...,\n",
      "        [-0.0072,  0.0435,  0.0079,  ...,  0.0355, -0.0247, -0.0132],\n",
      "        [ 0.0048, -0.0127,  0.0139,  ...,  0.0175, -0.0054,  0.0123],\n",
      "        [ 0.0225, -0.0025, -0.0505,  ..., -0.0489, -0.0309, -0.0122]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0063, -0.0220, -0.0052,  ..., -0.0153, -0.0065, -0.0085],\n",
      "        [ 0.0544,  0.0001, -0.0085,  ..., -0.0026, -0.0127,  0.0158],\n",
      "        [ 0.0036, -0.0011,  0.0141,  ..., -0.0309,  0.0009, -0.0025],\n",
      "        ...,\n",
      "        [-0.0072,  0.0435,  0.0079,  ...,  0.0355, -0.0247, -0.0132],\n",
      "        [ 0.0049, -0.0128,  0.0139,  ...,  0.0175, -0.0054,  0.0124],\n",
      "        [ 0.0225, -0.0026, -0.0505,  ..., -0.0489, -0.0310, -0.0122]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 6.3545e-03, -2.2026e-02, -5.2531e-03,  ..., -1.5387e-02,\n",
      "         -6.6192e-03, -8.3738e-03],\n",
      "        [ 5.4529e-02,  3.4914e-05, -8.5833e-03,  ..., -2.7319e-03,\n",
      "         -1.2830e-02,  1.5901e-02],\n",
      "        [ 3.7057e-03, -1.2445e-03,  1.3976e-02,  ..., -3.1065e-02,\n",
      "          8.1241e-04, -2.3858e-03],\n",
      "        ...,\n",
      "        [-7.2371e-03,  4.3515e-02,  7.9588e-03,  ...,  3.5522e-02,\n",
      "         -2.4719e-02, -1.3188e-02],\n",
      "        [ 4.8786e-03, -1.2865e-02,  1.3862e-02,  ...,  1.7478e-02,\n",
      "         -5.4234e-03,  1.2451e-02],\n",
      "        [ 2.2435e-02, -2.5437e-03, -5.0467e-02,  ..., -4.8936e-02,\n",
      "         -3.0979e-02, -1.2254e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 6.4572e-03, -2.2105e-02, -5.3251e-03,  ..., -1.5472e-02,\n",
      "         -6.6904e-03, -8.2751e-03],\n",
      "        [ 5.4630e-02, -5.7438e-05, -8.6688e-03,  ..., -2.8222e-03,\n",
      "         -1.2912e-02,  1.5995e-02],\n",
      "        [ 3.8334e-03, -1.3482e-03,  1.3878e-02,  ..., -3.1170e-02,\n",
      "          7.1315e-04, -2.2681e-03],\n",
      "        ...,\n",
      "        [-7.2613e-03,  4.3494e-02,  8.0018e-03,  ...,  3.5525e-02,\n",
      "         -2.4727e-02, -1.3163e-02],\n",
      "        [ 4.8897e-03, -1.2922e-02,  1.3841e-02,  ...,  1.7472e-02,\n",
      "         -5.4433e-03,  1.2493e-02],\n",
      "        [ 2.2392e-02, -2.5220e-03, -5.0444e-02,  ..., -4.8960e-02,\n",
      "         -3.1006e-02, -1.2258e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0066, -0.0222, -0.0054,  ..., -0.0156, -0.0068, -0.0082],\n",
      "        [ 0.0548, -0.0002, -0.0088,  ..., -0.0029, -0.0130,  0.0161],\n",
      "        [ 0.0040, -0.0015,  0.0138,  ..., -0.0313,  0.0006, -0.0021],\n",
      "        ...,\n",
      "        [-0.0073,  0.0435,  0.0081,  ...,  0.0355, -0.0247, -0.0132],\n",
      "        [ 0.0049, -0.0130,  0.0138,  ...,  0.0175, -0.0055,  0.0126],\n",
      "        [ 0.0224, -0.0025, -0.0504,  ..., -0.0490, -0.0310, -0.0123]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0067, -0.0223, -0.0055,  ..., -0.0156, -0.0068, -0.0081],\n",
      "        [ 0.0549, -0.0003, -0.0089,  ..., -0.0030, -0.0131,  0.0162],\n",
      "        [ 0.0041, -0.0016,  0.0137,  ..., -0.0314,  0.0005, -0.0020],\n",
      "        ...,\n",
      "        [-0.0073,  0.0434,  0.0081,  ...,  0.0356, -0.0247, -0.0131],\n",
      "        [ 0.0049, -0.0130,  0.0138,  ...,  0.0175, -0.0055,  0.0126],\n",
      "        [ 0.0223, -0.0025, -0.0504,  ..., -0.0490, -0.0310, -0.0123]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0067, -0.0223, -0.0055,  ..., -0.0157, -0.0069, -0.0080],\n",
      "        [ 0.0550, -0.0004, -0.0090,  ..., -0.0031, -0.0132,  0.0163],\n",
      "        [ 0.0042, -0.0017,  0.0136,  ..., -0.0315,  0.0004, -0.0019],\n",
      "        ...,\n",
      "        [-0.0073,  0.0434,  0.0082,  ...,  0.0356, -0.0247, -0.0131],\n",
      "        [ 0.0049, -0.0130,  0.0138,  ...,  0.0175, -0.0055,  0.0127],\n",
      "        [ 0.0223, -0.0024, -0.0504,  ..., -0.0490, -0.0311, -0.0123]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0068, -0.0224, -0.0056,  ..., -0.0158, -0.0070, -0.0079],\n",
      "        [ 0.0551, -0.0005, -0.0090,  ..., -0.0032, -0.0133,  0.0164],\n",
      "        [ 0.0044, -0.0018,  0.0135,  ..., -0.0316,  0.0003, -0.0018],\n",
      "        ...,\n",
      "        [-0.0073,  0.0434,  0.0082,  ...,  0.0356, -0.0248, -0.0131],\n",
      "        [ 0.0049, -0.0130,  0.0138,  ...,  0.0175, -0.0054,  0.0128],\n",
      "        [ 0.0223, -0.0024, -0.0504,  ..., -0.0490, -0.0311, -0.0123]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0069, -0.0225, -0.0057,  ..., -0.0159, -0.0070, -0.0078],\n",
      "        [ 0.0552, -0.0006, -0.0091,  ..., -0.0033, -0.0133,  0.0165],\n",
      "        [ 0.0045, -0.0019,  0.0134,  ..., -0.0317,  0.0002, -0.0017],\n",
      "        ...,\n",
      "        [-0.0073,  0.0433,  0.0083,  ...,  0.0356, -0.0248, -0.0132],\n",
      "        [ 0.0048, -0.0131,  0.0138,  ...,  0.0175, -0.0055,  0.0129],\n",
      "        [ 0.0222, -0.0024, -0.0504,  ..., -0.0491, -0.0311, -0.0123]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0070, -0.0225, -0.0057,  ..., -0.0159, -0.0071, -0.0077],\n",
      "        [ 0.0553, -0.0006, -0.0092,  ..., -0.0034, -0.0134,  0.0166],\n",
      "        [ 0.0046, -0.0020,  0.0133,  ..., -0.0318,  0.0002, -0.0016],\n",
      "        ...,\n",
      "        [-0.0073,  0.0433,  0.0083,  ...,  0.0356, -0.0248, -0.0132],\n",
      "        [ 0.0048, -0.0131,  0.0138,  ...,  0.0175, -0.0055,  0.0130],\n",
      "        [ 0.0222, -0.0024, -0.0503,  ..., -0.0491, -0.0311, -0.0122]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 7.1175e-03, -2.2629e-02, -5.7855e-03,  ..., -1.6025e-02,\n",
      "         -7.1725e-03, -7.6443e-03],\n",
      "        [ 5.5357e-02, -7.2507e-04, -9.2807e-03,  ..., -3.4985e-03,\n",
      "         -1.3473e-02,  1.6662e-02],\n",
      "        [ 4.6735e-03, -2.0693e-03,  1.3208e-02,  ..., -3.1920e-02,\n",
      "          9.1080e-05, -1.4840e-03],\n",
      "        ...,\n",
      "        [-7.3591e-03,  4.3218e-02,  8.3893e-03,  ...,  3.5577e-02,\n",
      "         -2.4784e-02, -1.3168e-02],\n",
      "        [ 4.8210e-03, -1.3123e-02,  1.3753e-02,  ...,  1.7477e-02,\n",
      "         -5.4789e-03,  1.3068e-02],\n",
      "        [ 2.2127e-02, -2.3997e-03, -5.0320e-02,  ..., -4.9092e-02,\n",
      "         -3.1166e-02, -1.2200e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 7.2004e-03, -2.2702e-02, -5.8452e-03,  ..., -1.6092e-02,\n",
      "         -7.2253e-03, -7.5729e-03],\n",
      "        [ 5.5419e-02, -7.8410e-04, -9.3345e-03,  ..., -3.5501e-03,\n",
      "         -1.3515e-02,  1.6711e-02],\n",
      "        [ 4.7467e-03, -2.1308e-03,  1.3149e-02,  ..., -3.1974e-02,\n",
      "          4.3994e-05, -1.4277e-03],\n",
      "        ...,\n",
      "        [-7.3868e-03,  4.3174e-02,  8.4525e-03,  ...,  3.5589e-02,\n",
      "         -2.4799e-02, -1.3168e-02],\n",
      "        [ 4.8225e-03, -1.3169e-02,  1.3753e-02,  ...,  1.7481e-02,\n",
      "         -5.4912e-03,  1.3139e-02],\n",
      "        [ 2.2086e-02, -2.4062e-03, -5.0286e-02,  ..., -4.9115e-02,\n",
      "         -3.1201e-02, -1.2175e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 7.2847e-03, -2.2772e-02, -5.9017e-03,  ..., -1.6158e-02,\n",
      "         -7.2735e-03, -7.5013e-03],\n",
      "        [ 5.5491e-02, -8.4844e-04, -9.3922e-03,  ..., -3.6074e-03,\n",
      "         -1.3554e-02,  1.6768e-02],\n",
      "        [ 4.8265e-03, -2.1966e-03,  1.3089e-02,  ..., -3.2034e-02,\n",
      "          2.6702e-06, -1.3668e-03],\n",
      "        ...,\n",
      "        [-7.4063e-03,  4.3153e-02,  8.5170e-03,  ...,  3.5595e-02,\n",
      "         -2.4814e-02, -1.3167e-02],\n",
      "        [ 4.8096e-03, -1.3225e-02,  1.3759e-02,  ...,  1.7477e-02,\n",
      "         -5.4943e-03,  1.3198e-02],\n",
      "        [ 2.2035e-02, -2.4125e-03, -5.0261e-02,  ..., -4.9123e-02,\n",
      "         -3.1226e-02, -1.2143e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 7.3829e-03, -2.2852e-02, -5.9684e-03,  ..., -1.6243e-02,\n",
      "         -7.3335e-03, -7.4189e-03],\n",
      "        [ 5.5572e-02, -9.1592e-04, -9.4521e-03,  ..., -3.6806e-03,\n",
      "         -1.3602e-02,  1.6833e-02],\n",
      "        [ 4.9123e-03, -2.2640e-03,  1.3030e-02,  ..., -3.2106e-02,\n",
      "         -4.5232e-05, -1.3006e-03],\n",
      "        ...,\n",
      "        [-7.4260e-03,  4.3127e-02,  8.5896e-03,  ...,  3.5589e-02,\n",
      "         -2.4827e-02, -1.3157e-02],\n",
      "        [ 4.8037e-03, -1.3297e-02,  1.3745e-02,  ...,  1.7474e-02,\n",
      "         -5.4810e-03,  1.3252e-02],\n",
      "        [ 2.2001e-02, -2.4273e-03, -5.0246e-02,  ..., -4.9127e-02,\n",
      "         -3.1249e-02, -1.2124e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 7.4847e-03, -2.2935e-02, -6.0398e-03,  ..., -1.6321e-02,\n",
      "         -7.3943e-03, -7.3402e-03],\n",
      "        [ 5.5656e-02, -9.8523e-04, -9.5166e-03,  ..., -3.7425e-03,\n",
      "         -1.3649e-02,  1.6891e-02],\n",
      "        [ 4.9957e-03, -2.3293e-03,  1.2969e-02,  ..., -3.2167e-02,\n",
      "         -8.8867e-05, -1.2461e-03],\n",
      "        ...,\n",
      "        [-7.4338e-03,  4.3114e-02,  8.6721e-03,  ...,  3.5597e-02,\n",
      "         -2.4838e-02, -1.3154e-02],\n",
      "        [ 4.7867e-03, -1.3370e-02,  1.3745e-02,  ...,  1.7459e-02,\n",
      "         -5.4673e-03,  1.3306e-02],\n",
      "        [ 2.1960e-02, -2.4324e-03, -5.0235e-02,  ..., -4.9131e-02,\n",
      "         -3.1277e-02, -1.2099e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0076, -0.0230, -0.0061,  ..., -0.0164, -0.0075, -0.0072],\n",
      "        [ 0.0557, -0.0011, -0.0096,  ..., -0.0038, -0.0137,  0.0170],\n",
      "        [ 0.0051, -0.0024,  0.0129,  ..., -0.0322, -0.0001, -0.0012],\n",
      "        ...,\n",
      "        [-0.0074,  0.0431,  0.0088,  ...,  0.0356, -0.0249, -0.0131],\n",
      "        [ 0.0048, -0.0134,  0.0137,  ...,  0.0175, -0.0055,  0.0134],\n",
      "        [ 0.0219, -0.0024, -0.0502,  ..., -0.0491, -0.0313, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0077, -0.0231, -0.0062,  ..., -0.0165, -0.0075, -0.0072],\n",
      "        [ 0.0558, -0.0011, -0.0097,  ..., -0.0039, -0.0137,  0.0170],\n",
      "        [ 0.0052, -0.0025,  0.0128,  ..., -0.0323, -0.0002, -0.0011],\n",
      "        ...,\n",
      "        [-0.0074,  0.0431,  0.0089,  ...,  0.0356, -0.0249, -0.0131],\n",
      "        [ 0.0047, -0.0135,  0.0137,  ...,  0.0174, -0.0054,  0.0134],\n",
      "        [ 0.0219, -0.0025, -0.0502,  ..., -0.0491, -0.0313, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0078, -0.0232, -0.0063,  ..., -0.0166, -0.0076, -0.0071],\n",
      "        [ 0.0559, -0.0012, -0.0097,  ..., -0.0039, -0.0138,  0.0171],\n",
      "        [ 0.0053, -0.0025,  0.0128,  ..., -0.0324, -0.0002, -0.0011],\n",
      "        ...,\n",
      "        [-0.0075,  0.0431,  0.0090,  ...,  0.0356, -0.0249, -0.0131],\n",
      "        [ 0.0047, -0.0135,  0.0138,  ...,  0.0174, -0.0054,  0.0135],\n",
      "        [ 0.0218, -0.0025, -0.0502,  ..., -0.0492, -0.0314, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0079, -0.0233, -0.0064,  ..., -0.0167, -0.0076, -0.0070],\n",
      "        [ 0.0560, -0.0012, -0.0098,  ..., -0.0040, -0.0138,  0.0171],\n",
      "        [ 0.0053, -0.0026,  0.0127,  ..., -0.0324, -0.0002, -0.0010],\n",
      "        ...,\n",
      "        [-0.0075,  0.0430,  0.0090,  ...,  0.0356, -0.0249, -0.0131],\n",
      "        [ 0.0047, -0.0136,  0.0138,  ...,  0.0174, -0.0054,  0.0135],\n",
      "        [ 0.0218, -0.0025, -0.0502,  ..., -0.0492, -0.0314, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0080, -0.0234, -0.0065,  ..., -0.0168, -0.0077, -0.0069],\n",
      "        [ 0.0560, -0.0013, -0.0098,  ..., -0.0040, -0.0138,  0.0171],\n",
      "        [ 0.0054, -0.0027,  0.0126,  ..., -0.0325, -0.0003, -0.0010],\n",
      "        ...,\n",
      "        [-0.0075,  0.0430,  0.0091,  ...,  0.0356, -0.0249, -0.0131],\n",
      "        [ 0.0046, -0.0136,  0.0139,  ...,  0.0174, -0.0055,  0.0136],\n",
      "        [ 0.0217, -0.0025, -0.0502,  ..., -0.0492, -0.0314, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0082, -0.0235, -0.0065,  ..., -0.0169, -0.0078, -0.0068],\n",
      "        [ 0.0561, -0.0014, -0.0099,  ..., -0.0041, -0.0138,  0.0172],\n",
      "        [ 0.0055, -0.0028,  0.0125,  ..., -0.0326, -0.0003, -0.0009],\n",
      "        ...,\n",
      "        [-0.0075,  0.0430,  0.0092,  ...,  0.0356, -0.0249, -0.0131],\n",
      "        [ 0.0046, -0.0137,  0.0139,  ...,  0.0173, -0.0055,  0.0136],\n",
      "        [ 0.0217, -0.0026, -0.0502,  ..., -0.0493, -0.0315, -0.0119]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0083, -0.0236, -0.0066,  ..., -0.0170, -0.0078, -0.0067],\n",
      "        [ 0.0562, -0.0015, -0.0100,  ..., -0.0042, -0.0139,  0.0173],\n",
      "        [ 0.0056, -0.0029,  0.0124,  ..., -0.0327, -0.0004, -0.0008],\n",
      "        ...,\n",
      "        [-0.0075,  0.0429,  0.0093,  ...,  0.0356, -0.0250, -0.0131],\n",
      "        [ 0.0045, -0.0137,  0.0140,  ...,  0.0173, -0.0056,  0.0137],\n",
      "        [ 0.0217, -0.0026, -0.0502,  ..., -0.0493, -0.0315, -0.0119]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0084, -0.0237, -0.0067,  ..., -0.0171, -0.0079, -0.0066],\n",
      "        [ 0.0563, -0.0016, -0.0101,  ..., -0.0042, -0.0139,  0.0174],\n",
      "        [ 0.0058, -0.0030,  0.0123,  ..., -0.0328, -0.0004, -0.0006],\n",
      "        ...,\n",
      "        [-0.0076,  0.0429,  0.0093,  ...,  0.0356, -0.0250, -0.0131],\n",
      "        [ 0.0045, -0.0138,  0.0141,  ...,  0.0172, -0.0056,  0.0137],\n",
      "        [ 0.0216, -0.0026, -0.0502,  ..., -0.0494, -0.0316, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0085, -0.0238, -0.0069,  ..., -0.0173, -0.0080, -0.0065],\n",
      "        [ 0.0564, -0.0017, -0.0102,  ..., -0.0044, -0.0140,  0.0175],\n",
      "        [ 0.0059, -0.0031,  0.0122,  ..., -0.0329, -0.0005, -0.0005],\n",
      "        ...,\n",
      "        [-0.0076,  0.0429,  0.0094,  ...,  0.0356, -0.0250, -0.0130],\n",
      "        [ 0.0044, -0.0138,  0.0141,  ...,  0.0171, -0.0057,  0.0138],\n",
      "        [ 0.0216, -0.0026, -0.0502,  ..., -0.0494, -0.0316, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0087, -0.0240, -0.0070,  ..., -0.0174, -0.0081, -0.0063],\n",
      "        [ 0.0566, -0.0018, -0.0103,  ..., -0.0045, -0.0141,  0.0176],\n",
      "        [ 0.0060, -0.0032,  0.0121,  ..., -0.0330, -0.0006, -0.0004],\n",
      "        ...,\n",
      "        [-0.0076,  0.0428,  0.0094,  ...,  0.0357, -0.0250, -0.0130],\n",
      "        [ 0.0044, -0.0138,  0.0141,  ...,  0.0171, -0.0057,  0.0138],\n",
      "        [ 0.0215, -0.0026, -0.0502,  ..., -0.0495, -0.0316, -0.0117]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0089, -0.0241, -0.0071,  ..., -0.0176, -0.0082, -0.0062],\n",
      "        [ 0.0567, -0.0020, -0.0105,  ..., -0.0046, -0.0142,  0.0177],\n",
      "        [ 0.0062, -0.0034,  0.0119,  ..., -0.0332, -0.0007, -0.0003],\n",
      "        ...,\n",
      "        [-0.0077,  0.0428,  0.0095,  ...,  0.0357, -0.0251, -0.0130],\n",
      "        [ 0.0043, -0.0138,  0.0141,  ...,  0.0171, -0.0057,  0.0139],\n",
      "        [ 0.0215, -0.0026, -0.0502,  ..., -0.0495, -0.0317, -0.0117]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0090, -0.0243, -0.0072,  ..., -0.0177, -0.0083, -0.0060],\n",
      "        [ 0.0569, -0.0021, -0.0106,  ..., -0.0048, -0.0143,  0.0179],\n",
      "        [ 0.0064, -0.0036,  0.0118,  ..., -0.0333, -0.0008, -0.0001],\n",
      "        ...,\n",
      "        [-0.0077,  0.0427,  0.0095,  ...,  0.0357, -0.0251, -0.0130],\n",
      "        [ 0.0043, -0.0138,  0.0142,  ...,  0.0171, -0.0058,  0.0139],\n",
      "        [ 0.0214, -0.0026, -0.0502,  ..., -0.0496, -0.0317, -0.0116]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 9.1859e-03, -2.4435e-02, -7.3727e-03,  ..., -1.7887e-02,\n",
      "         -8.4153e-03, -5.9042e-03],\n",
      "        [ 5.7003e-02, -2.2538e-03, -1.0755e-02,  ..., -4.8825e-03,\n",
      "         -1.4342e-02,  1.7976e-02],\n",
      "        [ 6.4875e-03, -3.6947e-03,  1.1681e-02,  ..., -3.3482e-02,\n",
      "         -8.7029e-04, -1.4682e-05],\n",
      "        ...,\n",
      "        [-7.7093e-03,  4.2666e-02,  9.5717e-03,  ...,  3.5755e-02,\n",
      "         -2.5088e-02, -1.2946e-02],\n",
      "        [ 4.2949e-03, -1.3791e-02,  1.4181e-02,  ...,  1.7028e-02,\n",
      "         -5.7885e-03,  1.3969e-02],\n",
      "        [ 2.1395e-02, -2.6512e-03, -5.0155e-02,  ..., -4.9602e-02,\n",
      "         -3.1740e-02, -1.1563e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0093, -0.0246, -0.0075,  ..., -0.0180, -0.0085, -0.0058],\n",
      "        [ 0.0571, -0.0024, -0.0109,  ..., -0.0050, -0.0144,  0.0181],\n",
      "        [ 0.0066, -0.0038,  0.0115,  ..., -0.0336, -0.0010,  0.0001],\n",
      "        ...,\n",
      "        [-0.0077,  0.0426,  0.0096,  ...,  0.0358, -0.0251, -0.0129],\n",
      "        [ 0.0043, -0.0138,  0.0142,  ...,  0.0170, -0.0058,  0.0140],\n",
      "        [ 0.0213, -0.0027, -0.0501,  ..., -0.0496, -0.0318, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0095, -0.0247, -0.0076,  ..., -0.0182, -0.0086, -0.0056],\n",
      "        [ 0.0573, -0.0026, -0.0110,  ..., -0.0052, -0.0145,  0.0182],\n",
      "        [ 0.0068, -0.0040,  0.0114,  ..., -0.0338, -0.0011,  0.0003],\n",
      "        ...,\n",
      "        [-0.0078,  0.0426,  0.0097,  ...,  0.0358, -0.0251, -0.0129],\n",
      "        [ 0.0042, -0.0138,  0.0142,  ...,  0.0170, -0.0058,  0.0141],\n",
      "        [ 0.0213, -0.0027, -0.0501,  ..., -0.0497, -0.0318, -0.0114]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0097, -0.0249, -0.0078,  ..., -0.0184, -0.0087, -0.0055],\n",
      "        [ 0.0574, -0.0027, -0.0112,  ..., -0.0053, -0.0146,  0.0184],\n",
      "        [ 0.0070, -0.0042,  0.0112,  ..., -0.0340, -0.0012,  0.0004],\n",
      "        ...,\n",
      "        [-0.0078,  0.0425,  0.0097,  ...,  0.0358, -0.0252, -0.0128],\n",
      "        [ 0.0042, -0.0138,  0.0143,  ...,  0.0171, -0.0059,  0.0142],\n",
      "        [ 0.0213, -0.0027, -0.0501,  ..., -0.0497, -0.0318, -0.0114]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0098, -0.0250, -0.0079,  ..., -0.0185, -0.0088, -0.0054],\n",
      "        [ 0.0576, -0.0028, -0.0113,  ..., -0.0054, -0.0147,  0.0185],\n",
      "        [ 0.0071, -0.0043,  0.0111,  ..., -0.0341, -0.0013,  0.0006],\n",
      "        ...,\n",
      "        [-0.0078,  0.0425,  0.0097,  ...,  0.0359, -0.0252, -0.0128],\n",
      "        [ 0.0042, -0.0138,  0.0143,  ...,  0.0171, -0.0059,  0.0143],\n",
      "        [ 0.0213, -0.0027, -0.0501,  ..., -0.0498, -0.0319, -0.0113]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0099, -0.0252, -0.0080,  ..., -0.0186, -0.0089, -0.0052],\n",
      "        [ 0.0577, -0.0030, -0.0114,  ..., -0.0056, -0.0148,  0.0186],\n",
      "        [ 0.0073, -0.0045,  0.0110,  ..., -0.0343, -0.0014,  0.0007],\n",
      "        ...,\n",
      "        [-0.0078,  0.0425,  0.0098,  ...,  0.0359, -0.0252, -0.0128],\n",
      "        [ 0.0042, -0.0137,  0.0144,  ...,  0.0170, -0.0060,  0.0144],\n",
      "        [ 0.0212, -0.0027, -0.0501,  ..., -0.0498, -0.0319, -0.0113]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0101, -0.0253, -0.0081,  ..., -0.0188, -0.0090, -0.0051],\n",
      "        [ 0.0578, -0.0031, -0.0115,  ..., -0.0057, -0.0149,  0.0187],\n",
      "        [ 0.0074, -0.0046,  0.0109,  ..., -0.0344, -0.0015,  0.0008],\n",
      "        ...,\n",
      "        [-0.0078,  0.0425,  0.0098,  ...,  0.0360, -0.0252, -0.0128],\n",
      "        [ 0.0042, -0.0137,  0.0144,  ...,  0.0170, -0.0061,  0.0145],\n",
      "        [ 0.0212, -0.0027, -0.0501,  ..., -0.0498, -0.0319, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0102, -0.0255, -0.0082,  ..., -0.0189, -0.0091, -0.0049],\n",
      "        [ 0.0580, -0.0033, -0.0117,  ..., -0.0059, -0.0150,  0.0189],\n",
      "        [ 0.0076, -0.0048,  0.0107,  ..., -0.0346, -0.0016,  0.0010],\n",
      "        ...,\n",
      "        [-0.0079,  0.0425,  0.0098,  ...,  0.0360, -0.0252, -0.0127],\n",
      "        [ 0.0042, -0.0136,  0.0145,  ...,  0.0170, -0.0061,  0.0146],\n",
      "        [ 0.0211, -0.0027, -0.0501,  ..., -0.0498, -0.0320, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0104, -0.0256, -0.0084,  ..., -0.0191, -0.0093, -0.0048],\n",
      "        [ 0.0581, -0.0034, -0.0118,  ..., -0.0060, -0.0151,  0.0190],\n",
      "        [ 0.0078, -0.0050,  0.0106,  ..., -0.0348, -0.0017,  0.0012],\n",
      "        ...,\n",
      "        [-0.0079,  0.0424,  0.0098,  ...,  0.0361, -0.0252, -0.0127],\n",
      "        [ 0.0042, -0.0136,  0.0145,  ...,  0.0170, -0.0062,  0.0147],\n",
      "        [ 0.0211, -0.0027, -0.0500,  ..., -0.0499, -0.0320, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0106, -0.0258, -0.0085,  ..., -0.0193, -0.0094, -0.0046],\n",
      "        [ 0.0583, -0.0036, -0.0120,  ..., -0.0062, -0.0152,  0.0192],\n",
      "        [ 0.0080, -0.0052,  0.0104,  ..., -0.0350, -0.0019,  0.0013],\n",
      "        ...,\n",
      "        [-0.0079,  0.0424,  0.0098,  ...,  0.0361, -0.0251, -0.0126],\n",
      "        [ 0.0043, -0.0136,  0.0146,  ...,  0.0169, -0.0063,  0.0147],\n",
      "        [ 0.0210, -0.0027, -0.0500,  ..., -0.0499, -0.0320, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0107, -0.0260, -0.0086,  ..., -0.0195, -0.0095, -0.0044],\n",
      "        [ 0.0585, -0.0038, -0.0121,  ..., -0.0063, -0.0153,  0.0193],\n",
      "        [ 0.0081, -0.0053,  0.0102,  ..., -0.0351, -0.0020,  0.0015],\n",
      "        ...,\n",
      "        [-0.0079,  0.0424,  0.0098,  ...,  0.0362, -0.0251, -0.0126],\n",
      "        [ 0.0043, -0.0137,  0.0146,  ...,  0.0169, -0.0063,  0.0148],\n",
      "        [ 0.0210, -0.0026, -0.0500,  ..., -0.0499, -0.0321, -0.0111]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0109, -0.0261, -0.0088,  ..., -0.0196, -0.0096, -0.0043],\n",
      "        [ 0.0586, -0.0039, -0.0123,  ..., -0.0065, -0.0154,  0.0195],\n",
      "        [ 0.0083, -0.0055,  0.0101,  ..., -0.0353, -0.0021,  0.0017],\n",
      "        ...,\n",
      "        [-0.0079,  0.0424,  0.0098,  ...,  0.0362, -0.0251, -0.0126],\n",
      "        [ 0.0043, -0.0137,  0.0147,  ...,  0.0169, -0.0064,  0.0149],\n",
      "        [ 0.0209, -0.0026, -0.0499,  ..., -0.0500, -0.0321, -0.0111]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0111, -0.0263, -0.0089,  ..., -0.0198, -0.0097, -0.0041],\n",
      "        [ 0.0588, -0.0041, -0.0124,  ..., -0.0066, -0.0155,  0.0196],\n",
      "        [ 0.0085, -0.0057,  0.0099,  ..., -0.0355, -0.0022,  0.0018],\n",
      "        ...,\n",
      "        [-0.0079,  0.0424,  0.0098,  ...,  0.0363, -0.0251, -0.0126],\n",
      "        [ 0.0043, -0.0137,  0.0147,  ...,  0.0168, -0.0065,  0.0150],\n",
      "        [ 0.0209, -0.0026, -0.0499,  ..., -0.0500, -0.0321, -0.0111]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0112, -0.0264, -0.0090,  ..., -0.0200, -0.0098, -0.0040],\n",
      "        [ 0.0589, -0.0042, -0.0125,  ..., -0.0068, -0.0156,  0.0198],\n",
      "        [ 0.0086, -0.0058,  0.0098,  ..., -0.0357, -0.0023,  0.0020],\n",
      "        ...,\n",
      "        [-0.0079,  0.0424,  0.0099,  ...,  0.0363, -0.0251, -0.0126],\n",
      "        [ 0.0042, -0.0138,  0.0147,  ...,  0.0168, -0.0065,  0.0151],\n",
      "        [ 0.0209, -0.0025, -0.0499,  ..., -0.0500, -0.0322, -0.0111]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0114, -0.0266, -0.0092,  ..., -0.0201, -0.0099, -0.0038],\n",
      "        [ 0.0590, -0.0044, -0.0127,  ..., -0.0069, -0.0157,  0.0199],\n",
      "        [ 0.0088, -0.0060,  0.0097,  ..., -0.0358, -0.0024,  0.0021],\n",
      "        ...,\n",
      "        [-0.0079,  0.0424,  0.0099,  ...,  0.0364, -0.0251, -0.0126],\n",
      "        [ 0.0042, -0.0138,  0.0147,  ...,  0.0167, -0.0066,  0.0152],\n",
      "        [ 0.0208, -0.0025, -0.0499,  ..., -0.0500, -0.0322, -0.0110]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0115, -0.0267, -0.0093,  ..., -0.0203, -0.0101, -0.0036],\n",
      "        [ 0.0592, -0.0045, -0.0128,  ..., -0.0071, -0.0158,  0.0200],\n",
      "        [ 0.0089, -0.0061,  0.0095,  ..., -0.0360, -0.0025,  0.0023],\n",
      "        ...,\n",
      "        [-0.0079,  0.0424,  0.0099,  ...,  0.0364, -0.0251, -0.0127],\n",
      "        [ 0.0042, -0.0139,  0.0148,  ...,  0.0166, -0.0067,  0.0153],\n",
      "        [ 0.0208, -0.0025, -0.0499,  ..., -0.0500, -0.0322, -0.0110]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0117, -0.0269, -0.0095,  ..., -0.0205, -0.0102, -0.0035],\n",
      "        [ 0.0594, -0.0047, -0.0130,  ..., -0.0072, -0.0159,  0.0202],\n",
      "        [ 0.0091, -0.0063,  0.0093,  ..., -0.0362, -0.0027,  0.0025],\n",
      "        ...,\n",
      "        [-0.0079,  0.0424,  0.0100,  ...,  0.0365, -0.0251, -0.0127],\n",
      "        [ 0.0041, -0.0140,  0.0148,  ...,  0.0166, -0.0068,  0.0155],\n",
      "        [ 0.0208, -0.0025, -0.0499,  ..., -0.0499, -0.0323, -0.0110]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0119, -0.0270, -0.0096,  ..., -0.0206, -0.0103, -0.0033],\n",
      "        [ 0.0595, -0.0048, -0.0131,  ..., -0.0073, -0.0160,  0.0203],\n",
      "        [ 0.0093, -0.0064,  0.0092,  ..., -0.0363, -0.0027,  0.0026],\n",
      "        ...,\n",
      "        [-0.0079,  0.0424,  0.0100,  ...,  0.0365, -0.0250, -0.0127],\n",
      "        [ 0.0041, -0.0140,  0.0148,  ...,  0.0165, -0.0069,  0.0156],\n",
      "        [ 0.0208, -0.0024, -0.0499,  ..., -0.0499, -0.0323, -0.0109]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0120, -0.0272, -0.0097,  ..., -0.0208, -0.0104, -0.0032],\n",
      "        [ 0.0596, -0.0049, -0.0132,  ..., -0.0075, -0.0161,  0.0205],\n",
      "        [ 0.0094, -0.0066,  0.0091,  ..., -0.0365, -0.0028,  0.0028],\n",
      "        ...,\n",
      "        [-0.0080,  0.0423,  0.0101,  ...,  0.0365, -0.0250, -0.0127],\n",
      "        [ 0.0041, -0.0141,  0.0148,  ...,  0.0165, -0.0069,  0.0157],\n",
      "        [ 0.0208, -0.0024, -0.0498,  ..., -0.0499, -0.0323, -0.0109]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0122, -0.0273, -0.0099,  ..., -0.0210, -0.0105, -0.0030],\n",
      "        [ 0.0598, -0.0051, -0.0134,  ..., -0.0076, -0.0162,  0.0206],\n",
      "        [ 0.0096, -0.0067,  0.0089,  ..., -0.0366, -0.0030,  0.0029],\n",
      "        ...,\n",
      "        [-0.0080,  0.0423,  0.0102,  ...,  0.0365, -0.0250, -0.0128],\n",
      "        [ 0.0040, -0.0142,  0.0149,  ...,  0.0164, -0.0070,  0.0158],\n",
      "        [ 0.0207, -0.0024, -0.0498,  ..., -0.0499, -0.0324, -0.0109]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0124, -0.0275, -0.0100,  ..., -0.0212, -0.0106, -0.0028],\n",
      "        [ 0.0599, -0.0052, -0.0135,  ..., -0.0078, -0.0163,  0.0208],\n",
      "        [ 0.0097, -0.0069,  0.0088,  ..., -0.0368, -0.0031,  0.0031],\n",
      "        ...,\n",
      "        [-0.0080,  0.0423,  0.0102,  ...,  0.0366, -0.0250, -0.0128],\n",
      "        [ 0.0040, -0.0142,  0.0149,  ...,  0.0163, -0.0071,  0.0159],\n",
      "        [ 0.0208, -0.0024, -0.0498,  ..., -0.0499, -0.0324, -0.0109]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0126, -0.0277, -0.0102,  ..., -0.0214, -0.0108, -0.0026],\n",
      "        [ 0.0601, -0.0054, -0.0137,  ..., -0.0080, -0.0164,  0.0209],\n",
      "        [ 0.0099, -0.0071,  0.0086,  ..., -0.0370, -0.0032,  0.0033],\n",
      "        ...,\n",
      "        [-0.0080,  0.0423,  0.0102,  ...,  0.0366, -0.0250, -0.0128],\n",
      "        [ 0.0040, -0.0143,  0.0150,  ...,  0.0162, -0.0072,  0.0160],\n",
      "        [ 0.0208, -0.0024, -0.0497,  ..., -0.0499, -0.0325, -0.0109]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0127, -0.0278, -0.0103,  ..., -0.0216, -0.0109, -0.0024],\n",
      "        [ 0.0603, -0.0056, -0.0138,  ..., -0.0082, -0.0166,  0.0211],\n",
      "        [ 0.0101, -0.0072,  0.0084,  ..., -0.0372, -0.0034,  0.0034],\n",
      "        ...,\n",
      "        [-0.0080,  0.0423,  0.0103,  ...,  0.0366, -0.0250, -0.0129],\n",
      "        [ 0.0041, -0.0144,  0.0150,  ...,  0.0162, -0.0073,  0.0161],\n",
      "        [ 0.0208, -0.0023, -0.0497,  ..., -0.0499, -0.0325, -0.0109]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0129, -0.0280, -0.0105,  ..., -0.0218, -0.0110, -0.0022],\n",
      "        [ 0.0604, -0.0057, -0.0140,  ..., -0.0083, -0.0167,  0.0213],\n",
      "        [ 0.0103, -0.0074,  0.0083,  ..., -0.0374, -0.0035,  0.0036],\n",
      "        ...,\n",
      "        [-0.0080,  0.0423,  0.0103,  ...,  0.0367, -0.0250, -0.0130],\n",
      "        [ 0.0041, -0.0145,  0.0151,  ...,  0.0161, -0.0074,  0.0162],\n",
      "        [ 0.0208, -0.0023, -0.0496,  ..., -0.0499, -0.0326, -0.0109]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0131, -0.0282, -0.0107,  ..., -0.0220, -0.0112, -0.0020],\n",
      "        [ 0.0606, -0.0059, -0.0142,  ..., -0.0085, -0.0168,  0.0215],\n",
      "        [ 0.0104, -0.0076,  0.0081,  ..., -0.0376, -0.0036,  0.0038],\n",
      "        ...,\n",
      "        [-0.0080,  0.0423,  0.0104,  ...,  0.0367, -0.0250, -0.0131],\n",
      "        [ 0.0040, -0.0146,  0.0151,  ...,  0.0159, -0.0075,  0.0163],\n",
      "        [ 0.0208, -0.0023, -0.0496,  ..., -0.0499, -0.0326, -0.0109]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0133, -0.0283, -0.0108,  ..., -0.0221, -0.0113, -0.0019],\n",
      "        [ 0.0608, -0.0060, -0.0143,  ..., -0.0087, -0.0169,  0.0216],\n",
      "        [ 0.0106, -0.0077,  0.0080,  ..., -0.0377, -0.0037,  0.0040],\n",
      "        ...,\n",
      "        [-0.0080,  0.0423,  0.0104,  ...,  0.0367, -0.0250, -0.0132],\n",
      "        [ 0.0040, -0.0147,  0.0152,  ...,  0.0158, -0.0076,  0.0164],\n",
      "        [ 0.0208, -0.0022, -0.0495,  ..., -0.0499, -0.0327, -0.0109]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0135, -0.0285, -0.0110,  ..., -0.0223, -0.0114, -0.0017],\n",
      "        [ 0.0609, -0.0062, -0.0144,  ..., -0.0089, -0.0170,  0.0218],\n",
      "        [ 0.0108, -0.0078,  0.0078,  ..., -0.0379, -0.0038,  0.0041],\n",
      "        ...,\n",
      "        [-0.0080,  0.0423,  0.0104,  ...,  0.0368, -0.0250, -0.0132],\n",
      "        [ 0.0040, -0.0148,  0.0152,  ...,  0.0157, -0.0077,  0.0165],\n",
      "        [ 0.0208, -0.0022, -0.0495,  ..., -0.0499, -0.0327, -0.0109]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0137, -0.0286, -0.0111,  ..., -0.0225, -0.0115, -0.0015],\n",
      "        [ 0.0611, -0.0063, -0.0146,  ..., -0.0090, -0.0171,  0.0220],\n",
      "        [ 0.0109, -0.0080,  0.0077,  ..., -0.0381, -0.0040,  0.0043],\n",
      "        ...,\n",
      "        [-0.0080,  0.0422,  0.0104,  ...,  0.0369, -0.0250, -0.0133],\n",
      "        [ 0.0040, -0.0149,  0.0153,  ...,  0.0156, -0.0077,  0.0166],\n",
      "        [ 0.0209, -0.0022, -0.0494,  ..., -0.0499, -0.0328, -0.0110]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0139, -0.0288, -0.0113,  ..., -0.0227, -0.0117, -0.0013],\n",
      "        [ 0.0613, -0.0065, -0.0148,  ..., -0.0092, -0.0173,  0.0221],\n",
      "        [ 0.0111, -0.0081,  0.0075,  ..., -0.0383, -0.0041,  0.0045],\n",
      "        ...,\n",
      "        [-0.0080,  0.0422,  0.0104,  ...,  0.0369, -0.0250, -0.0133],\n",
      "        [ 0.0040, -0.0150,  0.0153,  ...,  0.0155, -0.0078,  0.0167],\n",
      "        [ 0.0208, -0.0021, -0.0494,  ..., -0.0499, -0.0328, -0.0110]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0141, -0.0290, -0.0114,  ..., -0.0229, -0.0118, -0.0012],\n",
      "        [ 0.0615, -0.0066, -0.0149,  ..., -0.0094, -0.0174,  0.0223],\n",
      "        [ 0.0113, -0.0083,  0.0073,  ..., -0.0384, -0.0042,  0.0046],\n",
      "        ...,\n",
      "        [-0.0080,  0.0422,  0.0104,  ...,  0.0369, -0.0250, -0.0134],\n",
      "        [ 0.0039, -0.0151,  0.0153,  ...,  0.0154, -0.0079,  0.0168],\n",
      "        [ 0.0208, -0.0021, -0.0493,  ..., -0.0499, -0.0329, -0.0110]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0143, -0.0291, -0.0116,  ..., -0.0231, -0.0120, -0.0010],\n",
      "        [ 0.0617, -0.0068, -0.0151,  ..., -0.0096, -0.0175,  0.0225],\n",
      "        [ 0.0115, -0.0084,  0.0072,  ..., -0.0386, -0.0043,  0.0048],\n",
      "        ...,\n",
      "        [-0.0080,  0.0422,  0.0104,  ...,  0.0370, -0.0249, -0.0134],\n",
      "        [ 0.0039, -0.0152,  0.0154,  ...,  0.0153, -0.0080,  0.0169],\n",
      "        [ 0.0208, -0.0021, -0.0493,  ..., -0.0499, -0.0329, -0.0111]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0145, -0.0293, -0.0117,  ..., -0.0233, -0.0121, -0.0008],\n",
      "        [ 0.0619, -0.0069, -0.0153,  ..., -0.0098, -0.0177,  0.0227],\n",
      "        [ 0.0117, -0.0086,  0.0070,  ..., -0.0388, -0.0045,  0.0050],\n",
      "        ...,\n",
      "        [-0.0080,  0.0422,  0.0105,  ...,  0.0371, -0.0249, -0.0135],\n",
      "        [ 0.0039, -0.0153,  0.0154,  ...,  0.0152, -0.0080,  0.0170],\n",
      "        [ 0.0208, -0.0020, -0.0492,  ..., -0.0498, -0.0330, -0.0111]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0146, -0.0294, -0.0119,  ..., -0.0234, -0.0122, -0.0006],\n",
      "        [ 0.0620, -0.0071, -0.0154,  ..., -0.0099, -0.0178,  0.0228],\n",
      "        [ 0.0118, -0.0087,  0.0069,  ..., -0.0390, -0.0046,  0.0051],\n",
      "        ...,\n",
      "        [-0.0080,  0.0421,  0.0105,  ...,  0.0371, -0.0249, -0.0135],\n",
      "        [ 0.0038, -0.0154,  0.0154,  ...,  0.0152, -0.0081,  0.0171],\n",
      "        [ 0.0208, -0.0020, -0.0491,  ..., -0.0498, -0.0331, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0148, -0.0296, -0.0121,  ..., -0.0236, -0.0124, -0.0005],\n",
      "        [ 0.0622, -0.0073, -0.0156,  ..., -0.0101, -0.0179,  0.0230],\n",
      "        [ 0.0120, -0.0089,  0.0067,  ..., -0.0391, -0.0047,  0.0053],\n",
      "        ...,\n",
      "        [-0.0081,  0.0421,  0.0105,  ...,  0.0372, -0.0249, -0.0136],\n",
      "        [ 0.0038, -0.0155,  0.0153,  ...,  0.0151, -0.0082,  0.0172],\n",
      "        [ 0.0208, -0.0019, -0.0491,  ..., -0.0498, -0.0331, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0150, -0.0297, -0.0122,  ..., -0.0238, -0.0125, -0.0003],\n",
      "        [ 0.0624, -0.0074, -0.0158,  ..., -0.0103, -0.0181,  0.0232],\n",
      "        [ 0.0122, -0.0090,  0.0065,  ..., -0.0393, -0.0049,  0.0055],\n",
      "        ...,\n",
      "        [-0.0081,  0.0422,  0.0105,  ...,  0.0372, -0.0248, -0.0136],\n",
      "        [ 0.0038, -0.0157,  0.0153,  ...,  0.0150, -0.0082,  0.0172],\n",
      "        [ 0.0207, -0.0019, -0.0490,  ..., -0.0499, -0.0332, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.5214e-02, -2.9878e-02, -1.2389e-02,  ..., -2.4031e-02,\n",
      "         -1.2640e-02, -7.7653e-05],\n",
      "        [ 6.2591e-02, -7.5688e-03, -1.5953e-02,  ..., -1.0506e-02,\n",
      "         -1.8206e-02,  2.3352e-02],\n",
      "        [ 1.2412e-02, -9.2003e-03,  6.3442e-03,  ..., -3.9529e-02,\n",
      "         -4.9948e-03,  5.6611e-03],\n",
      "        ...,\n",
      "        [-8.1489e-03,  4.2164e-02,  1.0555e-02,  ...,  3.7283e-02,\n",
      "         -2.4843e-02, -1.3651e-02],\n",
      "        [ 3.7036e-03, -1.5758e-02,  1.5231e-02,  ...,  1.4932e-02,\n",
      "         -8.2426e-03,  1.7327e-02],\n",
      "        [ 2.0669e-02, -1.8871e-03, -4.9011e-02,  ..., -4.9864e-02,\n",
      "         -3.3259e-02, -1.1218e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0154, -0.0300, -0.0126,  ..., -0.0242, -0.0128,  0.0001],\n",
      "        [ 0.0628, -0.0077, -0.0161,  ..., -0.0107, -0.0183,  0.0235],\n",
      "        [ 0.0126, -0.0094,  0.0062,  ..., -0.0397, -0.0051,  0.0058],\n",
      "        ...,\n",
      "        [-0.0082,  0.0422,  0.0106,  ...,  0.0373, -0.0248, -0.0137],\n",
      "        [ 0.0037, -0.0159,  0.0152,  ...,  0.0149, -0.0083,  0.0174],\n",
      "        [ 0.0206, -0.0019, -0.0490,  ..., -0.0499, -0.0333, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0156, -0.0302, -0.0127,  ..., -0.0244, -0.0129,  0.0003],\n",
      "        [ 0.0630, -0.0079, -0.0163,  ..., -0.0109, -0.0185,  0.0237],\n",
      "        [ 0.0128, -0.0095,  0.0060,  ..., -0.0399, -0.0053,  0.0060],\n",
      "        ...,\n",
      "        [-0.0082,  0.0422,  0.0106,  ...,  0.0374, -0.0248, -0.0137],\n",
      "        [ 0.0036, -0.0160,  0.0151,  ...,  0.0148, -0.0083,  0.0175],\n",
      "        [ 0.0206, -0.0018, -0.0490,  ..., -0.0499, -0.0334, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0158, -0.0304, -0.0129,  ..., -0.0246, -0.0131,  0.0005],\n",
      "        [ 0.0631, -0.0081, -0.0165,  ..., -0.0111, -0.0186,  0.0239],\n",
      "        [ 0.0130, -0.0097,  0.0058,  ..., -0.0401, -0.0054,  0.0062],\n",
      "        ...,\n",
      "        [-0.0083,  0.0422,  0.0107,  ...,  0.0374, -0.0249, -0.0138],\n",
      "        [ 0.0036, -0.0161,  0.0151,  ...,  0.0148, -0.0083,  0.0176],\n",
      "        [ 0.0205, -0.0018, -0.0490,  ..., -0.0499, -0.0334, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0160, -0.0305, -0.0131,  ..., -0.0248, -0.0132,  0.0007],\n",
      "        [ 0.0633, -0.0082, -0.0167,  ..., -0.0113, -0.0188,  0.0241],\n",
      "        [ 0.0131, -0.0098,  0.0056,  ..., -0.0403, -0.0055,  0.0064],\n",
      "        ...,\n",
      "        [-0.0083,  0.0422,  0.0107,  ...,  0.0374, -0.0249, -0.0138],\n",
      "        [ 0.0035, -0.0162,  0.0150,  ...,  0.0147, -0.0083,  0.0177],\n",
      "        [ 0.0205, -0.0018, -0.0489,  ..., -0.0499, -0.0335, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0161, -0.0307, -0.0132,  ..., -0.0250, -0.0133,  0.0008],\n",
      "        [ 0.0635, -0.0084, -0.0168,  ..., -0.0115, -0.0189,  0.0242],\n",
      "        [ 0.0133, -0.0100,  0.0055,  ..., -0.0405, -0.0057,  0.0065],\n",
      "        ...,\n",
      "        [-0.0083,  0.0422,  0.0108,  ...,  0.0375, -0.0249, -0.0138],\n",
      "        [ 0.0035, -0.0162,  0.0150,  ...,  0.0147, -0.0084,  0.0178],\n",
      "        [ 0.0204, -0.0018, -0.0489,  ..., -0.0499, -0.0335, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0163, -0.0308, -0.0134,  ..., -0.0252, -0.0134,  0.0010],\n",
      "        [ 0.0637, -0.0085, -0.0170,  ..., -0.0116, -0.0190,  0.0244],\n",
      "        [ 0.0135, -0.0101,  0.0053,  ..., -0.0406, -0.0058,  0.0067],\n",
      "        ...,\n",
      "        [-0.0084,  0.0422,  0.0108,  ...,  0.0375, -0.0249, -0.0139],\n",
      "        [ 0.0035, -0.0163,  0.0149,  ...,  0.0146, -0.0084,  0.0179],\n",
      "        [ 0.0204, -0.0018, -0.0489,  ..., -0.0499, -0.0336, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0165, -0.0309, -0.0135,  ..., -0.0253, -0.0135,  0.0012],\n",
      "        [ 0.0638, -0.0086, -0.0171,  ..., -0.0118, -0.0191,  0.0246],\n",
      "        [ 0.0137, -0.0103,  0.0052,  ..., -0.0408, -0.0059,  0.0068],\n",
      "        ...,\n",
      "        [-0.0084,  0.0422,  0.0109,  ...,  0.0375, -0.0249, -0.0139],\n",
      "        [ 0.0035, -0.0164,  0.0149,  ...,  0.0146, -0.0084,  0.0181],\n",
      "        [ 0.0204, -0.0018, -0.0489,  ..., -0.0499, -0.0336, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0167, -0.0311, -0.0137,  ..., -0.0255, -0.0137,  0.0013],\n",
      "        [ 0.0640, -0.0088, -0.0173,  ..., -0.0120, -0.0192,  0.0247],\n",
      "        [ 0.0139, -0.0104,  0.0050,  ..., -0.0410, -0.0060,  0.0070],\n",
      "        ...,\n",
      "        [-0.0085,  0.0421,  0.0109,  ...,  0.0375, -0.0250, -0.0139],\n",
      "        [ 0.0035, -0.0165,  0.0149,  ...,  0.0145, -0.0085,  0.0181],\n",
      "        [ 0.0204, -0.0018, -0.0489,  ..., -0.0499, -0.0337, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0169, -0.0313, -0.0138,  ..., -0.0257, -0.0138,  0.0015],\n",
      "        [ 0.0642, -0.0090, -0.0174,  ..., -0.0122, -0.0194,  0.0249],\n",
      "        [ 0.0141, -0.0106,  0.0049,  ..., -0.0412, -0.0061,  0.0072],\n",
      "        ...,\n",
      "        [-0.0086,  0.0421,  0.0110,  ...,  0.0375, -0.0250, -0.0139],\n",
      "        [ 0.0034, -0.0166,  0.0149,  ...,  0.0144, -0.0085,  0.0182],\n",
      "        [ 0.0204, -0.0018, -0.0488,  ..., -0.0500, -0.0337, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0171, -0.0314, -0.0140,  ..., -0.0259, -0.0139,  0.0017],\n",
      "        [ 0.0644, -0.0091, -0.0176,  ..., -0.0123, -0.0195,  0.0251],\n",
      "        [ 0.0142, -0.0107,  0.0047,  ..., -0.0414, -0.0062,  0.0074],\n",
      "        ...,\n",
      "        [-0.0087,  0.0420,  0.0111,  ...,  0.0376, -0.0250, -0.0139],\n",
      "        [ 0.0034, -0.0166,  0.0149,  ...,  0.0143, -0.0086,  0.0183],\n",
      "        [ 0.0203, -0.0017, -0.0488,  ..., -0.0500, -0.0338, -0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0173, -0.0315, -0.0141,  ..., -0.0261, -0.0140,  0.0019],\n",
      "        [ 0.0645, -0.0092, -0.0177,  ..., -0.0125, -0.0196,  0.0252],\n",
      "        [ 0.0144, -0.0108,  0.0046,  ..., -0.0415, -0.0063,  0.0075],\n",
      "        ...,\n",
      "        [-0.0088,  0.0420,  0.0111,  ...,  0.0376, -0.0251, -0.0139],\n",
      "        [ 0.0034, -0.0167,  0.0150,  ...,  0.0143, -0.0086,  0.0184],\n",
      "        [ 0.0203, -0.0017, -0.0487,  ..., -0.0500, -0.0338, -0.0113]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0174, -0.0316, -0.0142,  ..., -0.0262, -0.0141,  0.0020],\n",
      "        [ 0.0647, -0.0093, -0.0178,  ..., -0.0126, -0.0196,  0.0254],\n",
      "        [ 0.0145, -0.0109,  0.0045,  ..., -0.0417, -0.0064,  0.0077],\n",
      "        ...,\n",
      "        [-0.0089,  0.0419,  0.0112,  ...,  0.0376, -0.0251, -0.0139],\n",
      "        [ 0.0035, -0.0167,  0.0150,  ...,  0.0142, -0.0086,  0.0185],\n",
      "        [ 0.0203, -0.0017, -0.0487,  ..., -0.0500, -0.0338, -0.0113]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0176, -0.0318, -0.0143,  ..., -0.0264, -0.0142,  0.0022],\n",
      "        [ 0.0648, -0.0094, -0.0179,  ..., -0.0128, -0.0197,  0.0255],\n",
      "        [ 0.0147, -0.0110,  0.0044,  ..., -0.0418, -0.0065,  0.0078],\n",
      "        ...,\n",
      "        [-0.0090,  0.0418,  0.0112,  ...,  0.0376, -0.0251, -0.0139],\n",
      "        [ 0.0035, -0.0168,  0.0150,  ...,  0.0141, -0.0087,  0.0185],\n",
      "        [ 0.0203, -0.0017, -0.0486,  ..., -0.0500, -0.0339, -0.0113]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0177, -0.0319, -0.0145,  ..., -0.0266, -0.0143,  0.0023],\n",
      "        [ 0.0650, -0.0095, -0.0181,  ..., -0.0129, -0.0198,  0.0256],\n",
      "        [ 0.0148, -0.0111,  0.0043,  ..., -0.0420, -0.0066,  0.0079],\n",
      "        ...,\n",
      "        [-0.0090,  0.0418,  0.0113,  ...,  0.0376, -0.0251, -0.0139],\n",
      "        [ 0.0035, -0.0169,  0.0151,  ...,  0.0140, -0.0087,  0.0186],\n",
      "        [ 0.0203, -0.0017, -0.0486,  ..., -0.0500, -0.0340, -0.0114]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0179, -0.0320, -0.0146,  ..., -0.0267, -0.0144,  0.0025],\n",
      "        [ 0.0651, -0.0096, -0.0182,  ..., -0.0131, -0.0198,  0.0258],\n",
      "        [ 0.0150, -0.0113,  0.0041,  ..., -0.0421, -0.0066,  0.0081],\n",
      "        ...,\n",
      "        [-0.0091,  0.0417,  0.0113,  ...,  0.0376, -0.0252, -0.0139],\n",
      "        [ 0.0035, -0.0169,  0.0151,  ...,  0.0140, -0.0087,  0.0187],\n",
      "        [ 0.0203, -0.0017, -0.0485,  ..., -0.0500, -0.0340, -0.0114]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "| epoch   1 step      200 |    200 batches | lr 0.0001 | ms/batch 30.45088 | loss 2.94338 | ppl    18.980\n",
      "Parameter containing:\n",
      "tensor([[ 0.0181, -0.0321, -0.0147,  ..., -0.0269, -0.0145,  0.0027],\n",
      "        [ 0.0653, -0.0097, -0.0183,  ..., -0.0132, -0.0199,  0.0260],\n",
      "        [ 0.0151, -0.0114,  0.0040,  ..., -0.0423, -0.0067,  0.0083],\n",
      "        ...,\n",
      "        [-0.0092,  0.0417,  0.0114,  ...,  0.0376, -0.0252, -0.0139],\n",
      "        [ 0.0035, -0.0170,  0.0151,  ...,  0.0139, -0.0087,  0.0188],\n",
      "        [ 0.0202, -0.0017, -0.0485,  ..., -0.0500, -0.0341, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0183, -0.0323, -0.0149,  ..., -0.0271, -0.0146,  0.0029],\n",
      "        [ 0.0654, -0.0098, -0.0184,  ..., -0.0134, -0.0200,  0.0261],\n",
      "        [ 0.0153, -0.0115,  0.0039,  ..., -0.0424, -0.0068,  0.0084],\n",
      "        ...,\n",
      "        [-0.0092,  0.0417,  0.0115,  ...,  0.0376, -0.0252, -0.0138],\n",
      "        [ 0.0035, -0.0171,  0.0152,  ...,  0.0139, -0.0086,  0.0189],\n",
      "        [ 0.0202, -0.0017, -0.0485,  ..., -0.0500, -0.0341, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0185, -0.0324, -0.0150,  ..., -0.0273, -0.0147,  0.0031],\n",
      "        [ 0.0656, -0.0100, -0.0185,  ..., -0.0135, -0.0201,  0.0263],\n",
      "        [ 0.0154, -0.0116,  0.0037,  ..., -0.0426, -0.0069,  0.0086],\n",
      "        ...,\n",
      "        [-0.0092,  0.0416,  0.0115,  ...,  0.0376, -0.0253, -0.0138],\n",
      "        [ 0.0035, -0.0172,  0.0152,  ...,  0.0138, -0.0086,  0.0190],\n",
      "        [ 0.0202, -0.0018, -0.0485,  ..., -0.0500, -0.0342, -0.0116]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0186, -0.0325, -0.0151,  ..., -0.0274, -0.0148,  0.0032],\n",
      "        [ 0.0657, -0.0100, -0.0186,  ..., -0.0137, -0.0201,  0.0264],\n",
      "        [ 0.0156, -0.0117,  0.0037,  ..., -0.0427, -0.0070,  0.0087],\n",
      "        ...,\n",
      "        [-0.0093,  0.0416,  0.0116,  ...,  0.0376, -0.0253, -0.0138],\n",
      "        [ 0.0035, -0.0173,  0.0152,  ...,  0.0137, -0.0086,  0.0191],\n",
      "        [ 0.0201, -0.0018, -0.0485,  ..., -0.0500, -0.0343, -0.0116]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0188, -0.0326, -0.0152,  ..., -0.0276, -0.0148,  0.0034],\n",
      "        [ 0.0659, -0.0101, -0.0187,  ..., -0.0138, -0.0202,  0.0266],\n",
      "        [ 0.0157, -0.0117,  0.0035,  ..., -0.0429, -0.0070,  0.0089],\n",
      "        ...,\n",
      "        [-0.0093,  0.0416,  0.0118,  ...,  0.0376, -0.0254, -0.0138],\n",
      "        [ 0.0034, -0.0174,  0.0152,  ...,  0.0136, -0.0086,  0.0192],\n",
      "        [ 0.0201, -0.0018, -0.0484,  ..., -0.0500, -0.0343, -0.0117]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0189, -0.0326, -0.0153,  ..., -0.0277, -0.0149,  0.0035],\n",
      "        [ 0.0660, -0.0102, -0.0188,  ..., -0.0139, -0.0202,  0.0267],\n",
      "        [ 0.0159, -0.0118,  0.0035,  ..., -0.0430, -0.0071,  0.0090],\n",
      "        ...,\n",
      "        [-0.0093,  0.0416,  0.0119,  ...,  0.0376, -0.0255, -0.0138],\n",
      "        [ 0.0034, -0.0175,  0.0152,  ...,  0.0135, -0.0086,  0.0193],\n",
      "        [ 0.0201, -0.0018, -0.0484,  ..., -0.0500, -0.0344, -0.0117]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0191, -0.0327, -0.0154,  ..., -0.0278, -0.0149,  0.0037],\n",
      "        [ 0.0661, -0.0102, -0.0189,  ..., -0.0140, -0.0202,  0.0268],\n",
      "        [ 0.0160, -0.0118,  0.0034,  ..., -0.0431, -0.0071,  0.0091],\n",
      "        ...,\n",
      "        [-0.0094,  0.0416,  0.0120,  ...,  0.0376, -0.0255, -0.0137],\n",
      "        [ 0.0033, -0.0176,  0.0152,  ...,  0.0134, -0.0087,  0.0193],\n",
      "        [ 0.0200, -0.0018, -0.0484,  ..., -0.0500, -0.0344, -0.0117]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0192, -0.0328, -0.0155,  ..., -0.0280, -0.0150,  0.0038],\n",
      "        [ 0.0662, -0.0103, -0.0190,  ..., -0.0141, -0.0203,  0.0269],\n",
      "        [ 0.0161, -0.0119,  0.0033,  ..., -0.0432, -0.0071,  0.0093],\n",
      "        ...,\n",
      "        [-0.0094,  0.0415,  0.0121,  ...,  0.0376, -0.0256, -0.0137],\n",
      "        [ 0.0033, -0.0177,  0.0152,  ...,  0.0133, -0.0087,  0.0194],\n",
      "        [ 0.0200, -0.0018, -0.0484,  ..., -0.0501, -0.0345, -0.0117]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0194, -0.0329, -0.0156,  ..., -0.0281, -0.0151,  0.0040],\n",
      "        [ 0.0664, -0.0103, -0.0190,  ..., -0.0143, -0.0203,  0.0271],\n",
      "        [ 0.0163, -0.0120,  0.0032,  ..., -0.0433, -0.0072,  0.0094],\n",
      "        ...,\n",
      "        [-0.0095,  0.0415,  0.0122,  ...,  0.0376, -0.0256, -0.0137],\n",
      "        [ 0.0033, -0.0178,  0.0152,  ...,  0.0133, -0.0087,  0.0195],\n",
      "        [ 0.0199, -0.0018, -0.0484,  ..., -0.0501, -0.0345, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0196, -0.0330, -0.0157,  ..., -0.0283, -0.0151,  0.0041],\n",
      "        [ 0.0665, -0.0104, -0.0191,  ..., -0.0144, -0.0204,  0.0272],\n",
      "        [ 0.0164, -0.0121,  0.0031,  ..., -0.0435, -0.0072,  0.0095],\n",
      "        ...,\n",
      "        [-0.0095,  0.0415,  0.0123,  ...,  0.0375, -0.0256, -0.0137],\n",
      "        [ 0.0032, -0.0179,  0.0152,  ...,  0.0132, -0.0087,  0.0196],\n",
      "        [ 0.0199, -0.0018, -0.0484,  ..., -0.0501, -0.0346, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0198, -0.0331, -0.0159,  ..., -0.0285, -0.0152,  0.0043],\n",
      "        [ 0.0667, -0.0105, -0.0192,  ..., -0.0145, -0.0204,  0.0274],\n",
      "        [ 0.0166, -0.0122,  0.0030,  ..., -0.0436, -0.0073,  0.0097],\n",
      "        ...,\n",
      "        [-0.0096,  0.0414,  0.0125,  ...,  0.0376, -0.0257, -0.0137],\n",
      "        [ 0.0032, -0.0180,  0.0152,  ...,  0.0131, -0.0088,  0.0197],\n",
      "        [ 0.0199, -0.0019, -0.0484,  ..., -0.0502, -0.0346, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0199, -0.0332, -0.0160,  ..., -0.0286, -0.0153,  0.0045],\n",
      "        [ 0.0668, -0.0106, -0.0194,  ..., -0.0147, -0.0205,  0.0275],\n",
      "        [ 0.0167, -0.0123,  0.0029,  ..., -0.0438, -0.0074,  0.0099],\n",
      "        ...,\n",
      "        [-0.0096,  0.0414,  0.0126,  ...,  0.0376, -0.0257, -0.0137],\n",
      "        [ 0.0032, -0.0181,  0.0152,  ...,  0.0130, -0.0088,  0.0198],\n",
      "        [ 0.0199, -0.0018, -0.0484,  ..., -0.0502, -0.0347, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0201, -0.0333, -0.0161,  ..., -0.0288, -0.0154,  0.0047],\n",
      "        [ 0.0670, -0.0107, -0.0195,  ..., -0.0148, -0.0206,  0.0277],\n",
      "        [ 0.0169, -0.0124,  0.0028,  ..., -0.0439, -0.0075,  0.0100],\n",
      "        ...,\n",
      "        [-0.0096,  0.0414,  0.0126,  ...,  0.0376, -0.0257, -0.0137],\n",
      "        [ 0.0031, -0.0182,  0.0152,  ...,  0.0129, -0.0089,  0.0199],\n",
      "        [ 0.0198, -0.0018, -0.0484,  ..., -0.0502, -0.0348, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0203, -0.0334, -0.0162,  ..., -0.0290, -0.0155,  0.0048],\n",
      "        [ 0.0672, -0.0108, -0.0196,  ..., -0.0150, -0.0206,  0.0278],\n",
      "        [ 0.0171, -0.0125,  0.0027,  ..., -0.0441, -0.0075,  0.0102],\n",
      "        ...,\n",
      "        [-0.0097,  0.0413,  0.0127,  ...,  0.0377, -0.0257, -0.0137],\n",
      "        [ 0.0031, -0.0184,  0.0152,  ...,  0.0128, -0.0089,  0.0199],\n",
      "        [ 0.0198, -0.0019, -0.0484,  ..., -0.0502, -0.0348, -0.0119]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0205, -0.0335, -0.0163,  ..., -0.0291, -0.0156,  0.0050],\n",
      "        [ 0.0673, -0.0109, -0.0197,  ..., -0.0151, -0.0207,  0.0280],\n",
      "        [ 0.0172, -0.0126,  0.0026,  ..., -0.0442, -0.0076,  0.0103],\n",
      "        ...,\n",
      "        [-0.0097,  0.0413,  0.0128,  ...,  0.0377, -0.0257, -0.0137],\n",
      "        [ 0.0031, -0.0185,  0.0151,  ...,  0.0127, -0.0089,  0.0201],\n",
      "        [ 0.0198, -0.0018, -0.0484,  ..., -0.0503, -0.0348, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0206, -0.0336, -0.0164,  ..., -0.0293, -0.0156,  0.0051],\n",
      "        [ 0.0675, -0.0109, -0.0197,  ..., -0.0152, -0.0207,  0.0281],\n",
      "        [ 0.0174, -0.0126,  0.0025,  ..., -0.0443, -0.0076,  0.0105],\n",
      "        ...,\n",
      "        [-0.0097,  0.0413,  0.0129,  ...,  0.0378, -0.0257, -0.0137],\n",
      "        [ 0.0030, -0.0186,  0.0151,  ...,  0.0125, -0.0090,  0.0202],\n",
      "        [ 0.0197, -0.0018, -0.0484,  ..., -0.0503, -0.0349, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0208, -0.0337, -0.0165,  ..., -0.0294, -0.0156,  0.0053],\n",
      "        [ 0.0676, -0.0110, -0.0198,  ..., -0.0154, -0.0207,  0.0282],\n",
      "        [ 0.0175, -0.0127,  0.0024,  ..., -0.0445, -0.0076,  0.0106],\n",
      "        ...,\n",
      "        [-0.0098,  0.0413,  0.0130,  ...,  0.0379, -0.0257, -0.0137],\n",
      "        [ 0.0030, -0.0188,  0.0151,  ...,  0.0124, -0.0090,  0.0203],\n",
      "        [ 0.0197, -0.0018, -0.0484,  ..., -0.0504, -0.0349, -0.0119]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0210, -0.0338, -0.0166,  ..., -0.0296, -0.0157,  0.0055],\n",
      "        [ 0.0678, -0.0111, -0.0199,  ..., -0.0155, -0.0208,  0.0284],\n",
      "        [ 0.0177, -0.0128,  0.0023,  ..., -0.0446, -0.0077,  0.0108],\n",
      "        ...,\n",
      "        [-0.0098,  0.0413,  0.0131,  ...,  0.0379, -0.0258, -0.0137],\n",
      "        [ 0.0029, -0.0189,  0.0151,  ...,  0.0123, -0.0091,  0.0204],\n",
      "        [ 0.0197, -0.0018, -0.0483,  ..., -0.0504, -0.0350, -0.0119]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0212, -0.0339, -0.0167,  ..., -0.0298, -0.0158,  0.0056],\n",
      "        [ 0.0679, -0.0112, -0.0200,  ..., -0.0157, -0.0209,  0.0285],\n",
      "        [ 0.0179, -0.0129,  0.0022,  ..., -0.0448, -0.0077,  0.0109],\n",
      "        ...,\n",
      "        [-0.0099,  0.0413,  0.0132,  ...,  0.0380, -0.0258, -0.0137],\n",
      "        [ 0.0029, -0.0190,  0.0151,  ...,  0.0122, -0.0091,  0.0206],\n",
      "        [ 0.0197, -0.0018, -0.0483,  ..., -0.0504, -0.0351, -0.0119]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0214, -0.0341, -0.0168,  ..., -0.0300, -0.0159,  0.0058],\n",
      "        [ 0.0681, -0.0113, -0.0201,  ..., -0.0158, -0.0209,  0.0287],\n",
      "        [ 0.0181, -0.0130,  0.0021,  ..., -0.0450, -0.0078,  0.0111],\n",
      "        ...,\n",
      "        [-0.0100,  0.0413,  0.0132,  ...,  0.0381, -0.0258, -0.0137],\n",
      "        [ 0.0028, -0.0190,  0.0150,  ...,  0.0121, -0.0092,  0.0207],\n",
      "        [ 0.0196, -0.0018, -0.0483,  ..., -0.0504, -0.0351, -0.0119]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0216, -0.0342, -0.0170,  ..., -0.0302, -0.0160,  0.0060],\n",
      "        [ 0.0683, -0.0114, -0.0202,  ..., -0.0160, -0.0210,  0.0289],\n",
      "        [ 0.0182, -0.0131,  0.0020,  ..., -0.0451, -0.0079,  0.0113],\n",
      "        ...,\n",
      "        [-0.0101,  0.0413,  0.0133,  ...,  0.0381, -0.0258, -0.0137],\n",
      "        [ 0.0027, -0.0191,  0.0150,  ...,  0.0120, -0.0093,  0.0208],\n",
      "        [ 0.0196, -0.0018, -0.0483,  ..., -0.0504, -0.0352, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0218, -0.0344, -0.0171,  ..., -0.0304, -0.0161,  0.0062],\n",
      "        [ 0.0685, -0.0115, -0.0203,  ..., -0.0162, -0.0210,  0.0290],\n",
      "        [ 0.0184, -0.0133,  0.0019,  ..., -0.0453, -0.0080,  0.0115],\n",
      "        ...,\n",
      "        [-0.0101,  0.0413,  0.0134,  ...,  0.0382, -0.0259, -0.0137],\n",
      "        [ 0.0026, -0.0191,  0.0150,  ...,  0.0119, -0.0093,  0.0209],\n",
      "        [ 0.0195, -0.0018, -0.0483,  ..., -0.0504, -0.0352, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0220, -0.0345, -0.0172,  ..., -0.0306, -0.0162,  0.0064],\n",
      "        [ 0.0687, -0.0117, -0.0204,  ..., -0.0164, -0.0211,  0.0292],\n",
      "        [ 0.0186, -0.0134,  0.0018,  ..., -0.0455, -0.0081,  0.0116],\n",
      "        ...,\n",
      "        [-0.0102,  0.0413,  0.0135,  ...,  0.0382, -0.0259, -0.0137],\n",
      "        [ 0.0025, -0.0192,  0.0150,  ...,  0.0119, -0.0094,  0.0210],\n",
      "        [ 0.0195, -0.0018, -0.0483,  ..., -0.0504, -0.0352, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0222, -0.0347, -0.0174,  ..., -0.0308, -0.0163,  0.0066],\n",
      "        [ 0.0689, -0.0118, -0.0205,  ..., -0.0165, -0.0212,  0.0294],\n",
      "        [ 0.0188, -0.0135,  0.0017,  ..., -0.0457, -0.0082,  0.0118],\n",
      "        ...,\n",
      "        [-0.0103,  0.0412,  0.0136,  ...,  0.0382, -0.0259, -0.0137],\n",
      "        [ 0.0024, -0.0192,  0.0150,  ...,  0.0118, -0.0095,  0.0211],\n",
      "        [ 0.0194, -0.0018, -0.0483,  ..., -0.0504, -0.0353, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0224, -0.0348, -0.0175,  ..., -0.0310, -0.0164,  0.0069],\n",
      "        [ 0.0690, -0.0119, -0.0207,  ..., -0.0167, -0.0213,  0.0296],\n",
      "        [ 0.0190, -0.0137,  0.0015,  ..., -0.0459, -0.0082,  0.0120],\n",
      "        ...,\n",
      "        [-0.0103,  0.0412,  0.0136,  ...,  0.0383, -0.0259, -0.0137],\n",
      "        [ 0.0023, -0.0192,  0.0150,  ...,  0.0117, -0.0095,  0.0213],\n",
      "        [ 0.0194, -0.0018, -0.0483,  ..., -0.0505, -0.0353, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0227, -0.0350, -0.0177,  ..., -0.0312, -0.0165,  0.0071],\n",
      "        [ 0.0692, -0.0120, -0.0208,  ..., -0.0169, -0.0214,  0.0297],\n",
      "        [ 0.0192, -0.0138,  0.0014,  ..., -0.0461, -0.0083,  0.0122],\n",
      "        ...,\n",
      "        [-0.0104,  0.0412,  0.0137,  ...,  0.0383, -0.0259, -0.0137],\n",
      "        [ 0.0023, -0.0193,  0.0151,  ...,  0.0116, -0.0096,  0.0214],\n",
      "        [ 0.0193, -0.0018, -0.0484,  ..., -0.0505, -0.0354, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0229, -0.0351, -0.0178,  ..., -0.0314, -0.0166,  0.0073],\n",
      "        [ 0.0694, -0.0122, -0.0209,  ..., -0.0170, -0.0214,  0.0299],\n",
      "        [ 0.0194, -0.0139,  0.0013,  ..., -0.0462, -0.0084,  0.0124],\n",
      "        ...,\n",
      "        [-0.0105,  0.0411,  0.0137,  ...,  0.0384, -0.0259, -0.0137],\n",
      "        [ 0.0023, -0.0193,  0.0151,  ...,  0.0115, -0.0097,  0.0215],\n",
      "        [ 0.0193, -0.0018, -0.0484,  ..., -0.0505, -0.0354, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0231, -0.0353, -0.0179,  ..., -0.0316, -0.0167,  0.0075],\n",
      "        [ 0.0696, -0.0123, -0.0210,  ..., -0.0172, -0.0215,  0.0301],\n",
      "        [ 0.0196, -0.0141,  0.0012,  ..., -0.0464, -0.0085,  0.0126],\n",
      "        ...,\n",
      "        [-0.0105,  0.0411,  0.0138,  ...,  0.0385, -0.0260, -0.0137],\n",
      "        [ 0.0023, -0.0193,  0.0152,  ...,  0.0114, -0.0098,  0.0216],\n",
      "        [ 0.0192, -0.0018, -0.0484,  ..., -0.0505, -0.0355, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0232, -0.0354, -0.0181,  ..., -0.0317, -0.0168,  0.0076],\n",
      "        [ 0.0697, -0.0124, -0.0211,  ..., -0.0174, -0.0216,  0.0303],\n",
      "        [ 0.0197, -0.0142,  0.0011,  ..., -0.0465, -0.0086,  0.0127],\n",
      "        ...,\n",
      "        [-0.0106,  0.0411,  0.0139,  ...,  0.0385, -0.0260, -0.0136],\n",
      "        [ 0.0023, -0.0194,  0.0152,  ...,  0.0114, -0.0098,  0.0217],\n",
      "        [ 0.0192, -0.0019, -0.0484,  ..., -0.0506, -0.0355, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0234, -0.0355, -0.0182,  ..., -0.0319, -0.0169,  0.0078],\n",
      "        [ 0.0699, -0.0125, -0.0212,  ..., -0.0175, -0.0216,  0.0304],\n",
      "        [ 0.0199, -0.0143,  0.0010,  ..., -0.0467, -0.0086,  0.0129],\n",
      "        ...,\n",
      "        [-0.0106,  0.0411,  0.0139,  ...,  0.0385, -0.0260, -0.0136],\n",
      "        [ 0.0023, -0.0194,  0.0153,  ...,  0.0113, -0.0099,  0.0217],\n",
      "        [ 0.0192, -0.0019, -0.0484,  ..., -0.0506, -0.0356, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0236, -0.0356, -0.0183,  ..., -0.0320, -0.0169,  0.0080],\n",
      "        [ 0.0700, -0.0126, -0.0213,  ..., -0.0176, -0.0217,  0.0306],\n",
      "        [ 0.0200, -0.0144,  0.0009,  ..., -0.0468, -0.0087,  0.0130],\n",
      "        ...,\n",
      "        [-0.0106,  0.0411,  0.0140,  ...,  0.0386, -0.0260, -0.0136],\n",
      "        [ 0.0023, -0.0194,  0.0153,  ...,  0.0112, -0.0100,  0.0218],\n",
      "        [ 0.0192, -0.0019, -0.0484,  ..., -0.0506, -0.0357, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0237, -0.0358, -0.0184,  ..., -0.0322, -0.0170,  0.0082],\n",
      "        [ 0.0702, -0.0127, -0.0214,  ..., -0.0178, -0.0218,  0.0307],\n",
      "        [ 0.0202, -0.0145,  0.0008,  ..., -0.0470, -0.0088,  0.0132],\n",
      "        ...,\n",
      "        [-0.0107,  0.0410,  0.0141,  ...,  0.0386, -0.0260, -0.0136],\n",
      "        [ 0.0023, -0.0194,  0.0153,  ...,  0.0111, -0.0101,  0.0218],\n",
      "        [ 0.0192, -0.0019, -0.0484,  ..., -0.0507, -0.0357, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0239, -0.0359, -0.0185,  ..., -0.0324, -0.0171,  0.0084],\n",
      "        [ 0.0704, -0.0129, -0.0215,  ..., -0.0179, -0.0218,  0.0309],\n",
      "        [ 0.0203, -0.0147,  0.0006,  ..., -0.0471, -0.0088,  0.0134],\n",
      "        ...,\n",
      "        [-0.0107,  0.0410,  0.0141,  ...,  0.0386, -0.0260, -0.0136],\n",
      "        [ 0.0022, -0.0195,  0.0153,  ...,  0.0111, -0.0101,  0.0219],\n",
      "        [ 0.0192, -0.0019, -0.0484,  ..., -0.0507, -0.0358, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0241, -0.0361, -0.0187,  ..., -0.0326, -0.0172,  0.0085],\n",
      "        [ 0.0705, -0.0130, -0.0216,  ..., -0.0181, -0.0219,  0.0311],\n",
      "        [ 0.0205, -0.0148,  0.0005,  ..., -0.0473, -0.0089,  0.0136],\n",
      "        ...,\n",
      "        [-0.0107,  0.0410,  0.0142,  ...,  0.0387, -0.0260, -0.0136],\n",
      "        [ 0.0023, -0.0195,  0.0154,  ...,  0.0110, -0.0102,  0.0220],\n",
      "        [ 0.0192, -0.0019, -0.0483,  ..., -0.0507, -0.0358, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0242, -0.0362, -0.0188,  ..., -0.0327, -0.0172,  0.0087],\n",
      "        [ 0.0707, -0.0131, -0.0217,  ..., -0.0183, -0.0219,  0.0312],\n",
      "        [ 0.0206, -0.0149,  0.0004,  ..., -0.0474, -0.0089,  0.0137],\n",
      "        ...,\n",
      "        [-0.0107,  0.0410,  0.0143,  ...,  0.0387, -0.0260, -0.0137],\n",
      "        [ 0.0023, -0.0195,  0.0154,  ...,  0.0110, -0.0103,  0.0221],\n",
      "        [ 0.0192, -0.0018, -0.0483,  ..., -0.0507, -0.0359, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0244, -0.0364, -0.0189,  ..., -0.0329, -0.0173,  0.0089],\n",
      "        [ 0.0708, -0.0132, -0.0218,  ..., -0.0184, -0.0219,  0.0314],\n",
      "        [ 0.0208, -0.0150,  0.0003,  ..., -0.0476, -0.0090,  0.0139],\n",
      "        ...,\n",
      "        [-0.0108,  0.0409,  0.0144,  ...,  0.0386, -0.0260, -0.0137],\n",
      "        [ 0.0023, -0.0195,  0.0155,  ...,  0.0109, -0.0103,  0.0222],\n",
      "        [ 0.0191, -0.0018, -0.0482,  ..., -0.0507, -0.0360, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0246, -0.0365, -0.0190,  ..., -0.0331, -0.0173,  0.0091],\n",
      "        [ 0.0710, -0.0133, -0.0219,  ..., -0.0186, -0.0220,  0.0315],\n",
      "        [ 0.0210, -0.0152,  0.0002,  ..., -0.0478, -0.0090,  0.0140],\n",
      "        ...,\n",
      "        [-0.0108,  0.0409,  0.0145,  ...,  0.0386, -0.0261, -0.0137],\n",
      "        [ 0.0023, -0.0195,  0.0155,  ...,  0.0109, -0.0104,  0.0223],\n",
      "        [ 0.0191, -0.0018, -0.0482,  ..., -0.0507, -0.0360, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0248, -0.0367, -0.0192,  ..., -0.0333, -0.0174,  0.0093],\n",
      "        [ 0.0711, -0.0135, -0.0221,  ..., -0.0188, -0.0220,  0.0317],\n",
      "        [ 0.0211, -0.0153,  0.0001,  ..., -0.0479, -0.0091,  0.0142],\n",
      "        ...,\n",
      "        [-0.0108,  0.0408,  0.0146,  ...,  0.0386, -0.0261, -0.0137],\n",
      "        [ 0.0023, -0.0196,  0.0155,  ...,  0.0108, -0.0104,  0.0224],\n",
      "        [ 0.0190, -0.0018, -0.0481,  ..., -0.0507, -0.0361, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4965e-02, -3.6849e-02, -1.9298e-02,  ..., -3.3520e-02,\n",
      "         -1.7452e-02,  9.4922e-03],\n",
      "        [ 7.1319e-02, -1.3634e-02, -2.2176e-02,  ..., -1.8948e-02,\n",
      "         -2.2045e-02,  3.1911e-02],\n",
      "        [ 2.1296e-02, -1.5457e-02, -2.1226e-06,  ..., -4.8115e-02,\n",
      "         -9.1252e-03,  1.4411e-02],\n",
      "        ...,\n",
      "        [-1.0837e-02,  4.0790e-02,  1.4650e-02,  ...,  3.8592e-02,\n",
      "         -2.6198e-02, -1.3641e-02],\n",
      "        [ 2.2990e-03, -1.9565e-02,  1.5483e-02,  ...,  1.0789e-02,\n",
      "         -1.0452e-02,  2.2523e-02],\n",
      "        [ 1.9011e-02, -1.7641e-03, -4.8109e-02,  ..., -5.0731e-02,\n",
      "         -3.6133e-02, -1.2126e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0252, -0.0370, -0.0194,  ..., -0.0337, -0.0175,  0.0097],\n",
      "        [ 0.0715, -0.0138, -0.0223,  ..., -0.0192, -0.0221,  0.0321],\n",
      "        [ 0.0215, -0.0156, -0.0001,  ..., -0.0483, -0.0092,  0.0146],\n",
      "        ...,\n",
      "        [-0.0109,  0.0408,  0.0147,  ...,  0.0386, -0.0262, -0.0136],\n",
      "        [ 0.0023, -0.0196,  0.0155,  ...,  0.0107, -0.0105,  0.0226],\n",
      "        [ 0.0190, -0.0017, -0.0481,  ..., -0.0507, -0.0362, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0254, -0.0372, -0.0196,  ..., -0.0340, -0.0176,  0.0099],\n",
      "        [ 0.0717, -0.0140, -0.0224,  ..., -0.0194, -0.0221,  0.0323],\n",
      "        [ 0.0217, -0.0158, -0.0002,  ..., -0.0485, -0.0092,  0.0148],\n",
      "        ...,\n",
      "        [-0.0110,  0.0407,  0.0148,  ...,  0.0386, -0.0263, -0.0136],\n",
      "        [ 0.0023, -0.0196,  0.0155,  ...,  0.0106, -0.0106,  0.0227],\n",
      "        [ 0.0189, -0.0017, -0.0481,  ..., -0.0508, -0.0363, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0256, -0.0374, -0.0197,  ..., -0.0342, -0.0176,  0.0102],\n",
      "        [ 0.0720, -0.0142, -0.0226,  ..., -0.0196, -0.0222,  0.0325],\n",
      "        [ 0.0219, -0.0160, -0.0004,  ..., -0.0488, -0.0093,  0.0150],\n",
      "        ...,\n",
      "        [-0.0110,  0.0407,  0.0148,  ...,  0.0387, -0.0263, -0.0136],\n",
      "        [ 0.0022, -0.0196,  0.0156,  ...,  0.0106, -0.0107,  0.0228],\n",
      "        [ 0.0189, -0.0017, -0.0480,  ..., -0.0508, -0.0363, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0258, -0.0376, -0.0198,  ..., -0.0344, -0.0177,  0.0104],\n",
      "        [ 0.0722, -0.0143, -0.0227,  ..., -0.0198, -0.0222,  0.0327],\n",
      "        [ 0.0221, -0.0162, -0.0005,  ..., -0.0490, -0.0093,  0.0152],\n",
      "        ...,\n",
      "        [-0.0111,  0.0406,  0.0149,  ...,  0.0387, -0.0263, -0.0135],\n",
      "        [ 0.0022, -0.0197,  0.0156,  ...,  0.0105, -0.0107,  0.0229],\n",
      "        [ 0.0188, -0.0017, -0.0480,  ..., -0.0508, -0.0364, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0261, -0.0378, -0.0200,  ..., -0.0347, -0.0178,  0.0106],\n",
      "        [ 0.0724, -0.0145, -0.0228,  ..., -0.0200, -0.0222,  0.0329],\n",
      "        [ 0.0223, -0.0163, -0.0006,  ..., -0.0492, -0.0094,  0.0154],\n",
      "        ...,\n",
      "        [-0.0111,  0.0406,  0.0149,  ...,  0.0387, -0.0263, -0.0135],\n",
      "        [ 0.0022, -0.0197,  0.0156,  ...,  0.0105, -0.0108,  0.0230],\n",
      "        [ 0.0188, -0.0016, -0.0480,  ..., -0.0508, -0.0365, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0263, -0.0380, -0.0201,  ..., -0.0349, -0.0178,  0.0108],\n",
      "        [ 0.0726, -0.0147, -0.0229,  ..., -0.0202, -0.0223,  0.0331],\n",
      "        [ 0.0225, -0.0165, -0.0008,  ..., -0.0494, -0.0094,  0.0156],\n",
      "        ...,\n",
      "        [-0.0112,  0.0405,  0.0150,  ...,  0.0387, -0.0263, -0.0135],\n",
      "        [ 0.0022, -0.0198,  0.0156,  ...,  0.0104, -0.0109,  0.0230],\n",
      "        [ 0.0187, -0.0016, -0.0480,  ..., -0.0508, -0.0365, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0265, -0.0382, -0.0203,  ..., -0.0351, -0.0179,  0.0110],\n",
      "        [ 0.0727, -0.0148, -0.0231,  ..., -0.0204, -0.0223,  0.0333],\n",
      "        [ 0.0227, -0.0167, -0.0009,  ..., -0.0496, -0.0094,  0.0158],\n",
      "        ...,\n",
      "        [-0.0113,  0.0405,  0.0151,  ...,  0.0387, -0.0263, -0.0134],\n",
      "        [ 0.0022, -0.0198,  0.0156,  ...,  0.0104, -0.0110,  0.0231],\n",
      "        [ 0.0186, -0.0016, -0.0480,  ..., -0.0508, -0.0366, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0267, -0.0384, -0.0204,  ..., -0.0353, -0.0179,  0.0112],\n",
      "        [ 0.0729, -0.0150, -0.0232,  ..., -0.0206, -0.0224,  0.0335],\n",
      "        [ 0.0229, -0.0168, -0.0010,  ..., -0.0498, -0.0095,  0.0160],\n",
      "        ...,\n",
      "        [-0.0113,  0.0404,  0.0152,  ...,  0.0388, -0.0263, -0.0134],\n",
      "        [ 0.0021, -0.0199,  0.0156,  ...,  0.0103, -0.0111,  0.0231],\n",
      "        [ 0.0186, -0.0016, -0.0480,  ..., -0.0508, -0.0366, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0269, -0.0386, -0.0206,  ..., -0.0356, -0.0180,  0.0114],\n",
      "        [ 0.0731, -0.0152, -0.0233,  ..., -0.0209, -0.0224,  0.0337],\n",
      "        [ 0.0231, -0.0170, -0.0011,  ..., -0.0500, -0.0095,  0.0162],\n",
      "        ...,\n",
      "        [-0.0115,  0.0404,  0.0152,  ...,  0.0388, -0.0263, -0.0133],\n",
      "        [ 0.0021, -0.0200,  0.0156,  ...,  0.0103, -0.0112,  0.0232],\n",
      "        [ 0.0185, -0.0015, -0.0480,  ..., -0.0508, -0.0367, -0.0119]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0271, -0.0388, -0.0207,  ..., -0.0358, -0.0180,  0.0117],\n",
      "        [ 0.0733, -0.0154, -0.0235,  ..., -0.0211, -0.0224,  0.0339],\n",
      "        [ 0.0233, -0.0172, -0.0013,  ..., -0.0502, -0.0095,  0.0164],\n",
      "        ...,\n",
      "        [-0.0116,  0.0403,  0.0153,  ...,  0.0388, -0.0263, -0.0133],\n",
      "        [ 0.0021, -0.0200,  0.0156,  ...,  0.0102, -0.0113,  0.0233],\n",
      "        [ 0.0185, -0.0015, -0.0479,  ..., -0.0508, -0.0367, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0274, -0.0390, -0.0209,  ..., -0.0361, -0.0181,  0.0119],\n",
      "        [ 0.0736, -0.0156, -0.0236,  ..., -0.0213, -0.0225,  0.0342],\n",
      "        [ 0.0235, -0.0174, -0.0014,  ..., -0.0505, -0.0096,  0.0167],\n",
      "        ...,\n",
      "        [-0.0117,  0.0402,  0.0154,  ...,  0.0388, -0.0263, -0.0132],\n",
      "        [ 0.0021, -0.0201,  0.0156,  ...,  0.0102, -0.0114,  0.0234],\n",
      "        [ 0.0184, -0.0015, -0.0479,  ..., -0.0508, -0.0367, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0276, -0.0392, -0.0211,  ..., -0.0363, -0.0182,  0.0121],\n",
      "        [ 0.0738, -0.0157, -0.0238,  ..., -0.0215, -0.0225,  0.0344],\n",
      "        [ 0.0237, -0.0176, -0.0016,  ..., -0.0507, -0.0097,  0.0169],\n",
      "        ...,\n",
      "        [-0.0118,  0.0402,  0.0154,  ...,  0.0388, -0.0263, -0.0132],\n",
      "        [ 0.0021, -0.0201,  0.0156,  ...,  0.0101, -0.0115,  0.0235],\n",
      "        [ 0.0184, -0.0015, -0.0479,  ..., -0.0509, -0.0368, -0.0117]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0278, -0.0394, -0.0212,  ..., -0.0365, -0.0183,  0.0123],\n",
      "        [ 0.0740, -0.0159, -0.0239,  ..., -0.0218, -0.0226,  0.0346],\n",
      "        [ 0.0239, -0.0177, -0.0017,  ..., -0.0509, -0.0097,  0.0171],\n",
      "        ...,\n",
      "        [-0.0119,  0.0401,  0.0155,  ...,  0.0388, -0.0263, -0.0131],\n",
      "        [ 0.0021, -0.0202,  0.0157,  ...,  0.0101, -0.0115,  0.0236],\n",
      "        [ 0.0183, -0.0015, -0.0479,  ..., -0.0509, -0.0368, -0.0117]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0280, -0.0395, -0.0213,  ..., -0.0367, -0.0183,  0.0125],\n",
      "        [ 0.0741, -0.0160, -0.0240,  ..., -0.0219, -0.0226,  0.0348],\n",
      "        [ 0.0241, -0.0179, -0.0018,  ..., -0.0511, -0.0097,  0.0173],\n",
      "        ...,\n",
      "        [-0.0120,  0.0400,  0.0156,  ...,  0.0389, -0.0264, -0.0131],\n",
      "        [ 0.0021, -0.0202,  0.0157,  ...,  0.0101, -0.0116,  0.0236],\n",
      "        [ 0.0182, -0.0015, -0.0479,  ..., -0.0509, -0.0368, -0.0116]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0281, -0.0397, -0.0214,  ..., -0.0369, -0.0183,  0.0127],\n",
      "        [ 0.0743, -0.0161, -0.0241,  ..., -0.0221, -0.0226,  0.0349],\n",
      "        [ 0.0243, -0.0180, -0.0019,  ..., -0.0513, -0.0097,  0.0174],\n",
      "        ...,\n",
      "        [-0.0120,  0.0400,  0.0156,  ...,  0.0389, -0.0264, -0.0131],\n",
      "        [ 0.0021, -0.0203,  0.0158,  ...,  0.0100, -0.0117,  0.0237],\n",
      "        [ 0.0182, -0.0015, -0.0479,  ..., -0.0509, -0.0368, -0.0116]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0283, -0.0398, -0.0215,  ..., -0.0371, -0.0184,  0.0129],\n",
      "        [ 0.0744, -0.0163, -0.0242,  ..., -0.0223, -0.0226,  0.0351],\n",
      "        [ 0.0244, -0.0181, -0.0020,  ..., -0.0515, -0.0097,  0.0176],\n",
      "        ...,\n",
      "        [-0.0121,  0.0399,  0.0157,  ...,  0.0389, -0.0264, -0.0130],\n",
      "        [ 0.0021, -0.0204,  0.0158,  ...,  0.0100, -0.0118,  0.0238],\n",
      "        [ 0.0181, -0.0015, -0.0479,  ..., -0.0508, -0.0369, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0285, -0.0400, -0.0217,  ..., -0.0373, -0.0184,  0.0131],\n",
      "        [ 0.0746, -0.0164, -0.0243,  ..., -0.0225, -0.0226,  0.0353],\n",
      "        [ 0.0246, -0.0182, -0.0021,  ..., -0.0516, -0.0098,  0.0178],\n",
      "        ...,\n",
      "        [-0.0122,  0.0399,  0.0158,  ...,  0.0389, -0.0264, -0.0130],\n",
      "        [ 0.0020, -0.0205,  0.0158,  ...,  0.0100, -0.0118,  0.0239],\n",
      "        [ 0.0180, -0.0016, -0.0478,  ..., -0.0508, -0.0369, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0287, -0.0401, -0.0218,  ..., -0.0375, -0.0184,  0.0133],\n",
      "        [ 0.0747, -0.0165, -0.0244,  ..., -0.0226, -0.0226,  0.0354],\n",
      "        [ 0.0247, -0.0184, -0.0022,  ..., -0.0518, -0.0098,  0.0179],\n",
      "        ...,\n",
      "        [-0.0123,  0.0399,  0.0158,  ...,  0.0389, -0.0264, -0.0130],\n",
      "        [ 0.0020, -0.0206,  0.0159,  ...,  0.0099, -0.0119,  0.0241],\n",
      "        [ 0.0179, -0.0016, -0.0478,  ..., -0.0508, -0.0370, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0288, -0.0402, -0.0218,  ..., -0.0376, -0.0184,  0.0134],\n",
      "        [ 0.0749, -0.0166, -0.0245,  ..., -0.0228, -0.0226,  0.0356],\n",
      "        [ 0.0249, -0.0185, -0.0023,  ..., -0.0520, -0.0098,  0.0181],\n",
      "        ...,\n",
      "        [-0.0124,  0.0398,  0.0159,  ...,  0.0390, -0.0264, -0.0130],\n",
      "        [ 0.0019, -0.0207,  0.0160,  ...,  0.0099, -0.0121,  0.0242],\n",
      "        [ 0.0178, -0.0016, -0.0477,  ..., -0.0508, -0.0370, -0.0114]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0290, -0.0403, -0.0219,  ..., -0.0378, -0.0184,  0.0136],\n",
      "        [ 0.0750, -0.0167, -0.0245,  ..., -0.0229, -0.0226,  0.0357],\n",
      "        [ 0.0250, -0.0186, -0.0024,  ..., -0.0521, -0.0098,  0.0183],\n",
      "        ...,\n",
      "        [-0.0125,  0.0398,  0.0160,  ...,  0.0390, -0.0264, -0.0130],\n",
      "        [ 0.0019, -0.0208,  0.0160,  ...,  0.0098, -0.0122,  0.0243],\n",
      "        [ 0.0177, -0.0016, -0.0477,  ..., -0.0508, -0.0371, -0.0114]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0291, -0.0404, -0.0220,  ..., -0.0379, -0.0184,  0.0138],\n",
      "        [ 0.0752, -0.0168, -0.0246,  ..., -0.0231, -0.0226,  0.0359],\n",
      "        [ 0.0252, -0.0187, -0.0025,  ..., -0.0523, -0.0097,  0.0184],\n",
      "        ...,\n",
      "        [-0.0126,  0.0398,  0.0160,  ...,  0.0390, -0.0264, -0.0130],\n",
      "        [ 0.0019, -0.0209,  0.0161,  ...,  0.0098, -0.0123,  0.0245],\n",
      "        [ 0.0176, -0.0016, -0.0477,  ..., -0.0507, -0.0371, -0.0114]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0293, -0.0405, -0.0221,  ..., -0.0381, -0.0184,  0.0139],\n",
      "        [ 0.0753, -0.0169, -0.0247,  ..., -0.0232, -0.0225,  0.0361],\n",
      "        [ 0.0253, -0.0188, -0.0025,  ..., -0.0524, -0.0097,  0.0186],\n",
      "        ...,\n",
      "        [-0.0127,  0.0398,  0.0161,  ...,  0.0391, -0.0264, -0.0130],\n",
      "        [ 0.0018, -0.0210,  0.0162,  ...,  0.0098, -0.0124,  0.0246],\n",
      "        [ 0.0175, -0.0016, -0.0477,  ..., -0.0507, -0.0372, -0.0114]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0294, -0.0407, -0.0221,  ..., -0.0383, -0.0183,  0.0141],\n",
      "        [ 0.0754, -0.0170, -0.0247,  ..., -0.0234, -0.0225,  0.0362],\n",
      "        [ 0.0255, -0.0189, -0.0026,  ..., -0.0526, -0.0097,  0.0187],\n",
      "        ...,\n",
      "        [-0.0128,  0.0398,  0.0161,  ...,  0.0392, -0.0264, -0.0130],\n",
      "        [ 0.0018, -0.0210,  0.0162,  ...,  0.0098, -0.0125,  0.0248],\n",
      "        [ 0.0174, -0.0016, -0.0477,  ..., -0.0507, -0.0372, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0296, -0.0408, -0.0222,  ..., -0.0384, -0.0183,  0.0142],\n",
      "        [ 0.0756, -0.0171, -0.0248,  ..., -0.0235, -0.0225,  0.0364],\n",
      "        [ 0.0256, -0.0190, -0.0027,  ..., -0.0527, -0.0096,  0.0189],\n",
      "        ...,\n",
      "        [-0.0129,  0.0399,  0.0162,  ...,  0.0392, -0.0264, -0.0130],\n",
      "        [ 0.0018, -0.0211,  0.0163,  ...,  0.0098, -0.0126,  0.0249],\n",
      "        [ 0.0173, -0.0016, -0.0477,  ..., -0.0507, -0.0372, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0298, -0.0409, -0.0223,  ..., -0.0386, -0.0183,  0.0144],\n",
      "        [ 0.0758, -0.0173, -0.0248,  ..., -0.0237, -0.0224,  0.0365],\n",
      "        [ 0.0258, -0.0192, -0.0027,  ..., -0.0529, -0.0096,  0.0191],\n",
      "        ...,\n",
      "        [-0.0131,  0.0399,  0.0163,  ...,  0.0393, -0.0264, -0.0130],\n",
      "        [ 0.0018, -0.0212,  0.0164,  ...,  0.0097, -0.0127,  0.0251],\n",
      "        [ 0.0171, -0.0015, -0.0477,  ..., -0.0506, -0.0372, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0300, -0.0411, -0.0224,  ..., -0.0388, -0.0183,  0.0146],\n",
      "        [ 0.0759, -0.0174, -0.0249,  ..., -0.0239, -0.0224,  0.0367],\n",
      "        [ 0.0260, -0.0193, -0.0028,  ..., -0.0531, -0.0096,  0.0192],\n",
      "        ...,\n",
      "        [-0.0132,  0.0399,  0.0163,  ...,  0.0393, -0.0264, -0.0130],\n",
      "        [ 0.0017, -0.0212,  0.0165,  ...,  0.0097, -0.0128,  0.0253],\n",
      "        [ 0.0171, -0.0016, -0.0477,  ..., -0.0507, -0.0373, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0302, -0.0412, -0.0225,  ..., -0.0390, -0.0183,  0.0148],\n",
      "        [ 0.0761, -0.0175, -0.0250,  ..., -0.0240, -0.0224,  0.0369],\n",
      "        [ 0.0262, -0.0195, -0.0029,  ..., -0.0533, -0.0096,  0.0194],\n",
      "        ...,\n",
      "        [-0.0132,  0.0400,  0.0164,  ...,  0.0394, -0.0265, -0.0130],\n",
      "        [ 0.0017, -0.0213,  0.0166,  ...,  0.0096, -0.0129,  0.0254],\n",
      "        [ 0.0170, -0.0016, -0.0477,  ..., -0.0507, -0.0373, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0303, -0.0414, -0.0226,  ..., -0.0391, -0.0183,  0.0150],\n",
      "        [ 0.0763, -0.0177, -0.0251,  ..., -0.0242, -0.0224,  0.0370],\n",
      "        [ 0.0263, -0.0196, -0.0030,  ..., -0.0534, -0.0096,  0.0196],\n",
      "        ...,\n",
      "        [-0.0133,  0.0400,  0.0165,  ...,  0.0395, -0.0265, -0.0130],\n",
      "        [ 0.0017, -0.0214,  0.0166,  ...,  0.0096, -0.0130,  0.0256],\n",
      "        [ 0.0169, -0.0016, -0.0477,  ..., -0.0507, -0.0373, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0305, -0.0415, -0.0227,  ..., -0.0393, -0.0183,  0.0151],\n",
      "        [ 0.0764, -0.0178, -0.0252,  ..., -0.0243, -0.0224,  0.0372],\n",
      "        [ 0.0265, -0.0197, -0.0031,  ..., -0.0536, -0.0096,  0.0197],\n",
      "        ...,\n",
      "        [-0.0134,  0.0400,  0.0165,  ...,  0.0395, -0.0265, -0.0130],\n",
      "        [ 0.0017, -0.0214,  0.0168,  ...,  0.0095, -0.0131,  0.0257],\n",
      "        [ 0.0169, -0.0016, -0.0477,  ..., -0.0507, -0.0374, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0307, -0.0417, -0.0228,  ..., -0.0395, -0.0183,  0.0153],\n",
      "        [ 0.0766, -0.0180, -0.0253,  ..., -0.0245, -0.0224,  0.0374],\n",
      "        [ 0.0267, -0.0199, -0.0032,  ..., -0.0538, -0.0096,  0.0199],\n",
      "        ...,\n",
      "        [-0.0135,  0.0400,  0.0166,  ...,  0.0396, -0.0265, -0.0129],\n",
      "        [ 0.0017, -0.0215,  0.0169,  ...,  0.0095, -0.0132,  0.0259],\n",
      "        [ 0.0168, -0.0016, -0.0477,  ..., -0.0507, -0.0375, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0309, -0.0418, -0.0229,  ..., -0.0397, -0.0183,  0.0155],\n",
      "        [ 0.0768, -0.0181, -0.0254,  ..., -0.0247, -0.0224,  0.0375],\n",
      "        [ 0.0268, -0.0200, -0.0033,  ..., -0.0539, -0.0096,  0.0201],\n",
      "        ...,\n",
      "        [-0.0136,  0.0400,  0.0166,  ...,  0.0397, -0.0264, -0.0129],\n",
      "        [ 0.0016, -0.0215,  0.0169,  ...,  0.0095, -0.0133,  0.0260],\n",
      "        [ 0.0167, -0.0016, -0.0477,  ..., -0.0507, -0.0375, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0311, -0.0420, -0.0230,  ..., -0.0399, -0.0183,  0.0157],\n",
      "        [ 0.0770, -0.0182, -0.0254,  ..., -0.0249, -0.0224,  0.0377],\n",
      "        [ 0.0270, -0.0202, -0.0034,  ..., -0.0541, -0.0096,  0.0202],\n",
      "        ...,\n",
      "        [-0.0137,  0.0400,  0.0166,  ...,  0.0398, -0.0264, -0.0129],\n",
      "        [ 0.0016, -0.0216,  0.0171,  ...,  0.0094, -0.0134,  0.0261],\n",
      "        [ 0.0167, -0.0016, -0.0477,  ..., -0.0507, -0.0375, -0.0115]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0313, -0.0421, -0.0231,  ..., -0.0401, -0.0183,  0.0159],\n",
      "        [ 0.0771, -0.0184, -0.0255,  ..., -0.0251, -0.0224,  0.0379],\n",
      "        [ 0.0272, -0.0203, -0.0035,  ..., -0.0543, -0.0096,  0.0204],\n",
      "        ...,\n",
      "        [-0.0138,  0.0399,  0.0167,  ...,  0.0399, -0.0263, -0.0129],\n",
      "        [ 0.0016, -0.0216,  0.0172,  ...,  0.0094, -0.0135,  0.0262],\n",
      "        [ 0.0167, -0.0016, -0.0477,  ..., -0.0508, -0.0376, -0.0116]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0315, -0.0423, -0.0232,  ..., -0.0403, -0.0182,  0.0161],\n",
      "        [ 0.0773, -0.0185, -0.0256,  ..., -0.0253, -0.0223,  0.0381],\n",
      "        [ 0.0274, -0.0204, -0.0036,  ..., -0.0545, -0.0096,  0.0206],\n",
      "        ...,\n",
      "        [-0.0139,  0.0399,  0.0167,  ...,  0.0399, -0.0263, -0.0129],\n",
      "        [ 0.0017, -0.0217,  0.0173,  ...,  0.0093, -0.0137,  0.0264],\n",
      "        [ 0.0166, -0.0016, -0.0476,  ..., -0.0508, -0.0376, -0.0116]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0317, -0.0424, -0.0233,  ..., -0.0405, -0.0182,  0.0163],\n",
      "        [ 0.0775, -0.0186, -0.0258,  ..., -0.0255, -0.0223,  0.0383],\n",
      "        [ 0.0276, -0.0206, -0.0037,  ..., -0.0547, -0.0095,  0.0208],\n",
      "        ...,\n",
      "        [-0.0140,  0.0399,  0.0168,  ...,  0.0400, -0.0263, -0.0128],\n",
      "        [ 0.0016, -0.0218,  0.0174,  ...,  0.0092, -0.0137,  0.0264],\n",
      "        [ 0.0166, -0.0017, -0.0476,  ..., -0.0508, -0.0377, -0.0116]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0319, -0.0425, -0.0234,  ..., -0.0407, -0.0181,  0.0165],\n",
      "        [ 0.0777, -0.0187, -0.0259,  ..., -0.0257, -0.0223,  0.0385],\n",
      "        [ 0.0278, -0.0207, -0.0038,  ..., -0.0549, -0.0095,  0.0211],\n",
      "        ...,\n",
      "        [-0.0141,  0.0398,  0.0168,  ...,  0.0401, -0.0263, -0.0128],\n",
      "        [ 0.0017, -0.0219,  0.0175,  ...,  0.0091, -0.0139,  0.0265],\n",
      "        [ 0.0166, -0.0017, -0.0476,  ..., -0.0508, -0.0378, -0.0116]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0321, -0.0426, -0.0235,  ..., -0.0409, -0.0181,  0.0167],\n",
      "        [ 0.0779, -0.0188, -0.0259,  ..., -0.0258, -0.0222,  0.0387],\n",
      "        [ 0.0280, -0.0208, -0.0039,  ..., -0.0551, -0.0095,  0.0212],\n",
      "        ...,\n",
      "        [-0.0142,  0.0398,  0.0169,  ...,  0.0402, -0.0262, -0.0128],\n",
      "        [ 0.0017, -0.0220,  0.0176,  ...,  0.0090, -0.0140,  0.0266],\n",
      "        [ 0.0166, -0.0018, -0.0476,  ..., -0.0508, -0.0378, -0.0117]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0323, -0.0428, -0.0236,  ..., -0.0411, -0.0180,  0.0170],\n",
      "        [ 0.0781, -0.0189, -0.0260,  ..., -0.0260, -0.0222,  0.0389],\n",
      "        [ 0.0282, -0.0209, -0.0039,  ..., -0.0553, -0.0094,  0.0214],\n",
      "        ...,\n",
      "        [-0.0143,  0.0398,  0.0170,  ...,  0.0402, -0.0262, -0.0128],\n",
      "        [ 0.0018, -0.0221,  0.0178,  ...,  0.0089, -0.0141,  0.0267],\n",
      "        [ 0.0165, -0.0018, -0.0476,  ..., -0.0508, -0.0378, -0.0117]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0325, -0.0429, -0.0237,  ..., -0.0413, -0.0180,  0.0172],\n",
      "        [ 0.0783, -0.0191, -0.0261,  ..., -0.0262, -0.0222,  0.0391],\n",
      "        [ 0.0284, -0.0210, -0.0040,  ..., -0.0555, -0.0094,  0.0217],\n",
      "        ...,\n",
      "        [-0.0144,  0.0397,  0.0170,  ...,  0.0403, -0.0261, -0.0127],\n",
      "        [ 0.0019, -0.0222,  0.0179,  ...,  0.0088, -0.0142,  0.0268],\n",
      "        [ 0.0165, -0.0018, -0.0476,  ..., -0.0508, -0.0379, -0.0118]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0328, -0.0431, -0.0238,  ..., -0.0416, -0.0180,  0.0174],\n",
      "        [ 0.0786, -0.0192, -0.0262,  ..., -0.0264, -0.0221,  0.0393],\n",
      "        [ 0.0286, -0.0212, -0.0041,  ..., -0.0557, -0.0094,  0.0219],\n",
      "        ...,\n",
      "        [-0.0145,  0.0397,  0.0171,  ...,  0.0404, -0.0261, -0.0127],\n",
      "        [ 0.0019, -0.0223,  0.0180,  ...,  0.0087, -0.0143,  0.0270],\n",
      "        [ 0.0165, -0.0019, -0.0475,  ..., -0.0508, -0.0379, -0.0119]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0330, -0.0432, -0.0239,  ..., -0.0418, -0.0179,  0.0176],\n",
      "        [ 0.0788, -0.0194, -0.0263,  ..., -0.0266, -0.0221,  0.0395],\n",
      "        [ 0.0289, -0.0213, -0.0042,  ..., -0.0559, -0.0093,  0.0221],\n",
      "        ...,\n",
      "        [-0.0146,  0.0396,  0.0171,  ...,  0.0405, -0.0260, -0.0127],\n",
      "        [ 0.0020, -0.0224,  0.0181,  ...,  0.0086, -0.0145,  0.0271],\n",
      "        [ 0.0164, -0.0019, -0.0475,  ..., -0.0508, -0.0379, -0.0119]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0332, -0.0434, -0.0240,  ..., -0.0420, -0.0178,  0.0178],\n",
      "        [ 0.0790, -0.0195, -0.0264,  ..., -0.0268, -0.0220,  0.0397],\n",
      "        [ 0.0291, -0.0215, -0.0043,  ..., -0.0561, -0.0092,  0.0223],\n",
      "        ...,\n",
      "        [-0.0146,  0.0396,  0.0172,  ...,  0.0406, -0.0260, -0.0127],\n",
      "        [ 0.0020, -0.0225,  0.0182,  ...,  0.0085, -0.0146,  0.0272],\n",
      "        [ 0.0164, -0.0020, -0.0475,  ..., -0.0508, -0.0380, -0.0120]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0334, -0.0435, -0.0240,  ..., -0.0422, -0.0177,  0.0180],\n",
      "        [ 0.0792, -0.0196, -0.0265,  ..., -0.0270, -0.0220,  0.0399],\n",
      "        [ 0.0293, -0.0216, -0.0044,  ..., -0.0563, -0.0092,  0.0224],\n",
      "        ...,\n",
      "        [-0.0147,  0.0396,  0.0172,  ...,  0.0407, -0.0260, -0.0127],\n",
      "        [ 0.0021, -0.0226,  0.0183,  ...,  0.0084, -0.0147,  0.0273],\n",
      "        [ 0.0163, -0.0020, -0.0475,  ..., -0.0508, -0.0380, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0336, -0.0436, -0.0241,  ..., -0.0423, -0.0177,  0.0182],\n",
      "        [ 0.0794, -0.0197, -0.0265,  ..., -0.0272, -0.0219,  0.0401],\n",
      "        [ 0.0295, -0.0217, -0.0045,  ..., -0.0565, -0.0092,  0.0226],\n",
      "        ...,\n",
      "        [-0.0147,  0.0396,  0.0173,  ...,  0.0408, -0.0260, -0.0127],\n",
      "        [ 0.0022, -0.0226,  0.0184,  ...,  0.0082, -0.0148,  0.0274],\n",
      "        [ 0.0163, -0.0021, -0.0475,  ..., -0.0509, -0.0380, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0338, -0.0437, -0.0242,  ..., -0.0425, -0.0176,  0.0184],\n",
      "        [ 0.0796, -0.0198, -0.0266,  ..., -0.0274, -0.0219,  0.0402],\n",
      "        [ 0.0297, -0.0218, -0.0045,  ..., -0.0567, -0.0091,  0.0228],\n",
      "        ...,\n",
      "        [-0.0148,  0.0396,  0.0174,  ...,  0.0409, -0.0260, -0.0127],\n",
      "        [ 0.0022, -0.0227,  0.0185,  ...,  0.0081, -0.0150,  0.0276],\n",
      "        [ 0.0163, -0.0021, -0.0475,  ..., -0.0508, -0.0381, -0.0121]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0340, -0.0439, -0.0242,  ..., -0.0427, -0.0176,  0.0186],\n",
      "        [ 0.0798, -0.0199, -0.0266,  ..., -0.0275, -0.0218,  0.0404],\n",
      "        [ 0.0299, -0.0219, -0.0046,  ..., -0.0568, -0.0091,  0.0230],\n",
      "        ...,\n",
      "        [-0.0149,  0.0396,  0.0174,  ...,  0.0410, -0.0259, -0.0127],\n",
      "        [ 0.0023, -0.0228,  0.0186,  ...,  0.0080, -0.0151,  0.0277],\n",
      "        [ 0.0162, -0.0021, -0.0475,  ..., -0.0508, -0.0381, -0.0122]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0343, -0.0440, -0.0243,  ..., -0.0429, -0.0175,  0.0188],\n",
      "        [ 0.0800, -0.0201, -0.0267,  ..., -0.0277, -0.0218,  0.0406],\n",
      "        [ 0.0301, -0.0221, -0.0046,  ..., -0.0570, -0.0091,  0.0232],\n",
      "        ...,\n",
      "        [-0.0149,  0.0397,  0.0175,  ...,  0.0410, -0.0259, -0.0127],\n",
      "        [ 0.0023, -0.0229,  0.0187,  ...,  0.0079, -0.0152,  0.0278],\n",
      "        [ 0.0162, -0.0021, -0.0474,  ..., -0.0508, -0.0382, -0.0122]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0345, -0.0442, -0.0243,  ..., -0.0431, -0.0176,  0.0190],\n",
      "        [ 0.0802, -0.0202, -0.0268,  ..., -0.0279, -0.0218,  0.0408],\n",
      "        [ 0.0303, -0.0222, -0.0047,  ..., -0.0572, -0.0091,  0.0234],\n",
      "        ...,\n",
      "        [-0.0150,  0.0397,  0.0176,  ...,  0.0411, -0.0259, -0.0127],\n",
      "        [ 0.0023, -0.0229,  0.0188,  ...,  0.0078, -0.0153,  0.0280],\n",
      "        [ 0.0161, -0.0022, -0.0474,  ..., -0.0508, -0.0382, -0.0123]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0347, -0.0443, -0.0244,  ..., -0.0433, -0.0176,  0.0193],\n",
      "        [ 0.0804, -0.0204, -0.0268,  ..., -0.0281, -0.0218,  0.0410],\n",
      "        [ 0.0305, -0.0224, -0.0048,  ..., -0.0574, -0.0091,  0.0236],\n",
      "        ...,\n",
      "        [-0.0151,  0.0396,  0.0177,  ...,  0.0412, -0.0259, -0.0127],\n",
      "        [ 0.0023, -0.0230,  0.0188,  ...,  0.0078, -0.0153,  0.0281],\n",
      "        [ 0.0160, -0.0022, -0.0475,  ..., -0.0508, -0.0382, -0.0123]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0349, -0.0445, -0.0244,  ..., -0.0435, -0.0175,  0.0195],\n",
      "        [ 0.0806, -0.0205, -0.0268,  ..., -0.0283, -0.0218,  0.0413],\n",
      "        [ 0.0307, -0.0225, -0.0048,  ..., -0.0576, -0.0091,  0.0238],\n",
      "        ...,\n",
      "        [-0.0151,  0.0396,  0.0178,  ...,  0.0412, -0.0259, -0.0128],\n",
      "        [ 0.0023, -0.0230,  0.0189,  ...,  0.0077, -0.0154,  0.0282],\n",
      "        [ 0.0160, -0.0022, -0.0475,  ..., -0.0508, -0.0383, -0.0124]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0352, -0.0447, -0.0245,  ..., -0.0437, -0.0175,  0.0197],\n",
      "        [ 0.0809, -0.0207, -0.0269,  ..., -0.0285, -0.0218,  0.0415],\n",
      "        [ 0.0309, -0.0227, -0.0049,  ..., -0.0578, -0.0091,  0.0240],\n",
      "        ...,\n",
      "        [-0.0152,  0.0396,  0.0179,  ...,  0.0413, -0.0260, -0.0128],\n",
      "        [ 0.0023, -0.0230,  0.0190,  ...,  0.0077, -0.0155,  0.0283],\n",
      "        [ 0.0159, -0.0022, -0.0475,  ..., -0.0507, -0.0383, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0354, -0.0448, -0.0245,  ..., -0.0440, -0.0176,  0.0199],\n",
      "        [ 0.0811, -0.0209, -0.0269,  ..., -0.0287, -0.0219,  0.0417],\n",
      "        [ 0.0311, -0.0229, -0.0049,  ..., -0.0580, -0.0092,  0.0242],\n",
      "        ...,\n",
      "        [-0.0152,  0.0396,  0.0180,  ...,  0.0413, -0.0260, -0.0128],\n",
      "        [ 0.0023, -0.0230,  0.0191,  ...,  0.0077, -0.0155,  0.0285],\n",
      "        [ 0.0158, -0.0021, -0.0475,  ..., -0.0507, -0.0384, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0356, -0.0450, -0.0246,  ..., -0.0442, -0.0176,  0.0202],\n",
      "        [ 0.0813, -0.0210, -0.0270,  ..., -0.0290, -0.0219,  0.0419],\n",
      "        [ 0.0314, -0.0230, -0.0050,  ..., -0.0582, -0.0092,  0.0245],\n",
      "        ...,\n",
      "        [-0.0153,  0.0396,  0.0181,  ...,  0.0413, -0.0260, -0.0128],\n",
      "        [ 0.0022, -0.0231,  0.0192,  ...,  0.0077, -0.0155,  0.0286],\n",
      "        [ 0.0157, -0.0022, -0.0475,  ..., -0.0507, -0.0384, -0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0359, -0.0452, -0.0246,  ..., -0.0444, -0.0176,  0.0204],\n",
      "        [ 0.0816, -0.0212, -0.0270,  ..., -0.0292, -0.0219,  0.0421],\n",
      "        [ 0.0316, -0.0232, -0.0050,  ..., -0.0585, -0.0092,  0.0247],\n",
      "        ...,\n",
      "        [-0.0153,  0.0395,  0.0182,  ...,  0.0414, -0.0260, -0.0128],\n",
      "        [ 0.0022, -0.0230,  0.0192,  ...,  0.0077, -0.0156,  0.0287],\n",
      "        [ 0.0156, -0.0022, -0.0475,  ..., -0.0506, -0.0385, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0361, -0.0454, -0.0246,  ..., -0.0447, -0.0176,  0.0207],\n",
      "        [ 0.0818, -0.0214, -0.0271,  ..., -0.0294, -0.0219,  0.0424],\n",
      "        [ 0.0319, -0.0234, -0.0051,  ..., -0.0587, -0.0092,  0.0249],\n",
      "        ...,\n",
      "        [-0.0154,  0.0395,  0.0183,  ...,  0.0415, -0.0261, -0.0128],\n",
      "        [ 0.0021, -0.0230,  0.0193,  ...,  0.0077, -0.0156,  0.0288],\n",
      "        [ 0.0155, -0.0022, -0.0475,  ..., -0.0506, -0.0386, -0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0364, -0.0456, -0.0246,  ..., -0.0449, -0.0175,  0.0209],\n",
      "        [ 0.0821, -0.0216, -0.0271,  ..., -0.0297, -0.0219,  0.0426],\n",
      "        [ 0.0321, -0.0236, -0.0051,  ..., -0.0589, -0.0092,  0.0252],\n",
      "        ...,\n",
      "        [-0.0155,  0.0395,  0.0184,  ...,  0.0416, -0.0261, -0.0128],\n",
      "        [ 0.0020, -0.0231,  0.0194,  ...,  0.0077, -0.0156,  0.0289],\n",
      "        [ 0.0154, -0.0022, -0.0475,  ..., -0.0506, -0.0387, -0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0366, -0.0458, -0.0246,  ..., -0.0451, -0.0174,  0.0211],\n",
      "        [ 0.0823, -0.0218, -0.0271,  ..., -0.0299, -0.0218,  0.0428],\n",
      "        [ 0.0323, -0.0237, -0.0051,  ..., -0.0591, -0.0092,  0.0254],\n",
      "        ...,\n",
      "        [-0.0155,  0.0396,  0.0185,  ...,  0.0417, -0.0261, -0.0128],\n",
      "        [ 0.0020, -0.0231,  0.0195,  ...,  0.0076, -0.0156,  0.0290],\n",
      "        [ 0.0154, -0.0022, -0.0476,  ..., -0.0507, -0.0387, -0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0368, -0.0459, -0.0246,  ..., -0.0453, -0.0174,  0.0213],\n",
      "        [ 0.0825, -0.0219, -0.0271,  ..., -0.0301, -0.0217,  0.0430],\n",
      "        [ 0.0325, -0.0239, -0.0052,  ..., -0.0593, -0.0091,  0.0256],\n",
      "        ...,\n",
      "        [-0.0155,  0.0396,  0.0186,  ...,  0.0417, -0.0262, -0.0127],\n",
      "        [ 0.0019, -0.0232,  0.0196,  ...,  0.0075, -0.0156,  0.0292],\n",
      "        [ 0.0153, -0.0022, -0.0476,  ..., -0.0507, -0.0388, -0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0370, -0.0461, -0.0246,  ..., -0.0455, -0.0173,  0.0215],\n",
      "        [ 0.0827, -0.0221, -0.0271,  ..., -0.0303, -0.0216,  0.0432],\n",
      "        [ 0.0327, -0.0240, -0.0052,  ..., -0.0595, -0.0091,  0.0258],\n",
      "        ...,\n",
      "        [-0.0155,  0.0396,  0.0187,  ...,  0.0417, -0.0262, -0.0127],\n",
      "        [ 0.0018, -0.0232,  0.0197,  ...,  0.0075, -0.0157,  0.0293],\n",
      "        [ 0.0152, -0.0022, -0.0476,  ..., -0.0507, -0.0389, -0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0373, -0.0462, -0.0246,  ..., -0.0457, -0.0172,  0.0218],\n",
      "        [ 0.0829, -0.0222, -0.0271,  ..., -0.0305, -0.0216,  0.0434],\n",
      "        [ 0.0329, -0.0242, -0.0052,  ..., -0.0597, -0.0090,  0.0260],\n",
      "        ...,\n",
      "        [-0.0155,  0.0396,  0.0188,  ...,  0.0418, -0.0263, -0.0127],\n",
      "        [ 0.0017, -0.0233,  0.0198,  ...,  0.0074, -0.0158,  0.0294],\n",
      "        [ 0.0152, -0.0022, -0.0476,  ..., -0.0508, -0.0391, -0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0375, -0.0463, -0.0246,  ..., -0.0460, -0.0170,  0.0220],\n",
      "        [ 0.0831, -0.0223, -0.0271,  ..., -0.0307, -0.0214,  0.0436],\n",
      "        [ 0.0332, -0.0243, -0.0052,  ..., -0.0599, -0.0089,  0.0262],\n",
      "        ...,\n",
      "        [-0.0156,  0.0396,  0.0189,  ...,  0.0419, -0.0263, -0.0127],\n",
      "        [ 0.0017, -0.0234,  0.0199,  ...,  0.0073, -0.0159,  0.0296],\n",
      "        [ 0.0151, -0.0022, -0.0477,  ..., -0.0508, -0.0392, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0377, -0.0465, -0.0245,  ..., -0.0462, -0.0169,  0.0222],\n",
      "        [ 0.0834, -0.0225, -0.0271,  ..., -0.0309, -0.0213,  0.0439],\n",
      "        [ 0.0334, -0.0245, -0.0052,  ..., -0.0601, -0.0088,  0.0264],\n",
      "        ...,\n",
      "        [-0.0156,  0.0396,  0.0190,  ...,  0.0419, -0.0264, -0.0127],\n",
      "        [ 0.0016, -0.0234,  0.0200,  ...,  0.0072, -0.0160,  0.0297],\n",
      "        [ 0.0150, -0.0022, -0.0477,  ..., -0.0509, -0.0393, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0380, -0.0466, -0.0245,  ..., -0.0464, -0.0168,  0.0224],\n",
      "        [ 0.0836, -0.0226, -0.0270,  ..., -0.0311, -0.0212,  0.0441],\n",
      "        [ 0.0336, -0.0246, -0.0052,  ..., -0.0603, -0.0087,  0.0266],\n",
      "        ...,\n",
      "        [-0.0157,  0.0396,  0.0191,  ...,  0.0420, -0.0265, -0.0126],\n",
      "        [ 0.0015, -0.0235,  0.0200,  ...,  0.0071, -0.0160,  0.0298],\n",
      "        [ 0.0149, -0.0023, -0.0477,  ..., -0.0510, -0.0394, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0382, -0.0468, -0.0244,  ..., -0.0466, -0.0166,  0.0226],\n",
      "        [ 0.0839, -0.0228, -0.0270,  ..., -0.0314, -0.0210,  0.0443],\n",
      "        [ 0.0338, -0.0248, -0.0052,  ..., -0.0605, -0.0086,  0.0268],\n",
      "        ...,\n",
      "        [-0.0157,  0.0395,  0.0192,  ...,  0.0421, -0.0265, -0.0127],\n",
      "        [ 0.0015, -0.0236,  0.0202,  ...,  0.0070, -0.0161,  0.0299],\n",
      "        [ 0.0148, -0.0022, -0.0477,  ..., -0.0510, -0.0395, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0385, -0.0470, -0.0244,  ..., -0.0469, -0.0165,  0.0229],\n",
      "        [ 0.0841, -0.0230, -0.0270,  ..., -0.0316, -0.0209,  0.0446],\n",
      "        [ 0.0341, -0.0250, -0.0052,  ..., -0.0608, -0.0085,  0.0270],\n",
      "        ...,\n",
      "        [-0.0158,  0.0395,  0.0193,  ...,  0.0421, -0.0266, -0.0127],\n",
      "        [ 0.0014, -0.0236,  0.0203,  ...,  0.0069, -0.0162,  0.0301],\n",
      "        [ 0.0147, -0.0022, -0.0477,  ..., -0.0510, -0.0396, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0387, -0.0472, -0.0243,  ..., -0.0471, -0.0164,  0.0231],\n",
      "        [ 0.0844, -0.0231, -0.0269,  ..., -0.0318, -0.0208,  0.0448],\n",
      "        [ 0.0343, -0.0251, -0.0051,  ..., -0.0610, -0.0084,  0.0273],\n",
      "        ...,\n",
      "        [-0.0159,  0.0395,  0.0194,  ...,  0.0422, -0.0266, -0.0127],\n",
      "        [ 0.0014, -0.0236,  0.0204,  ...,  0.0068, -0.0164,  0.0303],\n",
      "        [ 0.0146, -0.0022, -0.0477,  ..., -0.0510, -0.0396, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0390, -0.0473, -0.0243,  ..., -0.0473, -0.0162,  0.0234],\n",
      "        [ 0.0846, -0.0233, -0.0269,  ..., -0.0320, -0.0206,  0.0450],\n",
      "        [ 0.0346, -0.0253, -0.0051,  ..., -0.0612, -0.0083,  0.0275],\n",
      "        ...,\n",
      "        [-0.0160,  0.0394,  0.0195,  ...,  0.0423, -0.0266, -0.0128],\n",
      "        [ 0.0013, -0.0237,  0.0204,  ...,  0.0067, -0.0165,  0.0305],\n",
      "        [ 0.0145, -0.0022, -0.0477,  ..., -0.0510, -0.0397, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0392, -0.0474, -0.0242,  ..., -0.0476, -0.0161,  0.0236],\n",
      "        [ 0.0848, -0.0234, -0.0268,  ..., -0.0323, -0.0205,  0.0452],\n",
      "        [ 0.0348, -0.0254, -0.0050,  ..., -0.0614, -0.0081,  0.0277],\n",
      "        ...,\n",
      "        [-0.0161,  0.0394,  0.0197,  ...,  0.0424, -0.0266, -0.0129],\n",
      "        [ 0.0014, -0.0238,  0.0205,  ...,  0.0065, -0.0167,  0.0306],\n",
      "        [ 0.0144, -0.0022, -0.0477,  ..., -0.0511, -0.0398, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0395, -0.0475, -0.0241,  ..., -0.0478, -0.0159,  0.0238],\n",
      "        [ 0.0851, -0.0235, -0.0268,  ..., -0.0325, -0.0203,  0.0454],\n",
      "        [ 0.0350, -0.0255, -0.0050,  ..., -0.0616, -0.0080,  0.0279],\n",
      "        ...,\n",
      "        [-0.0161,  0.0393,  0.0198,  ...,  0.0425, -0.0267, -0.0129],\n",
      "        [ 0.0013, -0.0240,  0.0206,  ...,  0.0064, -0.0168,  0.0308],\n",
      "        [ 0.0143, -0.0022, -0.0476,  ..., -0.0511, -0.0399, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0397, -0.0476, -0.0240,  ..., -0.0480, -0.0157,  0.0240],\n",
      "        [ 0.0853, -0.0236, -0.0267,  ..., -0.0327, -0.0201,  0.0456],\n",
      "        [ 0.0353, -0.0256, -0.0049,  ..., -0.0619, -0.0078,  0.0281],\n",
      "        ...,\n",
      "        [-0.0162,  0.0393,  0.0200,  ...,  0.0425, -0.0267, -0.0130],\n",
      "        [ 0.0013, -0.0241,  0.0207,  ...,  0.0062, -0.0170,  0.0310],\n",
      "        [ 0.0142, -0.0021, -0.0476,  ..., -0.0510, -0.0400, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0399, -0.0478, -0.0239,  ..., -0.0482, -0.0156,  0.0242],\n",
      "        [ 0.0855, -0.0237, -0.0266,  ..., -0.0329, -0.0199,  0.0458],\n",
      "        [ 0.0355, -0.0257, -0.0049,  ..., -0.0621, -0.0076,  0.0283],\n",
      "        ...,\n",
      "        [-0.0163,  0.0392,  0.0202,  ...,  0.0426, -0.0267, -0.0130],\n",
      "        [ 0.0013, -0.0242,  0.0208,  ...,  0.0061, -0.0172,  0.0312],\n",
      "        [ 0.0141, -0.0021, -0.0475,  ..., -0.0510, -0.0401, -0.0125]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0402, -0.0479, -0.0239,  ..., -0.0484, -0.0154,  0.0244],\n",
      "        [ 0.0857, -0.0238, -0.0266,  ..., -0.0331, -0.0197,  0.0461],\n",
      "        [ 0.0357, -0.0258, -0.0048,  ..., -0.0623, -0.0075,  0.0285],\n",
      "        ...,\n",
      "        [-0.0164,  0.0392,  0.0203,  ...,  0.0427, -0.0266, -0.0131],\n",
      "        [ 0.0013, -0.0244,  0.0209,  ...,  0.0059, -0.0173,  0.0315],\n",
      "        [ 0.0140, -0.0022, -0.0475,  ..., -0.0510, -0.0402, -0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0404, -0.0480, -0.0238,  ..., -0.0487, -0.0153,  0.0247],\n",
      "        [ 0.0860, -0.0239, -0.0266,  ..., -0.0333, -0.0196,  0.0463],\n",
      "        [ 0.0360, -0.0259, -0.0048,  ..., -0.0625, -0.0074,  0.0288],\n",
      "        ...,\n",
      "        [-0.0165,  0.0391,  0.0205,  ...,  0.0429, -0.0266, -0.0132],\n",
      "        [ 0.0013, -0.0245,  0.0210,  ...,  0.0058, -0.0175,  0.0316],\n",
      "        [ 0.0139, -0.0022, -0.0474,  ..., -0.0510, -0.0403, -0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0407, -0.0481, -0.0237,  ..., -0.0489, -0.0151,  0.0249],\n",
      "        [ 0.0862, -0.0240, -0.0265,  ..., -0.0336, -0.0194,  0.0465],\n",
      "        [ 0.0362, -0.0261, -0.0048,  ..., -0.0627, -0.0072,  0.0290],\n",
      "        ...,\n",
      "        [-0.0166,  0.0391,  0.0207,  ...,  0.0429, -0.0266, -0.0133],\n",
      "        [ 0.0013, -0.0247,  0.0211,  ...,  0.0057, -0.0176,  0.0318],\n",
      "        [ 0.0138, -0.0022, -0.0474,  ..., -0.0510, -0.0404, -0.0126]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0409, -0.0482, -0.0236,  ..., -0.0491, -0.0149,  0.0252],\n",
      "        [ 0.0865, -0.0241, -0.0264,  ..., -0.0338, -0.0192,  0.0468],\n",
      "        [ 0.0365, -0.0262, -0.0047,  ..., -0.0629, -0.0070,  0.0292],\n",
      "        ...,\n",
      "        [-0.0167,  0.0391,  0.0208,  ...,  0.0430, -0.0266, -0.0134],\n",
      "        [ 0.0014, -0.0248,  0.0212,  ...,  0.0056, -0.0178,  0.0320],\n",
      "        [ 0.0137, -0.0023, -0.0473,  ..., -0.0510, -0.0405, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0412, -0.0484, -0.0236,  ..., -0.0493, -0.0147,  0.0254],\n",
      "        [ 0.0867, -0.0243, -0.0264,  ..., -0.0340, -0.0190,  0.0470],\n",
      "        [ 0.0367, -0.0263, -0.0047,  ..., -0.0631, -0.0069,  0.0295],\n",
      "        ...,\n",
      "        [-0.0169,  0.0390,  0.0210,  ...,  0.0431, -0.0266, -0.0134],\n",
      "        [ 0.0014, -0.0249,  0.0213,  ...,  0.0055, -0.0179,  0.0321],\n",
      "        [ 0.0136, -0.0023, -0.0473,  ..., -0.0511, -0.0406, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0414, -0.0485, -0.0235,  ..., -0.0495, -0.0145,  0.0256],\n",
      "        [ 0.0870, -0.0244, -0.0264,  ..., -0.0342, -0.0188,  0.0472],\n",
      "        [ 0.0369, -0.0264, -0.0047,  ..., -0.0633, -0.0067,  0.0297],\n",
      "        ...,\n",
      "        [-0.0170,  0.0390,  0.0212,  ...,  0.0432, -0.0266, -0.0135],\n",
      "        [ 0.0013, -0.0250,  0.0214,  ...,  0.0054, -0.0180,  0.0323],\n",
      "        [ 0.0135, -0.0023, -0.0472,  ..., -0.0511, -0.0407, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0416, -0.0486, -0.0235,  ..., -0.0498, -0.0143,  0.0259],\n",
      "        [ 0.0872, -0.0245, -0.0264,  ..., -0.0344, -0.0186,  0.0475],\n",
      "        [ 0.0372, -0.0266, -0.0047,  ..., -0.0636, -0.0066,  0.0299],\n",
      "        ...,\n",
      "        [-0.0171,  0.0390,  0.0214,  ...,  0.0433, -0.0266, -0.0135],\n",
      "        [ 0.0013, -0.0251,  0.0216,  ...,  0.0054, -0.0180,  0.0324],\n",
      "        [ 0.0134, -0.0024, -0.0471,  ..., -0.0511, -0.0408, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0419, -0.0488, -0.0234,  ..., -0.0500, -0.0141,  0.0261],\n",
      "        [ 0.0874, -0.0246, -0.0263,  ..., -0.0346, -0.0185,  0.0477],\n",
      "        [ 0.0374, -0.0267, -0.0047,  ..., -0.0637, -0.0064,  0.0301],\n",
      "        ...,\n",
      "        [-0.0172,  0.0389,  0.0216,  ...,  0.0434, -0.0267, -0.0136],\n",
      "        [ 0.0012, -0.0251,  0.0217,  ...,  0.0053, -0.0181,  0.0325],\n",
      "        [ 0.0133, -0.0024, -0.0471,  ..., -0.0511, -0.0409, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0421, -0.0488, -0.0233,  ..., -0.0502, -0.0139,  0.0263],\n",
      "        [ 0.0876, -0.0247, -0.0263,  ..., -0.0348, -0.0183,  0.0479],\n",
      "        [ 0.0376, -0.0268, -0.0047,  ..., -0.0639, -0.0063,  0.0303],\n",
      "        ...,\n",
      "        [-0.0172,  0.0389,  0.0218,  ...,  0.0434, -0.0267, -0.0136],\n",
      "        [ 0.0012, -0.0253,  0.0218,  ...,  0.0052, -0.0182,  0.0327],\n",
      "        [ 0.0132, -0.0024, -0.0471,  ..., -0.0511, -0.0410, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0423, -0.0489, -0.0233,  ..., -0.0503, -0.0137,  0.0265],\n",
      "        [ 0.0878, -0.0248, -0.0262,  ..., -0.0350, -0.0181,  0.0481],\n",
      "        [ 0.0378, -0.0268, -0.0046,  ..., -0.0641, -0.0061,  0.0305],\n",
      "        ...,\n",
      "        [-0.0173,  0.0388,  0.0219,  ...,  0.0435, -0.0268, -0.0136],\n",
      "        [ 0.0012, -0.0254,  0.0219,  ...,  0.0051, -0.0182,  0.0328],\n",
      "        [ 0.0132, -0.0025, -0.0470,  ..., -0.0511, -0.0411, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0424, -0.0490, -0.0231,  ..., -0.0505, -0.0135,  0.0267],\n",
      "        [ 0.0880, -0.0249, -0.0261,  ..., -0.0352, -0.0179,  0.0483],\n",
      "        [ 0.0379, -0.0269, -0.0046,  ..., -0.0643, -0.0059,  0.0307],\n",
      "        ...,\n",
      "        [-0.0173,  0.0388,  0.0221,  ...,  0.0435, -0.0268, -0.0137],\n",
      "        [ 0.0012, -0.0255,  0.0220,  ...,  0.0051, -0.0183,  0.0329],\n",
      "        [ 0.0132, -0.0026, -0.0470,  ..., -0.0511, -0.0412, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0426, -0.0491, -0.0231,  ..., -0.0507, -0.0134,  0.0269],\n",
      "        [ 0.0882, -0.0249, -0.0261,  ..., -0.0354, -0.0177,  0.0485],\n",
      "        [ 0.0381, -0.0269, -0.0046,  ..., -0.0645, -0.0058,  0.0310],\n",
      "        ...,\n",
      "        [-0.0174,  0.0388,  0.0223,  ...,  0.0436, -0.0268, -0.0137],\n",
      "        [ 0.0011, -0.0257,  0.0221,  ...,  0.0050, -0.0184,  0.0331],\n",
      "        [ 0.0131, -0.0027, -0.0469,  ..., -0.0512, -0.0414, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0429, -0.0492, -0.0231,  ..., -0.0509, -0.0132,  0.0271],\n",
      "        [ 0.0884, -0.0251, -0.0261,  ..., -0.0356, -0.0176,  0.0487],\n",
      "        [ 0.0384, -0.0270, -0.0046,  ..., -0.0647, -0.0057,  0.0312],\n",
      "        ...,\n",
      "        [-0.0175,  0.0388,  0.0225,  ...,  0.0437, -0.0268, -0.0138],\n",
      "        [ 0.0011, -0.0258,  0.0222,  ...,  0.0049, -0.0183,  0.0331],\n",
      "        [ 0.0131, -0.0028, -0.0469,  ..., -0.0513, -0.0415, -0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0431, -0.0493, -0.0231,  ..., -0.0512, -0.0131,  0.0273],\n",
      "        [ 0.0886, -0.0252, -0.0261,  ..., -0.0358, -0.0175,  0.0490],\n",
      "        [ 0.0386, -0.0272, -0.0046,  ..., -0.0649, -0.0056,  0.0314],\n",
      "        ...,\n",
      "        [-0.0176,  0.0387,  0.0226,  ...,  0.0438, -0.0268, -0.0138],\n",
      "        [ 0.0010, -0.0259,  0.0223,  ...,  0.0049, -0.0183,  0.0332],\n",
      "        [ 0.0130, -0.0030, -0.0468,  ..., -0.0513, -0.0416, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0433, -0.0494, -0.0231,  ..., -0.0514, -0.0130,  0.0275],\n",
      "        [ 0.0888, -0.0253, -0.0261,  ..., -0.0360, -0.0174,  0.0492],\n",
      "        [ 0.0387, -0.0273, -0.0047,  ..., -0.0651, -0.0055,  0.0316],\n",
      "        ...,\n",
      "        [-0.0177,  0.0387,  0.0228,  ...,  0.0438, -0.0269, -0.0137],\n",
      "        [ 0.0009, -0.0260,  0.0224,  ...,  0.0048, -0.0183,  0.0332],\n",
      "        [ 0.0130, -0.0031, -0.0467,  ..., -0.0514, -0.0416, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0434, -0.0496, -0.0231,  ..., -0.0516, -0.0129,  0.0277],\n",
      "        [ 0.0890, -0.0254, -0.0261,  ..., -0.0362, -0.0173,  0.0494],\n",
      "        [ 0.0389, -0.0274, -0.0047,  ..., -0.0653, -0.0054,  0.0318],\n",
      "        ...,\n",
      "        [-0.0177,  0.0387,  0.0229,  ...,  0.0439, -0.0270, -0.0137],\n",
      "        [ 0.0009, -0.0261,  0.0225,  ...,  0.0047, -0.0183,  0.0333],\n",
      "        [ 0.0130, -0.0033, -0.0467,  ..., -0.0515, -0.0418, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0436, -0.0497, -0.0230,  ..., -0.0517, -0.0128,  0.0279],\n",
      "        [ 0.0892, -0.0256, -0.0261,  ..., -0.0364, -0.0172,  0.0495],\n",
      "        [ 0.0391, -0.0275, -0.0047,  ..., -0.0654, -0.0053,  0.0320],\n",
      "        ...,\n",
      "        [-0.0177,  0.0386,  0.0231,  ...,  0.0439, -0.0271, -0.0136],\n",
      "        [ 0.0008, -0.0262,  0.0226,  ...,  0.0046, -0.0184,  0.0333],\n",
      "        [ 0.0130, -0.0034, -0.0467,  ..., -0.0517, -0.0419, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0438, -0.0498, -0.0230,  ..., -0.0519, -0.0127,  0.0281],\n",
      "        [ 0.0894, -0.0257, -0.0261,  ..., -0.0365, -0.0171,  0.0497],\n",
      "        [ 0.0393, -0.0276, -0.0047,  ..., -0.0656, -0.0053,  0.0321],\n",
      "        ...,\n",
      "        [-0.0177,  0.0385,  0.0232,  ...,  0.0439, -0.0271, -0.0136],\n",
      "        [ 0.0008, -0.0263,  0.0227,  ...,  0.0045, -0.0184,  0.0333],\n",
      "        [ 0.0131, -0.0036, -0.0466,  ..., -0.0518, -0.0421, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0439, -0.0499, -0.0229,  ..., -0.0521, -0.0126,  0.0282],\n",
      "        [ 0.0895, -0.0258, -0.0261,  ..., -0.0367, -0.0170,  0.0499],\n",
      "        [ 0.0394, -0.0277, -0.0047,  ..., -0.0658, -0.0052,  0.0323],\n",
      "        ...,\n",
      "        [-0.0177,  0.0384,  0.0233,  ...,  0.0439, -0.0273, -0.0135],\n",
      "        [ 0.0008, -0.0265,  0.0227,  ...,  0.0043, -0.0184,  0.0334],\n",
      "        [ 0.0131, -0.0038, -0.0466,  ..., -0.0520, -0.0422, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0441, -0.0500, -0.0229,  ..., -0.0522, -0.0125,  0.0284],\n",
      "        [ 0.0897, -0.0259, -0.0261,  ..., -0.0369, -0.0169,  0.0500],\n",
      "        [ 0.0396, -0.0278, -0.0047,  ..., -0.0659, -0.0051,  0.0325],\n",
      "        ...,\n",
      "        [-0.0177,  0.0383,  0.0234,  ...,  0.0439, -0.0274, -0.0134],\n",
      "        [ 0.0007, -0.0266,  0.0228,  ...,  0.0042, -0.0184,  0.0334],\n",
      "        [ 0.0132, -0.0040, -0.0466,  ..., -0.0521, -0.0424, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0443, -0.0501, -0.0229,  ..., -0.0524, -0.0125,  0.0286],\n",
      "        [ 0.0898, -0.0260, -0.0261,  ..., -0.0370, -0.0169,  0.0502],\n",
      "        [ 0.0397, -0.0280, -0.0047,  ..., -0.0661, -0.0051,  0.0326],\n",
      "        ...,\n",
      "        [-0.0177,  0.0382,  0.0235,  ...,  0.0440, -0.0275, -0.0134],\n",
      "        [ 0.0007, -0.0266,  0.0229,  ...,  0.0041, -0.0184,  0.0334],\n",
      "        [ 0.0132, -0.0041, -0.0466,  ..., -0.0522, -0.0425, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0444, -0.0502, -0.0229,  ..., -0.0526, -0.0124,  0.0287],\n",
      "        [ 0.0900, -0.0261, -0.0261,  ..., -0.0372, -0.0168,  0.0504],\n",
      "        [ 0.0399, -0.0281, -0.0047,  ..., -0.0662, -0.0051,  0.0328],\n",
      "        ...,\n",
      "        [-0.0178,  0.0383,  0.0236,  ...,  0.0441, -0.0275, -0.0134],\n",
      "        [ 0.0007, -0.0268,  0.0229,  ...,  0.0039, -0.0185,  0.0335],\n",
      "        [ 0.0132, -0.0043, -0.0466,  ..., -0.0523, -0.0427, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0446, -0.0503, -0.0228,  ..., -0.0527, -0.0123,  0.0289],\n",
      "        [ 0.0902, -0.0263, -0.0261,  ..., -0.0374, -0.0168,  0.0505],\n",
      "        [ 0.0401, -0.0282, -0.0047,  ..., -0.0664, -0.0050,  0.0330],\n",
      "        ...,\n",
      "        [-0.0179,  0.0383,  0.0237,  ...,  0.0442, -0.0276, -0.0133],\n",
      "        [ 0.0008, -0.0269,  0.0230,  ...,  0.0038, -0.0185,  0.0335],\n",
      "        [ 0.0132, -0.0044, -0.0466,  ..., -0.0524, -0.0428, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0447, -0.0505, -0.0228,  ..., -0.0529, -0.0122,  0.0290],\n",
      "        [ 0.0903, -0.0264, -0.0261,  ..., -0.0375, -0.0167,  0.0507],\n",
      "        [ 0.0402, -0.0283, -0.0047,  ..., -0.0666, -0.0049,  0.0331],\n",
      "        ...,\n",
      "        [-0.0179,  0.0383,  0.0238,  ...,  0.0443, -0.0277, -0.0133],\n",
      "        [ 0.0008, -0.0270,  0.0231,  ...,  0.0037, -0.0185,  0.0336],\n",
      "        [ 0.0132, -0.0045, -0.0465,  ..., -0.0525, -0.0429, -0.0129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0449, -0.0506, -0.0228,  ..., -0.0531, -0.0122,  0.0292],\n",
      "        [ 0.0905, -0.0266, -0.0261,  ..., -0.0377, -0.0167,  0.0508],\n",
      "        [ 0.0404, -0.0285, -0.0047,  ..., -0.0667, -0.0049,  0.0333],\n",
      "        ...,\n",
      "        [-0.0180,  0.0384,  0.0239,  ...,  0.0444, -0.0278, -0.0132],\n",
      "        [ 0.0008, -0.0272,  0.0232,  ...,  0.0035, -0.0185,  0.0337],\n",
      "        [ 0.0132, -0.0045, -0.0465,  ..., -0.0525, -0.0430, -0.0129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0451, -0.0507, -0.0227,  ..., -0.0532, -0.0121,  0.0293],\n",
      "        [ 0.0907, -0.0267, -0.0261,  ..., -0.0379, -0.0166,  0.0510],\n",
      "        [ 0.0405, -0.0286, -0.0047,  ..., -0.0669, -0.0048,  0.0334],\n",
      "        ...,\n",
      "        [-0.0181,  0.0384,  0.0240,  ...,  0.0445, -0.0278, -0.0133],\n",
      "        [ 0.0009, -0.0273,  0.0233,  ...,  0.0034, -0.0186,  0.0338],\n",
      "        [ 0.0131, -0.0046, -0.0465,  ..., -0.0526, -0.0431, -0.0129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0452, -0.0509, -0.0227,  ..., -0.0534, -0.0120,  0.0294],\n",
      "        [ 0.0908, -0.0268, -0.0261,  ..., -0.0380, -0.0165,  0.0511],\n",
      "        [ 0.0407, -0.0287, -0.0047,  ..., -0.0670, -0.0048,  0.0335],\n",
      "        ...,\n",
      "        [-0.0181,  0.0384,  0.0241,  ...,  0.0446, -0.0279, -0.0132],\n",
      "        [ 0.0010, -0.0274,  0.0233,  ...,  0.0032, -0.0187,  0.0339],\n",
      "        [ 0.0132, -0.0047, -0.0465,  ..., -0.0527, -0.0432, -0.0129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0453, -0.0510, -0.0227,  ..., -0.0536, -0.0119,  0.0296],\n",
      "        [ 0.0910, -0.0269, -0.0261,  ..., -0.0382, -0.0165,  0.0512],\n",
      "        [ 0.0408, -0.0289, -0.0047,  ..., -0.0672, -0.0047,  0.0337],\n",
      "        ...,\n",
      "        [-0.0181,  0.0383,  0.0242,  ...,  0.0447, -0.0280, -0.0132],\n",
      "        [ 0.0011, -0.0276,  0.0233,  ...,  0.0030, -0.0189,  0.0340],\n",
      "        [ 0.0132, -0.0048, -0.0465,  ..., -0.0527, -0.0433, -0.0129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0455, -0.0511, -0.0227,  ..., -0.0537, -0.0119,  0.0297],\n",
      "        [ 0.0911, -0.0271, -0.0261,  ..., -0.0383, -0.0164,  0.0514],\n",
      "        [ 0.0410, -0.0290, -0.0048,  ..., -0.0673, -0.0047,  0.0338],\n",
      "        ...,\n",
      "        [-0.0181,  0.0383,  0.0243,  ...,  0.0447, -0.0281, -0.0131],\n",
      "        [ 0.0012, -0.0278,  0.0233,  ...,  0.0028, -0.0190,  0.0342],\n",
      "        [ 0.0132, -0.0049, -0.0465,  ..., -0.0528, -0.0434, -0.0129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0457, -0.0512, -0.0227,  ..., -0.0539, -0.0118,  0.0299],\n",
      "        [ 0.0913, -0.0272, -0.0261,  ..., -0.0385, -0.0163,  0.0515],\n",
      "        [ 0.0411, -0.0291, -0.0048,  ..., -0.0675, -0.0046,  0.0339],\n",
      "        ...,\n",
      "        [-0.0182,  0.0382,  0.0244,  ...,  0.0448, -0.0282, -0.0131],\n",
      "        [ 0.0014, -0.0280,  0.0233,  ...,  0.0025, -0.0192,  0.0343],\n",
      "        [ 0.0133, -0.0051, -0.0466,  ..., -0.0530, -0.0435, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0458, -0.0513, -0.0226,  ..., -0.0541, -0.0117,  0.0300],\n",
      "        [ 0.0914, -0.0273, -0.0261,  ..., -0.0387, -0.0163,  0.0517],\n",
      "        [ 0.0413, -0.0292, -0.0048,  ..., -0.0677, -0.0045,  0.0341],\n",
      "        ...,\n",
      "        [-0.0183,  0.0382,  0.0245,  ...,  0.0449, -0.0282, -0.0132],\n",
      "        [ 0.0015, -0.0282,  0.0233,  ...,  0.0023, -0.0194,  0.0345],\n",
      "        [ 0.0133, -0.0052, -0.0466,  ..., -0.0531, -0.0437, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0460, -0.0514, -0.0226,  ..., -0.0542, -0.0116,  0.0302],\n",
      "        [ 0.0916, -0.0274, -0.0260,  ..., -0.0388, -0.0162,  0.0518],\n",
      "        [ 0.0415, -0.0293, -0.0048,  ..., -0.0678, -0.0045,  0.0343],\n",
      "        ...,\n",
      "        [-0.0184,  0.0382,  0.0247,  ...,  0.0450, -0.0283, -0.0132],\n",
      "        [ 0.0016, -0.0285,  0.0232,  ...,  0.0021, -0.0196,  0.0346],\n",
      "        [ 0.0134, -0.0054, -0.0466,  ..., -0.0533, -0.0439, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0462, -0.0515, -0.0226,  ..., -0.0544, -0.0115,  0.0303],\n",
      "        [ 0.0918, -0.0275, -0.0260,  ..., -0.0390, -0.0161,  0.0520],\n",
      "        [ 0.0416, -0.0295, -0.0047,  ..., -0.0680, -0.0044,  0.0344],\n",
      "        ...,\n",
      "        [-0.0185,  0.0382,  0.0248,  ...,  0.0451, -0.0283, -0.0133],\n",
      "        [ 0.0017, -0.0287,  0.0232,  ...,  0.0020, -0.0197,  0.0347],\n",
      "        [ 0.0134, -0.0056, -0.0467,  ..., -0.0534, -0.0440, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0464, -0.0516, -0.0225,  ..., -0.0546, -0.0114,  0.0305],\n",
      "        [ 0.0920, -0.0277, -0.0259,  ..., -0.0392, -0.0160,  0.0522],\n",
      "        [ 0.0418, -0.0296, -0.0047,  ..., -0.0682, -0.0043,  0.0346],\n",
      "        ...,\n",
      "        [-0.0186,  0.0381,  0.0250,  ...,  0.0452, -0.0283, -0.0133],\n",
      "        [ 0.0018, -0.0289,  0.0232,  ...,  0.0019, -0.0198,  0.0348],\n",
      "        [ 0.0134, -0.0057, -0.0467,  ..., -0.0534, -0.0441, -0.0128]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0465, -0.0518, -0.0225,  ..., -0.0548, -0.0114,  0.0307],\n",
      "        [ 0.0921, -0.0278, -0.0259,  ..., -0.0394, -0.0160,  0.0524],\n",
      "        [ 0.0420, -0.0297, -0.0046,  ..., -0.0684, -0.0043,  0.0348],\n",
      "        ...,\n",
      "        [-0.0187,  0.0381,  0.0251,  ...,  0.0453, -0.0283, -0.0133],\n",
      "        [ 0.0018, -0.0291,  0.0232,  ...,  0.0017, -0.0199,  0.0349],\n",
      "        [ 0.0133, -0.0058, -0.0467,  ..., -0.0535, -0.0442, -0.0129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0467, -0.0519, -0.0224,  ..., -0.0549, -0.0113,  0.0309],\n",
      "        [ 0.0923, -0.0279, -0.0258,  ..., -0.0395, -0.0160,  0.0525],\n",
      "        [ 0.0422, -0.0299, -0.0046,  ..., -0.0686, -0.0043,  0.0350],\n",
      "        ...,\n",
      "        [-0.0187,  0.0381,  0.0253,  ...,  0.0454, -0.0283, -0.0133],\n",
      "        [ 0.0019, -0.0293,  0.0231,  ...,  0.0016, -0.0201,  0.0349],\n",
      "        [ 0.0133, -0.0060, -0.0467,  ..., -0.0535, -0.0443, -0.0129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0469, -0.0521, -0.0223,  ..., -0.0551, -0.0112,  0.0310],\n",
      "        [ 0.0925, -0.0281, -0.0258,  ..., -0.0397, -0.0159,  0.0527],\n",
      "        [ 0.0424, -0.0300, -0.0045,  ..., -0.0688, -0.0042,  0.0352],\n",
      "        ...,\n",
      "        [-0.0189,  0.0382,  0.0254,  ...,  0.0455, -0.0283, -0.0133],\n",
      "        [ 0.0019, -0.0295,  0.0232,  ...,  0.0014, -0.0201,  0.0349],\n",
      "        [ 0.0132, -0.0061, -0.0466,  ..., -0.0536, -0.0443, -0.0130]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0471, -0.0522, -0.0222,  ..., -0.0553, -0.0112,  0.0312],\n",
      "        [ 0.0927, -0.0282, -0.0257,  ..., -0.0399, -0.0159,  0.0529],\n",
      "        [ 0.0426, -0.0302, -0.0044,  ..., -0.0689, -0.0042,  0.0353],\n",
      "        ...,\n",
      "        [-0.0190,  0.0382,  0.0256,  ...,  0.0456, -0.0283, -0.0133],\n",
      "        [ 0.0019, -0.0296,  0.0232,  ...,  0.0013, -0.0202,  0.0350],\n",
      "        [ 0.0132, -0.0062, -0.0466,  ..., -0.0536, -0.0443, -0.0131]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0473, -0.0523, -0.0222,  ..., -0.0555, -0.0111,  0.0314],\n",
      "        [ 0.0928, -0.0283, -0.0257,  ..., -0.0401, -0.0159,  0.0530],\n",
      "        [ 0.0427, -0.0303, -0.0044,  ..., -0.0691, -0.0041,  0.0355],\n",
      "        ...,\n",
      "        [-0.0191,  0.0382,  0.0257,  ...,  0.0457, -0.0284, -0.0132],\n",
      "        [ 0.0019, -0.0297,  0.0232,  ...,  0.0013, -0.0202,  0.0350],\n",
      "        [ 0.0131, -0.0062, -0.0466,  ..., -0.0536, -0.0444, -0.0131]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0475, -0.0524, -0.0220,  ..., -0.0557, -0.0109,  0.0316],\n",
      "        [ 0.0930, -0.0285, -0.0256,  ..., -0.0403, -0.0157,  0.0532],\n",
      "        [ 0.0429, -0.0304, -0.0043,  ..., -0.0693, -0.0040,  0.0357],\n",
      "        ...,\n",
      "        [-0.0193,  0.0383,  0.0259,  ...,  0.0458, -0.0283, -0.0132],\n",
      "        [ 0.0019, -0.0298,  0.0232,  ...,  0.0012, -0.0203,  0.0350],\n",
      "        [ 0.0130, -0.0063, -0.0465,  ..., -0.0536, -0.0444, -0.0132]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0476, -0.0526, -0.0219,  ..., -0.0559, -0.0108,  0.0317],\n",
      "        [ 0.0932, -0.0286, -0.0255,  ..., -0.0404, -0.0157,  0.0534],\n",
      "        [ 0.0431, -0.0306, -0.0042,  ..., -0.0695, -0.0039,  0.0359],\n",
      "        ...,\n",
      "        [-0.0194,  0.0383,  0.0260,  ...,  0.0459, -0.0284, -0.0132],\n",
      "        [ 0.0020, -0.0299,  0.0232,  ...,  0.0010, -0.0204,  0.0351],\n",
      "        [ 0.0129, -0.0064, -0.0465,  ..., -0.0536, -0.0445, -0.0133]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0478, -0.0527, -0.0218,  ..., -0.0561, -0.0107,  0.0319],\n",
      "        [ 0.0933, -0.0288, -0.0255,  ..., -0.0406, -0.0156,  0.0535],\n",
      "        [ 0.0433, -0.0307, -0.0041,  ..., -0.0697, -0.0038,  0.0360],\n",
      "        ...,\n",
      "        [-0.0195,  0.0383,  0.0262,  ...,  0.0460, -0.0284, -0.0131],\n",
      "        [ 0.0020, -0.0300,  0.0232,  ...,  0.0009, -0.0205,  0.0351],\n",
      "        [ 0.0128, -0.0064, -0.0466,  ..., -0.0536, -0.0446, -0.0134]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0480, -0.0529, -0.0218,  ..., -0.0563, -0.0105,  0.0321],\n",
      "        [ 0.0935, -0.0289, -0.0254,  ..., -0.0408, -0.0155,  0.0537],\n",
      "        [ 0.0435, -0.0309, -0.0041,  ..., -0.0699, -0.0036,  0.0362],\n",
      "        ...,\n",
      "        [-0.0196,  0.0383,  0.0264,  ...,  0.0461, -0.0284, -0.0130],\n",
      "        [ 0.0019, -0.0301,  0.0233,  ...,  0.0009, -0.0205,  0.0351],\n",
      "        [ 0.0128, -0.0065, -0.0466,  ..., -0.0537, -0.0446, -0.0134]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0482, -0.0530, -0.0217,  ..., -0.0565, -0.0104,  0.0323],\n",
      "        [ 0.0937, -0.0291, -0.0254,  ..., -0.0410, -0.0154,  0.0539],\n",
      "        [ 0.0437, -0.0310, -0.0040,  ..., -0.0701, -0.0035,  0.0364],\n",
      "        ...,\n",
      "        [-0.0197,  0.0383,  0.0265,  ...,  0.0462, -0.0284, -0.0130],\n",
      "        [ 0.0019, -0.0302,  0.0233,  ...,  0.0008, -0.0206,  0.0352],\n",
      "        [ 0.0126, -0.0065, -0.0465,  ..., -0.0537, -0.0447, -0.0135]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0484, -0.0532, -0.0216,  ..., -0.0567, -0.0103,  0.0325],\n",
      "        [ 0.0939, -0.0292, -0.0253,  ..., -0.0412, -0.0153,  0.0541],\n",
      "        [ 0.0439, -0.0312, -0.0039,  ..., -0.0703, -0.0034,  0.0366],\n",
      "        ...,\n",
      "        [-0.0199,  0.0383,  0.0267,  ...,  0.0463, -0.0285, -0.0129],\n",
      "        [ 0.0019, -0.0302,  0.0234,  ...,  0.0007, -0.0206,  0.0352],\n",
      "        [ 0.0125, -0.0065, -0.0465,  ..., -0.0537, -0.0447, -0.0136]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0486, -0.0534, -0.0215,  ..., -0.0569, -0.0102,  0.0327],\n",
      "        [ 0.0941, -0.0294, -0.0252,  ..., -0.0414, -0.0152,  0.0542],\n",
      "        [ 0.0441, -0.0314, -0.0038,  ..., -0.0705, -0.0033,  0.0368],\n",
      "        ...,\n",
      "        [-0.0200,  0.0383,  0.0269,  ...,  0.0464, -0.0285, -0.0129],\n",
      "        [ 0.0018, -0.0302,  0.0235,  ...,  0.0007, -0.0206,  0.0352],\n",
      "        [ 0.0124, -0.0065, -0.0465,  ..., -0.0537, -0.0447, -0.0136]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0488, -0.0536, -0.0215,  ..., -0.0571, -0.0102,  0.0328],\n",
      "        [ 0.0942, -0.0296, -0.0252,  ..., -0.0416, -0.0152,  0.0544],\n",
      "        [ 0.0442, -0.0316, -0.0038,  ..., -0.0707, -0.0032,  0.0369],\n",
      "        ...,\n",
      "        [-0.0201,  0.0383,  0.0271,  ...,  0.0465, -0.0285, -0.0128],\n",
      "        [ 0.0018, -0.0302,  0.0235,  ...,  0.0006, -0.0206,  0.0352],\n",
      "        [ 0.0123, -0.0065, -0.0465,  ..., -0.0537, -0.0447, -0.0137]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0490, -0.0538, -0.0215,  ..., -0.0573, -0.0102,  0.0330],\n",
      "        [ 0.0944, -0.0298, -0.0252,  ..., -0.0418, -0.0152,  0.0546],\n",
      "        [ 0.0444, -0.0318, -0.0038,  ..., -0.0709, -0.0032,  0.0371],\n",
      "        ...,\n",
      "        [-0.0203,  0.0383,  0.0272,  ...,  0.0467, -0.0285, -0.0128],\n",
      "        [ 0.0017, -0.0302,  0.0236,  ...,  0.0006, -0.0206,  0.0353],\n",
      "        [ 0.0121, -0.0065, -0.0464,  ..., -0.0536, -0.0448, -0.0138]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0492, -0.0540, -0.0215,  ..., -0.0575, -0.0102,  0.0332],\n",
      "        [ 0.0946, -0.0301, -0.0252,  ..., -0.0419, -0.0152,  0.0547],\n",
      "        [ 0.0446, -0.0320, -0.0038,  ..., -0.0711, -0.0031,  0.0373],\n",
      "        ...,\n",
      "        [-0.0204,  0.0383,  0.0274,  ...,  0.0468, -0.0285, -0.0128],\n",
      "        [ 0.0017, -0.0302,  0.0237,  ...,  0.0006, -0.0206,  0.0353],\n",
      "        [ 0.0120, -0.0065, -0.0464,  ..., -0.0536, -0.0448, -0.0138]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0493, -0.0542, -0.0215,  ..., -0.0577, -0.0102,  0.0334],\n",
      "        [ 0.0947, -0.0303, -0.0253,  ..., -0.0421, -0.0152,  0.0549],\n",
      "        [ 0.0448, -0.0323, -0.0038,  ..., -0.0713, -0.0031,  0.0375],\n",
      "        ...,\n",
      "        [-0.0206,  0.0384,  0.0276,  ...,  0.0470, -0.0285, -0.0128],\n",
      "        [ 0.0017, -0.0302,  0.0238,  ...,  0.0006, -0.0206,  0.0353],\n",
      "        [ 0.0118, -0.0064, -0.0463,  ..., -0.0535, -0.0448, -0.0140]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0495, -0.0544, -0.0216,  ..., -0.0579, -0.0102,  0.0336],\n",
      "        [ 0.0949, -0.0305, -0.0254,  ..., -0.0423, -0.0152,  0.0550],\n",
      "        [ 0.0450, -0.0325, -0.0038,  ..., -0.0715, -0.0030,  0.0376],\n",
      "        ...,\n",
      "        [-0.0208,  0.0384,  0.0277,  ...,  0.0471, -0.0284, -0.0128],\n",
      "        [ 0.0016, -0.0302,  0.0239,  ...,  0.0005, -0.0206,  0.0353],\n",
      "        [ 0.0116, -0.0063, -0.0463,  ..., -0.0534, -0.0447, -0.0141]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0497, -0.0546, -0.0216,  ..., -0.0581, -0.0102,  0.0337],\n",
      "        [ 0.0951, -0.0307, -0.0254,  ..., -0.0425, -0.0152,  0.0552],\n",
      "        [ 0.0452, -0.0327, -0.0039,  ..., -0.0717, -0.0030,  0.0378],\n",
      "        ...,\n",
      "        [-0.0209,  0.0383,  0.0279,  ...,  0.0472, -0.0284, -0.0129],\n",
      "        [ 0.0016, -0.0303,  0.0241,  ...,  0.0005, -0.0206,  0.0354],\n",
      "        [ 0.0115, -0.0063, -0.0462,  ..., -0.0534, -0.0448, -0.0142]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0499, -0.0548, -0.0217,  ..., -0.0583, -0.0102,  0.0339],\n",
      "        [ 0.0952, -0.0309, -0.0255,  ..., -0.0427, -0.0152,  0.0554],\n",
      "        [ 0.0454, -0.0329, -0.0039,  ..., -0.0719, -0.0030,  0.0380],\n",
      "        ...,\n",
      "        [-0.0210,  0.0383,  0.0280,  ...,  0.0473, -0.0285, -0.0129],\n",
      "        [ 0.0016, -0.0303,  0.0242,  ...,  0.0004, -0.0207,  0.0355],\n",
      "        [ 0.0113, -0.0063, -0.0461,  ..., -0.0533, -0.0447, -0.0142]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0501, -0.0550, -0.0216,  ..., -0.0585, -0.0101,  0.0341],\n",
      "        [ 0.0954, -0.0311, -0.0255,  ..., -0.0428, -0.0151,  0.0555],\n",
      "        [ 0.0455, -0.0331, -0.0039,  ..., -0.0721, -0.0029,  0.0382],\n",
      "        ...,\n",
      "        [-0.0211,  0.0382,  0.0282,  ...,  0.0474, -0.0285, -0.0129],\n",
      "        [ 0.0017, -0.0304,  0.0242,  ...,  0.0003, -0.0208,  0.0356],\n",
      "        [ 0.0112, -0.0063, -0.0461,  ..., -0.0533, -0.0448, -0.0143]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0502, -0.0552, -0.0217,  ..., -0.0586, -0.0101,  0.0342],\n",
      "        [ 0.0955, -0.0312, -0.0256,  ..., -0.0430, -0.0151,  0.0556],\n",
      "        [ 0.0457, -0.0332, -0.0039,  ..., -0.0723, -0.0029,  0.0383],\n",
      "        ...,\n",
      "        [-0.0212,  0.0381,  0.0283,  ...,  0.0475, -0.0285, -0.0129],\n",
      "        [ 0.0017, -0.0305,  0.0243,  ...,  0.0002, -0.0209,  0.0357],\n",
      "        [ 0.0111, -0.0062, -0.0460,  ..., -0.0533, -0.0448, -0.0144]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 5.0344e-02, -5.5344e-02, -2.1709e-02,  ..., -5.8809e-02,\n",
      "         -1.0073e-02,  3.4355e-02],\n",
      "        [ 9.5667e-02, -3.1414e-02, -2.5578e-02,  ..., -4.3155e-02,\n",
      "         -1.5074e-02,  5.5779e-02],\n",
      "        [ 4.5858e-02, -3.3414e-02, -3.9433e-03,  ..., -7.2458e-02,\n",
      "         -2.8043e-03,  3.8457e-02],\n",
      "        ...,\n",
      "        [-2.1342e-02,  3.8064e-02,  2.8495e-02,  ...,  4.7613e-02,\n",
      "         -2.8438e-02, -1.2934e-02],\n",
      "        [ 1.7392e-03, -3.0572e-02,  2.4394e-02,  ...,  4.8005e-05,\n",
      "         -2.0980e-02,  3.5894e-02],\n",
      "        [ 1.0964e-02, -6.2500e-03, -4.5946e-02,  ..., -5.3270e-02,\n",
      "         -4.4801e-02, -1.4443e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0505, -0.0555, -0.0217,  ..., -0.0590, -0.0100,  0.0345],\n",
      "        [ 0.0958, -0.0316, -0.0256,  ..., -0.0433, -0.0150,  0.0559],\n",
      "        [ 0.0460, -0.0336, -0.0039,  ..., -0.0726, -0.0028,  0.0386],\n",
      "        ...,\n",
      "        [-0.0214,  0.0380,  0.0286,  ...,  0.0477, -0.0284, -0.0129],\n",
      "        [ 0.0018, -0.0307,  0.0244,  ..., -0.0001, -0.0211,  0.0361],\n",
      "        [ 0.0109, -0.0063, -0.0459,  ..., -0.0533, -0.0449, -0.0145]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0506, -0.0557, -0.0217,  ..., -0.0591, -0.0100,  0.0346],\n",
      "        [ 0.0959, -0.0317, -0.0256,  ..., -0.0434, -0.0150,  0.0560],\n",
      "        [ 0.0462, -0.0338, -0.0039,  ..., -0.0728, -0.0027,  0.0387],\n",
      "        ...,\n",
      "        [-0.0215,  0.0379,  0.0288,  ...,  0.0478, -0.0284, -0.0130],\n",
      "        [ 0.0019, -0.0308,  0.0244,  ..., -0.0003, -0.0213,  0.0362],\n",
      "        [ 0.0109, -0.0063, -0.0459,  ..., -0.0533, -0.0449, -0.0145]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0507, -0.0558, -0.0216,  ..., -0.0593, -0.0099,  0.0347],\n",
      "        [ 0.0961, -0.0319, -0.0255,  ..., -0.0436, -0.0149,  0.0562],\n",
      "        [ 0.0463, -0.0339, -0.0038,  ..., -0.0729, -0.0026,  0.0389],\n",
      "        ...,\n",
      "        [-0.0216,  0.0379,  0.0290,  ...,  0.0479, -0.0284, -0.0130],\n",
      "        [ 0.0019, -0.0309,  0.0245,  ..., -0.0004, -0.0214,  0.0364],\n",
      "        [ 0.0108, -0.0063, -0.0459,  ..., -0.0534, -0.0450, -0.0145]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0509, -0.0560, -0.0215,  ..., -0.0594, -0.0098,  0.0349],\n",
      "        [ 0.0962, -0.0321, -0.0255,  ..., -0.0438, -0.0148,  0.0563],\n",
      "        [ 0.0465, -0.0341, -0.0038,  ..., -0.0731, -0.0025,  0.0390],\n",
      "        ...,\n",
      "        [-0.0217,  0.0379,  0.0291,  ...,  0.0480, -0.0283, -0.0131],\n",
      "        [ 0.0020, -0.0309,  0.0246,  ..., -0.0005, -0.0215,  0.0365],\n",
      "        [ 0.0108, -0.0063, -0.0459,  ..., -0.0534, -0.0451, -0.0145]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0510, -0.0562, -0.0214,  ..., -0.0596, -0.0097,  0.0350],\n",
      "        [ 0.0964, -0.0323, -0.0254,  ..., -0.0439, -0.0147,  0.0564],\n",
      "        [ 0.0466, -0.0343, -0.0037,  ..., -0.0733, -0.0023,  0.0392],\n",
      "        ...,\n",
      "        [-0.0219,  0.0379,  0.0293,  ...,  0.0482, -0.0283, -0.0131],\n",
      "        [ 0.0020, -0.0309,  0.0246,  ..., -0.0005, -0.0216,  0.0366],\n",
      "        [ 0.0107, -0.0063, -0.0459,  ..., -0.0534, -0.0451, -0.0146]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0512, -0.0564, -0.0213,  ..., -0.0597, -0.0096,  0.0352],\n",
      "        [ 0.0965, -0.0325, -0.0253,  ..., -0.0441, -0.0146,  0.0566],\n",
      "        [ 0.0468, -0.0345, -0.0036,  ..., -0.0735, -0.0022,  0.0393],\n",
      "        ...,\n",
      "        [-0.0220,  0.0379,  0.0294,  ...,  0.0483, -0.0282, -0.0131],\n",
      "        [ 0.0020, -0.0309,  0.0247,  ..., -0.0006, -0.0216,  0.0367],\n",
      "        [ 0.0106, -0.0063, -0.0458,  ..., -0.0534, -0.0452, -0.0147]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0513, -0.0566, -0.0213,  ..., -0.0599, -0.0095,  0.0353],\n",
      "        [ 0.0967, -0.0327, -0.0252,  ..., -0.0443, -0.0144,  0.0568],\n",
      "        [ 0.0470, -0.0347, -0.0035,  ..., -0.0736, -0.0020,  0.0395],\n",
      "        ...,\n",
      "        [-0.0220,  0.0379,  0.0296,  ...,  0.0483, -0.0282, -0.0132],\n",
      "        [ 0.0020, -0.0309,  0.0248,  ..., -0.0006, -0.0217,  0.0369],\n",
      "        [ 0.0105, -0.0063, -0.0458,  ..., -0.0534, -0.0452, -0.0147]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0515, -0.0568, -0.0211,  ..., -0.0601, -0.0094,  0.0355],\n",
      "        [ 0.0969, -0.0329, -0.0252,  ..., -0.0444, -0.0143,  0.0569],\n",
      "        [ 0.0471, -0.0349, -0.0034,  ..., -0.0738, -0.0019,  0.0397],\n",
      "        ...,\n",
      "        [-0.0221,  0.0378,  0.0297,  ...,  0.0484, -0.0282, -0.0132],\n",
      "        [ 0.0020, -0.0309,  0.0249,  ..., -0.0007, -0.0218,  0.0370],\n",
      "        [ 0.0105, -0.0063, -0.0458,  ..., -0.0534, -0.0453, -0.0148]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0516, -0.0569, -0.0211,  ..., -0.0602, -0.0093,  0.0356],\n",
      "        [ 0.0970, -0.0331, -0.0251,  ..., -0.0446, -0.0141,  0.0571],\n",
      "        [ 0.0473, -0.0351, -0.0033,  ..., -0.0740, -0.0017,  0.0398],\n",
      "        ...,\n",
      "        [-0.0222,  0.0377,  0.0298,  ...,  0.0484, -0.0282, -0.0132],\n",
      "        [ 0.0020, -0.0308,  0.0250,  ..., -0.0007, -0.0218,  0.0370],\n",
      "        [ 0.0104, -0.0062, -0.0458,  ..., -0.0534, -0.0453, -0.0148]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0518, -0.0571, -0.0210,  ..., -0.0604, -0.0092,  0.0358],\n",
      "        [ 0.0972, -0.0333, -0.0250,  ..., -0.0448, -0.0140,  0.0572],\n",
      "        [ 0.0475, -0.0353, -0.0033,  ..., -0.0742, -0.0016,  0.0400],\n",
      "        ...,\n",
      "        [-0.0222,  0.0377,  0.0299,  ...,  0.0484, -0.0282, -0.0131],\n",
      "        [ 0.0020, -0.0308,  0.0251,  ..., -0.0007, -0.0218,  0.0371],\n",
      "        [ 0.0103, -0.0062, -0.0457,  ..., -0.0534, -0.0453, -0.0149]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0520, -0.0573, -0.0210,  ..., -0.0606, -0.0091,  0.0359],\n",
      "        [ 0.0974, -0.0335, -0.0250,  ..., -0.0449, -0.0138,  0.0574],\n",
      "        [ 0.0477, -0.0355, -0.0032,  ..., -0.0744, -0.0014,  0.0402],\n",
      "        ...,\n",
      "        [-0.0222,  0.0376,  0.0300,  ...,  0.0485, -0.0283, -0.0131],\n",
      "        [ 0.0019, -0.0308,  0.0252,  ..., -0.0007, -0.0218,  0.0371],\n",
      "        [ 0.0102, -0.0062, -0.0457,  ..., -0.0533, -0.0453, -0.0150]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0521, -0.0574, -0.0209,  ..., -0.0607, -0.0089,  0.0361],\n",
      "        [ 0.0976, -0.0336, -0.0249,  ..., -0.0451, -0.0136,  0.0576],\n",
      "        [ 0.0478, -0.0357, -0.0031,  ..., -0.0745, -0.0012,  0.0403],\n",
      "        ...,\n",
      "        [-0.0223,  0.0375,  0.0302,  ...,  0.0485, -0.0283, -0.0131],\n",
      "        [ 0.0019, -0.0308,  0.0253,  ..., -0.0007, -0.0219,  0.0372],\n",
      "        [ 0.0101, -0.0062, -0.0457,  ..., -0.0533, -0.0454, -0.0150]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0523, -0.0576, -0.0208,  ..., -0.0609, -0.0088,  0.0362],\n",
      "        [ 0.0977, -0.0338, -0.0248,  ..., -0.0453, -0.0134,  0.0577],\n",
      "        [ 0.0480, -0.0359, -0.0030,  ..., -0.0747, -0.0009,  0.0405],\n",
      "        ...,\n",
      "        [-0.0223,  0.0375,  0.0303,  ...,  0.0486, -0.0283, -0.0131],\n",
      "        [ 0.0018, -0.0308,  0.0254,  ..., -0.0007, -0.0219,  0.0373],\n",
      "        [ 0.0100, -0.0061, -0.0457,  ..., -0.0533, -0.0454, -0.0150]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0525, -0.0578, -0.0207,  ..., -0.0611, -0.0086,  0.0364],\n",
      "        [ 0.0979, -0.0340, -0.0247,  ..., -0.0455, -0.0132,  0.0579],\n",
      "        [ 0.0482, -0.0360, -0.0029,  ..., -0.0749, -0.0007,  0.0407],\n",
      "        ...,\n",
      "        [-0.0224,  0.0374,  0.0304,  ...,  0.0487, -0.0283, -0.0131],\n",
      "        [ 0.0018, -0.0307,  0.0255,  ..., -0.0008, -0.0220,  0.0373],\n",
      "        [ 0.0099, -0.0061, -0.0457,  ..., -0.0533, -0.0454, -0.0151]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0526, -0.0579, -0.0206,  ..., -0.0612, -0.0084,  0.0365],\n",
      "        [ 0.0981, -0.0341, -0.0246,  ..., -0.0457, -0.0129,  0.0581],\n",
      "        [ 0.0484, -0.0362, -0.0028,  ..., -0.0751, -0.0005,  0.0408],\n",
      "        ...,\n",
      "        [-0.0225,  0.0373,  0.0305,  ...,  0.0487, -0.0284, -0.0131],\n",
      "        [ 0.0018, -0.0307,  0.0255,  ..., -0.0008, -0.0220,  0.0374],\n",
      "        [ 0.0099, -0.0061, -0.0457,  ..., -0.0533, -0.0455, -0.0151]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0528, -0.0580, -0.0205,  ..., -0.0614, -0.0082,  0.0367],\n",
      "        [ 0.0983, -0.0342, -0.0245,  ..., -0.0458, -0.0127,  0.0582],\n",
      "        [ 0.0486, -0.0363, -0.0027,  ..., -0.0752, -0.0003,  0.0410],\n",
      "        ...,\n",
      "        [-0.0225,  0.0372,  0.0307,  ...,  0.0487, -0.0285, -0.0130],\n",
      "        [ 0.0018, -0.0307,  0.0256,  ..., -0.0009, -0.0221,  0.0375],\n",
      "        [ 0.0097, -0.0060, -0.0457,  ..., -0.0532, -0.0455, -0.0152]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 5.2938e-02, -5.8165e-02, -2.0379e-02,  ..., -6.1530e-02,\n",
      "         -8.0176e-03,  3.6824e-02],\n",
      "        [ 9.8431e-02, -3.4347e-02, -2.4402e-02,  ..., -4.5973e-02,\n",
      "         -1.2460e-02,  5.8376e-02],\n",
      "        [ 4.8728e-02, -3.6485e-02, -2.6181e-03,  ..., -7.5398e-02,\n",
      "         -2.1261e-05,  4.1138e-02],\n",
      "        ...,\n",
      "        [-2.2573e-02,  3.7026e-02,  3.0787e-02,  ...,  4.8758e-02,\n",
      "         -2.8567e-02, -1.2968e-02],\n",
      "        [ 1.7884e-03, -3.0659e-02,  2.5616e-02,  ..., -9.3895e-04,\n",
      "         -2.2130e-02,  3.7606e-02],\n",
      "        [ 9.5871e-03, -5.9899e-03, -4.5642e-02,  ..., -5.3146e-02,\n",
      "         -4.5482e-02, -1.5331e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0531, -0.0583, -0.0203,  ..., -0.0617, -0.0078,  0.0370],\n",
      "        [ 0.0986, -0.0345, -0.0243,  ..., -0.0461, -0.0122,  0.0585],\n",
      "        [ 0.0489, -0.0366, -0.0025,  ..., -0.0756,  0.0002,  0.0413],\n",
      "        ...,\n",
      "        [-0.0226,  0.0369,  0.0309,  ...,  0.0488, -0.0286, -0.0130],\n",
      "        [ 0.0018, -0.0306,  0.0257,  ..., -0.0010, -0.0222,  0.0377],\n",
      "        [ 0.0095, -0.0059, -0.0456,  ..., -0.0531, -0.0455, -0.0154]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0532, -0.0584, -0.0201,  ..., -0.0618, -0.0076,  0.0371],\n",
      "        [ 0.0987, -0.0345, -0.0242,  ..., -0.0463, -0.0119,  0.0587],\n",
      "        [ 0.0490, -0.0367, -0.0024,  ..., -0.0757,  0.0005,  0.0414],\n",
      "        ...,\n",
      "        [-0.0227,  0.0368,  0.0310,  ...,  0.0489, -0.0287, -0.0130],\n",
      "        [ 0.0018, -0.0306,  0.0257,  ..., -0.0010, -0.0223,  0.0378],\n",
      "        [ 0.0094, -0.0059, -0.0456,  ..., -0.0531, -0.0456, -0.0154]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0534, -0.0584, -0.0200,  ..., -0.0619, -0.0073,  0.0372],\n",
      "        [ 0.0989, -0.0346, -0.0240,  ..., -0.0464, -0.0117,  0.0588],\n",
      "        [ 0.0492, -0.0368, -0.0022,  ..., -0.0758,  0.0008,  0.0416],\n",
      "        ...,\n",
      "        [-0.0227,  0.0367,  0.0311,  ...,  0.0489, -0.0288, -0.0129],\n",
      "        [ 0.0018, -0.0306,  0.0258,  ..., -0.0011, -0.0223,  0.0378],\n",
      "        [ 0.0093, -0.0059, -0.0457,  ..., -0.0530, -0.0456, -0.0155]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0535, -0.0585, -0.0200,  ..., -0.0620, -0.0071,  0.0373],\n",
      "        [ 0.0990, -0.0347, -0.0240,  ..., -0.0465, -0.0114,  0.0589],\n",
      "        [ 0.0493, -0.0369, -0.0022,  ..., -0.0760,  0.0010,  0.0417],\n",
      "        ...,\n",
      "        [-0.0228,  0.0366,  0.0312,  ...,  0.0489, -0.0288, -0.0129],\n",
      "        [ 0.0018, -0.0306,  0.0258,  ..., -0.0012, -0.0224,  0.0379],\n",
      "        [ 0.0092, -0.0058, -0.0457,  ..., -0.0530, -0.0457, -0.0155]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0537, -0.0586, -0.0199,  ..., -0.0622, -0.0069,  0.0375],\n",
      "        [ 0.0992, -0.0347, -0.0238,  ..., -0.0467, -0.0111,  0.0591],\n",
      "        [ 0.0495, -0.0370, -0.0020,  ..., -0.0761,  0.0013,  0.0419],\n",
      "        ...,\n",
      "        [-0.0228,  0.0365,  0.0313,  ...,  0.0489, -0.0289, -0.0128],\n",
      "        [ 0.0018, -0.0306,  0.0259,  ..., -0.0012, -0.0225,  0.0380],\n",
      "        [ 0.0092, -0.0058, -0.0457,  ..., -0.0530, -0.0457, -0.0155]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0538, -0.0587, -0.0198,  ..., -0.0623, -0.0066,  0.0376],\n",
      "        [ 0.0993, -0.0348, -0.0237,  ..., -0.0468, -0.0109,  0.0592],\n",
      "        [ 0.0497, -0.0371, -0.0019,  ..., -0.0763,  0.0016,  0.0420],\n",
      "        ...,\n",
      "        [-0.0229,  0.0364,  0.0314,  ...,  0.0490, -0.0289, -0.0128],\n",
      "        [ 0.0018, -0.0306,  0.0259,  ..., -0.0013, -0.0226,  0.0381],\n",
      "        [ 0.0091, -0.0058, -0.0457,  ..., -0.0530, -0.0457, -0.0155]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0539, -0.0588, -0.0197,  ..., -0.0624, -0.0064,  0.0378],\n",
      "        [ 0.0995, -0.0349, -0.0236,  ..., -0.0469, -0.0106,  0.0594],\n",
      "        [ 0.0498, -0.0372, -0.0018,  ..., -0.0764,  0.0018,  0.0422],\n",
      "        ...,\n",
      "        [-0.0230,  0.0363,  0.0315,  ...,  0.0491, -0.0289, -0.0127],\n",
      "        [ 0.0017, -0.0305,  0.0260,  ..., -0.0014, -0.0226,  0.0382],\n",
      "        [ 0.0090, -0.0057, -0.0457,  ..., -0.0530, -0.0458, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0541, -0.0588, -0.0196,  ..., -0.0625, -0.0063,  0.0379],\n",
      "        [ 0.0996, -0.0350, -0.0235,  ..., -0.0470, -0.0104,  0.0595],\n",
      "        [ 0.0500, -0.0372, -0.0017,  ..., -0.0765,  0.0021,  0.0423],\n",
      "        ...,\n",
      "        [-0.0230,  0.0362,  0.0316,  ...,  0.0492, -0.0290, -0.0127],\n",
      "        [ 0.0018, -0.0305,  0.0260,  ..., -0.0014, -0.0227,  0.0383],\n",
      "        [ 0.0090, -0.0057, -0.0458,  ..., -0.0530, -0.0458, -0.0155]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0542, -0.0589, -0.0196,  ..., -0.0626, -0.0061,  0.0380],\n",
      "        [ 0.0998, -0.0351, -0.0234,  ..., -0.0472, -0.0103,  0.0596],\n",
      "        [ 0.0501, -0.0373, -0.0016,  ..., -0.0766,  0.0023,  0.0424],\n",
      "        ...,\n",
      "        [-0.0231,  0.0361,  0.0317,  ...,  0.0492, -0.0290, -0.0127],\n",
      "        [ 0.0018, -0.0305,  0.0260,  ..., -0.0015, -0.0227,  0.0383],\n",
      "        [ 0.0089, -0.0057, -0.0458,  ..., -0.0529, -0.0458, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0543, -0.0590, -0.0195,  ..., -0.0627, -0.0060,  0.0380],\n",
      "        [ 0.0999, -0.0351, -0.0233,  ..., -0.0472, -0.0101,  0.0597],\n",
      "        [ 0.0502, -0.0374, -0.0015,  ..., -0.0767,  0.0024,  0.0425],\n",
      "        ...,\n",
      "        [-0.0231,  0.0360,  0.0318,  ...,  0.0493, -0.0290, -0.0126],\n",
      "        [ 0.0018, -0.0305,  0.0260,  ..., -0.0015, -0.0227,  0.0384],\n",
      "        [ 0.0088, -0.0056, -0.0458,  ..., -0.0529, -0.0458, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0544, -0.0590, -0.0195,  ..., -0.0627, -0.0059,  0.0381],\n",
      "        [ 0.1000, -0.0352, -0.0233,  ..., -0.0473, -0.0100,  0.0598],\n",
      "        [ 0.0503, -0.0374, -0.0014,  ..., -0.0768,  0.0026,  0.0426],\n",
      "        ...,\n",
      "        [-0.0231,  0.0359,  0.0319,  ...,  0.0493, -0.0291, -0.0126],\n",
      "        [ 0.0018, -0.0305,  0.0260,  ..., -0.0016, -0.0227,  0.0385],\n",
      "        [ 0.0088, -0.0056, -0.0458,  ..., -0.0529, -0.0458, -0.0157]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0545, -0.0590, -0.0195,  ..., -0.0628, -0.0057,  0.0382],\n",
      "        [ 0.1001, -0.0352, -0.0232,  ..., -0.0474, -0.0098,  0.0599],\n",
      "        [ 0.0504, -0.0375, -0.0014,  ..., -0.0769,  0.0028,  0.0427],\n",
      "        ...,\n",
      "        [-0.0231,  0.0358,  0.0320,  ...,  0.0494, -0.0291, -0.0125],\n",
      "        [ 0.0018, -0.0305,  0.0261,  ..., -0.0016, -0.0228,  0.0386],\n",
      "        [ 0.0087, -0.0055, -0.0458,  ..., -0.0528, -0.0458, -0.0157]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0545, -0.0591, -0.0194,  ..., -0.0629, -0.0056,  0.0383],\n",
      "        [ 0.1002, -0.0353, -0.0231,  ..., -0.0475, -0.0097,  0.0600],\n",
      "        [ 0.0505, -0.0375, -0.0013,  ..., -0.0770,  0.0029,  0.0428],\n",
      "        ...,\n",
      "        [-0.0232,  0.0357,  0.0320,  ...,  0.0494, -0.0291, -0.0125],\n",
      "        [ 0.0017, -0.0305,  0.0262,  ..., -0.0016, -0.0228,  0.0387],\n",
      "        [ 0.0087, -0.0055, -0.0458,  ..., -0.0528, -0.0459, -0.0157]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0546, -0.0591, -0.0194,  ..., -0.0630, -0.0055,  0.0384],\n",
      "        [ 0.1003, -0.0353, -0.0231,  ..., -0.0476, -0.0096,  0.0601],\n",
      "        [ 0.0506, -0.0375, -0.0012,  ..., -0.0771,  0.0030,  0.0429],\n",
      "        ...,\n",
      "        [-0.0232,  0.0355,  0.0321,  ...,  0.0494, -0.0292, -0.0125],\n",
      "        [ 0.0017, -0.0305,  0.0263,  ..., -0.0016, -0.0228,  0.0388],\n",
      "        [ 0.0086, -0.0055, -0.0458,  ..., -0.0528, -0.0459, -0.0158]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0547, -0.0591, -0.0194,  ..., -0.0630, -0.0054,  0.0384],\n",
      "        [ 0.1004, -0.0354, -0.0230,  ..., -0.0477, -0.0095,  0.0602],\n",
      "        [ 0.0507, -0.0376, -0.0012,  ..., -0.0772,  0.0032,  0.0430],\n",
      "        ...,\n",
      "        [-0.0232,  0.0354,  0.0321,  ...,  0.0493, -0.0292, -0.0124],\n",
      "        [ 0.0017, -0.0305,  0.0263,  ..., -0.0016, -0.0229,  0.0389],\n",
      "        [ 0.0086, -0.0056, -0.0458,  ..., -0.0528, -0.0460, -0.0158]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0548, -0.0592, -0.0193,  ..., -0.0631, -0.0053,  0.0385],\n",
      "        [ 0.1005, -0.0354, -0.0230,  ..., -0.0478, -0.0094,  0.0603],\n",
      "        [ 0.0509, -0.0376, -0.0011,  ..., -0.0773,  0.0033,  0.0431],\n",
      "        ...,\n",
      "        [-0.0232,  0.0354,  0.0323,  ...,  0.0494, -0.0293, -0.0123],\n",
      "        [ 0.0017, -0.0305,  0.0264,  ..., -0.0016, -0.0229,  0.0390],\n",
      "        [ 0.0086, -0.0056, -0.0458,  ..., -0.0528, -0.0460, -0.0158]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0549, -0.0592, -0.0193,  ..., -0.0632, -0.0052,  0.0386],\n",
      "        [ 0.1006, -0.0355, -0.0229,  ..., -0.0479, -0.0093,  0.0604],\n",
      "        [ 0.0510, -0.0377, -0.0010,  ..., -0.0775,  0.0035,  0.0433],\n",
      "        ...,\n",
      "        [-0.0232,  0.0353,  0.0324,  ...,  0.0494, -0.0293, -0.0123],\n",
      "        [ 0.0017, -0.0305,  0.0264,  ..., -0.0016, -0.0230,  0.0391],\n",
      "        [ 0.0086, -0.0057, -0.0458,  ..., -0.0528, -0.0461, -0.0158]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "| epoch   1 step      400 |    400 batches | lr 0.0001 | ms/batch 31.03735 | loss 2.41892 | ppl    11.234\n",
      "Parameter containing:\n",
      "tensor([[ 0.0551, -0.0593, -0.0192,  ..., -0.0633, -0.0051,  0.0388],\n",
      "        [ 0.1007, -0.0356, -0.0228,  ..., -0.0480, -0.0092,  0.0605],\n",
      "        [ 0.0511, -0.0378, -0.0009,  ..., -0.0776,  0.0036,  0.0434],\n",
      "        ...,\n",
      "        [-0.0232,  0.0352,  0.0324,  ...,  0.0494, -0.0294, -0.0122],\n",
      "        [ 0.0017, -0.0305,  0.0264,  ..., -0.0016, -0.0230,  0.0392],\n",
      "        [ 0.0086, -0.0058, -0.0459,  ..., -0.0529, -0.0462, -0.0158]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0552, -0.0593, -0.0191,  ..., -0.0634, -0.0050,  0.0389],\n",
      "        [ 0.1009, -0.0356, -0.0227,  ..., -0.0481, -0.0091,  0.0606],\n",
      "        [ 0.0513, -0.0378, -0.0008,  ..., -0.0777,  0.0037,  0.0435],\n",
      "        ...,\n",
      "        [-0.0231,  0.0351,  0.0325,  ...,  0.0494, -0.0296, -0.0121],\n",
      "        [ 0.0016, -0.0304,  0.0265,  ..., -0.0017, -0.0230,  0.0393],\n",
      "        [ 0.0087, -0.0059, -0.0460,  ..., -0.0530, -0.0464, -0.0158]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0553, -0.0593, -0.0190,  ..., -0.0635, -0.0049,  0.0390],\n",
      "        [ 0.1010, -0.0356, -0.0225,  ..., -0.0482, -0.0089,  0.0608],\n",
      "        [ 0.0514, -0.0378, -0.0006,  ..., -0.0778,  0.0039,  0.0437],\n",
      "        ...,\n",
      "        [-0.0231,  0.0351,  0.0325,  ...,  0.0494, -0.0297, -0.0121],\n",
      "        [ 0.0016, -0.0304,  0.0265,  ..., -0.0017, -0.0230,  0.0394],\n",
      "        [ 0.0088, -0.0060, -0.0460,  ..., -0.0531, -0.0465, -0.0158]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0554, -0.0593, -0.0188,  ..., -0.0636, -0.0047,  0.0391],\n",
      "        [ 0.1011, -0.0356, -0.0224,  ..., -0.0483, -0.0088,  0.0609],\n",
      "        [ 0.0515, -0.0379, -0.0005,  ..., -0.0779,  0.0041,  0.0438],\n",
      "        ...,\n",
      "        [-0.0231,  0.0350,  0.0325,  ...,  0.0494, -0.0298, -0.0120],\n",
      "        [ 0.0016, -0.0304,  0.0266,  ..., -0.0017, -0.0230,  0.0395],\n",
      "        [ 0.0089, -0.0061, -0.0462,  ..., -0.0532, -0.0467, -0.0157]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0555, -0.0594, -0.0187,  ..., -0.0637, -0.0045,  0.0392],\n",
      "        [ 0.1012, -0.0357, -0.0222,  ..., -0.0484, -0.0086,  0.0610],\n",
      "        [ 0.0517, -0.0379, -0.0003,  ..., -0.0780,  0.0043,  0.0439],\n",
      "        ...,\n",
      "        [-0.0231,  0.0349,  0.0326,  ...,  0.0495, -0.0298, -0.0120],\n",
      "        [ 0.0016, -0.0304,  0.0266,  ..., -0.0017, -0.0230,  0.0396],\n",
      "        [ 0.0090, -0.0063, -0.0463,  ..., -0.0534, -0.0469, -0.0157]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0557, -0.0594, -0.0185,  ..., -0.0638, -0.0044,  0.0393],\n",
      "        [ 0.1014, -0.0357, -0.0221,  ..., -0.0485, -0.0084,  0.0612],\n",
      "        [ 0.0518, -0.0379, -0.0001,  ..., -0.0782,  0.0045,  0.0441],\n",
      "        ...,\n",
      "        [-0.0231,  0.0348,  0.0326,  ...,  0.0495, -0.0299, -0.0119],\n",
      "        [ 0.0015, -0.0304,  0.0267,  ..., -0.0017, -0.0230,  0.0396],\n",
      "        [ 0.0090, -0.0065, -0.0464,  ..., -0.0535, -0.0470, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 5.5747e-02, -5.9421e-02, -1.8409e-02,  ..., -6.3907e-02,\n",
      "         -4.2641e-03,  3.9431e-02],\n",
      "        [ 1.0146e-01, -3.5737e-02, -2.1945e-02,  ..., -4.8596e-02,\n",
      "         -8.3369e-03,  6.1255e-02],\n",
      "        [ 5.1885e-02, -3.7959e-02,  4.1300e-05,  ..., -7.8261e-02,\n",
      "          4.6680e-03,  4.4163e-02],\n",
      "        ...,\n",
      "        [-2.3045e-02,  3.4727e-02,  3.2713e-02,  ...,  4.9563e-02,\n",
      "         -2.9985e-02, -1.1854e-02],\n",
      "        [ 1.4387e-03, -3.0351e-02,  2.6706e-02,  ..., -1.7382e-03,\n",
      "         -2.2988e-02,  3.9665e-02],\n",
      "        [ 9.0579e-03, -6.6325e-03, -4.6527e-02,  ..., -5.3596e-02,\n",
      "         -4.7213e-02, -1.5582e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0558, -0.0594, -0.0183,  ..., -0.0640, -0.0042,  0.0395],\n",
      "        [ 0.1015, -0.0358, -0.0218,  ..., -0.0487, -0.0082,  0.0614],\n",
      "        [ 0.0520, -0.0380,  0.0002,  ..., -0.0784,  0.0048,  0.0443],\n",
      "        ...,\n",
      "        [-0.0231,  0.0347,  0.0328,  ...,  0.0496, -0.0300, -0.0118],\n",
      "        [ 0.0014, -0.0304,  0.0267,  ..., -0.0017, -0.0230,  0.0397],\n",
      "        [ 0.0091, -0.0068, -0.0467,  ..., -0.0537, -0.0474, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0559, -0.0595, -0.0182,  ..., -0.0641, -0.0041,  0.0396],\n",
      "        [ 0.1016, -0.0358, -0.0217,  ..., -0.0488, -0.0082,  0.0614],\n",
      "        [ 0.0521, -0.0380,  0.0004,  ..., -0.0785,  0.0049,  0.0444],\n",
      "        ...,\n",
      "        [-0.0231,  0.0346,  0.0329,  ...,  0.0497, -0.0301, -0.0117],\n",
      "        [ 0.0013, -0.0304,  0.0268,  ..., -0.0017, -0.0230,  0.0397],\n",
      "        [ 0.0091, -0.0069, -0.0468,  ..., -0.0538, -0.0475, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0560, -0.0595, -0.0180,  ..., -0.0641, -0.0040,  0.0397],\n",
      "        [ 0.1017, -0.0358, -0.0215,  ..., -0.0488, -0.0081,  0.0615],\n",
      "        [ 0.0521, -0.0380,  0.0005,  ..., -0.0785,  0.0050,  0.0445],\n",
      "        ...,\n",
      "        [-0.0231,  0.0345,  0.0329,  ...,  0.0497, -0.0302, -0.0117],\n",
      "        [ 0.0013, -0.0304,  0.0268,  ..., -0.0017, -0.0229,  0.0397],\n",
      "        [ 0.0092, -0.0071, -0.0469,  ..., -0.0538, -0.0477, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0560, -0.0595, -0.0179,  ..., -0.0642, -0.0040,  0.0397],\n",
      "        [ 0.1018, -0.0358, -0.0214,  ..., -0.0489, -0.0080,  0.0616],\n",
      "        [ 0.0522, -0.0380,  0.0007,  ..., -0.0786,  0.0052,  0.0446],\n",
      "        ...,\n",
      "        [-0.0231,  0.0345,  0.0330,  ...,  0.0498, -0.0302, -0.0118],\n",
      "        [ 0.0013, -0.0304,  0.0268,  ..., -0.0018, -0.0229,  0.0397],\n",
      "        [ 0.0093, -0.0073, -0.0471,  ..., -0.0539, -0.0479, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0561, -0.0595, -0.0178,  ..., -0.0642, -0.0039,  0.0398],\n",
      "        [ 0.1019, -0.0358, -0.0213,  ..., -0.0490, -0.0079,  0.0617],\n",
      "        [ 0.0523, -0.0380,  0.0008,  ..., -0.0787,  0.0053,  0.0447],\n",
      "        ...,\n",
      "        [-0.0232,  0.0345,  0.0331,  ...,  0.0499, -0.0302, -0.0118],\n",
      "        [ 0.0013, -0.0305,  0.0268,  ..., -0.0018, -0.0230,  0.0398],\n",
      "        [ 0.0093, -0.0075, -0.0472,  ..., -0.0539, -0.0480, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0562, -0.0596, -0.0177,  ..., -0.0643, -0.0038,  0.0399],\n",
      "        [ 0.1020, -0.0359, -0.0211,  ..., -0.0491, -0.0078,  0.0618],\n",
      "        [ 0.0524, -0.0381,  0.0009,  ..., -0.0788,  0.0054,  0.0448],\n",
      "        ...,\n",
      "        [-0.0233,  0.0346,  0.0332,  ...,  0.0500, -0.0302, -0.0119],\n",
      "        [ 0.0014, -0.0306,  0.0267,  ..., -0.0019, -0.0230,  0.0398],\n",
      "        [ 0.0093, -0.0076, -0.0473,  ..., -0.0540, -0.0482, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0563, -0.0597, -0.0176,  ..., -0.0644, -0.0037,  0.0400],\n",
      "        [ 0.1021, -0.0360, -0.0210,  ..., -0.0491, -0.0077,  0.0619],\n",
      "        [ 0.0526, -0.0381,  0.0011,  ..., -0.0789,  0.0055,  0.0449],\n",
      "        ...,\n",
      "        [-0.0234,  0.0346,  0.0333,  ...,  0.0501, -0.0302, -0.0119],\n",
      "        [ 0.0014, -0.0307,  0.0267,  ..., -0.0020, -0.0231,  0.0399],\n",
      "        [ 0.0094, -0.0076, -0.0474,  ..., -0.0540, -0.0483, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0564, -0.0597, -0.0175,  ..., -0.0645, -0.0036,  0.0401],\n",
      "        [ 0.1022, -0.0360, -0.0208,  ..., -0.0492, -0.0076,  0.0620],\n",
      "        [ 0.0527, -0.0382,  0.0012,  ..., -0.0790,  0.0057,  0.0450],\n",
      "        ...,\n",
      "        [-0.0235,  0.0346,  0.0334,  ...,  0.0502, -0.0302, -0.0120],\n",
      "        [ 0.0015, -0.0308,  0.0266,  ..., -0.0021, -0.0232,  0.0401],\n",
      "        [ 0.0094, -0.0077, -0.0475,  ..., -0.0541, -0.0484, -0.0156]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0566, -0.0598, -0.0173,  ..., -0.0646, -0.0035,  0.0402],\n",
      "        [ 0.1023, -0.0361, -0.0207,  ..., -0.0493, -0.0075,  0.0621],\n",
      "        [ 0.0528, -0.0382,  0.0014,  ..., -0.0791,  0.0058,  0.0451],\n",
      "        ...,\n",
      "        [-0.0235,  0.0347,  0.0335,  ...,  0.0503, -0.0302, -0.0120],\n",
      "        [ 0.0016, -0.0309,  0.0265,  ..., -0.0021, -0.0233,  0.0402],\n",
      "        [ 0.0094, -0.0078, -0.0476,  ..., -0.0541, -0.0485, -0.0157]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0567, -0.0598, -0.0172,  ..., -0.0647, -0.0033,  0.0403],\n",
      "        [ 0.1024, -0.0361, -0.0205,  ..., -0.0494, -0.0073,  0.0622],\n",
      "        [ 0.0529, -0.0382,  0.0015,  ..., -0.0792,  0.0060,  0.0452],\n",
      "        ...,\n",
      "        [-0.0236,  0.0347,  0.0336,  ...,  0.0503, -0.0302, -0.0121],\n",
      "        [ 0.0017, -0.0310,  0.0265,  ..., -0.0022, -0.0234,  0.0403],\n",
      "        [ 0.0094, -0.0079, -0.0477,  ..., -0.0542, -0.0486, -0.0157]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0568, -0.0599, -0.0170,  ..., -0.0648, -0.0032,  0.0405],\n",
      "        [ 0.1025, -0.0361, -0.0203,  ..., -0.0495, -0.0072,  0.0623],\n",
      "        [ 0.0530, -0.0383,  0.0017,  ..., -0.0793,  0.0061,  0.0453],\n",
      "        ...,\n",
      "        [-0.0236,  0.0347,  0.0337,  ...,  0.0504, -0.0302, -0.0121],\n",
      "        [ 0.0017, -0.0311,  0.0265,  ..., -0.0023, -0.0235,  0.0404],\n",
      "        [ 0.0093, -0.0079, -0.0478,  ..., -0.0542, -0.0487, -0.0157]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0569, -0.0599, -0.0168,  ..., -0.0649, -0.0031,  0.0406],\n",
      "        [ 0.1026, -0.0362, -0.0202,  ..., -0.0496, -0.0071,  0.0624],\n",
      "        [ 0.0531, -0.0383,  0.0019,  ..., -0.0794,  0.0063,  0.0454],\n",
      "        ...,\n",
      "        [-0.0237,  0.0348,  0.0338,  ...,  0.0505, -0.0302, -0.0121],\n",
      "        [ 0.0017, -0.0312,  0.0264,  ..., -0.0024, -0.0236,  0.0405],\n",
      "        [ 0.0093, -0.0080, -0.0478,  ..., -0.0542, -0.0488, -0.0158]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0570, -0.0600, -0.0166,  ..., -0.0650, -0.0030,  0.0407],\n",
      "        [ 0.1027, -0.0363, -0.0200,  ..., -0.0497, -0.0070,  0.0625],\n",
      "        [ 0.0532, -0.0384,  0.0021,  ..., -0.0795,  0.0064,  0.0456],\n",
      "        ...,\n",
      "        [-0.0238,  0.0348,  0.0339,  ...,  0.0505, -0.0302, -0.0122],\n",
      "        [ 0.0017, -0.0313,  0.0264,  ..., -0.0024, -0.0237,  0.0406],\n",
      "        [ 0.0092, -0.0080, -0.0479,  ..., -0.0542, -0.0489, -0.0158]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0571, -0.0601, -0.0164,  ..., -0.0651, -0.0029,  0.0408],\n",
      "        [ 0.1028, -0.0364, -0.0199,  ..., -0.0498, -0.0069,  0.0625],\n",
      "        [ 0.0534, -0.0385,  0.0022,  ..., -0.0796,  0.0066,  0.0457],\n",
      "        ...,\n",
      "        [-0.0238,  0.0349,  0.0340,  ...,  0.0506, -0.0302, -0.0123],\n",
      "        [ 0.0018, -0.0314,  0.0263,  ..., -0.0025, -0.0238,  0.0407],\n",
      "        [ 0.0091, -0.0081, -0.0479,  ..., -0.0542, -0.0490, -0.0159]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0572, -0.0602, -0.0163,  ..., -0.0652, -0.0028,  0.0409],\n",
      "        [ 0.1029, -0.0365, -0.0198,  ..., -0.0499, -0.0069,  0.0626],\n",
      "        [ 0.0535, -0.0386,  0.0024,  ..., -0.0797,  0.0067,  0.0458],\n",
      "        ...,\n",
      "        [-0.0239,  0.0350,  0.0341,  ...,  0.0507, -0.0301, -0.0123],\n",
      "        [ 0.0018, -0.0316,  0.0263,  ..., -0.0026, -0.0239,  0.0408],\n",
      "        [ 0.0091, -0.0081, -0.0479,  ..., -0.0542, -0.0490, -0.0160]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0573, -0.0604, -0.0161,  ..., -0.0653, -0.0027,  0.0409],\n",
      "        [ 0.1030, -0.0366, -0.0196,  ..., -0.0500, -0.0068,  0.0627],\n",
      "        [ 0.0536, -0.0386,  0.0026,  ..., -0.0798,  0.0068,  0.0459],\n",
      "        ...,\n",
      "        [-0.0239,  0.0351,  0.0342,  ...,  0.0508, -0.0301, -0.0124],\n",
      "        [ 0.0019, -0.0317,  0.0263,  ..., -0.0026, -0.0240,  0.0410],\n",
      "        [ 0.0090, -0.0082, -0.0479,  ..., -0.0543, -0.0491, -0.0160]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0574, -0.0604, -0.0160,  ..., -0.0654, -0.0026,  0.0410],\n",
      "        [ 0.1031, -0.0366, -0.0195,  ..., -0.0501, -0.0067,  0.0628],\n",
      "        [ 0.0537, -0.0387,  0.0027,  ..., -0.0799,  0.0070,  0.0460],\n",
      "        ...,\n",
      "        [-0.0240,  0.0351,  0.0344,  ...,  0.0509, -0.0301, -0.0125],\n",
      "        [ 0.0019, -0.0318,  0.0263,  ..., -0.0027, -0.0241,  0.0410],\n",
      "        [ 0.0089, -0.0082, -0.0478,  ..., -0.0542, -0.0491, -0.0162]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0576, -0.0605, -0.0158,  ..., -0.0655, -0.0025,  0.0412],\n",
      "        [ 0.1032, -0.0367, -0.0194,  ..., -0.0501, -0.0067,  0.0629],\n",
      "        [ 0.0538, -0.0388,  0.0029,  ..., -0.0800,  0.0071,  0.0461],\n",
      "        ...,\n",
      "        [-0.0241,  0.0351,  0.0345,  ...,  0.0510, -0.0300, -0.0126],\n",
      "        [ 0.0019, -0.0319,  0.0263,  ..., -0.0027, -0.0241,  0.0411],\n",
      "        [ 0.0088, -0.0082, -0.0478,  ..., -0.0542, -0.0491, -0.0163]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0577, -0.0606, -0.0157,  ..., -0.0656, -0.0024,  0.0413],\n",
      "        [ 0.1033, -0.0368, -0.0193,  ..., -0.0502, -0.0066,  0.0629],\n",
      "        [ 0.0540, -0.0388,  0.0031,  ..., -0.0801,  0.0073,  0.0462],\n",
      "        ...,\n",
      "        [-0.0242,  0.0352,  0.0347,  ...,  0.0511, -0.0300, -0.0127],\n",
      "        [ 0.0019, -0.0320,  0.0263,  ..., -0.0028, -0.0242,  0.0412],\n",
      "        [ 0.0087, -0.0083, -0.0478,  ..., -0.0541, -0.0491, -0.0163]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0578, -0.0606, -0.0156,  ..., -0.0657, -0.0023,  0.0414],\n",
      "        [ 0.1033, -0.0368, -0.0192,  ..., -0.0503, -0.0065,  0.0630],\n",
      "        [ 0.0541, -0.0389,  0.0032,  ..., -0.0802,  0.0074,  0.0464],\n",
      "        ...,\n",
      "        [-0.0243,  0.0352,  0.0348,  ...,  0.0511, -0.0299, -0.0128],\n",
      "        [ 0.0020, -0.0321,  0.0263,  ..., -0.0028, -0.0243,  0.0412],\n",
      "        [ 0.0086, -0.0083, -0.0477,  ..., -0.0541, -0.0491, -0.0164]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0579, -0.0607, -0.0155,  ..., -0.0659, -0.0022,  0.0415],\n",
      "        [ 0.1034, -0.0369, -0.0191,  ..., -0.0504, -0.0065,  0.0631],\n",
      "        [ 0.0542, -0.0389,  0.0033,  ..., -0.0803,  0.0075,  0.0465],\n",
      "        ...,\n",
      "        [-0.0244,  0.0351,  0.0350,  ...,  0.0512, -0.0299, -0.0128],\n",
      "        [ 0.0019, -0.0320,  0.0264,  ..., -0.0028, -0.0242,  0.0412],\n",
      "        [ 0.0085, -0.0083, -0.0476,  ..., -0.0540, -0.0491, -0.0165]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0580, -0.0608, -0.0154,  ..., -0.0660, -0.0021,  0.0416],\n",
      "        [ 0.1035, -0.0370, -0.0191,  ..., -0.0505, -0.0064,  0.0632],\n",
      "        [ 0.0543, -0.0390,  0.0034,  ..., -0.0804,  0.0075,  0.0466],\n",
      "        ...,\n",
      "        [-0.0245,  0.0351,  0.0351,  ...,  0.0512, -0.0298, -0.0129],\n",
      "        [ 0.0019, -0.0320,  0.0265,  ..., -0.0027, -0.0242,  0.0412],\n",
      "        [ 0.0083, -0.0083, -0.0475,  ..., -0.0539, -0.0490, -0.0166]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0581, -0.0609, -0.0154,  ..., -0.0661, -0.0021,  0.0417],\n",
      "        [ 0.1036, -0.0371, -0.0190,  ..., -0.0506, -0.0064,  0.0633],\n",
      "        [ 0.0544, -0.0391,  0.0035,  ..., -0.0805,  0.0076,  0.0467],\n",
      "        ...,\n",
      "        [-0.0245,  0.0351,  0.0353,  ...,  0.0513, -0.0298, -0.0129],\n",
      "        [ 0.0018, -0.0319,  0.0266,  ..., -0.0026, -0.0241,  0.0412],\n",
      "        [ 0.0082, -0.0082, -0.0474,  ..., -0.0538, -0.0489, -0.0167]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0582, -0.0610, -0.0153,  ..., -0.0662, -0.0021,  0.0417],\n",
      "        [ 0.1037, -0.0373, -0.0190,  ..., -0.0507, -0.0064,  0.0634],\n",
      "        [ 0.0545, -0.0392,  0.0035,  ..., -0.0806,  0.0076,  0.0467],\n",
      "        ...,\n",
      "        [-0.0246,  0.0351,  0.0354,  ...,  0.0513, -0.0298, -0.0129],\n",
      "        [ 0.0017, -0.0318,  0.0267,  ..., -0.0025, -0.0240,  0.0411],\n",
      "        [ 0.0080, -0.0081, -0.0473,  ..., -0.0537, -0.0489, -0.0169]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0583, -0.0611, -0.0153,  ..., -0.0663, -0.0021,  0.0418],\n",
      "        [ 0.1038, -0.0374, -0.0190,  ..., -0.0508, -0.0064,  0.0634],\n",
      "        [ 0.0546, -0.0393,  0.0036,  ..., -0.0807,  0.0076,  0.0468],\n",
      "        ...,\n",
      "        [-0.0247,  0.0351,  0.0355,  ...,  0.0514, -0.0298, -0.0129],\n",
      "        [ 0.0016, -0.0318,  0.0268,  ..., -0.0024, -0.0239,  0.0411],\n",
      "        [ 0.0079, -0.0080, -0.0471,  ..., -0.0535, -0.0488, -0.0170]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0583, -0.0611, -0.0153,  ..., -0.0663, -0.0020,  0.0418],\n",
      "        [ 0.1038, -0.0374, -0.0190,  ..., -0.0509, -0.0064,  0.0635],\n",
      "        [ 0.0547, -0.0393,  0.0037,  ..., -0.0808,  0.0077,  0.0469],\n",
      "        ...,\n",
      "        [-0.0248,  0.0351,  0.0357,  ...,  0.0514, -0.0298, -0.0128],\n",
      "        [ 0.0015, -0.0317,  0.0269,  ..., -0.0023, -0.0239,  0.0410],\n",
      "        [ 0.0077, -0.0079, -0.0470,  ..., -0.0534, -0.0487, -0.0171]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0584, -0.0612, -0.0152,  ..., -0.0664, -0.0019,  0.0419],\n",
      "        [ 0.1039, -0.0375, -0.0190,  ..., -0.0510, -0.0063,  0.0635],\n",
      "        [ 0.0547, -0.0393,  0.0037,  ..., -0.0809,  0.0078,  0.0469],\n",
      "        ...,\n",
      "        [-0.0249,  0.0350,  0.0358,  ...,  0.0514, -0.0298, -0.0128],\n",
      "        [ 0.0014, -0.0316,  0.0270,  ..., -0.0023, -0.0238,  0.0410],\n",
      "        [ 0.0076, -0.0079, -0.0470,  ..., -0.0534, -0.0487, -0.0172]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0585, -0.0612, -0.0152,  ..., -0.0665, -0.0019,  0.0419],\n",
      "        [ 0.1040, -0.0375, -0.0189,  ..., -0.0510, -0.0063,  0.0635],\n",
      "        [ 0.0548, -0.0394,  0.0038,  ..., -0.0810,  0.0078,  0.0470],\n",
      "        ...,\n",
      "        [-0.0249,  0.0350,  0.0359,  ...,  0.0515, -0.0298, -0.0127],\n",
      "        [ 0.0014, -0.0315,  0.0270,  ..., -0.0022, -0.0238,  0.0409],\n",
      "        [ 0.0076, -0.0079, -0.0469,  ..., -0.0533, -0.0487, -0.0172]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0585, -0.0613, -0.0151,  ..., -0.0666, -0.0018,  0.0419],\n",
      "        [ 0.1040, -0.0376, -0.0189,  ..., -0.0511, -0.0063,  0.0635],\n",
      "        [ 0.0549, -0.0394,  0.0038,  ..., -0.0810,  0.0079,  0.0470],\n",
      "        ...,\n",
      "        [-0.0250,  0.0350,  0.0360,  ...,  0.0515, -0.0298, -0.0127],\n",
      "        [ 0.0013, -0.0315,  0.0270,  ..., -0.0022, -0.0238,  0.0409],\n",
      "        [ 0.0075, -0.0079, -0.0470,  ..., -0.0533, -0.0488, -0.0172]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0586, -0.0613, -0.0151,  ..., -0.0666, -0.0018,  0.0419],\n",
      "        [ 0.1041, -0.0377, -0.0189,  ..., -0.0512, -0.0063,  0.0636],\n",
      "        [ 0.0549, -0.0394,  0.0039,  ..., -0.0811,  0.0079,  0.0470],\n",
      "        ...,\n",
      "        [-0.0250,  0.0350,  0.0361,  ...,  0.0515, -0.0299, -0.0126],\n",
      "        [ 0.0013, -0.0314,  0.0270,  ..., -0.0022, -0.0238,  0.0409],\n",
      "        [ 0.0075, -0.0079, -0.0470,  ..., -0.0533, -0.0488, -0.0172]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0586, -0.0614, -0.0150,  ..., -0.0667, -0.0018,  0.0420],\n",
      "        [ 0.1041, -0.0377, -0.0188,  ..., -0.0512, -0.0063,  0.0636],\n",
      "        [ 0.0550, -0.0395,  0.0040,  ..., -0.0812,  0.0079,  0.0471],\n",
      "        ...,\n",
      "        [-0.0250,  0.0350,  0.0361,  ...,  0.0515, -0.0300, -0.0125],\n",
      "        [ 0.0013, -0.0314,  0.0270,  ..., -0.0021, -0.0238,  0.0408],\n",
      "        [ 0.0075, -0.0080, -0.0470,  ..., -0.0534, -0.0489, -0.0172]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0587, -0.0614, -0.0150,  ..., -0.0668, -0.0018,  0.0420],\n",
      "        [ 0.1041, -0.0378, -0.0188,  ..., -0.0513, -0.0063,  0.0636],\n",
      "        [ 0.0550, -0.0396,  0.0041,  ..., -0.0813,  0.0079,  0.0471],\n",
      "        ...,\n",
      "        [-0.0250,  0.0349,  0.0362,  ...,  0.0515, -0.0301, -0.0124],\n",
      "        [ 0.0013, -0.0313,  0.0271,  ..., -0.0021, -0.0238,  0.0408],\n",
      "        [ 0.0075, -0.0080, -0.0471,  ..., -0.0534, -0.0490, -0.0172]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0587, -0.0615, -0.0149,  ..., -0.0669, -0.0018,  0.0420],\n",
      "        [ 0.1042, -0.0378, -0.0188,  ..., -0.0514, -0.0063,  0.0636],\n",
      "        [ 0.0551, -0.0396,  0.0041,  ..., -0.0813,  0.0080,  0.0471],\n",
      "        ...,\n",
      "        [-0.0250,  0.0348,  0.0362,  ...,  0.0515, -0.0301, -0.0124],\n",
      "        [ 0.0013, -0.0313,  0.0271,  ..., -0.0021, -0.0238,  0.0408],\n",
      "        [ 0.0075, -0.0081, -0.0471,  ..., -0.0534, -0.0491, -0.0172]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0588, -0.0615, -0.0149,  ..., -0.0670, -0.0017,  0.0420],\n",
      "        [ 0.1042, -0.0379, -0.0187,  ..., -0.0514, -0.0062,  0.0637],\n",
      "        [ 0.0551, -0.0396,  0.0042,  ..., -0.0814,  0.0081,  0.0472],\n",
      "        ...,\n",
      "        [-0.0251,  0.0348,  0.0363,  ...,  0.0515, -0.0302, -0.0123],\n",
      "        [ 0.0013, -0.0314,  0.0271,  ..., -0.0021, -0.0239,  0.0409],\n",
      "        [ 0.0075, -0.0081, -0.0472,  ..., -0.0535, -0.0492, -0.0171]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0589, -0.0615, -0.0148,  ..., -0.0670, -0.0016,  0.0421],\n",
      "        [ 0.1043, -0.0379, -0.0186,  ..., -0.0515, -0.0061,  0.0637],\n",
      "        [ 0.0552, -0.0397,  0.0043,  ..., -0.0815,  0.0082,  0.0473],\n",
      "        ...,\n",
      "        [-0.0251,  0.0348,  0.0363,  ...,  0.0515, -0.0302, -0.0123],\n",
      "        [ 0.0013, -0.0314,  0.0271,  ..., -0.0021, -0.0239,  0.0409],\n",
      "        [ 0.0075, -0.0082, -0.0472,  ..., -0.0535, -0.0493, -0.0171]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0589, -0.0616, -0.0147,  ..., -0.0671, -0.0016,  0.0422],\n",
      "        [ 0.1044, -0.0379, -0.0186,  ..., -0.0516, -0.0061,  0.0638],\n",
      "        [ 0.0553, -0.0397,  0.0044,  ..., -0.0816,  0.0083,  0.0473],\n",
      "        ...,\n",
      "        [-0.0251,  0.0348,  0.0363,  ...,  0.0515, -0.0302, -0.0123],\n",
      "        [ 0.0013, -0.0314,  0.0271,  ..., -0.0022, -0.0240,  0.0409],\n",
      "        [ 0.0075, -0.0082, -0.0473,  ..., -0.0536, -0.0494, -0.0171]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0590, -0.0616, -0.0146,  ..., -0.0672, -0.0015,  0.0422],\n",
      "        [ 0.1044, -0.0379, -0.0185,  ..., -0.0517, -0.0060,  0.0638],\n",
      "        [ 0.0553, -0.0397,  0.0045,  ..., -0.0816,  0.0084,  0.0474],\n",
      "        ...,\n",
      "        [-0.0252,  0.0348,  0.0364,  ...,  0.0516, -0.0302, -0.0123],\n",
      "        [ 0.0013, -0.0314,  0.0272,  ..., -0.0022, -0.0240,  0.0410],\n",
      "        [ 0.0076, -0.0083, -0.0474,  ..., -0.0536, -0.0496, -0.0171]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0591, -0.0616, -0.0145,  ..., -0.0673, -0.0014,  0.0423],\n",
      "        [ 0.1045, -0.0379, -0.0184,  ..., -0.0517, -0.0059,  0.0639],\n",
      "        [ 0.0554, -0.0396,  0.0046,  ..., -0.0817,  0.0085,  0.0475],\n",
      "        ...,\n",
      "        [-0.0252,  0.0348,  0.0364,  ...,  0.0516, -0.0302, -0.0123],\n",
      "        [ 0.0013, -0.0315,  0.0272,  ..., -0.0022, -0.0241,  0.0410],\n",
      "        [ 0.0076, -0.0084, -0.0475,  ..., -0.0537, -0.0497, -0.0170]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0591, -0.0616, -0.0145,  ..., -0.0673, -0.0014,  0.0424],\n",
      "        [ 0.1046, -0.0378, -0.0183,  ..., -0.0518, -0.0058,  0.0640],\n",
      "        [ 0.0555, -0.0396,  0.0046,  ..., -0.0818,  0.0086,  0.0475],\n",
      "        ...,\n",
      "        [-0.0252,  0.0347,  0.0364,  ...,  0.0516, -0.0302, -0.0123],\n",
      "        [ 0.0013, -0.0315,  0.0272,  ..., -0.0022, -0.0241,  0.0410],\n",
      "        [ 0.0077, -0.0084, -0.0476,  ..., -0.0538, -0.0499, -0.0169]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0592, -0.0616, -0.0144,  ..., -0.0674, -0.0013,  0.0424],\n",
      "        [ 0.1047, -0.0379, -0.0183,  ..., -0.0519, -0.0057,  0.0640],\n",
      "        [ 0.0555, -0.0396,  0.0047,  ..., -0.0818,  0.0087,  0.0476],\n",
      "        ...,\n",
      "        [-0.0253,  0.0348,  0.0365,  ...,  0.0516, -0.0302, -0.0123],\n",
      "        [ 0.0013, -0.0315,  0.0272,  ..., -0.0022, -0.0242,  0.0410],\n",
      "        [ 0.0077, -0.0085, -0.0477,  ..., -0.0538, -0.0500, -0.0169]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0593, -0.0616, -0.0143,  ..., -0.0674, -0.0012,  0.0425],\n",
      "        [ 0.1047, -0.0379, -0.0182,  ..., -0.0519, -0.0056,  0.0641],\n",
      "        [ 0.0556, -0.0396,  0.0048,  ..., -0.0819,  0.0088,  0.0476],\n",
      "        ...,\n",
      "        [-0.0253,  0.0348,  0.0366,  ...,  0.0517, -0.0302, -0.0123],\n",
      "        [ 0.0013, -0.0315,  0.0271,  ..., -0.0022, -0.0242,  0.0411],\n",
      "        [ 0.0077, -0.0085, -0.0478,  ..., -0.0538, -0.0501, -0.0169]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0593, -0.0616, -0.0142,  ..., -0.0675, -0.0011,  0.0425],\n",
      "        [ 0.1048, -0.0378, -0.0180,  ..., -0.0520, -0.0055,  0.0641],\n",
      "        [ 0.0556, -0.0396,  0.0050,  ..., -0.0819,  0.0089,  0.0477],\n",
      "        ...,\n",
      "        [-0.0254,  0.0348,  0.0366,  ...,  0.0518, -0.0302, -0.0123],\n",
      "        [ 0.0013, -0.0315,  0.0271,  ..., -0.0022, -0.0243,  0.0411],\n",
      "        [ 0.0077, -0.0086, -0.0479,  ..., -0.0539, -0.0501, -0.0169]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0594, -0.0616, -0.0141,  ..., -0.0675, -0.0010,  0.0425],\n",
      "        [ 0.1048, -0.0378, -0.0179,  ..., -0.0520, -0.0054,  0.0641],\n",
      "        [ 0.0557, -0.0396,  0.0051,  ..., -0.0820,  0.0091,  0.0477],\n",
      "        ...,\n",
      "        [-0.0255,  0.0349,  0.0367,  ...,  0.0518, -0.0302, -0.0123],\n",
      "        [ 0.0013, -0.0315,  0.0271,  ..., -0.0022, -0.0243,  0.0411],\n",
      "        [ 0.0077, -0.0086, -0.0480,  ..., -0.0539, -0.0502, -0.0169]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0594, -0.0615, -0.0140,  ..., -0.0676, -0.0009,  0.0425],\n",
      "        [ 0.1049, -0.0378, -0.0178,  ..., -0.0521, -0.0054,  0.0642],\n",
      "        [ 0.0558, -0.0396,  0.0052,  ..., -0.0821,  0.0092,  0.0477],\n",
      "        ...,\n",
      "        [-0.0254,  0.0348,  0.0367,  ...,  0.0518, -0.0302, -0.0123],\n",
      "        [ 0.0013, -0.0315,  0.0272,  ..., -0.0022, -0.0243,  0.0411],\n",
      "        [ 0.0077, -0.0087, -0.0481,  ..., -0.0540, -0.0503, -0.0169]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0594, -0.0615, -0.0139,  ..., -0.0676, -0.0009,  0.0426],\n",
      "        [ 0.1049, -0.0378, -0.0177,  ..., -0.0521, -0.0053,  0.0642],\n",
      "        [ 0.0558, -0.0396,  0.0053,  ..., -0.0821,  0.0093,  0.0478],\n",
      "        ...,\n",
      "        [-0.0255,  0.0348,  0.0367,  ...,  0.0518, -0.0302, -0.0123],\n",
      "        [ 0.0013, -0.0315,  0.0272,  ..., -0.0022, -0.0243,  0.0411],\n",
      "        [ 0.0078, -0.0087, -0.0482,  ..., -0.0540, -0.0504, -0.0169]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0595, -0.0615, -0.0138,  ..., -0.0677, -0.0008,  0.0426],\n",
      "        [ 0.1050, -0.0378, -0.0176,  ..., -0.0522, -0.0052,  0.0643],\n",
      "        [ 0.0559, -0.0396,  0.0054,  ..., -0.0822,  0.0094,  0.0478],\n",
      "        ...,\n",
      "        [-0.0255,  0.0349,  0.0367,  ...,  0.0519, -0.0302, -0.0124],\n",
      "        [ 0.0012, -0.0315,  0.0272,  ..., -0.0022, -0.0243,  0.0411],\n",
      "        [ 0.0078, -0.0087, -0.0482,  ..., -0.0540, -0.0505, -0.0170]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0595, -0.0615, -0.0136,  ..., -0.0677, -0.0007,  0.0427],\n",
      "        [ 0.1050, -0.0378, -0.0175,  ..., -0.0522, -0.0052,  0.0643],\n",
      "        [ 0.0559, -0.0396,  0.0056,  ..., -0.0822,  0.0095,  0.0479],\n",
      "        ...,\n",
      "        [-0.0256,  0.0349,  0.0368,  ...,  0.0520, -0.0301, -0.0125],\n",
      "        [ 0.0012, -0.0314,  0.0272,  ..., -0.0021, -0.0243,  0.0411],\n",
      "        [ 0.0077, -0.0087, -0.0483,  ..., -0.0541, -0.0505, -0.0170]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0596, -0.0616, -0.0135,  ..., -0.0678, -0.0006,  0.0427],\n",
      "        [ 0.1051, -0.0379, -0.0175,  ..., -0.0523, -0.0051,  0.0644],\n",
      "        [ 0.0560, -0.0396,  0.0057,  ..., -0.0823,  0.0096,  0.0479],\n",
      "        ...,\n",
      "        [-0.0256,  0.0349,  0.0368,  ...,  0.0520, -0.0301, -0.0126],\n",
      "        [ 0.0012, -0.0314,  0.0272,  ..., -0.0021, -0.0244,  0.0411],\n",
      "        [ 0.0077, -0.0087, -0.0484,  ..., -0.0541, -0.0506, -0.0171]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0596, -0.0616, -0.0134,  ..., -0.0678, -0.0005,  0.0427],\n",
      "        [ 0.1051, -0.0379, -0.0174,  ..., -0.0523, -0.0050,  0.0644],\n",
      "        [ 0.0560, -0.0396,  0.0058,  ..., -0.0823,  0.0097,  0.0480],\n",
      "        ...,\n",
      "        [-0.0257,  0.0350,  0.0368,  ...,  0.0521, -0.0301, -0.0127],\n",
      "        [ 0.0012, -0.0315,  0.0272,  ..., -0.0021, -0.0244,  0.0411],\n",
      "        [ 0.0077, -0.0087, -0.0484,  ..., -0.0541, -0.0506, -0.0172]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0597, -0.0616, -0.0133,  ..., -0.0679, -0.0004,  0.0427],\n",
      "        [ 0.1052, -0.0379, -0.0173,  ..., -0.0524, -0.0050,  0.0644],\n",
      "        [ 0.0561, -0.0397,  0.0059,  ..., -0.0824,  0.0098,  0.0480],\n",
      "        ...,\n",
      "        [-0.0258,  0.0350,  0.0369,  ...,  0.0522, -0.0299, -0.0128],\n",
      "        [ 0.0012, -0.0315,  0.0272,  ..., -0.0022, -0.0245,  0.0412],\n",
      "        [ 0.0077, -0.0087, -0.0485,  ..., -0.0541, -0.0506, -0.0173]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0597, -0.0616, -0.0132,  ..., -0.0679, -0.0003,  0.0428],\n",
      "        [ 0.1052, -0.0379, -0.0172,  ..., -0.0524, -0.0049,  0.0645],\n",
      "        [ 0.0562, -0.0397,  0.0060,  ..., -0.0824,  0.0100,  0.0481],\n",
      "        ...,\n",
      "        [-0.0259,  0.0351,  0.0370,  ...,  0.0523, -0.0298, -0.0129],\n",
      "        [ 0.0011, -0.0315,  0.0272,  ..., -0.0022, -0.0245,  0.0412],\n",
      "        [ 0.0076, -0.0087, -0.0486,  ..., -0.0541, -0.0506, -0.0173]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0598, -0.0616, -0.0132,  ..., -0.0680, -0.0002,  0.0428],\n",
      "        [ 0.1053, -0.0380, -0.0171,  ..., -0.0525, -0.0048,  0.0645],\n",
      "        [ 0.0562, -0.0397,  0.0060,  ..., -0.0825,  0.0101,  0.0481],\n",
      "        ...,\n",
      "        [-0.0260,  0.0352,  0.0370,  ...,  0.0524, -0.0298, -0.0130],\n",
      "        [ 0.0011, -0.0315,  0.0272,  ..., -0.0022, -0.0246,  0.0412],\n",
      "        [ 0.0077, -0.0086, -0.0486,  ..., -0.0541, -0.0507, -0.0174]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 5.9832e-02, -6.1620e-02, -1.3107e-02,  ..., -6.8031e-02,\n",
      "         -7.5897e-05,  4.2851e-02],\n",
      "        [ 1.0534e-01, -3.7974e-02, -1.7084e-02,  ..., -5.2525e-02,\n",
      "         -4.7071e-03,  6.4579e-02],\n",
      "        [ 5.6307e-02, -3.9657e-02,  6.1148e-03,  ..., -8.2554e-02,\n",
      "          1.0222e-02,  4.8177e-02],\n",
      "        ...,\n",
      "        [-2.6102e-02,  3.5298e-02,  3.7058e-02,  ...,  5.2509e-02,\n",
      "         -2.9682e-02, -1.3121e-02],\n",
      "        [ 1.1547e-03, -3.1544e-02,  2.7149e-02,  ..., -2.2511e-03,\n",
      "         -2.4618e-02,  4.1255e-02],\n",
      "        [ 7.6169e-03, -8.6050e-03, -4.8689e-02,  ..., -5.4096e-02,\n",
      "         -5.0701e-02, -1.7410e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 5.9883e-02, -6.1622e-02, -1.3053e-02,  ..., -6.8088e-02,\n",
      "          4.4010e-05,  4.2888e-02],\n",
      "        [ 1.0540e-01, -3.7990e-02, -1.7041e-02,  ..., -5.2579e-02,\n",
      "         -4.6242e-03,  6.4628e-02],\n",
      "        [ 5.6367e-02, -3.9654e-02,  6.1749e-03,  ..., -8.2613e-02,\n",
      "          1.0355e-02,  4.8224e-02],\n",
      "        ...,\n",
      "        [-2.6165e-02,  3.5345e-02,  3.7094e-02,  ...,  5.2572e-02,\n",
      "         -2.9619e-02, -1.3203e-02],\n",
      "        [ 1.2064e-03, -3.1582e-02,  2.7109e-02,  ..., -2.3156e-03,\n",
      "         -2.4680e-02,  4.1297e-02],\n",
      "        [ 7.5979e-03, -8.5812e-03, -4.8744e-02,  ..., -5.4127e-02,\n",
      "         -5.0721e-02, -1.7465e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0599, -0.0616, -0.0130,  ..., -0.0681,  0.0002,  0.0429],\n",
      "        [ 0.1055, -0.0380, -0.0170,  ..., -0.0526, -0.0045,  0.0647],\n",
      "        [ 0.0564, -0.0396,  0.0062,  ..., -0.0827,  0.0105,  0.0483],\n",
      "        ...,\n",
      "        [-0.0262,  0.0354,  0.0371,  ...,  0.0526, -0.0296, -0.0133],\n",
      "        [ 0.0013, -0.0316,  0.0271,  ..., -0.0024, -0.0247,  0.0414],\n",
      "        [ 0.0076, -0.0086, -0.0488,  ..., -0.0542, -0.0508, -0.0175]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0600, -0.0616, -0.0129,  ..., -0.0682,  0.0003,  0.0430],\n",
      "        [ 0.1055, -0.0380, -0.0169,  ..., -0.0527, -0.0044,  0.0648],\n",
      "        [ 0.0565, -0.0396,  0.0063,  ..., -0.0827,  0.0107,  0.0484],\n",
      "        ...,\n",
      "        [-0.0263,  0.0354,  0.0372,  ...,  0.0527, -0.0295, -0.0134],\n",
      "        [ 0.0013, -0.0316,  0.0270,  ..., -0.0025, -0.0248,  0.0414],\n",
      "        [ 0.0076, -0.0085, -0.0488,  ..., -0.0542, -0.0508, -0.0175]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0601, -0.0616, -0.0129,  ..., -0.0683,  0.0004,  0.0431],\n",
      "        [ 0.1056, -0.0380, -0.0169,  ..., -0.0528, -0.0043,  0.0648],\n",
      "        [ 0.0566, -0.0396,  0.0064,  ..., -0.0828,  0.0108,  0.0484],\n",
      "        ...,\n",
      "        [-0.0264,  0.0354,  0.0372,  ...,  0.0527, -0.0295, -0.0134],\n",
      "        [ 0.0014, -0.0316,  0.0270,  ..., -0.0025, -0.0249,  0.0415],\n",
      "        [ 0.0075, -0.0085, -0.0488,  ..., -0.0542, -0.0508, -0.0176]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0601, -0.0617, -0.0128,  ..., -0.0684,  0.0005,  0.0431],\n",
      "        [ 0.1057, -0.0381, -0.0168,  ..., -0.0529, -0.0042,  0.0649],\n",
      "        [ 0.0567, -0.0396,  0.0064,  ..., -0.0829,  0.0109,  0.0485],\n",
      "        ...,\n",
      "        [-0.0264,  0.0354,  0.0373,  ...,  0.0527, -0.0294, -0.0135],\n",
      "        [ 0.0014, -0.0316,  0.0270,  ..., -0.0025, -0.0249,  0.0416],\n",
      "        [ 0.0074, -0.0084, -0.0488,  ..., -0.0542, -0.0507, -0.0177]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0602, -0.0617, -0.0127,  ..., -0.0685,  0.0006,  0.0432],\n",
      "        [ 0.1058, -0.0381, -0.0168,  ..., -0.0530, -0.0041,  0.0650],\n",
      "        [ 0.0567, -0.0396,  0.0065,  ..., -0.0830,  0.0110,  0.0486],\n",
      "        ...,\n",
      "        [-0.0264,  0.0355,  0.0373,  ...,  0.0527, -0.0294, -0.0136],\n",
      "        [ 0.0014, -0.0316,  0.0271,  ..., -0.0025, -0.0250,  0.0416],\n",
      "        [ 0.0073, -0.0083, -0.0487,  ..., -0.0541, -0.0507, -0.0177]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0603, -0.0617, -0.0127,  ..., -0.0685,  0.0007,  0.0433],\n",
      "        [ 0.1058, -0.0381, -0.0168,  ..., -0.0530, -0.0040,  0.0651],\n",
      "        [ 0.0568, -0.0396,  0.0065,  ..., -0.0831,  0.0111,  0.0487],\n",
      "        ...,\n",
      "        [-0.0265,  0.0355,  0.0373,  ...,  0.0527, -0.0294, -0.0137],\n",
      "        [ 0.0014, -0.0316,  0.0270,  ..., -0.0026, -0.0250,  0.0417],\n",
      "        [ 0.0072, -0.0083, -0.0487,  ..., -0.0541, -0.0507, -0.0178]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0603, -0.0617, -0.0127,  ..., -0.0686,  0.0007,  0.0434],\n",
      "        [ 0.1059, -0.0381, -0.0167,  ..., -0.0531, -0.0039,  0.0651],\n",
      "        [ 0.0569, -0.0396,  0.0065,  ..., -0.0831,  0.0111,  0.0488],\n",
      "        ...,\n",
      "        [-0.0266,  0.0355,  0.0374,  ...,  0.0527, -0.0293, -0.0137],\n",
      "        [ 0.0014, -0.0315,  0.0270,  ..., -0.0026, -0.0250,  0.0417],\n",
      "        [ 0.0072, -0.0082, -0.0487,  ..., -0.0541, -0.0507, -0.0178]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0604, -0.0617, -0.0126,  ..., -0.0686,  0.0008,  0.0435],\n",
      "        [ 0.1060, -0.0380, -0.0167,  ..., -0.0531, -0.0038,  0.0652],\n",
      "        [ 0.0569, -0.0396,  0.0065,  ..., -0.0832,  0.0113,  0.0489],\n",
      "        ...,\n",
      "        [-0.0266,  0.0356,  0.0374,  ...,  0.0527, -0.0293, -0.0138],\n",
      "        [ 0.0014, -0.0315,  0.0270,  ..., -0.0026, -0.0251,  0.0417],\n",
      "        [ 0.0071, -0.0082, -0.0487,  ..., -0.0542, -0.0507, -0.0179]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0605, -0.0616, -0.0126,  ..., -0.0687,  0.0009,  0.0435],\n",
      "        [ 0.1061, -0.0380, -0.0167,  ..., -0.0532, -0.0037,  0.0653],\n",
      "        [ 0.0570, -0.0396,  0.0065,  ..., -0.0832,  0.0113,  0.0489],\n",
      "        ...,\n",
      "        [-0.0267,  0.0356,  0.0374,  ...,  0.0527, -0.0293, -0.0139],\n",
      "        [ 0.0015, -0.0316,  0.0269,  ..., -0.0027, -0.0252,  0.0418],\n",
      "        [ 0.0071, -0.0083, -0.0487,  ..., -0.0542, -0.0508, -0.0179]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0605, -0.0616, -0.0126,  ..., -0.0687,  0.0010,  0.0436],\n",
      "        [ 0.1061, -0.0380, -0.0167,  ..., -0.0532, -0.0037,  0.0653],\n",
      "        [ 0.0570, -0.0395,  0.0066,  ..., -0.0833,  0.0114,  0.0490],\n",
      "        ...,\n",
      "        [-0.0267,  0.0355,  0.0374,  ...,  0.0527, -0.0294, -0.0139],\n",
      "        [ 0.0015, -0.0316,  0.0269,  ..., -0.0027, -0.0252,  0.0419],\n",
      "        [ 0.0072, -0.0084, -0.0488,  ..., -0.0543, -0.0509, -0.0179]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0606, -0.0616, -0.0125,  ..., -0.0687,  0.0010,  0.0436],\n",
      "        [ 0.1061, -0.0379, -0.0166,  ..., -0.0532, -0.0036,  0.0654],\n",
      "        [ 0.0571, -0.0395,  0.0066,  ..., -0.0833,  0.0115,  0.0491],\n",
      "        ...,\n",
      "        [-0.0267,  0.0355,  0.0374,  ...,  0.0527, -0.0294, -0.0139],\n",
      "        [ 0.0016, -0.0316,  0.0268,  ..., -0.0028, -0.0253,  0.0419],\n",
      "        [ 0.0072, -0.0084, -0.0489,  ..., -0.0544, -0.0510, -0.0179]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0606, -0.0615, -0.0124,  ..., -0.0688,  0.0011,  0.0437],\n",
      "        [ 0.1062, -0.0379, -0.0166,  ..., -0.0533, -0.0035,  0.0655],\n",
      "        [ 0.0571, -0.0394,  0.0067,  ..., -0.0834,  0.0116,  0.0491],\n",
      "        ...,\n",
      "        [-0.0267,  0.0355,  0.0373,  ...,  0.0526, -0.0294, -0.0140],\n",
      "        [ 0.0016, -0.0316,  0.0268,  ..., -0.0028, -0.0254,  0.0420],\n",
      "        [ 0.0072, -0.0084, -0.0489,  ..., -0.0544, -0.0510, -0.0180]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0607, -0.0615, -0.0124,  ..., -0.0688,  0.0012,  0.0438],\n",
      "        [ 0.1063, -0.0379, -0.0166,  ..., -0.0533, -0.0035,  0.0655],\n",
      "        [ 0.0572, -0.0394,  0.0067,  ..., -0.0834,  0.0117,  0.0492],\n",
      "        ...,\n",
      "        [-0.0267,  0.0355,  0.0373,  ...,  0.0526, -0.0295, -0.0140],\n",
      "        [ 0.0017, -0.0316,  0.0267,  ..., -0.0029, -0.0254,  0.0421],\n",
      "        [ 0.0072, -0.0084, -0.0490,  ..., -0.0545, -0.0511, -0.0180]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0608, -0.0614, -0.0124,  ..., -0.0689,  0.0013,  0.0439],\n",
      "        [ 0.1064, -0.0378, -0.0165,  ..., -0.0534, -0.0034,  0.0656],\n",
      "        [ 0.0573, -0.0394,  0.0067,  ..., -0.0835,  0.0117,  0.0493],\n",
      "        ...,\n",
      "        [-0.0267,  0.0354,  0.0373,  ...,  0.0525, -0.0295, -0.0139],\n",
      "        [ 0.0017, -0.0316,  0.0267,  ..., -0.0029, -0.0255,  0.0422],\n",
      "        [ 0.0072, -0.0085, -0.0490,  ..., -0.0545, -0.0511, -0.0180]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0609, -0.0614, -0.0123,  ..., -0.0689,  0.0014,  0.0439],\n",
      "        [ 0.1064, -0.0378, -0.0164,  ..., -0.0534, -0.0033,  0.0657],\n",
      "        [ 0.0573, -0.0394,  0.0068,  ..., -0.0835,  0.0118,  0.0493],\n",
      "        ...,\n",
      "        [-0.0266,  0.0354,  0.0372,  ...,  0.0525, -0.0296, -0.0139],\n",
      "        [ 0.0018, -0.0316,  0.0266,  ..., -0.0030, -0.0256,  0.0423],\n",
      "        [ 0.0073, -0.0085, -0.0491,  ..., -0.0545, -0.0512, -0.0179]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0609, -0.0614, -0.0123,  ..., -0.0690,  0.0015,  0.0440],\n",
      "        [ 0.1064, -0.0378, -0.0164,  ..., -0.0534, -0.0032,  0.0657],\n",
      "        [ 0.0574, -0.0393,  0.0068,  ..., -0.0835,  0.0119,  0.0494],\n",
      "        ...,\n",
      "        [-0.0266,  0.0354,  0.0372,  ...,  0.0524, -0.0296, -0.0138],\n",
      "        [ 0.0018, -0.0316,  0.0266,  ..., -0.0030, -0.0257,  0.0424],\n",
      "        [ 0.0073, -0.0085, -0.0492,  ..., -0.0546, -0.0513, -0.0179]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0610, -0.0613, -0.0122,  ..., -0.0690,  0.0016,  0.0440],\n",
      "        [ 0.1065, -0.0377, -0.0163,  ..., -0.0535, -0.0031,  0.0658],\n",
      "        [ 0.0574, -0.0393,  0.0069,  ..., -0.0836,  0.0120,  0.0494],\n",
      "        ...,\n",
      "        [-0.0265,  0.0353,  0.0372,  ...,  0.0524, -0.0297, -0.0137],\n",
      "        [ 0.0019, -0.0316,  0.0266,  ..., -0.0031, -0.0257,  0.0425],\n",
      "        [ 0.0073, -0.0085, -0.0492,  ..., -0.0546, -0.0513, -0.0179]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0610, -0.0613, -0.0122,  ..., -0.0691,  0.0017,  0.0441],\n",
      "        [ 0.1066, -0.0377, -0.0163,  ..., -0.0535, -0.0030,  0.0658],\n",
      "        [ 0.0574, -0.0393,  0.0069,  ..., -0.0836,  0.0121,  0.0495],\n",
      "        ...,\n",
      "        [-0.0264,  0.0352,  0.0371,  ...,  0.0523, -0.0298, -0.0136],\n",
      "        [ 0.0019, -0.0315,  0.0266,  ..., -0.0030, -0.0257,  0.0426],\n",
      "        [ 0.0072, -0.0085, -0.0492,  ..., -0.0545, -0.0512, -0.0179]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0611, -0.0613, -0.0122,  ..., -0.0691,  0.0017,  0.0442],\n",
      "        [ 0.1066, -0.0377, -0.0163,  ..., -0.0536, -0.0029,  0.0659],\n",
      "        [ 0.0575, -0.0393,  0.0069,  ..., -0.0836,  0.0121,  0.0495],\n",
      "        ...,\n",
      "        [-0.0263,  0.0351,  0.0371,  ...,  0.0521, -0.0299, -0.0135],\n",
      "        [ 0.0018, -0.0315,  0.0266,  ..., -0.0030, -0.0256,  0.0426],\n",
      "        [ 0.0072, -0.0084, -0.0492,  ..., -0.0545, -0.0512, -0.0180]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0611, -0.0613, -0.0122,  ..., -0.0692,  0.0018,  0.0442],\n",
      "        [ 0.1066, -0.0376, -0.0163,  ..., -0.0536, -0.0029,  0.0660],\n",
      "        [ 0.0575, -0.0393,  0.0069,  ..., -0.0837,  0.0122,  0.0496],\n",
      "        ...,\n",
      "        [-0.0263,  0.0351,  0.0370,  ...,  0.0521, -0.0300, -0.0134],\n",
      "        [ 0.0018, -0.0314,  0.0266,  ..., -0.0029, -0.0256,  0.0426],\n",
      "        [ 0.0071, -0.0084, -0.0492,  ..., -0.0544, -0.0512, -0.0180]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0611, -0.0613, -0.0122,  ..., -0.0692,  0.0019,  0.0443],\n",
      "        [ 0.1067, -0.0376, -0.0162,  ..., -0.0536, -0.0028,  0.0660],\n",
      "        [ 0.0575, -0.0393,  0.0070,  ..., -0.0837,  0.0122,  0.0496],\n",
      "        ...,\n",
      "        [-0.0262,  0.0350,  0.0370,  ...,  0.0520, -0.0300, -0.0134],\n",
      "        [ 0.0018, -0.0313,  0.0266,  ..., -0.0029, -0.0256,  0.0427],\n",
      "        [ 0.0071, -0.0084, -0.0492,  ..., -0.0544, -0.0512, -0.0180]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0612, -0.0612, -0.0122,  ..., -0.0692,  0.0019,  0.0443],\n",
      "        [ 0.1068, -0.0376, -0.0162,  ..., -0.0537, -0.0027,  0.0661],\n",
      "        [ 0.0576, -0.0393,  0.0070,  ..., -0.0838,  0.0123,  0.0497],\n",
      "        ...,\n",
      "        [-0.0261,  0.0349,  0.0370,  ...,  0.0519, -0.0301, -0.0133],\n",
      "        [ 0.0018, -0.0313,  0.0266,  ..., -0.0029, -0.0256,  0.0428],\n",
      "        [ 0.0070, -0.0084, -0.0492,  ..., -0.0544, -0.0512, -0.0181]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0613, -0.0612, -0.0121,  ..., -0.0693,  0.0020,  0.0444],\n",
      "        [ 0.1068, -0.0375, -0.0162,  ..., -0.0537, -0.0026,  0.0661],\n",
      "        [ 0.0577, -0.0393,  0.0070,  ..., -0.0838,  0.0124,  0.0498],\n",
      "        ...,\n",
      "        [-0.0261,  0.0348,  0.0370,  ...,  0.0519, -0.0302, -0.0133],\n",
      "        [ 0.0019, -0.0313,  0.0266,  ..., -0.0029, -0.0256,  0.0428],\n",
      "        [ 0.0070, -0.0084, -0.0493,  ..., -0.0544, -0.0512, -0.0181]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0613, -0.0612, -0.0121,  ..., -0.0694,  0.0021,  0.0445],\n",
      "        [ 0.1069, -0.0375, -0.0161,  ..., -0.0538, -0.0025,  0.0662],\n",
      "        [ 0.0577, -0.0392,  0.0071,  ..., -0.0839,  0.0125,  0.0498],\n",
      "        ...,\n",
      "        [-0.0261,  0.0348,  0.0371,  ...,  0.0519, -0.0302, -0.0133],\n",
      "        [ 0.0019, -0.0313,  0.0265,  ..., -0.0029, -0.0256,  0.0429],\n",
      "        [ 0.0070, -0.0084, -0.0493,  ..., -0.0544, -0.0513, -0.0181]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0614, -0.0612, -0.0120,  ..., -0.0694,  0.0022,  0.0445],\n",
      "        [ 0.1069, -0.0375, -0.0161,  ..., -0.0538, -0.0025,  0.0663],\n",
      "        [ 0.0577, -0.0392,  0.0071,  ..., -0.0839,  0.0125,  0.0498],\n",
      "        ...,\n",
      "        [-0.0261,  0.0347,  0.0371,  ...,  0.0518, -0.0302, -0.0133],\n",
      "        [ 0.0019, -0.0313,  0.0265,  ..., -0.0029, -0.0256,  0.0429],\n",
      "        [ 0.0071, -0.0084, -0.0494,  ..., -0.0544, -0.0513, -0.0181]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0614, -0.0612, -0.0121,  ..., -0.0695,  0.0022,  0.0446],\n",
      "        [ 0.1070, -0.0374, -0.0161,  ..., -0.0539, -0.0024,  0.0663],\n",
      "        [ 0.0578, -0.0392,  0.0071,  ..., -0.0840,  0.0126,  0.0499],\n",
      "        ...,\n",
      "        [-0.0261,  0.0347,  0.0371,  ...,  0.0518, -0.0302, -0.0132],\n",
      "        [ 0.0020, -0.0313,  0.0264,  ..., -0.0029, -0.0257,  0.0430],\n",
      "        [ 0.0071, -0.0085, -0.0495,  ..., -0.0544, -0.0514, -0.0181]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0615, -0.0612, -0.0121,  ..., -0.0695,  0.0022,  0.0446],\n",
      "        [ 0.1070, -0.0374, -0.0160,  ..., -0.0539, -0.0024,  0.0664],\n",
      "        [ 0.0578, -0.0392,  0.0071,  ..., -0.0840,  0.0126,  0.0499],\n",
      "        ...,\n",
      "        [-0.0261,  0.0346,  0.0372,  ...,  0.0517, -0.0303, -0.0132],\n",
      "        [ 0.0020, -0.0312,  0.0264,  ..., -0.0029, -0.0257,  0.0430],\n",
      "        [ 0.0071, -0.0085, -0.0495,  ..., -0.0545, -0.0514, -0.0181]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0615, -0.0612, -0.0121,  ..., -0.0696,  0.0023,  0.0447],\n",
      "        [ 0.1071, -0.0374, -0.0160,  ..., -0.0540, -0.0023,  0.0664],\n",
      "        [ 0.0578, -0.0392,  0.0071,  ..., -0.0840,  0.0126,  0.0500],\n",
      "        ...,\n",
      "        [-0.0261,  0.0346,  0.0372,  ...,  0.0517, -0.0303, -0.0132],\n",
      "        [ 0.0020, -0.0312,  0.0264,  ..., -0.0029, -0.0257,  0.0430],\n",
      "        [ 0.0072, -0.0085, -0.0497,  ..., -0.0545, -0.0515, -0.0181]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0615, -0.0611, -0.0120,  ..., -0.0696,  0.0023,  0.0447],\n",
      "        [ 0.1071, -0.0374, -0.0160,  ..., -0.0540, -0.0023,  0.0665],\n",
      "        [ 0.0578, -0.0392,  0.0071,  ..., -0.0840,  0.0126,  0.0500],\n",
      "        ...,\n",
      "        [-0.0261,  0.0345,  0.0372,  ...,  0.0517, -0.0303, -0.0132],\n",
      "        [ 0.0021, -0.0311,  0.0263,  ..., -0.0029, -0.0257,  0.0430],\n",
      "        [ 0.0073, -0.0086, -0.0498,  ..., -0.0546, -0.0516, -0.0181]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0615, -0.0611, -0.0120,  ..., -0.0696,  0.0024,  0.0447],\n",
      "        [ 0.1071, -0.0374, -0.0160,  ..., -0.0540, -0.0022,  0.0665],\n",
      "        [ 0.0579, -0.0392,  0.0071,  ..., -0.0841,  0.0127,  0.0500],\n",
      "        ...,\n",
      "        [-0.0261,  0.0345,  0.0372,  ...,  0.0516, -0.0304, -0.0132],\n",
      "        [ 0.0021, -0.0311,  0.0263,  ..., -0.0029, -0.0257,  0.0430],\n",
      "        [ 0.0073, -0.0087, -0.0499,  ..., -0.0547, -0.0517, -0.0181]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0616, -0.0612, -0.0120,  ..., -0.0697,  0.0024,  0.0448],\n",
      "        [ 0.1072, -0.0373, -0.0159,  ..., -0.0541, -0.0022,  0.0666],\n",
      "        [ 0.0579, -0.0392,  0.0072,  ..., -0.0841,  0.0128,  0.0501],\n",
      "        ...,\n",
      "        [-0.0261,  0.0344,  0.0373,  ...,  0.0516, -0.0304, -0.0132],\n",
      "        [ 0.0022, -0.0311,  0.0262,  ..., -0.0030, -0.0257,  0.0431],\n",
      "        [ 0.0074, -0.0087, -0.0499,  ..., -0.0548, -0.0517, -0.0181]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0616, -0.0611, -0.0119,  ..., -0.0697,  0.0025,  0.0448],\n",
      "        [ 0.1072, -0.0373, -0.0159,  ..., -0.0541, -0.0021,  0.0666],\n",
      "        [ 0.0579, -0.0392,  0.0072,  ..., -0.0841,  0.0128,  0.0501],\n",
      "        ...,\n",
      "        [-0.0261,  0.0344,  0.0373,  ...,  0.0516, -0.0304, -0.0132],\n",
      "        [ 0.0022, -0.0311,  0.0262,  ..., -0.0030, -0.0257,  0.0431],\n",
      "        [ 0.0074, -0.0088, -0.0500,  ..., -0.0549, -0.0518, -0.0181]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0616, -0.0611, -0.0119,  ..., -0.0698,  0.0025,  0.0448],\n",
      "        [ 0.1073, -0.0373, -0.0159,  ..., -0.0541, -0.0020,  0.0667],\n",
      "        [ 0.0580, -0.0391,  0.0072,  ..., -0.0841,  0.0129,  0.0501],\n",
      "        ...,\n",
      "        [-0.0261,  0.0343,  0.0373,  ...,  0.0516, -0.0304, -0.0132],\n",
      "        [ 0.0023, -0.0312,  0.0261,  ..., -0.0031, -0.0257,  0.0432],\n",
      "        [ 0.0074, -0.0089, -0.0501,  ..., -0.0549, -0.0519, -0.0182]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0617, -0.0611, -0.0118,  ..., -0.0698,  0.0026,  0.0449],\n",
      "        [ 0.1073, -0.0373, -0.0158,  ..., -0.0542, -0.0020,  0.0667],\n",
      "        [ 0.0580, -0.0391,  0.0073,  ..., -0.0842,  0.0129,  0.0502],\n",
      "        ...,\n",
      "        [-0.0261,  0.0343,  0.0374,  ...,  0.0516, -0.0303, -0.0133],\n",
      "        [ 0.0024, -0.0312,  0.0260,  ..., -0.0031, -0.0258,  0.0432],\n",
      "        [ 0.0075, -0.0090, -0.0501,  ..., -0.0550, -0.0519, -0.0182]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0617, -0.0611, -0.0118,  ..., -0.0698,  0.0026,  0.0449],\n",
      "        [ 0.1073, -0.0372, -0.0157,  ..., -0.0542, -0.0019,  0.0667],\n",
      "        [ 0.0580, -0.0391,  0.0074,  ..., -0.0842,  0.0130,  0.0502],\n",
      "        ...,\n",
      "        [-0.0262,  0.0344,  0.0374,  ...,  0.0516, -0.0303, -0.0134],\n",
      "        [ 0.0024, -0.0312,  0.0259,  ..., -0.0032, -0.0258,  0.0433],\n",
      "        [ 0.0075, -0.0090, -0.0502,  ..., -0.0550, -0.0520, -0.0182]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0617, -0.0610, -0.0117,  ..., -0.0699,  0.0027,  0.0449],\n",
      "        [ 0.1074, -0.0372, -0.0157,  ..., -0.0542, -0.0019,  0.0668],\n",
      "        [ 0.0580, -0.0390,  0.0074,  ..., -0.0842,  0.0131,  0.0503],\n",
      "        ...,\n",
      "        [-0.0263,  0.0344,  0.0376,  ...,  0.0517, -0.0302, -0.0135],\n",
      "        [ 0.0025, -0.0313,  0.0259,  ..., -0.0033, -0.0259,  0.0434],\n",
      "        [ 0.0075, -0.0091, -0.0503,  ..., -0.0551, -0.0520, -0.0182]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0617, -0.0610, -0.0117,  ..., -0.0699,  0.0028,  0.0450],\n",
      "        [ 0.1075, -0.0371, -0.0156,  ..., -0.0543, -0.0018,  0.0668],\n",
      "        [ 0.0581, -0.0390,  0.0075,  ..., -0.0842,  0.0131,  0.0503],\n",
      "        ...,\n",
      "        [-0.0264,  0.0344,  0.0377,  ...,  0.0518, -0.0301, -0.0136],\n",
      "        [ 0.0026, -0.0313,  0.0258,  ..., -0.0033, -0.0259,  0.0434],\n",
      "        [ 0.0075, -0.0091, -0.0503,  ..., -0.0551, -0.0521, -0.0183]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0618, -0.0609, -0.0116,  ..., -0.0699,  0.0028,  0.0450],\n",
      "        [ 0.1075, -0.0370, -0.0156,  ..., -0.0543, -0.0017,  0.0669],\n",
      "        [ 0.0581, -0.0389,  0.0076,  ..., -0.0843,  0.0132,  0.0503],\n",
      "        ...,\n",
      "        [-0.0265,  0.0344,  0.0378,  ...,  0.0519, -0.0301, -0.0136],\n",
      "        [ 0.0026, -0.0313,  0.0257,  ..., -0.0033, -0.0260,  0.0435],\n",
      "        [ 0.0075, -0.0092, -0.0503,  ..., -0.0551, -0.0521, -0.0183]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0618, -0.0608, -0.0116,  ..., -0.0699,  0.0029,  0.0450],\n",
      "        [ 0.1075, -0.0369, -0.0155,  ..., -0.0543, -0.0016,  0.0669],\n",
      "        [ 0.0581, -0.0388,  0.0076,  ..., -0.0843,  0.0133,  0.0504],\n",
      "        ...,\n",
      "        [-0.0266,  0.0345,  0.0378,  ...,  0.0519, -0.0300, -0.0137],\n",
      "        [ 0.0027, -0.0314,  0.0257,  ..., -0.0034, -0.0260,  0.0436],\n",
      "        [ 0.0076, -0.0093, -0.0503,  ..., -0.0551, -0.0521, -0.0184]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0618, -0.0608, -0.0115,  ..., -0.0699,  0.0029,  0.0451],\n",
      "        [ 0.1075, -0.0369, -0.0155,  ..., -0.0543, -0.0016,  0.0669],\n",
      "        [ 0.0581, -0.0387,  0.0077,  ..., -0.0843,  0.0134,  0.0504],\n",
      "        ...,\n",
      "        [-0.0267,  0.0345,  0.0379,  ...,  0.0520, -0.0299, -0.0138],\n",
      "        [ 0.0027, -0.0314,  0.0256,  ..., -0.0035, -0.0261,  0.0436],\n",
      "        [ 0.0075, -0.0093, -0.0503,  ..., -0.0551, -0.0521, -0.0185]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0618, -0.0607, -0.0115,  ..., -0.0699,  0.0030,  0.0450],\n",
      "        [ 0.1076, -0.0368, -0.0154,  ..., -0.0543, -0.0015,  0.0669],\n",
      "        [ 0.0582, -0.0387,  0.0077,  ..., -0.0843,  0.0135,  0.0504],\n",
      "        ...,\n",
      "        [-0.0267,  0.0345,  0.0380,  ...,  0.0520, -0.0298, -0.0139],\n",
      "        [ 0.0028, -0.0314,  0.0255,  ..., -0.0035, -0.0261,  0.0436],\n",
      "        [ 0.0076, -0.0094, -0.0504,  ..., -0.0551, -0.0522, -0.0185]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0618, -0.0607, -0.0115,  ..., -0.0699,  0.0030,  0.0450],\n",
      "        [ 0.1076, -0.0368, -0.0154,  ..., -0.0543, -0.0015,  0.0670],\n",
      "        [ 0.0582, -0.0386,  0.0078,  ..., -0.0843,  0.0135,  0.0505],\n",
      "        ...,\n",
      "        [-0.0268,  0.0345,  0.0380,  ...,  0.0520, -0.0298, -0.0139],\n",
      "        [ 0.0028, -0.0314,  0.0255,  ..., -0.0034, -0.0261,  0.0436],\n",
      "        [ 0.0076, -0.0095, -0.0504,  ..., -0.0552, -0.0522, -0.0186]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0618, -0.0606, -0.0114,  ..., -0.0698,  0.0030,  0.0450],\n",
      "        [ 0.1076, -0.0368, -0.0154,  ..., -0.0543, -0.0015,  0.0670],\n",
      "        [ 0.0582, -0.0386,  0.0078,  ..., -0.0843,  0.0136,  0.0505],\n",
      "        ...,\n",
      "        [-0.0268,  0.0345,  0.0380,  ...,  0.0520, -0.0298, -0.0140],\n",
      "        [ 0.0027, -0.0313,  0.0255,  ..., -0.0034, -0.0260,  0.0436],\n",
      "        [ 0.0076, -0.0095, -0.0504,  ..., -0.0551, -0.0522, -0.0186]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0618, -0.0606, -0.0114,  ..., -0.0698,  0.0030,  0.0450],\n",
      "        [ 0.1076, -0.0367, -0.0154,  ..., -0.0543, -0.0015,  0.0670],\n",
      "        [ 0.0582, -0.0386,  0.0078,  ..., -0.0843,  0.0136,  0.0505],\n",
      "        ...,\n",
      "        [-0.0268,  0.0344,  0.0381,  ...,  0.0520, -0.0298, -0.0140],\n",
      "        [ 0.0026, -0.0313,  0.0255,  ..., -0.0033, -0.0259,  0.0436],\n",
      "        [ 0.0075, -0.0095, -0.0504,  ..., -0.0551, -0.0522, -0.0187]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0618, -0.0606, -0.0113,  ..., -0.0698,  0.0030,  0.0451],\n",
      "        [ 0.1076, -0.0367, -0.0153,  ..., -0.0543, -0.0014,  0.0670],\n",
      "        [ 0.0582, -0.0385,  0.0079,  ..., -0.0843,  0.0137,  0.0505],\n",
      "        ...,\n",
      "        [-0.0268,  0.0345,  0.0381,  ...,  0.0521, -0.0297, -0.0141],\n",
      "        [ 0.0026, -0.0312,  0.0256,  ..., -0.0032, -0.0258,  0.0435],\n",
      "        [ 0.0075, -0.0095, -0.0504,  ..., -0.0551, -0.0523, -0.0188]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0618, -0.0605, -0.0113,  ..., -0.0698,  0.0031,  0.0451],\n",
      "        [ 0.1076, -0.0366, -0.0153,  ..., -0.0543, -0.0014,  0.0670],\n",
      "        [ 0.0582, -0.0384,  0.0079,  ..., -0.0843,  0.0137,  0.0506],\n",
      "        ...,\n",
      "        [-0.0269,  0.0345,  0.0381,  ...,  0.0521, -0.0297, -0.0142],\n",
      "        [ 0.0025, -0.0311,  0.0256,  ..., -0.0032, -0.0257,  0.0435],\n",
      "        [ 0.0075, -0.0096, -0.0504,  ..., -0.0551, -0.0523, -0.0188]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0618, -0.0604, -0.0112,  ..., -0.0698,  0.0031,  0.0451],\n",
      "        [ 0.1076, -0.0365, -0.0153,  ..., -0.0543, -0.0014,  0.0670],\n",
      "        [ 0.0583, -0.0384,  0.0080,  ..., -0.0843,  0.0138,  0.0506],\n",
      "        ...,\n",
      "        [-0.0269,  0.0345,  0.0382,  ...,  0.0522, -0.0296, -0.0142],\n",
      "        [ 0.0025, -0.0311,  0.0256,  ..., -0.0031, -0.0257,  0.0434],\n",
      "        [ 0.0075, -0.0096, -0.0504,  ..., -0.0551, -0.0523, -0.0189]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0619, -0.0603, -0.0112,  ..., -0.0698,  0.0032,  0.0451],\n",
      "        [ 0.1077, -0.0365, -0.0153,  ..., -0.0543, -0.0013,  0.0671],\n",
      "        [ 0.0583, -0.0383,  0.0080,  ..., -0.0843,  0.0139,  0.0507],\n",
      "        ...,\n",
      "        [-0.0270,  0.0346,  0.0383,  ...,  0.0523, -0.0295, -0.0143],\n",
      "        [ 0.0024, -0.0311,  0.0256,  ..., -0.0031, -0.0256,  0.0434],\n",
      "        [ 0.0075, -0.0097, -0.0504,  ..., -0.0551, -0.0523, -0.0190]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0619, -0.0603, -0.0112,  ..., -0.0698,  0.0033,  0.0451],\n",
      "        [ 0.1077, -0.0364, -0.0152,  ..., -0.0543, -0.0012,  0.0671],\n",
      "        [ 0.0584, -0.0383,  0.0081,  ..., -0.0844,  0.0140,  0.0507],\n",
      "        ...,\n",
      "        [-0.0271,  0.0346,  0.0383,  ...,  0.0523, -0.0294, -0.0144],\n",
      "        [ 0.0024, -0.0311,  0.0256,  ..., -0.0031, -0.0256,  0.0434],\n",
      "        [ 0.0074, -0.0097, -0.0504,  ..., -0.0551, -0.0523, -0.0191]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0619, -0.0603, -0.0111,  ..., -0.0699,  0.0033,  0.0452],\n",
      "        [ 0.1078, -0.0364, -0.0152,  ..., -0.0544, -0.0011,  0.0672],\n",
      "        [ 0.0584, -0.0383,  0.0081,  ..., -0.0844,  0.0141,  0.0508],\n",
      "        ...,\n",
      "        [-0.0271,  0.0346,  0.0384,  ...,  0.0524, -0.0294, -0.0145],\n",
      "        [ 0.0023, -0.0310,  0.0256,  ..., -0.0030, -0.0255,  0.0434],\n",
      "        [ 0.0074, -0.0097, -0.0504,  ..., -0.0551, -0.0523, -0.0192]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0620, -0.0602, -0.0111,  ..., -0.0699,  0.0034,  0.0452],\n",
      "        [ 0.1078, -0.0363, -0.0152,  ..., -0.0544, -0.0011,  0.0672],\n",
      "        [ 0.0585, -0.0382,  0.0081,  ..., -0.0845,  0.0141,  0.0508],\n",
      "        ...,\n",
      "        [-0.0272,  0.0347,  0.0384,  ...,  0.0524, -0.0293, -0.0146],\n",
      "        [ 0.0023, -0.0310,  0.0255,  ..., -0.0030, -0.0255,  0.0434],\n",
      "        [ 0.0073, -0.0097, -0.0504,  ..., -0.0550, -0.0523, -0.0193]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0621, -0.0602, -0.0111,  ..., -0.0700,  0.0035,  0.0453],\n",
      "        [ 0.1079, -0.0363, -0.0152,  ..., -0.0545, -0.0010,  0.0673],\n",
      "        [ 0.0586, -0.0382,  0.0081,  ..., -0.0845,  0.0142,  0.0509],\n",
      "        ...,\n",
      "        [-0.0273,  0.0347,  0.0385,  ...,  0.0525, -0.0292, -0.0147],\n",
      "        [ 0.0023, -0.0310,  0.0255,  ..., -0.0030, -0.0255,  0.0434],\n",
      "        [ 0.0073, -0.0097, -0.0504,  ..., -0.0550, -0.0523, -0.0193]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0621, -0.0601, -0.0111,  ..., -0.0700,  0.0036,  0.0453],\n",
      "        [ 0.1079, -0.0362, -0.0152,  ..., -0.0545, -0.0009,  0.0673],\n",
      "        [ 0.0586, -0.0381,  0.0081,  ..., -0.0846,  0.0143,  0.0509],\n",
      "        ...,\n",
      "        [-0.0273,  0.0348,  0.0385,  ...,  0.0526, -0.0292, -0.0148],\n",
      "        [ 0.0023, -0.0311,  0.0254,  ..., -0.0031, -0.0255,  0.0434],\n",
      "        [ 0.0072, -0.0097, -0.0504,  ..., -0.0550, -0.0523, -0.0194]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0622, -0.0600, -0.0111,  ..., -0.0700,  0.0036,  0.0454],\n",
      "        [ 0.1080, -0.0362, -0.0152,  ..., -0.0545, -0.0008,  0.0674],\n",
      "        [ 0.0587, -0.0381,  0.0082,  ..., -0.0846,  0.0144,  0.0510],\n",
      "        ...,\n",
      "        [-0.0274,  0.0349,  0.0386,  ...,  0.0527, -0.0291, -0.0149],\n",
      "        [ 0.0023, -0.0311,  0.0254,  ..., -0.0031, -0.0254,  0.0434],\n",
      "        [ 0.0072, -0.0097, -0.0504,  ..., -0.0550, -0.0523, -0.0195]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0622, -0.0600, -0.0111,  ..., -0.0700,  0.0037,  0.0454],\n",
      "        [ 0.1080, -0.0361, -0.0152,  ..., -0.0546, -0.0008,  0.0674],\n",
      "        [ 0.0587, -0.0380,  0.0082,  ..., -0.0846,  0.0145,  0.0510],\n",
      "        ...,\n",
      "        [-0.0275,  0.0350,  0.0386,  ...,  0.0527, -0.0291, -0.0149],\n",
      "        [ 0.0023, -0.0311,  0.0254,  ..., -0.0031, -0.0254,  0.0434],\n",
      "        [ 0.0071, -0.0097, -0.0504,  ..., -0.0550, -0.0523, -0.0196]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0622, -0.0599, -0.0111,  ..., -0.0700,  0.0038,  0.0454],\n",
      "        [ 0.1080, -0.0360, -0.0152,  ..., -0.0546, -0.0007,  0.0674],\n",
      "        [ 0.0588, -0.0380,  0.0082,  ..., -0.0846,  0.0145,  0.0511],\n",
      "        ...,\n",
      "        [-0.0275,  0.0350,  0.0386,  ...,  0.0528, -0.0291, -0.0149],\n",
      "        [ 0.0023, -0.0311,  0.0253,  ..., -0.0031, -0.0254,  0.0434],\n",
      "        [ 0.0071, -0.0097, -0.0504,  ..., -0.0549, -0.0523, -0.0196]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0623, -0.0598, -0.0110,  ..., -0.0700,  0.0038,  0.0455],\n",
      "        [ 0.1081, -0.0360, -0.0152,  ..., -0.0546, -0.0006,  0.0675],\n",
      "        [ 0.0588, -0.0379,  0.0082,  ..., -0.0847,  0.0146,  0.0511],\n",
      "        ...,\n",
      "        [-0.0275,  0.0351,  0.0386,  ...,  0.0528, -0.0290, -0.0150],\n",
      "        [ 0.0023, -0.0311,  0.0253,  ..., -0.0031, -0.0254,  0.0434],\n",
      "        [ 0.0071, -0.0097, -0.0504,  ..., -0.0549, -0.0524, -0.0197]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0623, -0.0597, -0.0110,  ..., -0.0700,  0.0038,  0.0455],\n",
      "        [ 0.1081, -0.0359, -0.0152,  ..., -0.0546, -0.0006,  0.0675],\n",
      "        [ 0.0588, -0.0379,  0.0082,  ..., -0.0847,  0.0146,  0.0511],\n",
      "        ...,\n",
      "        [-0.0275,  0.0351,  0.0386,  ...,  0.0528, -0.0291, -0.0150],\n",
      "        [ 0.0024, -0.0312,  0.0252,  ..., -0.0032, -0.0254,  0.0435],\n",
      "        [ 0.0070, -0.0097, -0.0505,  ..., -0.0549, -0.0524, -0.0197]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0624, -0.0597, -0.0110,  ..., -0.0701,  0.0039,  0.0455],\n",
      "        [ 0.1081, -0.0359, -0.0152,  ..., -0.0546, -0.0005,  0.0675],\n",
      "        [ 0.0589, -0.0378,  0.0083,  ..., -0.0847,  0.0147,  0.0512],\n",
      "        ...,\n",
      "        [-0.0275,  0.0351,  0.0386,  ...,  0.0528, -0.0290, -0.0150],\n",
      "        [ 0.0024, -0.0312,  0.0252,  ..., -0.0032, -0.0254,  0.0435],\n",
      "        [ 0.0070, -0.0097, -0.0504,  ..., -0.0549, -0.0524, -0.0197]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0624, -0.0597, -0.0110,  ..., -0.0701,  0.0040,  0.0455],\n",
      "        [ 0.1082, -0.0358, -0.0152,  ..., -0.0546, -0.0005,  0.0676],\n",
      "        [ 0.0589, -0.0378,  0.0083,  ..., -0.0847,  0.0148,  0.0512],\n",
      "        ...,\n",
      "        [-0.0275,  0.0352,  0.0386,  ...,  0.0528, -0.0290, -0.0150],\n",
      "        [ 0.0024, -0.0312,  0.0252,  ..., -0.0033, -0.0254,  0.0435],\n",
      "        [ 0.0070, -0.0098, -0.0505,  ..., -0.0550, -0.0524, -0.0197]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0624, -0.0597, -0.0109,  ..., -0.0701,  0.0040,  0.0455],\n",
      "        [ 0.1082, -0.0358, -0.0152,  ..., -0.0546, -0.0004,  0.0676],\n",
      "        [ 0.0590, -0.0378,  0.0083,  ..., -0.0848,  0.0148,  0.0512],\n",
      "        ...,\n",
      "        [-0.0275,  0.0352,  0.0386,  ...,  0.0528, -0.0290, -0.0151],\n",
      "        [ 0.0025, -0.0313,  0.0252,  ..., -0.0033, -0.0254,  0.0436],\n",
      "        [ 0.0070, -0.0098, -0.0505,  ..., -0.0550, -0.0524, -0.0197]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0625, -0.0596, -0.0109,  ..., -0.0701,  0.0041,  0.0456],\n",
      "        [ 0.1083, -0.0358, -0.0152,  ..., -0.0547, -0.0003,  0.0676],\n",
      "        [ 0.0590, -0.0378,  0.0083,  ..., -0.0848,  0.0149,  0.0513],\n",
      "        ...,\n",
      "        [-0.0274,  0.0351,  0.0385,  ...,  0.0528, -0.0291, -0.0150],\n",
      "        [ 0.0025, -0.0313,  0.0251,  ..., -0.0034, -0.0254,  0.0437],\n",
      "        [ 0.0070, -0.0098, -0.0505,  ..., -0.0550, -0.0524, -0.0198]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0625, -0.0596, -0.0109,  ..., -0.0701,  0.0041,  0.0456],\n",
      "        [ 0.1083, -0.0358, -0.0152,  ..., -0.0547, -0.0003,  0.0676],\n",
      "        [ 0.0591, -0.0378,  0.0083,  ..., -0.0849,  0.0150,  0.0513],\n",
      "        ...,\n",
      "        [-0.0274,  0.0351,  0.0385,  ...,  0.0527, -0.0291, -0.0150],\n",
      "        [ 0.0025, -0.0313,  0.0251,  ..., -0.0034, -0.0254,  0.0437],\n",
      "        [ 0.0070, -0.0097, -0.0505,  ..., -0.0550, -0.0524, -0.0198]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0625, -0.0596, -0.0109,  ..., -0.0702,  0.0042,  0.0456],\n",
      "        [ 0.1083, -0.0358, -0.0152,  ..., -0.0548, -0.0002,  0.0676],\n",
      "        [ 0.0592, -0.0378,  0.0082,  ..., -0.0849,  0.0150,  0.0514],\n",
      "        ...,\n",
      "        [-0.0273,  0.0350,  0.0384,  ...,  0.0527, -0.0292, -0.0150],\n",
      "        [ 0.0026, -0.0313,  0.0250,  ..., -0.0034, -0.0254,  0.0438],\n",
      "        [ 0.0070, -0.0098, -0.0505,  ..., -0.0550, -0.0524, -0.0198]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0626, -0.0596, -0.0109,  ..., -0.0702,  0.0042,  0.0456],\n",
      "        [ 0.1084, -0.0358, -0.0152,  ..., -0.0548, -0.0002,  0.0677],\n",
      "        [ 0.0592, -0.0378,  0.0082,  ..., -0.0850,  0.0150,  0.0514],\n",
      "        ...,\n",
      "        [-0.0272,  0.0349,  0.0384,  ...,  0.0525, -0.0293, -0.0149],\n",
      "        [ 0.0026, -0.0313,  0.0250,  ..., -0.0034, -0.0254,  0.0439],\n",
      "        [ 0.0070, -0.0098, -0.0506,  ..., -0.0550, -0.0525, -0.0198]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0626, -0.0597, -0.0109,  ..., -0.0702,  0.0042,  0.0456],\n",
      "        [ 0.1084, -0.0358, -0.0152,  ..., -0.0548, -0.0002,  0.0677],\n",
      "        [ 0.0592, -0.0378,  0.0082,  ..., -0.0850,  0.0150,  0.0515],\n",
      "        ...,\n",
      "        [-0.0271,  0.0349,  0.0383,  ...,  0.0524, -0.0293, -0.0149],\n",
      "        [ 0.0027, -0.0313,  0.0249,  ..., -0.0035, -0.0254,  0.0440],\n",
      "        [ 0.0070, -0.0098, -0.0506,  ..., -0.0550, -0.0525, -0.0198]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0626, -0.0596, -0.0108,  ..., -0.0702,  0.0043,  0.0457],\n",
      "        [ 0.1084, -0.0357, -0.0152,  ..., -0.0548, -0.0001,  0.0677],\n",
      "        [ 0.0593, -0.0378,  0.0082,  ..., -0.0850,  0.0150,  0.0515],\n",
      "        ...,\n",
      "        [-0.0270,  0.0348,  0.0383,  ...,  0.0523, -0.0295, -0.0148],\n",
      "        [ 0.0027, -0.0313,  0.0249,  ..., -0.0035, -0.0254,  0.0440],\n",
      "        [ 0.0070, -0.0098, -0.0507,  ..., -0.0551, -0.0525, -0.0198]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0626, -0.0596, -0.0108,  ..., -0.0703,  0.0043,  0.0457],\n",
      "        [ 0.1084, -0.0357, -0.0152,  ..., -0.0549, -0.0001,  0.0678],\n",
      "        [ 0.0593, -0.0378,  0.0082,  ..., -0.0851,  0.0150,  0.0515],\n",
      "        ...,\n",
      "        [-0.0269,  0.0347,  0.0382,  ...,  0.0522, -0.0296, -0.0147],\n",
      "        [ 0.0027, -0.0312,  0.0249,  ..., -0.0035, -0.0253,  0.0440],\n",
      "        [ 0.0070, -0.0098, -0.0507,  ..., -0.0551, -0.0525, -0.0199]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 6.2628e-02, -5.9546e-02, -1.0794e-02,  ..., -7.0281e-02,\n",
      "          4.3336e-03,  4.5744e-02],\n",
      "        [ 1.0846e-01, -3.5693e-02, -1.5208e-02,  ..., -5.4905e-02,\n",
      "         -7.1221e-05,  6.7814e-02],\n",
      "        [ 5.9308e-02, -3.7752e-02,  8.1531e-03,  ..., -8.5096e-02,\n",
      "          1.5063e-02,  5.1589e-02],\n",
      "        ...,\n",
      "        [-2.6815e-02,  3.4625e-02,  3.8112e-02,  ...,  5.2115e-02,\n",
      "         -2.9659e-02, -1.4661e-02],\n",
      "        [ 2.6906e-03, -3.1187e-02,  2.4932e-02,  ..., -3.4275e-03,\n",
      "         -2.5276e-02,  4.4083e-02],\n",
      "        [ 6.9857e-03, -9.8782e-03, -5.0771e-02,  ..., -5.5098e-02,\n",
      "         -5.2535e-02, -1.9893e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 6.2660e-02, -5.9539e-02, -1.0782e-02,  ..., -7.0315e-02,\n",
      "          4.3627e-03,  4.5788e-02],\n",
      "        [ 1.0850e-01, -3.5689e-02, -1.5210e-02,  ..., -5.4945e-02,\n",
      "         -4.2720e-05,  6.7860e-02],\n",
      "        [ 5.9344e-02, -3.7745e-02,  8.1453e-03,  ..., -8.5136e-02,\n",
      "          1.5077e-02,  5.1642e-02],\n",
      "        ...,\n",
      "        [-2.6717e-02,  3.4543e-02,  3.8051e-02,  ...,  5.2009e-02,\n",
      "         -2.9767e-02, -1.4582e-02],\n",
      "        [ 2.6488e-03, -3.1104e-02,  2.4977e-02,  ..., -3.3666e-03,\n",
      "         -2.5192e-02,  4.4099e-02],\n",
      "        [ 6.9732e-03, -9.9099e-03, -5.0800e-02,  ..., -5.5102e-02,\n",
      "         -5.2550e-02, -1.9902e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 6.2692e-02, -5.9521e-02, -1.0739e-02,  ..., -7.0352e-02,\n",
      "          4.4290e-03,  4.5828e-02],\n",
      "        [ 1.0853e-01, -3.5676e-02, -1.5178e-02,  ..., -5.4988e-02,\n",
      "          1.9418e-05,  6.7902e-02],\n",
      "        [ 5.9376e-02, -3.7739e-02,  8.1614e-03,  ..., -8.5179e-02,\n",
      "          1.5114e-02,  5.1692e-02],\n",
      "        ...,\n",
      "        [-2.6675e-02,  3.4501e-02,  3.8022e-02,  ...,  5.1957e-02,\n",
      "         -2.9825e-02, -1.4550e-02],\n",
      "        [ 2.6085e-03, -3.1021e-02,  2.5018e-02,  ..., -3.3046e-03,\n",
      "         -2.5112e-02,  4.4119e-02],\n",
      "        [ 6.9353e-03, -9.9155e-03, -5.0814e-02,  ..., -5.5080e-02,\n",
      "         -5.2538e-02, -1.9931e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0627, -0.0595, -0.0107,  ..., -0.0704,  0.0045,  0.0459],\n",
      "        [ 0.1086, -0.0357, -0.0151,  ..., -0.0550,  0.0001,  0.0679],\n",
      "        [ 0.0594, -0.0377,  0.0082,  ..., -0.0852,  0.0152,  0.0517],\n",
      "        ...,\n",
      "        [-0.0267,  0.0345,  0.0380,  ...,  0.0519, -0.0299, -0.0145],\n",
      "        [ 0.0026, -0.0310,  0.0250,  ..., -0.0033, -0.0251,  0.0442],\n",
      "        [ 0.0069, -0.0099, -0.0508,  ..., -0.0551, -0.0525, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0628, -0.0595, -0.0107,  ..., -0.0704,  0.0046,  0.0459],\n",
      "        [ 0.1086, -0.0356, -0.0151,  ..., -0.0551,  0.0002,  0.0680],\n",
      "        [ 0.0594, -0.0377,  0.0082,  ..., -0.0853,  0.0153,  0.0518],\n",
      "        ...,\n",
      "        [-0.0267,  0.0345,  0.0380,  ...,  0.0519, -0.0299, -0.0145],\n",
      "        [ 0.0026, -0.0309,  0.0251,  ..., -0.0032, -0.0250,  0.0442],\n",
      "        [ 0.0069, -0.0099, -0.0508,  ..., -0.0550, -0.0525, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0628, -0.0595, -0.0107,  ..., -0.0705,  0.0047,  0.0459],\n",
      "        [ 0.1086, -0.0356, -0.0151,  ..., -0.0551,  0.0003,  0.0680],\n",
      "        [ 0.0595, -0.0377,  0.0082,  ..., -0.0853,  0.0153,  0.0518],\n",
      "        ...,\n",
      "        [-0.0267,  0.0345,  0.0381,  ...,  0.0519, -0.0299, -0.0145],\n",
      "        [ 0.0025, -0.0308,  0.0251,  ..., -0.0032, -0.0250,  0.0442],\n",
      "        [ 0.0068, -0.0099, -0.0508,  ..., -0.0550, -0.0525, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0628, -0.0595, -0.0107,  ..., -0.0705,  0.0047,  0.0460],\n",
      "        [ 0.1087, -0.0356, -0.0151,  ..., -0.0551,  0.0003,  0.0680],\n",
      "        [ 0.0595, -0.0377,  0.0082,  ..., -0.0854,  0.0154,  0.0519],\n",
      "        ...,\n",
      "        [-0.0267,  0.0345,  0.0381,  ...,  0.0519, -0.0299, -0.0145],\n",
      "        [ 0.0025, -0.0308,  0.0251,  ..., -0.0032, -0.0249,  0.0443],\n",
      "        [ 0.0068, -0.0099, -0.0508,  ..., -0.0550, -0.0525, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0629, -0.0595, -0.0107,  ..., -0.0706,  0.0048,  0.0460],\n",
      "        [ 0.1087, -0.0356, -0.0151,  ..., -0.0552,  0.0004,  0.0681],\n",
      "        [ 0.0596, -0.0377,  0.0082,  ..., -0.0854,  0.0155,  0.0520],\n",
      "        ...,\n",
      "        [-0.0267,  0.0345,  0.0381,  ...,  0.0520, -0.0299, -0.0145],\n",
      "        [ 0.0025, -0.0308,  0.0251,  ..., -0.0032, -0.0249,  0.0443],\n",
      "        [ 0.0067, -0.0099, -0.0508,  ..., -0.0550, -0.0526, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0630, -0.0595, -0.0107,  ..., -0.0706,  0.0049,  0.0461],\n",
      "        [ 0.1088, -0.0355, -0.0151,  ..., -0.0552,  0.0005,  0.0681],\n",
      "        [ 0.0596, -0.0376,  0.0082,  ..., -0.0854,  0.0155,  0.0520],\n",
      "        ...,\n",
      "        [-0.0267,  0.0346,  0.0382,  ...,  0.0520, -0.0299, -0.0146],\n",
      "        [ 0.0024, -0.0307,  0.0251,  ..., -0.0031, -0.0249,  0.0444],\n",
      "        [ 0.0067, -0.0098, -0.0508,  ..., -0.0549, -0.0526, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0630, -0.0594, -0.0107,  ..., -0.0706,  0.0049,  0.0461],\n",
      "        [ 0.1088, -0.0355, -0.0150,  ..., -0.0552,  0.0006,  0.0681],\n",
      "        [ 0.0597, -0.0376,  0.0082,  ..., -0.0855,  0.0156,  0.0521],\n",
      "        ...,\n",
      "        [-0.0267,  0.0346,  0.0382,  ...,  0.0520, -0.0299, -0.0146],\n",
      "        [ 0.0024, -0.0307,  0.0251,  ..., -0.0031, -0.0249,  0.0445],\n",
      "        [ 0.0066, -0.0098, -0.0508,  ..., -0.0549, -0.0526, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0631, -0.0594, -0.0107,  ..., -0.0707,  0.0050,  0.0462],\n",
      "        [ 0.1089, -0.0355, -0.0150,  ..., -0.0553,  0.0007,  0.0682],\n",
      "        [ 0.0598, -0.0376,  0.0083,  ..., -0.0855,  0.0157,  0.0522],\n",
      "        ...,\n",
      "        [-0.0268,  0.0346,  0.0382,  ...,  0.0521, -0.0300, -0.0147],\n",
      "        [ 0.0024, -0.0306,  0.0251,  ..., -0.0031, -0.0249,  0.0445],\n",
      "        [ 0.0066, -0.0098, -0.0508,  ..., -0.0549, -0.0526, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0631, -0.0594, -0.0107,  ..., -0.0707,  0.0051,  0.0462],\n",
      "        [ 0.1089, -0.0355, -0.0149,  ..., -0.0553,  0.0008,  0.0683],\n",
      "        [ 0.0598, -0.0376,  0.0083,  ..., -0.0856,  0.0158,  0.0522],\n",
      "        ...,\n",
      "        [-0.0268,  0.0347,  0.0383,  ...,  0.0521, -0.0299, -0.0147],\n",
      "        [ 0.0024, -0.0306,  0.0252,  ..., -0.0031, -0.0248,  0.0446],\n",
      "        [ 0.0065, -0.0098, -0.0508,  ..., -0.0549, -0.0526, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0632, -0.0594, -0.0107,  ..., -0.0707,  0.0052,  0.0463],\n",
      "        [ 0.1090, -0.0354, -0.0149,  ..., -0.0554,  0.0009,  0.0683],\n",
      "        [ 0.0599, -0.0376,  0.0083,  ..., -0.0856,  0.0159,  0.0523],\n",
      "        ...,\n",
      "        [-0.0269,  0.0348,  0.0383,  ...,  0.0522, -0.0299, -0.0148],\n",
      "        [ 0.0024, -0.0306,  0.0252,  ..., -0.0031, -0.0248,  0.0446],\n",
      "        [ 0.0065, -0.0098, -0.0508,  ..., -0.0549, -0.0526, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0632, -0.0594, -0.0106,  ..., -0.0708,  0.0052,  0.0463],\n",
      "        [ 0.1090, -0.0354, -0.0148,  ..., -0.0554,  0.0010,  0.0684],\n",
      "        [ 0.0599, -0.0376,  0.0084,  ..., -0.0857,  0.0160,  0.0524],\n",
      "        ...,\n",
      "        [-0.0270,  0.0348,  0.0383,  ...,  0.0522, -0.0299, -0.0149],\n",
      "        [ 0.0023, -0.0306,  0.0252,  ..., -0.0031, -0.0248,  0.0447],\n",
      "        [ 0.0065, -0.0099, -0.0507,  ..., -0.0550, -0.0527, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0632, -0.0594, -0.0105,  ..., -0.0708,  0.0053,  0.0463],\n",
      "        [ 0.1091, -0.0354, -0.0147,  ..., -0.0555,  0.0011,  0.0684],\n",
      "        [ 0.0600, -0.0376,  0.0085,  ..., -0.0857,  0.0161,  0.0524],\n",
      "        ...,\n",
      "        [-0.0271,  0.0349,  0.0384,  ...,  0.0523, -0.0298, -0.0149],\n",
      "        [ 0.0023, -0.0306,  0.0252,  ..., -0.0032, -0.0248,  0.0447],\n",
      "        [ 0.0064, -0.0099, -0.0507,  ..., -0.0550, -0.0527, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0633, -0.0594, -0.0105,  ..., -0.0709,  0.0054,  0.0464],\n",
      "        [ 0.1091, -0.0354, -0.0146,  ..., -0.0555,  0.0012,  0.0685],\n",
      "        [ 0.0601, -0.0376,  0.0085,  ..., -0.0858,  0.0161,  0.0525],\n",
      "        ...,\n",
      "        [-0.0272,  0.0350,  0.0385,  ...,  0.0524, -0.0297, -0.0150],\n",
      "        [ 0.0024, -0.0306,  0.0253,  ..., -0.0032, -0.0248,  0.0447],\n",
      "        [ 0.0064, -0.0099, -0.0507,  ..., -0.0550, -0.0527, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0633, -0.0594, -0.0104,  ..., -0.0709,  0.0054,  0.0464],\n",
      "        [ 0.1092, -0.0354, -0.0146,  ..., -0.0556,  0.0013,  0.0685],\n",
      "        [ 0.0601, -0.0376,  0.0086,  ..., -0.0858,  0.0162,  0.0526],\n",
      "        ...,\n",
      "        [-0.0273,  0.0351,  0.0386,  ...,  0.0525, -0.0296, -0.0151],\n",
      "        [ 0.0024, -0.0306,  0.0253,  ..., -0.0032, -0.0248,  0.0448],\n",
      "        [ 0.0064, -0.0099, -0.0507,  ..., -0.0550, -0.0527, -0.0200]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0634, -0.0594, -0.0104,  ..., -0.0709,  0.0055,  0.0464],\n",
      "        [ 0.1092, -0.0354, -0.0145,  ..., -0.0556,  0.0014,  0.0686],\n",
      "        [ 0.0602, -0.0375,  0.0086,  ..., -0.0859,  0.0163,  0.0526],\n",
      "        ...,\n",
      "        [-0.0274,  0.0351,  0.0386,  ...,  0.0526, -0.0296, -0.0152],\n",
      "        [ 0.0023, -0.0305,  0.0253,  ..., -0.0032, -0.0248,  0.0448],\n",
      "        [ 0.0063, -0.0098, -0.0506,  ..., -0.0550, -0.0527, -0.0201]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0634, -0.0594, -0.0103,  ..., -0.0710,  0.0056,  0.0465],\n",
      "        [ 0.1093, -0.0354, -0.0145,  ..., -0.0557,  0.0015,  0.0686],\n",
      "        [ 0.0602, -0.0375,  0.0087,  ..., -0.0859,  0.0164,  0.0527],\n",
      "        ...,\n",
      "        [-0.0274,  0.0351,  0.0387,  ...,  0.0527, -0.0295, -0.0153],\n",
      "        [ 0.0023, -0.0305,  0.0254,  ..., -0.0032, -0.0248,  0.0448],\n",
      "        [ 0.0062, -0.0098, -0.0506,  ..., -0.0549, -0.0527, -0.0201]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0634, -0.0594, -0.0103,  ..., -0.0710,  0.0056,  0.0465],\n",
      "        [ 0.1094, -0.0354, -0.0144,  ..., -0.0557,  0.0016,  0.0687],\n",
      "        [ 0.0603, -0.0375,  0.0087,  ..., -0.0860,  0.0165,  0.0528],\n",
      "        ...,\n",
      "        [-0.0275,  0.0352,  0.0387,  ...,  0.0527, -0.0295, -0.0154],\n",
      "        [ 0.0023, -0.0304,  0.0254,  ..., -0.0031, -0.0247,  0.0448],\n",
      "        [ 0.0062, -0.0097, -0.0505,  ..., -0.0549, -0.0526, -0.0201]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0635, -0.0594, -0.0102,  ..., -0.0710,  0.0057,  0.0465],\n",
      "        [ 0.1094, -0.0354, -0.0143,  ..., -0.0558,  0.0016,  0.0688],\n",
      "        [ 0.0603, -0.0375,  0.0088,  ..., -0.0860,  0.0166,  0.0528],\n",
      "        ...,\n",
      "        [-0.0276,  0.0352,  0.0387,  ...,  0.0528, -0.0295, -0.0154],\n",
      "        [ 0.0022, -0.0303,  0.0254,  ..., -0.0031, -0.0247,  0.0447],\n",
      "        [ 0.0061, -0.0097, -0.0505,  ..., -0.0548, -0.0526, -0.0202]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0635, -0.0594, -0.0102,  ..., -0.0710,  0.0057,  0.0465],\n",
      "        [ 0.1094, -0.0354, -0.0143,  ..., -0.0558,  0.0016,  0.0688],\n",
      "        [ 0.0603, -0.0375,  0.0089,  ..., -0.0860,  0.0166,  0.0529],\n",
      "        ...,\n",
      "        [-0.0276,  0.0352,  0.0388,  ...,  0.0528, -0.0294, -0.0155],\n",
      "        [ 0.0022, -0.0303,  0.0255,  ..., -0.0030, -0.0246,  0.0447],\n",
      "        [ 0.0061, -0.0097, -0.0505,  ..., -0.0548, -0.0526, -0.0202]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0635, -0.0594, -0.0101,  ..., -0.0711,  0.0057,  0.0466],\n",
      "        [ 0.1094, -0.0354, -0.0142,  ..., -0.0558,  0.0016,  0.0688],\n",
      "        [ 0.0604, -0.0375,  0.0089,  ..., -0.0861,  0.0166,  0.0529],\n",
      "        ...,\n",
      "        [-0.0277,  0.0352,  0.0388,  ...,  0.0529, -0.0294, -0.0155],\n",
      "        [ 0.0021, -0.0302,  0.0256,  ..., -0.0029, -0.0245,  0.0446],\n",
      "        [ 0.0060, -0.0096, -0.0504,  ..., -0.0548, -0.0526, -0.0203]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0635, -0.0593, -0.0101,  ..., -0.0711,  0.0057,  0.0466],\n",
      "        [ 0.1095, -0.0353, -0.0141,  ..., -0.0559,  0.0017,  0.0688],\n",
      "        [ 0.0604, -0.0375,  0.0090,  ..., -0.0861,  0.0166,  0.0530],\n",
      "        ...,\n",
      "        [-0.0277,  0.0352,  0.0388,  ...,  0.0529, -0.0294, -0.0155],\n",
      "        [ 0.0021, -0.0301,  0.0256,  ..., -0.0029, -0.0245,  0.0446],\n",
      "        [ 0.0059, -0.0096, -0.0504,  ..., -0.0547, -0.0526, -0.0203]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0635, -0.0593, -0.0100,  ..., -0.0711,  0.0058,  0.0466],\n",
      "        [ 0.1095, -0.0353, -0.0141,  ..., -0.0559,  0.0017,  0.0688],\n",
      "        [ 0.0604, -0.0375,  0.0090,  ..., -0.0861,  0.0167,  0.0530],\n",
      "        ...,\n",
      "        [-0.0276,  0.0351,  0.0388,  ...,  0.0528, -0.0295, -0.0155],\n",
      "        [ 0.0020, -0.0300,  0.0257,  ..., -0.0028, -0.0244,  0.0445],\n",
      "        [ 0.0059, -0.0096, -0.0504,  ..., -0.0547, -0.0526, -0.0204]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0635, -0.0593, -0.0100,  ..., -0.0711,  0.0058,  0.0466],\n",
      "        [ 0.1095, -0.0353, -0.0141,  ..., -0.0559,  0.0017,  0.0689],\n",
      "        [ 0.0604, -0.0375,  0.0091,  ..., -0.0861,  0.0167,  0.0530],\n",
      "        ...,\n",
      "        [-0.0276,  0.0351,  0.0387,  ...,  0.0527, -0.0296, -0.0155],\n",
      "        [ 0.0020, -0.0299,  0.0258,  ..., -0.0028, -0.0243,  0.0445],\n",
      "        [ 0.0058, -0.0096, -0.0504,  ..., -0.0547, -0.0526, -0.0204]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0636, -0.0593, -0.0100,  ..., -0.0711,  0.0058,  0.0466],\n",
      "        [ 0.1095, -0.0353, -0.0140,  ..., -0.0559,  0.0017,  0.0689],\n",
      "        [ 0.0605, -0.0375,  0.0091,  ..., -0.0862,  0.0167,  0.0531],\n",
      "        ...,\n",
      "        [-0.0276,  0.0350,  0.0387,  ...,  0.0527, -0.0296, -0.0155],\n",
      "        [ 0.0020, -0.0299,  0.0258,  ..., -0.0027, -0.0242,  0.0445],\n",
      "        [ 0.0058, -0.0096, -0.0504,  ..., -0.0547, -0.0526, -0.0204]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0636, -0.0593, -0.0100,  ..., -0.0712,  0.0058,  0.0467],\n",
      "        [ 0.1096, -0.0353, -0.0140,  ..., -0.0560,  0.0017,  0.0689],\n",
      "        [ 0.0605, -0.0374,  0.0091,  ..., -0.0862,  0.0167,  0.0531],\n",
      "        ...,\n",
      "        [-0.0275,  0.0349,  0.0387,  ...,  0.0527, -0.0297, -0.0155],\n",
      "        [ 0.0019, -0.0298,  0.0258,  ..., -0.0027, -0.0242,  0.0445],\n",
      "        [ 0.0058, -0.0096, -0.0504,  ..., -0.0547, -0.0526, -0.0204]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0636, -0.0592, -0.0099,  ..., -0.0712,  0.0058,  0.0467],\n",
      "        [ 0.1096, -0.0353, -0.0140,  ..., -0.0560,  0.0017,  0.0690],\n",
      "        [ 0.0606, -0.0374,  0.0091,  ..., -0.0862,  0.0168,  0.0532],\n",
      "        ...,\n",
      "        [-0.0275,  0.0349,  0.0387,  ...,  0.0526, -0.0297, -0.0154],\n",
      "        [ 0.0019, -0.0298,  0.0259,  ..., -0.0027, -0.0241,  0.0444],\n",
      "        [ 0.0057, -0.0096, -0.0503,  ..., -0.0547, -0.0526, -0.0205]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0637, -0.0592, -0.0099,  ..., -0.0712,  0.0059,  0.0468],\n",
      "        [ 0.1097, -0.0353, -0.0140,  ..., -0.0560,  0.0018,  0.0690],\n",
      "        [ 0.0606, -0.0374,  0.0092,  ..., -0.0863,  0.0168,  0.0532],\n",
      "        ...,\n",
      "        [-0.0275,  0.0348,  0.0387,  ...,  0.0526, -0.0298, -0.0154],\n",
      "        [ 0.0019, -0.0297,  0.0259,  ..., -0.0026, -0.0241,  0.0444],\n",
      "        [ 0.0057, -0.0096, -0.0504,  ..., -0.0547, -0.0527, -0.0205]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0637, -0.0592, -0.0098,  ..., -0.0713,  0.0059,  0.0468],\n",
      "        [ 0.1097, -0.0352, -0.0139,  ..., -0.0560,  0.0018,  0.0691],\n",
      "        [ 0.0606, -0.0374,  0.0092,  ..., -0.0863,  0.0168,  0.0532],\n",
      "        ...,\n",
      "        [-0.0275,  0.0348,  0.0387,  ...,  0.0525, -0.0298, -0.0154],\n",
      "        [ 0.0018, -0.0297,  0.0260,  ..., -0.0026, -0.0240,  0.0444],\n",
      "        [ 0.0057, -0.0096, -0.0503,  ..., -0.0546, -0.0527, -0.0206]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0637, -0.0592, -0.0098,  ..., -0.0713,  0.0060,  0.0468],\n",
      "        [ 0.1097, -0.0352, -0.0139,  ..., -0.0561,  0.0019,  0.0691],\n",
      "        [ 0.0607, -0.0373,  0.0093,  ..., -0.0863,  0.0169,  0.0533],\n",
      "        ...,\n",
      "        [-0.0275,  0.0347,  0.0387,  ...,  0.0525, -0.0299, -0.0154],\n",
      "        [ 0.0018, -0.0297,  0.0260,  ..., -0.0026, -0.0240,  0.0443],\n",
      "        [ 0.0056, -0.0095, -0.0503,  ..., -0.0546, -0.0526, -0.0207]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0638, -0.0591, -0.0098,  ..., -0.0713,  0.0060,  0.0468],\n",
      "        [ 0.1098, -0.0351, -0.0139,  ..., -0.0561,  0.0019,  0.0691],\n",
      "        [ 0.0607, -0.0372,  0.0093,  ..., -0.0863,  0.0169,  0.0533],\n",
      "        ...,\n",
      "        [-0.0275,  0.0347,  0.0387,  ...,  0.0525, -0.0299, -0.0154],\n",
      "        [ 0.0018, -0.0296,  0.0260,  ..., -0.0026, -0.0239,  0.0443],\n",
      "        [ 0.0056, -0.0095, -0.0503,  ..., -0.0546, -0.0526, -0.0207]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0638, -0.0591, -0.0097,  ..., -0.0713,  0.0061,  0.0469],\n",
      "        [ 0.1098, -0.0351, -0.0139,  ..., -0.0561,  0.0020,  0.0692],\n",
      "        [ 0.0607, -0.0372,  0.0094,  ..., -0.0863,  0.0170,  0.0534],\n",
      "        ...,\n",
      "        [-0.0275,  0.0347,  0.0388,  ...,  0.0525, -0.0299, -0.0154],\n",
      "        [ 0.0018, -0.0296,  0.0260,  ..., -0.0026, -0.0239,  0.0442],\n",
      "        [ 0.0056, -0.0095, -0.0503,  ..., -0.0546, -0.0526, -0.0208]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0638, -0.0590, -0.0097,  ..., -0.0713,  0.0061,  0.0469],\n",
      "        [ 0.1099, -0.0350, -0.0138,  ..., -0.0561,  0.0021,  0.0692],\n",
      "        [ 0.0608, -0.0371,  0.0094,  ..., -0.0863,  0.0171,  0.0534],\n",
      "        ...,\n",
      "        [-0.0276,  0.0347,  0.0388,  ...,  0.0526, -0.0299, -0.0154],\n",
      "        [ 0.0019, -0.0296,  0.0259,  ..., -0.0026, -0.0239,  0.0442],\n",
      "        [ 0.0056, -0.0095, -0.0504,  ..., -0.0546, -0.0526, -0.0208]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0639, -0.0590, -0.0097,  ..., -0.0713,  0.0061,  0.0469],\n",
      "        [ 0.1099, -0.0349, -0.0138,  ..., -0.0562,  0.0021,  0.0692],\n",
      "        [ 0.0608, -0.0371,  0.0095,  ..., -0.0864,  0.0171,  0.0534],\n",
      "        ...,\n",
      "        [-0.0276,  0.0346,  0.0389,  ...,  0.0525, -0.0300, -0.0154],\n",
      "        [ 0.0019, -0.0296,  0.0259,  ..., -0.0027, -0.0239,  0.0442],\n",
      "        [ 0.0055, -0.0096, -0.0504,  ..., -0.0547, -0.0527, -0.0209]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0639, -0.0590, -0.0097,  ..., -0.0714,  0.0062,  0.0469],\n",
      "        [ 0.1099, -0.0349, -0.0138,  ..., -0.0562,  0.0022,  0.0692],\n",
      "        [ 0.0608, -0.0371,  0.0095,  ..., -0.0864,  0.0172,  0.0534],\n",
      "        ...,\n",
      "        [-0.0276,  0.0346,  0.0389,  ...,  0.0525, -0.0300, -0.0155],\n",
      "        [ 0.0020, -0.0296,  0.0259,  ..., -0.0027, -0.0239,  0.0442],\n",
      "        [ 0.0055, -0.0096, -0.0504,  ..., -0.0547, -0.0527, -0.0209]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0639, -0.0590, -0.0097,  ..., -0.0714,  0.0062,  0.0470],\n",
      "        [ 0.1100, -0.0349, -0.0139,  ..., -0.0562,  0.0022,  0.0693],\n",
      "        [ 0.0609, -0.0371,  0.0096,  ..., -0.0864,  0.0172,  0.0535],\n",
      "        ...,\n",
      "        [-0.0276,  0.0346,  0.0390,  ...,  0.0525, -0.0300, -0.0155],\n",
      "        [ 0.0020, -0.0296,  0.0258,  ..., -0.0027, -0.0239,  0.0442],\n",
      "        [ 0.0055, -0.0096, -0.0505,  ..., -0.0547, -0.0527, -0.0209]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0640, -0.0590, -0.0097,  ..., -0.0715,  0.0062,  0.0470],\n",
      "        [ 0.1100, -0.0349, -0.0139,  ..., -0.0563,  0.0022,  0.0693],\n",
      "        [ 0.0609, -0.0370,  0.0096,  ..., -0.0864,  0.0172,  0.0535],\n",
      "        ...,\n",
      "        [-0.0276,  0.0346,  0.0390,  ...,  0.0525, -0.0300, -0.0155],\n",
      "        [ 0.0021, -0.0296,  0.0258,  ..., -0.0027, -0.0239,  0.0442],\n",
      "        [ 0.0056, -0.0096, -0.0505,  ..., -0.0548, -0.0528, -0.0210]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0640, -0.0590, -0.0098,  ..., -0.0715,  0.0062,  0.0471],\n",
      "        [ 0.1101, -0.0349, -0.0139,  ..., -0.0563,  0.0022,  0.0693],\n",
      "        [ 0.0610, -0.0370,  0.0096,  ..., -0.0865,  0.0173,  0.0536],\n",
      "        ...,\n",
      "        [-0.0276,  0.0346,  0.0390,  ...,  0.0525, -0.0300, -0.0154],\n",
      "        [ 0.0021, -0.0296,  0.0258,  ..., -0.0027, -0.0239,  0.0442],\n",
      "        [ 0.0056, -0.0096, -0.0506,  ..., -0.0548, -0.0528, -0.0210]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0641, -0.0590, -0.0098,  ..., -0.0715,  0.0062,  0.0471],\n",
      "        [ 0.1101, -0.0349, -0.0139,  ..., -0.0563,  0.0022,  0.0694],\n",
      "        [ 0.0610, -0.0370,  0.0096,  ..., -0.0865,  0.0173,  0.0536],\n",
      "        ...,\n",
      "        [-0.0275,  0.0346,  0.0390,  ...,  0.0525, -0.0301, -0.0154],\n",
      "        [ 0.0022, -0.0296,  0.0257,  ..., -0.0028, -0.0239,  0.0443],\n",
      "        [ 0.0055, -0.0096, -0.0506,  ..., -0.0548, -0.0528, -0.0211]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0641, -0.0590, -0.0098,  ..., -0.0716,  0.0062,  0.0472],\n",
      "        [ 0.1101, -0.0349, -0.0140,  ..., -0.0563,  0.0022,  0.0694],\n",
      "        [ 0.0611, -0.0370,  0.0096,  ..., -0.0865,  0.0173,  0.0537],\n",
      "        ...,\n",
      "        [-0.0275,  0.0345,  0.0390,  ...,  0.0524, -0.0301, -0.0154],\n",
      "        [ 0.0022, -0.0297,  0.0257,  ..., -0.0028, -0.0240,  0.0443],\n",
      "        [ 0.0055, -0.0096, -0.0506,  ..., -0.0548, -0.0528, -0.0211]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0590, -0.0099,  ..., -0.0716,  0.0062,  0.0472],\n",
      "        [ 0.1101, -0.0349, -0.0140,  ..., -0.0563,  0.0022,  0.0694],\n",
      "        [ 0.0611, -0.0370,  0.0096,  ..., -0.0866,  0.0173,  0.0537],\n",
      "        ...,\n",
      "        [-0.0275,  0.0346,  0.0390,  ...,  0.0524, -0.0301, -0.0154],\n",
      "        [ 0.0023, -0.0297,  0.0256,  ..., -0.0029, -0.0241,  0.0444],\n",
      "        [ 0.0055, -0.0096, -0.0506,  ..., -0.0548, -0.0528, -0.0212]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0590, -0.0099,  ..., -0.0717,  0.0063,  0.0473],\n",
      "        [ 0.1102, -0.0348, -0.0141,  ..., -0.0564,  0.0022,  0.0694],\n",
      "        [ 0.0611, -0.0370,  0.0096,  ..., -0.0866,  0.0174,  0.0537],\n",
      "        ...,\n",
      "        [-0.0275,  0.0345,  0.0390,  ...,  0.0524, -0.0301, -0.0154],\n",
      "        [ 0.0024, -0.0297,  0.0256,  ..., -0.0029, -0.0241,  0.0445],\n",
      "        [ 0.0055, -0.0096, -0.0507,  ..., -0.0548, -0.0528, -0.0212]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0589, -0.0099,  ..., -0.0717,  0.0063,  0.0473],\n",
      "        [ 0.1102, -0.0348, -0.0141,  ..., -0.0564,  0.0023,  0.0694],\n",
      "        [ 0.0611, -0.0370,  0.0096,  ..., -0.0866,  0.0174,  0.0538],\n",
      "        ...,\n",
      "        [-0.0275,  0.0345,  0.0390,  ...,  0.0524, -0.0301, -0.0154],\n",
      "        [ 0.0024, -0.0297,  0.0255,  ..., -0.0030, -0.0242,  0.0445],\n",
      "        [ 0.0055, -0.0096, -0.0507,  ..., -0.0548, -0.0529, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0589, -0.0099,  ..., -0.0718,  0.0064,  0.0473],\n",
      "        [ 0.1102, -0.0348, -0.0141,  ..., -0.0564,  0.0023,  0.0694],\n",
      "        [ 0.0612, -0.0369,  0.0096,  ..., -0.0866,  0.0174,  0.0538],\n",
      "        ...,\n",
      "        [-0.0274,  0.0345,  0.0390,  ...,  0.0523, -0.0301, -0.0154],\n",
      "        [ 0.0025, -0.0297,  0.0255,  ..., -0.0030, -0.0242,  0.0446],\n",
      "        [ 0.0055, -0.0097, -0.0507,  ..., -0.0548, -0.0529, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0589, -0.0099,  ..., -0.0718,  0.0064,  0.0474],\n",
      "        [ 0.1102, -0.0347, -0.0141,  ..., -0.0564,  0.0023,  0.0695],\n",
      "        [ 0.0612, -0.0369,  0.0097,  ..., -0.0866,  0.0175,  0.0538],\n",
      "        ...,\n",
      "        [-0.0274,  0.0345,  0.0390,  ...,  0.0523, -0.0302, -0.0153],\n",
      "        [ 0.0025, -0.0297,  0.0254,  ..., -0.0030, -0.0243,  0.0446],\n",
      "        [ 0.0055, -0.0097, -0.0508,  ..., -0.0548, -0.0530, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0588, -0.0099,  ..., -0.0718,  0.0064,  0.0474],\n",
      "        [ 0.1102, -0.0347, -0.0141,  ..., -0.0563,  0.0023,  0.0694],\n",
      "        [ 0.0612, -0.0368,  0.0097,  ..., -0.0866,  0.0175,  0.0538],\n",
      "        ...,\n",
      "        [-0.0274,  0.0344,  0.0390,  ...,  0.0523, -0.0302, -0.0153],\n",
      "        [ 0.0026, -0.0298,  0.0254,  ..., -0.0031, -0.0244,  0.0446],\n",
      "        [ 0.0056, -0.0098, -0.0508,  ..., -0.0549, -0.0531, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0588, -0.0099,  ..., -0.0718,  0.0064,  0.0474],\n",
      "        [ 0.1102, -0.0346, -0.0141,  ..., -0.0563,  0.0023,  0.0694],\n",
      "        [ 0.0612, -0.0368,  0.0097,  ..., -0.0866,  0.0174,  0.0538],\n",
      "        ...,\n",
      "        [-0.0273,  0.0344,  0.0389,  ...,  0.0522, -0.0303, -0.0153],\n",
      "        [ 0.0027, -0.0298,  0.0253,  ..., -0.0032, -0.0244,  0.0447],\n",
      "        [ 0.0057, -0.0099, -0.0509,  ..., -0.0551, -0.0532, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0587, -0.0099,  ..., -0.0718,  0.0064,  0.0474],\n",
      "        [ 0.1102, -0.0346, -0.0141,  ..., -0.0563,  0.0023,  0.0694],\n",
      "        [ 0.0612, -0.0367,  0.0097,  ..., -0.0866,  0.0175,  0.0538],\n",
      "        ...,\n",
      "        [-0.0273,  0.0343,  0.0389,  ...,  0.0521, -0.0303, -0.0153],\n",
      "        [ 0.0028, -0.0299,  0.0253,  ..., -0.0033, -0.0245,  0.0447],\n",
      "        [ 0.0058, -0.0100, -0.0510,  ..., -0.0552, -0.0533, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0587, -0.0098,  ..., -0.0718,  0.0064,  0.0474],\n",
      "        [ 0.1102, -0.0346, -0.0141,  ..., -0.0563,  0.0022,  0.0694],\n",
      "        [ 0.0612, -0.0367,  0.0097,  ..., -0.0866,  0.0175,  0.0538],\n",
      "        ...,\n",
      "        [-0.0272,  0.0343,  0.0389,  ...,  0.0521, -0.0303, -0.0153],\n",
      "        [ 0.0029, -0.0299,  0.0252,  ..., -0.0033, -0.0246,  0.0447],\n",
      "        [ 0.0058, -0.0102, -0.0511,  ..., -0.0552, -0.0534, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0587, -0.0098,  ..., -0.0718,  0.0064,  0.0474],\n",
      "        [ 0.1102, -0.0346, -0.0141,  ..., -0.0563,  0.0022,  0.0694],\n",
      "        [ 0.0612, -0.0367,  0.0097,  ..., -0.0866,  0.0175,  0.0538],\n",
      "        ...,\n",
      "        [-0.0272,  0.0343,  0.0388,  ...,  0.0521, -0.0304, -0.0152],\n",
      "        [ 0.0030, -0.0299,  0.0252,  ..., -0.0034, -0.0246,  0.0447],\n",
      "        [ 0.0059, -0.0102, -0.0512,  ..., -0.0553, -0.0535, -0.0214]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0587, -0.0098,  ..., -0.0718,  0.0064,  0.0474],\n",
      "        [ 0.1102, -0.0346, -0.0141,  ..., -0.0563,  0.0022,  0.0694],\n",
      "        [ 0.0612, -0.0367,  0.0097,  ..., -0.0866,  0.0175,  0.0538],\n",
      "        ...,\n",
      "        [-0.0271,  0.0342,  0.0388,  ...,  0.0520, -0.0304, -0.0152],\n",
      "        [ 0.0030, -0.0299,  0.0252,  ..., -0.0034, -0.0246,  0.0447],\n",
      "        [ 0.0059, -0.0103, -0.0512,  ..., -0.0554, -0.0535, -0.0214]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0587, -0.0097,  ..., -0.0718,  0.0064,  0.0474],\n",
      "        [ 0.1102, -0.0346, -0.0141,  ..., -0.0563,  0.0022,  0.0694],\n",
      "        [ 0.0612, -0.0367,  0.0098,  ..., -0.0866,  0.0175,  0.0538],\n",
      "        ...,\n",
      "        [-0.0271,  0.0342,  0.0388,  ...,  0.0520, -0.0304, -0.0152],\n",
      "        [ 0.0031, -0.0299,  0.0252,  ..., -0.0034, -0.0246,  0.0447],\n",
      "        [ 0.0060, -0.0103, -0.0513,  ..., -0.0554, -0.0536, -0.0214]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0586, -0.0097,  ..., -0.0718,  0.0065,  0.0474],\n",
      "        [ 0.1102, -0.0345, -0.0140,  ..., -0.0563,  0.0022,  0.0694],\n",
      "        [ 0.0612, -0.0366,  0.0099,  ..., -0.0866,  0.0175,  0.0538],\n",
      "        ...,\n",
      "        [-0.0271,  0.0342,  0.0388,  ...,  0.0520, -0.0305, -0.0151],\n",
      "        [ 0.0031, -0.0299,  0.0252,  ..., -0.0034, -0.0246,  0.0447],\n",
      "        [ 0.0060, -0.0104, -0.0514,  ..., -0.0555, -0.0537, -0.0214]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0585, -0.0096,  ..., -0.0718,  0.0065,  0.0474],\n",
      "        [ 0.1102, -0.0345, -0.0140,  ..., -0.0563,  0.0022,  0.0694],\n",
      "        [ 0.0612, -0.0366,  0.0099,  ..., -0.0866,  0.0175,  0.0538],\n",
      "        ...,\n",
      "        [-0.0271,  0.0342,  0.0388,  ...,  0.0520, -0.0305, -0.0151],\n",
      "        [ 0.0032, -0.0299,  0.0251,  ..., -0.0035, -0.0247,  0.0447],\n",
      "        [ 0.0061, -0.0105, -0.0515,  ..., -0.0556, -0.0538, -0.0214]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0585, -0.0095,  ..., -0.0717,  0.0065,  0.0474],\n",
      "        [ 0.1102, -0.0344, -0.0139,  ..., -0.0562,  0.0022,  0.0694],\n",
      "        [ 0.0612, -0.0365,  0.0100,  ..., -0.0865,  0.0175,  0.0538],\n",
      "        ...,\n",
      "        [-0.0271,  0.0343,  0.0388,  ...,  0.0520, -0.0304, -0.0152],\n",
      "        [ 0.0033, -0.0299,  0.0251,  ..., -0.0035, -0.0247,  0.0448],\n",
      "        [ 0.0062, -0.0106, -0.0516,  ..., -0.0557, -0.0539, -0.0214]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0584, -0.0095,  ..., -0.0717,  0.0066,  0.0474],\n",
      "        [ 0.1102, -0.0343, -0.0139,  ..., -0.0562,  0.0022,  0.0694],\n",
      "        [ 0.0612, -0.0365,  0.0100,  ..., -0.0865,  0.0176,  0.0537],\n",
      "        ...,\n",
      "        [-0.0272,  0.0343,  0.0388,  ...,  0.0521, -0.0304, -0.0152],\n",
      "        [ 0.0033, -0.0299,  0.0250,  ..., -0.0036, -0.0247,  0.0448],\n",
      "        [ 0.0063, -0.0107, -0.0517,  ..., -0.0558, -0.0540, -0.0214]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0583, -0.0094,  ..., -0.0717,  0.0066,  0.0474],\n",
      "        [ 0.1101, -0.0343, -0.0139,  ..., -0.0562,  0.0022,  0.0694],\n",
      "        [ 0.0611, -0.0365,  0.0100,  ..., -0.0865,  0.0176,  0.0537],\n",
      "        ...,\n",
      "        [-0.0272,  0.0343,  0.0388,  ...,  0.0521, -0.0304, -0.0152],\n",
      "        [ 0.0034, -0.0299,  0.0250,  ..., -0.0036, -0.0248,  0.0448],\n",
      "        [ 0.0064, -0.0108, -0.0519,  ..., -0.0559, -0.0541, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0583, -0.0095,  ..., -0.0717,  0.0067,  0.0473],\n",
      "        [ 0.1101, -0.0343, -0.0140,  ..., -0.0562,  0.0022,  0.0694],\n",
      "        [ 0.0611, -0.0365,  0.0100,  ..., -0.0865,  0.0176,  0.0537],\n",
      "        ...,\n",
      "        [-0.0272,  0.0344,  0.0388,  ...,  0.0521, -0.0304, -0.0152],\n",
      "        [ 0.0035, -0.0299,  0.0249,  ..., -0.0037, -0.0248,  0.0448],\n",
      "        [ 0.0065, -0.0109, -0.0520,  ..., -0.0560, -0.0543, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0583, -0.0095,  ..., -0.0717,  0.0067,  0.0473],\n",
      "        [ 0.1101, -0.0343, -0.0140,  ..., -0.0562,  0.0022,  0.0694],\n",
      "        [ 0.0611, -0.0364,  0.0100,  ..., -0.0865,  0.0176,  0.0537],\n",
      "        ...,\n",
      "        [-0.0273,  0.0344,  0.0388,  ...,  0.0522, -0.0304, -0.0152],\n",
      "        [ 0.0035, -0.0299,  0.0249,  ..., -0.0037, -0.0248,  0.0448],\n",
      "        [ 0.0065, -0.0110, -0.0521,  ..., -0.0561, -0.0543, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0582, -0.0095,  ..., -0.0717,  0.0067,  0.0473],\n",
      "        [ 0.1101, -0.0342, -0.0140,  ..., -0.0562,  0.0022,  0.0693],\n",
      "        [ 0.0611, -0.0364,  0.0101,  ..., -0.0865,  0.0176,  0.0536],\n",
      "        ...,\n",
      "        [-0.0273,  0.0344,  0.0388,  ...,  0.0522, -0.0304, -0.0152],\n",
      "        [ 0.0036, -0.0299,  0.0249,  ..., -0.0037, -0.0248,  0.0448],\n",
      "        [ 0.0065, -0.0110, -0.0521,  ..., -0.0561, -0.0544, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0582, -0.0095,  ..., -0.0717,  0.0068,  0.0473],\n",
      "        [ 0.1101, -0.0342, -0.0140,  ..., -0.0562,  0.0023,  0.0693],\n",
      "        [ 0.0611, -0.0364,  0.0101,  ..., -0.0865,  0.0177,  0.0536],\n",
      "        ...,\n",
      "        [-0.0274,  0.0345,  0.0389,  ...,  0.0523, -0.0303, -0.0153],\n",
      "        [ 0.0036, -0.0299,  0.0249,  ..., -0.0037, -0.0247,  0.0448],\n",
      "        [ 0.0066, -0.0111, -0.0522,  ..., -0.0562, -0.0545, -0.0214]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0582, -0.0095,  ..., -0.0717,  0.0068,  0.0473],\n",
      "        [ 0.1101, -0.0342, -0.0140,  ..., -0.0562,  0.0023,  0.0693],\n",
      "        [ 0.0611, -0.0364,  0.0101,  ..., -0.0865,  0.0177,  0.0536],\n",
      "        ...,\n",
      "        [-0.0274,  0.0345,  0.0389,  ...,  0.0523, -0.0303, -0.0153],\n",
      "        [ 0.0036, -0.0299,  0.0249,  ..., -0.0038, -0.0247,  0.0447],\n",
      "        [ 0.0066, -0.0111, -0.0522,  ..., -0.0562, -0.0545, -0.0215]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0582, -0.0094,  ..., -0.0717,  0.0069,  0.0473],\n",
      "        [ 0.1101, -0.0342, -0.0140,  ..., -0.0562,  0.0023,  0.0693],\n",
      "        [ 0.0611, -0.0364,  0.0102,  ..., -0.0865,  0.0178,  0.0536],\n",
      "        ...,\n",
      "        [-0.0274,  0.0345,  0.0389,  ...,  0.0524, -0.0303, -0.0154],\n",
      "        [ 0.0036, -0.0298,  0.0249,  ..., -0.0038, -0.0247,  0.0447],\n",
      "        [ 0.0066, -0.0112, -0.0522,  ..., -0.0563, -0.0545, -0.0216]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0582, -0.0094,  ..., -0.0717,  0.0069,  0.0473],\n",
      "        [ 0.1101, -0.0342, -0.0140,  ..., -0.0562,  0.0024,  0.0693],\n",
      "        [ 0.0611, -0.0364,  0.0102,  ..., -0.0865,  0.0178,  0.0536],\n",
      "        ...,\n",
      "        [-0.0275,  0.0346,  0.0390,  ...,  0.0524, -0.0302, -0.0155],\n",
      "        [ 0.0036, -0.0298,  0.0250,  ..., -0.0038, -0.0246,  0.0447],\n",
      "        [ 0.0066, -0.0112, -0.0523,  ..., -0.0563, -0.0545, -0.0216]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0581, -0.0094,  ..., -0.0717,  0.0070,  0.0473],\n",
      "        [ 0.1101, -0.0342, -0.0140,  ..., -0.0562,  0.0024,  0.0693],\n",
      "        [ 0.0611, -0.0364,  0.0103,  ..., -0.0865,  0.0179,  0.0536],\n",
      "        ...,\n",
      "        [-0.0276,  0.0346,  0.0390,  ...,  0.0525, -0.0302, -0.0155],\n",
      "        [ 0.0036, -0.0298,  0.0250,  ..., -0.0038, -0.0246,  0.0447],\n",
      "        [ 0.0066, -0.0112, -0.0523,  ..., -0.0563, -0.0545, -0.0217]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0580, -0.0093,  ..., -0.0717,  0.0070,  0.0473],\n",
      "        [ 0.1101, -0.0341, -0.0140,  ..., -0.0562,  0.0025,  0.0693],\n",
      "        [ 0.0610, -0.0363,  0.0103,  ..., -0.0865,  0.0180,  0.0536],\n",
      "        ...,\n",
      "        [-0.0276,  0.0346,  0.0391,  ...,  0.0525, -0.0302, -0.0156],\n",
      "        [ 0.0036, -0.0297,  0.0250,  ..., -0.0038, -0.0245,  0.0447],\n",
      "        [ 0.0066, -0.0113, -0.0524,  ..., -0.0564, -0.0546, -0.0218]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0580, -0.0093,  ..., -0.0717,  0.0071,  0.0473],\n",
      "        [ 0.1101, -0.0341, -0.0140,  ..., -0.0562,  0.0025,  0.0693],\n",
      "        [ 0.0611, -0.0363,  0.0103,  ..., -0.0865,  0.0180,  0.0536],\n",
      "        ...,\n",
      "        [-0.0276,  0.0346,  0.0391,  ...,  0.0526, -0.0301, -0.0157],\n",
      "        [ 0.0036, -0.0297,  0.0250,  ..., -0.0038, -0.0245,  0.0447],\n",
      "        [ 0.0066, -0.0113, -0.0524,  ..., -0.0564, -0.0546, -0.0219]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0642, -0.0580, -0.0093,  ..., -0.0718,  0.0071,  0.0474],\n",
      "        [ 0.1101, -0.0341, -0.0140,  ..., -0.0563,  0.0025,  0.0694],\n",
      "        [ 0.0611, -0.0363,  0.0104,  ..., -0.0865,  0.0181,  0.0537],\n",
      "        ...,\n",
      "        [-0.0277,  0.0347,  0.0391,  ...,  0.0526, -0.0301, -0.0158],\n",
      "        [ 0.0036, -0.0297,  0.0250,  ..., -0.0038, -0.0245,  0.0447],\n",
      "        [ 0.0065, -0.0112, -0.0524,  ..., -0.0564, -0.0545, -0.0219]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0580, -0.0093,  ..., -0.0718,  0.0071,  0.0474],\n",
      "        [ 0.1102, -0.0341, -0.0140,  ..., -0.0563,  0.0026,  0.0694],\n",
      "        [ 0.0611, -0.0363,  0.0104,  ..., -0.0866,  0.0181,  0.0537],\n",
      "        ...,\n",
      "        [-0.0277,  0.0347,  0.0392,  ...,  0.0527, -0.0300, -0.0159],\n",
      "        [ 0.0036, -0.0297,  0.0251,  ..., -0.0038, -0.0245,  0.0447],\n",
      "        [ 0.0065, -0.0111, -0.0524,  ..., -0.0563, -0.0545, -0.0221]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "| epoch   1 step      600 |    600 batches | lr 0.0001 | ms/batch 30.89383 | loss 2.21195 | ppl     9.134\n",
      "Parameter containing:\n",
      "tensor([[ 0.0643, -0.0580, -0.0093,  ..., -0.0718,  0.0072,  0.0474],\n",
      "        [ 0.1102, -0.0341, -0.0140,  ..., -0.0563,  0.0026,  0.0694],\n",
      "        [ 0.0612, -0.0363,  0.0104,  ..., -0.0866,  0.0182,  0.0537],\n",
      "        ...,\n",
      "        [-0.0277,  0.0347,  0.0392,  ...,  0.0527, -0.0300, -0.0159],\n",
      "        [ 0.0036, -0.0297,  0.0251,  ..., -0.0037, -0.0245,  0.0446],\n",
      "        [ 0.0064, -0.0111, -0.0523,  ..., -0.0563, -0.0544, -0.0222]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0644, -0.0580, -0.0094,  ..., -0.0719,  0.0072,  0.0474],\n",
      "        [ 0.1102, -0.0341, -0.0141,  ..., -0.0563,  0.0026,  0.0694],\n",
      "        [ 0.0612, -0.0362,  0.0104,  ..., -0.0866,  0.0183,  0.0537],\n",
      "        ...,\n",
      "        [-0.0278,  0.0347,  0.0393,  ...,  0.0527, -0.0300, -0.0160],\n",
      "        [ 0.0036, -0.0296,  0.0251,  ..., -0.0037, -0.0244,  0.0446],\n",
      "        [ 0.0063, -0.0110, -0.0523,  ..., -0.0562, -0.0544, -0.0223]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0644, -0.0579, -0.0094,  ..., -0.0719,  0.0073,  0.0475],\n",
      "        [ 0.1102, -0.0341, -0.0141,  ..., -0.0564,  0.0026,  0.0694],\n",
      "        [ 0.0612, -0.0362,  0.0104,  ..., -0.0867,  0.0183,  0.0537],\n",
      "        ...,\n",
      "        [-0.0278,  0.0347,  0.0393,  ...,  0.0527, -0.0300, -0.0160],\n",
      "        [ 0.0036, -0.0296,  0.0252,  ..., -0.0037, -0.0244,  0.0446],\n",
      "        [ 0.0062, -0.0109, -0.0523,  ..., -0.0561, -0.0543, -0.0224]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0644, -0.0578, -0.0094,  ..., -0.0719,  0.0073,  0.0475],\n",
      "        [ 0.1102, -0.0340, -0.0141,  ..., -0.0564,  0.0027,  0.0695],\n",
      "        [ 0.0612, -0.0361,  0.0104,  ..., -0.0867,  0.0184,  0.0538],\n",
      "        ...,\n",
      "        [-0.0278,  0.0347,  0.0394,  ...,  0.0527, -0.0300, -0.0160],\n",
      "        [ 0.0036, -0.0296,  0.0252,  ..., -0.0036, -0.0243,  0.0445],\n",
      "        [ 0.0061, -0.0109, -0.0523,  ..., -0.0561, -0.0543, -0.0225]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0644, -0.0577, -0.0094,  ..., -0.0719,  0.0074,  0.0475],\n",
      "        [ 0.1102, -0.0339, -0.0141,  ..., -0.0564,  0.0027,  0.0695],\n",
      "        [ 0.0612, -0.0361,  0.0105,  ..., -0.0867,  0.0185,  0.0538],\n",
      "        ...,\n",
      "        [-0.0278,  0.0347,  0.0394,  ...,  0.0527, -0.0300, -0.0161],\n",
      "        [ 0.0035, -0.0295,  0.0253,  ..., -0.0036, -0.0243,  0.0445],\n",
      "        [ 0.0061, -0.0108, -0.0522,  ..., -0.0560, -0.0542, -0.0226]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0644, -0.0577, -0.0094,  ..., -0.0719,  0.0074,  0.0476],\n",
      "        [ 0.1103, -0.0338, -0.0141,  ..., -0.0564,  0.0027,  0.0695],\n",
      "        [ 0.0612, -0.0360,  0.0105,  ..., -0.0867,  0.0185,  0.0538],\n",
      "        ...,\n",
      "        [-0.0278,  0.0347,  0.0394,  ...,  0.0527, -0.0301, -0.0161],\n",
      "        [ 0.0035, -0.0295,  0.0254,  ..., -0.0035, -0.0242,  0.0444],\n",
      "        [ 0.0060, -0.0107, -0.0522,  ..., -0.0560, -0.0542, -0.0227]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0644, -0.0576, -0.0094,  ..., -0.0719,  0.0075,  0.0476],\n",
      "        [ 0.1103, -0.0338, -0.0141,  ..., -0.0564,  0.0028,  0.0695],\n",
      "        [ 0.0613, -0.0359,  0.0105,  ..., -0.0867,  0.0186,  0.0538],\n",
      "        ...,\n",
      "        [-0.0277,  0.0346,  0.0394,  ...,  0.0527, -0.0301, -0.0161],\n",
      "        [ 0.0034, -0.0294,  0.0254,  ..., -0.0034, -0.0242,  0.0444],\n",
      "        [ 0.0059, -0.0107, -0.0522,  ..., -0.0560, -0.0542, -0.0227]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0645, -0.0575, -0.0094,  ..., -0.0719,  0.0075,  0.0476],\n",
      "        [ 0.1103, -0.0338, -0.0141,  ..., -0.0564,  0.0028,  0.0695],\n",
      "        [ 0.0613, -0.0359,  0.0105,  ..., -0.0867,  0.0187,  0.0538],\n",
      "        ...,\n",
      "        [-0.0277,  0.0346,  0.0394,  ...,  0.0527, -0.0301, -0.0161],\n",
      "        [ 0.0034, -0.0294,  0.0254,  ..., -0.0034, -0.0241,  0.0444],\n",
      "        [ 0.0059, -0.0107, -0.0522,  ..., -0.0559, -0.0542, -0.0228]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0645, -0.0575, -0.0093,  ..., -0.0720,  0.0076,  0.0476],\n",
      "        [ 0.1103, -0.0337, -0.0141,  ..., -0.0564,  0.0029,  0.0695],\n",
      "        [ 0.0613, -0.0358,  0.0106,  ..., -0.0867,  0.0188,  0.0538],\n",
      "        ...,\n",
      "        [-0.0277,  0.0346,  0.0394,  ...,  0.0527, -0.0301, -0.0161],\n",
      "        [ 0.0034, -0.0293,  0.0255,  ..., -0.0034, -0.0241,  0.0443],\n",
      "        [ 0.0058, -0.0106, -0.0522,  ..., -0.0559, -0.0541, -0.0230]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0645, -0.0574, -0.0094,  ..., -0.0720,  0.0076,  0.0476],\n",
      "        [ 0.1103, -0.0337, -0.0141,  ..., -0.0564,  0.0029,  0.0695],\n",
      "        [ 0.0613, -0.0358,  0.0106,  ..., -0.0867,  0.0189,  0.0539],\n",
      "        ...,\n",
      "        [-0.0277,  0.0346,  0.0393,  ...,  0.0527, -0.0302, -0.0160],\n",
      "        [ 0.0034, -0.0293,  0.0255,  ..., -0.0033, -0.0240,  0.0443],\n",
      "        [ 0.0058, -0.0106, -0.0522,  ..., -0.0558, -0.0541, -0.0231]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0645, -0.0574, -0.0094,  ..., -0.0720,  0.0076,  0.0476],\n",
      "        [ 0.1103, -0.0337, -0.0141,  ..., -0.0564,  0.0030,  0.0695],\n",
      "        [ 0.0613, -0.0357,  0.0106,  ..., -0.0868,  0.0189,  0.0539],\n",
      "        ...,\n",
      "        [-0.0276,  0.0345,  0.0393,  ...,  0.0527, -0.0302, -0.0160],\n",
      "        [ 0.0033, -0.0292,  0.0256,  ..., -0.0033, -0.0240,  0.0442],\n",
      "        [ 0.0057, -0.0106, -0.0523,  ..., -0.0558, -0.0541, -0.0232]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0645, -0.0574, -0.0094,  ..., -0.0720,  0.0077,  0.0477],\n",
      "        [ 0.1103, -0.0336, -0.0141,  ..., -0.0565,  0.0030,  0.0696],\n",
      "        [ 0.0614, -0.0357,  0.0106,  ..., -0.0868,  0.0190,  0.0539],\n",
      "        ...,\n",
      "        [-0.0276,  0.0345,  0.0393,  ...,  0.0527, -0.0302, -0.0160],\n",
      "        [ 0.0033, -0.0292,  0.0256,  ..., -0.0032, -0.0239,  0.0442],\n",
      "        [ 0.0057, -0.0106, -0.0523,  ..., -0.0558, -0.0541, -0.0233]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0646, -0.0573, -0.0094,  ..., -0.0720,  0.0077,  0.0477],\n",
      "        [ 0.1104, -0.0335, -0.0141,  ..., -0.0565,  0.0031,  0.0696],\n",
      "        [ 0.0614, -0.0356,  0.0106,  ..., -0.0868,  0.0191,  0.0540],\n",
      "        ...,\n",
      "        [-0.0276,  0.0345,  0.0393,  ...,  0.0526, -0.0302, -0.0160],\n",
      "        [ 0.0032, -0.0291,  0.0256,  ..., -0.0032, -0.0239,  0.0441],\n",
      "        [ 0.0056, -0.0106, -0.0523,  ..., -0.0557, -0.0540, -0.0235]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0646, -0.0572, -0.0094,  ..., -0.0720,  0.0078,  0.0477],\n",
      "        [ 0.1104, -0.0335, -0.0141,  ..., -0.0565,  0.0031,  0.0696],\n",
      "        [ 0.0614, -0.0356,  0.0106,  ..., -0.0868,  0.0191,  0.0540],\n",
      "        ...,\n",
      "        [-0.0276,  0.0344,  0.0393,  ...,  0.0526, -0.0302, -0.0159],\n",
      "        [ 0.0032, -0.0291,  0.0257,  ..., -0.0031, -0.0238,  0.0441],\n",
      "        [ 0.0056, -0.0106, -0.0523,  ..., -0.0557, -0.0540, -0.0235]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0646, -0.0572, -0.0094,  ..., -0.0720,  0.0079,  0.0477],\n",
      "        [ 0.1104, -0.0334, -0.0141,  ..., -0.0565,  0.0032,  0.0696],\n",
      "        [ 0.0614, -0.0355,  0.0106,  ..., -0.0868,  0.0192,  0.0540],\n",
      "        ...,\n",
      "        [-0.0276,  0.0344,  0.0393,  ...,  0.0525, -0.0303, -0.0159],\n",
      "        [ 0.0032, -0.0290,  0.0257,  ..., -0.0030, -0.0238,  0.0440],\n",
      "        [ 0.0055, -0.0106, -0.0523,  ..., -0.0557, -0.0540, -0.0236]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0646, -0.0571, -0.0094,  ..., -0.0720,  0.0080,  0.0478],\n",
      "        [ 0.1104, -0.0334, -0.0141,  ..., -0.0565,  0.0033,  0.0696],\n",
      "        [ 0.0615, -0.0355,  0.0105,  ..., -0.0869,  0.0193,  0.0541],\n",
      "        ...,\n",
      "        [-0.0275,  0.0343,  0.0392,  ...,  0.0525, -0.0303, -0.0159],\n",
      "        [ 0.0031, -0.0290,  0.0258,  ..., -0.0030, -0.0237,  0.0440],\n",
      "        [ 0.0054, -0.0106, -0.0523,  ..., -0.0557, -0.0540, -0.0237]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0647, -0.0571, -0.0095,  ..., -0.0721,  0.0080,  0.0478],\n",
      "        [ 0.1104, -0.0333, -0.0141,  ..., -0.0566,  0.0034,  0.0697],\n",
      "        [ 0.0615, -0.0354,  0.0105,  ..., -0.0869,  0.0194,  0.0541],\n",
      "        ...,\n",
      "        [-0.0275,  0.0342,  0.0392,  ...,  0.0524, -0.0304, -0.0158],\n",
      "        [ 0.0031, -0.0289,  0.0258,  ..., -0.0029, -0.0236,  0.0439],\n",
      "        [ 0.0054, -0.0106, -0.0523,  ..., -0.0557, -0.0540, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0647, -0.0571, -0.0095,  ..., -0.0721,  0.0081,  0.0478],\n",
      "        [ 0.1105, -0.0333, -0.0141,  ..., -0.0566,  0.0034,  0.0697],\n",
      "        [ 0.0616, -0.0354,  0.0105,  ..., -0.0870,  0.0195,  0.0541],\n",
      "        ...,\n",
      "        [-0.0274,  0.0341,  0.0391,  ...,  0.0523, -0.0305, -0.0157],\n",
      "        [ 0.0031, -0.0289,  0.0258,  ..., -0.0029, -0.0236,  0.0439],\n",
      "        [ 0.0054, -0.0106, -0.0523,  ..., -0.0557, -0.0540, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0647, -0.0571, -0.0095,  ..., -0.0721,  0.0082,  0.0478],\n",
      "        [ 0.1105, -0.0333, -0.0141,  ..., -0.0567,  0.0035,  0.0697],\n",
      "        [ 0.0616, -0.0354,  0.0105,  ..., -0.0870,  0.0196,  0.0541],\n",
      "        ...,\n",
      "        [-0.0274,  0.0341,  0.0391,  ...,  0.0522, -0.0307, -0.0157],\n",
      "        [ 0.0031, -0.0288,  0.0258,  ..., -0.0028, -0.0235,  0.0439],\n",
      "        [ 0.0054, -0.0106, -0.0523,  ..., -0.0557, -0.0541, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0648, -0.0571, -0.0096,  ..., -0.0722,  0.0083,  0.0478],\n",
      "        [ 0.1106, -0.0333, -0.0142,  ..., -0.0567,  0.0036,  0.0697],\n",
      "        [ 0.0617, -0.0354,  0.0104,  ..., -0.0871,  0.0197,  0.0542],\n",
      "        ...,\n",
      "        [-0.0273,  0.0340,  0.0391,  ...,  0.0522, -0.0307, -0.0156],\n",
      "        [ 0.0031, -0.0288,  0.0259,  ..., -0.0028, -0.0235,  0.0439],\n",
      "        [ 0.0053, -0.0107, -0.0523,  ..., -0.0557, -0.0541, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0648, -0.0571, -0.0096,  ..., -0.0722,  0.0083,  0.0478],\n",
      "        [ 0.1106, -0.0332, -0.0142,  ..., -0.0568,  0.0037,  0.0697],\n",
      "        [ 0.0617, -0.0354,  0.0104,  ..., -0.0871,  0.0198,  0.0542],\n",
      "        ...,\n",
      "        [-0.0273,  0.0339,  0.0390,  ...,  0.0521, -0.0308, -0.0155],\n",
      "        [ 0.0031, -0.0288,  0.0259,  ..., -0.0028, -0.0235,  0.0439],\n",
      "        [ 0.0053, -0.0107, -0.0523,  ..., -0.0557, -0.0541, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0649, -0.0571, -0.0097,  ..., -0.0722,  0.0084,  0.0479],\n",
      "        [ 0.1107, -0.0332, -0.0143,  ..., -0.0568,  0.0038,  0.0698],\n",
      "        [ 0.0617, -0.0354,  0.0103,  ..., -0.0871,  0.0199,  0.0542],\n",
      "        ...,\n",
      "        [-0.0272,  0.0338,  0.0390,  ...,  0.0520, -0.0310, -0.0154],\n",
      "        [ 0.0030, -0.0287,  0.0259,  ..., -0.0027, -0.0235,  0.0438],\n",
      "        [ 0.0053, -0.0107, -0.0523,  ..., -0.0557, -0.0542, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0649, -0.0571, -0.0098,  ..., -0.0723,  0.0085,  0.0479],\n",
      "        [ 0.1107, -0.0332, -0.0143,  ..., -0.0569,  0.0039,  0.0698],\n",
      "        [ 0.0618, -0.0353,  0.0103,  ..., -0.0872,  0.0200,  0.0543],\n",
      "        ...,\n",
      "        [-0.0271,  0.0337,  0.0389,  ...,  0.0519, -0.0311, -0.0153],\n",
      "        [ 0.0031, -0.0287,  0.0259,  ..., -0.0027, -0.0235,  0.0439],\n",
      "        [ 0.0053, -0.0107, -0.0524,  ..., -0.0557, -0.0542, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0649, -0.0571, -0.0098,  ..., -0.0723,  0.0085,  0.0479],\n",
      "        [ 0.1107, -0.0332, -0.0144,  ..., -0.0569,  0.0039,  0.0698],\n",
      "        [ 0.0618, -0.0353,  0.0103,  ..., -0.0872,  0.0201,  0.0543],\n",
      "        ...,\n",
      "        [-0.0271,  0.0336,  0.0389,  ...,  0.0518, -0.0311, -0.0153],\n",
      "        [ 0.0031, -0.0287,  0.0259,  ..., -0.0027, -0.0235,  0.0439],\n",
      "        [ 0.0053, -0.0107, -0.0524,  ..., -0.0557, -0.0543, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0650, -0.0571, -0.0098,  ..., -0.0723,  0.0086,  0.0479],\n",
      "        [ 0.1108, -0.0332, -0.0144,  ..., -0.0569,  0.0040,  0.0698],\n",
      "        [ 0.0618, -0.0353,  0.0103,  ..., -0.0872,  0.0201,  0.0543],\n",
      "        ...,\n",
      "        [-0.0270,  0.0335,  0.0388,  ...,  0.0517, -0.0312, -0.0152],\n",
      "        [ 0.0031, -0.0287,  0.0259,  ..., -0.0027, -0.0235,  0.0439],\n",
      "        [ 0.0053, -0.0107, -0.0525,  ..., -0.0558, -0.0544, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0650, -0.0571, -0.0098,  ..., -0.0724,  0.0087,  0.0479],\n",
      "        [ 0.1108, -0.0332, -0.0144,  ..., -0.0570,  0.0041,  0.0699],\n",
      "        [ 0.0619, -0.0354,  0.0102,  ..., -0.0873,  0.0202,  0.0543],\n",
      "        ...,\n",
      "        [-0.0269,  0.0335,  0.0388,  ...,  0.0517, -0.0313, -0.0152],\n",
      "        [ 0.0031, -0.0288,  0.0258,  ..., -0.0027, -0.0235,  0.0440],\n",
      "        [ 0.0053, -0.0108, -0.0526,  ..., -0.0558, -0.0544, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0651, -0.0571, -0.0099,  ..., -0.0724,  0.0088,  0.0479],\n",
      "        [ 0.1109, -0.0332, -0.0144,  ..., -0.0570,  0.0042,  0.0699],\n",
      "        [ 0.0619, -0.0353,  0.0102,  ..., -0.0873,  0.0203,  0.0543],\n",
      "        ...,\n",
      "        [-0.0269,  0.0334,  0.0388,  ...,  0.0516, -0.0313, -0.0152],\n",
      "        [ 0.0032, -0.0288,  0.0258,  ..., -0.0028, -0.0235,  0.0440],\n",
      "        [ 0.0053, -0.0108, -0.0526,  ..., -0.0558, -0.0545, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0651, -0.0571, -0.0099,  ..., -0.0724,  0.0089,  0.0480],\n",
      "        [ 0.1109, -0.0332, -0.0144,  ..., -0.0571,  0.0043,  0.0699],\n",
      "        [ 0.0619, -0.0353,  0.0102,  ..., -0.0873,  0.0204,  0.0543],\n",
      "        ...,\n",
      "        [-0.0269,  0.0334,  0.0387,  ...,  0.0516, -0.0313, -0.0152],\n",
      "        [ 0.0032, -0.0288,  0.0257,  ..., -0.0028, -0.0235,  0.0441],\n",
      "        [ 0.0053, -0.0108, -0.0527,  ..., -0.0558, -0.0545, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0652, -0.0571, -0.0099,  ..., -0.0725,  0.0090,  0.0480],\n",
      "        [ 0.1110, -0.0332, -0.0144,  ..., -0.0571,  0.0043,  0.0699],\n",
      "        [ 0.0620, -0.0353,  0.0102,  ..., -0.0873,  0.0204,  0.0544],\n",
      "        ...,\n",
      "        [-0.0269,  0.0334,  0.0387,  ...,  0.0516, -0.0313, -0.0152],\n",
      "        [ 0.0033, -0.0288,  0.0257,  ..., -0.0028, -0.0236,  0.0442],\n",
      "        [ 0.0053, -0.0108, -0.0527,  ..., -0.0558, -0.0545, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0652, -0.0571, -0.0100,  ..., -0.0725,  0.0090,  0.0480],\n",
      "        [ 0.1110, -0.0332, -0.0144,  ..., -0.0571,  0.0044,  0.0699],\n",
      "        [ 0.0620, -0.0354,  0.0102,  ..., -0.0874,  0.0205,  0.0544],\n",
      "        ...,\n",
      "        [-0.0269,  0.0334,  0.0387,  ...,  0.0516, -0.0313, -0.0153],\n",
      "        [ 0.0033, -0.0288,  0.0256,  ..., -0.0028, -0.0236,  0.0442],\n",
      "        [ 0.0054, -0.0108, -0.0527,  ..., -0.0558, -0.0546, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0652, -0.0571, -0.0100,  ..., -0.0725,  0.0091,  0.0480],\n",
      "        [ 0.1110, -0.0332, -0.0145,  ..., -0.0571,  0.0044,  0.0699],\n",
      "        [ 0.0620, -0.0354,  0.0101,  ..., -0.0874,  0.0205,  0.0544],\n",
      "        ...,\n",
      "        [-0.0269,  0.0334,  0.0387,  ...,  0.0516, -0.0313, -0.0153],\n",
      "        [ 0.0033, -0.0289,  0.0255,  ..., -0.0029, -0.0236,  0.0443],\n",
      "        [ 0.0054, -0.0108, -0.0528,  ..., -0.0559, -0.0546, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0653, -0.0571, -0.0100,  ..., -0.0725,  0.0091,  0.0480],\n",
      "        [ 0.1111, -0.0332, -0.0145,  ..., -0.0572,  0.0045,  0.0700],\n",
      "        [ 0.0621, -0.0354,  0.0101,  ..., -0.0874,  0.0205,  0.0544],\n",
      "        ...,\n",
      "        [-0.0269,  0.0334,  0.0388,  ...,  0.0516, -0.0313, -0.0153],\n",
      "        [ 0.0034, -0.0289,  0.0255,  ..., -0.0029, -0.0237,  0.0444],\n",
      "        [ 0.0053, -0.0108, -0.0528,  ..., -0.0559, -0.0547, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0653, -0.0572, -0.0100,  ..., -0.0726,  0.0092,  0.0480],\n",
      "        [ 0.1111, -0.0333, -0.0145,  ..., -0.0572,  0.0046,  0.0700],\n",
      "        [ 0.0621, -0.0354,  0.0101,  ..., -0.0874,  0.0206,  0.0544],\n",
      "        ...,\n",
      "        [-0.0269,  0.0334,  0.0388,  ...,  0.0516, -0.0313, -0.0154],\n",
      "        [ 0.0034, -0.0289,  0.0255,  ..., -0.0029, -0.0237,  0.0444],\n",
      "        [ 0.0053, -0.0108, -0.0529,  ..., -0.0559, -0.0547, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0654, -0.0572, -0.0100,  ..., -0.0726,  0.0093,  0.0480],\n",
      "        [ 0.1112, -0.0333, -0.0145,  ..., -0.0572,  0.0046,  0.0700],\n",
      "        [ 0.0622, -0.0355,  0.0101,  ..., -0.0874,  0.0206,  0.0544],\n",
      "        ...,\n",
      "        [-0.0270,  0.0335,  0.0388,  ...,  0.0516, -0.0313, -0.0154],\n",
      "        [ 0.0035, -0.0289,  0.0254,  ..., -0.0030, -0.0237,  0.0444],\n",
      "        [ 0.0053, -0.0108, -0.0529,  ..., -0.0559, -0.0548, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0654, -0.0572, -0.0101,  ..., -0.0726,  0.0093,  0.0480],\n",
      "        [ 0.1112, -0.0333, -0.0145,  ..., -0.0573,  0.0047,  0.0700],\n",
      "        [ 0.0622, -0.0355,  0.0100,  ..., -0.0875,  0.0207,  0.0545],\n",
      "        ...,\n",
      "        [-0.0270,  0.0335,  0.0389,  ...,  0.0517, -0.0312, -0.0154],\n",
      "        [ 0.0035, -0.0289,  0.0253,  ..., -0.0030, -0.0237,  0.0445],\n",
      "        [ 0.0053, -0.0109, -0.0530,  ..., -0.0559, -0.0548, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0655, -0.0573, -0.0101,  ..., -0.0727,  0.0094,  0.0480],\n",
      "        [ 0.1113, -0.0333, -0.0146,  ..., -0.0573,  0.0047,  0.0700],\n",
      "        [ 0.0622, -0.0356,  0.0099,  ..., -0.0875,  0.0207,  0.0545],\n",
      "        ...,\n",
      "        [-0.0270,  0.0335,  0.0389,  ...,  0.0517, -0.0312, -0.0154],\n",
      "        [ 0.0036, -0.0289,  0.0253,  ..., -0.0030, -0.0237,  0.0445],\n",
      "        [ 0.0053, -0.0109, -0.0530,  ..., -0.0560, -0.0548, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0655, -0.0573, -0.0102,  ..., -0.0727,  0.0094,  0.0480],\n",
      "        [ 0.1113, -0.0334, -0.0146,  ..., -0.0573,  0.0048,  0.0701],\n",
      "        [ 0.0623, -0.0356,  0.0099,  ..., -0.0875,  0.0207,  0.0545],\n",
      "        ...,\n",
      "        [-0.0270,  0.0335,  0.0389,  ...,  0.0517, -0.0312, -0.0154],\n",
      "        [ 0.0036, -0.0289,  0.0253,  ..., -0.0030, -0.0237,  0.0445],\n",
      "        [ 0.0053, -0.0108, -0.0530,  ..., -0.0560, -0.0549, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0656, -0.0574, -0.0102,  ..., -0.0727,  0.0094,  0.0480],\n",
      "        [ 0.1113, -0.0334, -0.0147,  ..., -0.0574,  0.0048,  0.0701],\n",
      "        [ 0.0623, -0.0357,  0.0099,  ..., -0.0875,  0.0207,  0.0545],\n",
      "        ...,\n",
      "        [-0.0271,  0.0335,  0.0390,  ...,  0.0517, -0.0312, -0.0155],\n",
      "        [ 0.0036, -0.0289,  0.0252,  ..., -0.0030, -0.0237,  0.0445],\n",
      "        [ 0.0053, -0.0108, -0.0530,  ..., -0.0559, -0.0549, -0.0240]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0656, -0.0575, -0.0102,  ..., -0.0728,  0.0095,  0.0480],\n",
      "        [ 0.1114, -0.0335, -0.0147,  ..., -0.0574,  0.0048,  0.0701],\n",
      "        [ 0.0623, -0.0358,  0.0098,  ..., -0.0876,  0.0207,  0.0545],\n",
      "        ...,\n",
      "        [-0.0271,  0.0336,  0.0390,  ...,  0.0518, -0.0312, -0.0155],\n",
      "        [ 0.0036, -0.0288,  0.0252,  ..., -0.0030, -0.0237,  0.0445],\n",
      "        [ 0.0052, -0.0108, -0.0530,  ..., -0.0559, -0.0549, -0.0240]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0657, -0.0575, -0.0102,  ..., -0.0728,  0.0095,  0.0481],\n",
      "        [ 0.1114, -0.0335, -0.0147,  ..., -0.0574,  0.0048,  0.0701],\n",
      "        [ 0.0624, -0.0358,  0.0098,  ..., -0.0876,  0.0207,  0.0545],\n",
      "        ...,\n",
      "        [-0.0272,  0.0336,  0.0391,  ...,  0.0518, -0.0311, -0.0155],\n",
      "        [ 0.0036, -0.0288,  0.0252,  ..., -0.0030, -0.0237,  0.0445],\n",
      "        [ 0.0052, -0.0108, -0.0530,  ..., -0.0559, -0.0549, -0.0241]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0657, -0.0576, -0.0103,  ..., -0.0729,  0.0095,  0.0481],\n",
      "        [ 0.1114, -0.0336, -0.0147,  ..., -0.0575,  0.0049,  0.0701],\n",
      "        [ 0.0624, -0.0359,  0.0098,  ..., -0.0876,  0.0208,  0.0545],\n",
      "        ...,\n",
      "        [-0.0273,  0.0336,  0.0392,  ...,  0.0519, -0.0311, -0.0156],\n",
      "        [ 0.0036, -0.0288,  0.0252,  ..., -0.0030, -0.0237,  0.0445],\n",
      "        [ 0.0051, -0.0107, -0.0530,  ..., -0.0559, -0.0549, -0.0241]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0658, -0.0576, -0.0102,  ..., -0.0729,  0.0096,  0.0481],\n",
      "        [ 0.1115, -0.0336, -0.0147,  ..., -0.0575,  0.0049,  0.0701],\n",
      "        [ 0.0625, -0.0359,  0.0098,  ..., -0.0877,  0.0208,  0.0546],\n",
      "        ...,\n",
      "        [-0.0274,  0.0337,  0.0393,  ...,  0.0519, -0.0310, -0.0156],\n",
      "        [ 0.0036, -0.0288,  0.0252,  ..., -0.0030, -0.0236,  0.0445],\n",
      "        [ 0.0050, -0.0107, -0.0530,  ..., -0.0558, -0.0549, -0.0242]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0658, -0.0576, -0.0102,  ..., -0.0729,  0.0096,  0.0481],\n",
      "        [ 0.1115, -0.0336, -0.0147,  ..., -0.0575,  0.0050,  0.0702],\n",
      "        [ 0.0625, -0.0359,  0.0098,  ..., -0.0877,  0.0208,  0.0546],\n",
      "        ...,\n",
      "        [-0.0275,  0.0337,  0.0394,  ...,  0.0520, -0.0309, -0.0157],\n",
      "        [ 0.0036, -0.0287,  0.0253,  ..., -0.0029, -0.0236,  0.0445],\n",
      "        [ 0.0050, -0.0106, -0.0529,  ..., -0.0558, -0.0549, -0.0242]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0658, -0.0576, -0.0102,  ..., -0.0730,  0.0097,  0.0481],\n",
      "        [ 0.1115, -0.0335, -0.0146,  ..., -0.0575,  0.0050,  0.0702],\n",
      "        [ 0.0625, -0.0360,  0.0098,  ..., -0.0877,  0.0209,  0.0546],\n",
      "        ...,\n",
      "        [-0.0275,  0.0338,  0.0394,  ...,  0.0521, -0.0309, -0.0158],\n",
      "        [ 0.0036, -0.0287,  0.0253,  ..., -0.0029, -0.0235,  0.0445],\n",
      "        [ 0.0049, -0.0106, -0.0529,  ..., -0.0557, -0.0548, -0.0242]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0658, -0.0577, -0.0102,  ..., -0.0730,  0.0097,  0.0481],\n",
      "        [ 0.1116, -0.0335, -0.0146,  ..., -0.0575,  0.0051,  0.0702],\n",
      "        [ 0.0625, -0.0360,  0.0099,  ..., -0.0878,  0.0210,  0.0546],\n",
      "        ...,\n",
      "        [-0.0276,  0.0338,  0.0395,  ...,  0.0521, -0.0308, -0.0158],\n",
      "        [ 0.0036, -0.0287,  0.0253,  ..., -0.0029, -0.0235,  0.0445],\n",
      "        [ 0.0048, -0.0105, -0.0529,  ..., -0.0557, -0.0548, -0.0243]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0659, -0.0577, -0.0101,  ..., -0.0730,  0.0098,  0.0481],\n",
      "        [ 0.1116, -0.0335, -0.0146,  ..., -0.0576,  0.0052,  0.0702],\n",
      "        [ 0.0626, -0.0360,  0.0099,  ..., -0.0878,  0.0210,  0.0546],\n",
      "        ...,\n",
      "        [-0.0276,  0.0338,  0.0395,  ...,  0.0521, -0.0308, -0.0159],\n",
      "        [ 0.0036, -0.0287,  0.0253,  ..., -0.0029, -0.0235,  0.0445],\n",
      "        [ 0.0048, -0.0104, -0.0528,  ..., -0.0556, -0.0548, -0.0243]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0659, -0.0577, -0.0101,  ..., -0.0730,  0.0098,  0.0481],\n",
      "        [ 0.1116, -0.0335, -0.0145,  ..., -0.0576,  0.0052,  0.0702],\n",
      "        [ 0.0626, -0.0360,  0.0099,  ..., -0.0878,  0.0211,  0.0547],\n",
      "        ...,\n",
      "        [-0.0276,  0.0337,  0.0395,  ...,  0.0522, -0.0308, -0.0159],\n",
      "        [ 0.0036, -0.0287,  0.0253,  ..., -0.0029, -0.0235,  0.0446],\n",
      "        [ 0.0047, -0.0103, -0.0528,  ..., -0.0556, -0.0548, -0.0243]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0659, -0.0577, -0.0101,  ..., -0.0730,  0.0099,  0.0481],\n",
      "        [ 0.1116, -0.0335, -0.0145,  ..., -0.0576,  0.0053,  0.0703],\n",
      "        [ 0.0626, -0.0360,  0.0100,  ..., -0.0878,  0.0212,  0.0547],\n",
      "        ...,\n",
      "        [-0.0276,  0.0337,  0.0395,  ...,  0.0522, -0.0308, -0.0159],\n",
      "        [ 0.0036, -0.0287,  0.0253,  ..., -0.0029, -0.0235,  0.0446],\n",
      "        [ 0.0046, -0.0103, -0.0528,  ..., -0.0556, -0.0548, -0.0243]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0659, -0.0577, -0.0100,  ..., -0.0730,  0.0099,  0.0481],\n",
      "        [ 0.1116, -0.0335, -0.0144,  ..., -0.0576,  0.0053,  0.0702],\n",
      "        [ 0.0626, -0.0360,  0.0100,  ..., -0.0879,  0.0212,  0.0547],\n",
      "        ...,\n",
      "        [-0.0276,  0.0337,  0.0395,  ...,  0.0522, -0.0308, -0.0159],\n",
      "        [ 0.0036, -0.0287,  0.0253,  ..., -0.0029, -0.0234,  0.0446],\n",
      "        [ 0.0046, -0.0103, -0.0528,  ..., -0.0556, -0.0548, -0.0243]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0659, -0.0577, -0.0100,  ..., -0.0730,  0.0100,  0.0480],\n",
      "        [ 0.1116, -0.0334, -0.0144,  ..., -0.0576,  0.0054,  0.0702],\n",
      "        [ 0.0627, -0.0360,  0.0101,  ..., -0.0879,  0.0213,  0.0547],\n",
      "        ...,\n",
      "        [-0.0276,  0.0337,  0.0395,  ...,  0.0521, -0.0308, -0.0159],\n",
      "        [ 0.0036, -0.0286,  0.0252,  ..., -0.0030, -0.0234,  0.0446],\n",
      "        [ 0.0046, -0.0103, -0.0529,  ..., -0.0556, -0.0549, -0.0242]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0658, -0.0576, -0.0099,  ..., -0.0730,  0.0100,  0.0480],\n",
      "        [ 0.1116, -0.0334, -0.0144,  ..., -0.0576,  0.0055,  0.0702],\n",
      "        [ 0.0627, -0.0360,  0.0101,  ..., -0.0879,  0.0214,  0.0547],\n",
      "        ...,\n",
      "        [-0.0276,  0.0336,  0.0395,  ...,  0.0521, -0.0309, -0.0159],\n",
      "        [ 0.0036, -0.0286,  0.0252,  ..., -0.0030, -0.0234,  0.0447],\n",
      "        [ 0.0046, -0.0103, -0.0529,  ..., -0.0556, -0.0549, -0.0242]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0658, -0.0575, -0.0099,  ..., -0.0730,  0.0101,  0.0480],\n",
      "        [ 0.1116, -0.0333, -0.0143,  ..., -0.0576,  0.0055,  0.0702],\n",
      "        [ 0.0627, -0.0359,  0.0101,  ..., -0.0879,  0.0214,  0.0547],\n",
      "        ...,\n",
      "        [-0.0276,  0.0335,  0.0395,  ...,  0.0521, -0.0309, -0.0158],\n",
      "        [ 0.0036, -0.0287,  0.0252,  ..., -0.0030, -0.0234,  0.0447],\n",
      "        [ 0.0046, -0.0103, -0.0529,  ..., -0.0556, -0.0550, -0.0241]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0658, -0.0575, -0.0099,  ..., -0.0730,  0.0102,  0.0479],\n",
      "        [ 0.1116, -0.0333, -0.0143,  ..., -0.0577,  0.0056,  0.0702],\n",
      "        [ 0.0627, -0.0359,  0.0102,  ..., -0.0880,  0.0215,  0.0547],\n",
      "        ...,\n",
      "        [-0.0276,  0.0335,  0.0395,  ...,  0.0520, -0.0310, -0.0158],\n",
      "        [ 0.0037, -0.0287,  0.0252,  ..., -0.0030, -0.0234,  0.0447],\n",
      "        [ 0.0046, -0.0103, -0.0530,  ..., -0.0556, -0.0550, -0.0241]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0658, -0.0574, -0.0099,  ..., -0.0730,  0.0102,  0.0479],\n",
      "        [ 0.1117, -0.0332, -0.0143,  ..., -0.0577,  0.0057,  0.0702],\n",
      "        [ 0.0627, -0.0359,  0.0102,  ..., -0.0880,  0.0216,  0.0547],\n",
      "        ...,\n",
      "        [-0.0275,  0.0334,  0.0395,  ...,  0.0520, -0.0310, -0.0158],\n",
      "        [ 0.0037, -0.0287,  0.0251,  ..., -0.0031, -0.0234,  0.0448],\n",
      "        [ 0.0046, -0.0104, -0.0530,  ..., -0.0557, -0.0551, -0.0240]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0659, -0.0574, -0.0099,  ..., -0.0730,  0.0103,  0.0479],\n",
      "        [ 0.1117, -0.0332, -0.0143,  ..., -0.0577,  0.0057,  0.0702],\n",
      "        [ 0.0627, -0.0359,  0.0102,  ..., -0.0880,  0.0216,  0.0547],\n",
      "        ...,\n",
      "        [-0.0275,  0.0333,  0.0394,  ...,  0.0519, -0.0310, -0.0157],\n",
      "        [ 0.0038, -0.0287,  0.0251,  ..., -0.0031, -0.0235,  0.0448],\n",
      "        [ 0.0046, -0.0104, -0.0531,  ..., -0.0557, -0.0552, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0659, -0.0573, -0.0099,  ..., -0.0730,  0.0103,  0.0479],\n",
      "        [ 0.1117, -0.0332, -0.0143,  ..., -0.0577,  0.0058,  0.0702],\n",
      "        [ 0.0628, -0.0358,  0.0102,  ..., -0.0880,  0.0217,  0.0547],\n",
      "        ...,\n",
      "        [-0.0274,  0.0332,  0.0394,  ...,  0.0519, -0.0311, -0.0157],\n",
      "        [ 0.0038, -0.0287,  0.0250,  ..., -0.0031, -0.0235,  0.0449],\n",
      "        [ 0.0045, -0.0104, -0.0531,  ..., -0.0557, -0.0553, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0659, -0.0573, -0.0099,  ..., -0.0730,  0.0104,  0.0479],\n",
      "        [ 0.1117, -0.0331, -0.0143,  ..., -0.0577,  0.0059,  0.0703],\n",
      "        [ 0.0628, -0.0358,  0.0102,  ..., -0.0880,  0.0218,  0.0547],\n",
      "        ...,\n",
      "        [-0.0274,  0.0332,  0.0394,  ...,  0.0519, -0.0311, -0.0156],\n",
      "        [ 0.0038, -0.0287,  0.0250,  ..., -0.0031, -0.0235,  0.0449],\n",
      "        [ 0.0045, -0.0104, -0.0531,  ..., -0.0557, -0.0553, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0659, -0.0572, -0.0099,  ..., -0.0730,  0.0105,  0.0480],\n",
      "        [ 0.1117, -0.0331, -0.0142,  ..., -0.0577,  0.0059,  0.0703],\n",
      "        [ 0.0628, -0.0358,  0.0102,  ..., -0.0881,  0.0218,  0.0548],\n",
      "        ...,\n",
      "        [-0.0274,  0.0331,  0.0394,  ...,  0.0518, -0.0311, -0.0156],\n",
      "        [ 0.0037, -0.0287,  0.0250,  ..., -0.0031, -0.0234,  0.0449],\n",
      "        [ 0.0045, -0.0104, -0.0531,  ..., -0.0556, -0.0554, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0572, -0.0099,  ..., -0.0730,  0.0105,  0.0480],\n",
      "        [ 0.1118, -0.0331, -0.0142,  ..., -0.0578,  0.0060,  0.0703],\n",
      "        [ 0.0628, -0.0358,  0.0102,  ..., -0.0881,  0.0219,  0.0548],\n",
      "        ...,\n",
      "        [-0.0273,  0.0331,  0.0394,  ...,  0.0518, -0.0311, -0.0156],\n",
      "        [ 0.0037, -0.0286,  0.0250,  ..., -0.0031, -0.0234,  0.0449],\n",
      "        [ 0.0045, -0.0104, -0.0532,  ..., -0.0557, -0.0554, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0572, -0.0099,  ..., -0.0731,  0.0106,  0.0480],\n",
      "        [ 0.1118, -0.0330, -0.0142,  ..., -0.0578,  0.0060,  0.0703],\n",
      "        [ 0.0629, -0.0357,  0.0103,  ..., -0.0881,  0.0219,  0.0548],\n",
      "        ...,\n",
      "        [-0.0273,  0.0330,  0.0394,  ...,  0.0518, -0.0312, -0.0155],\n",
      "        [ 0.0037, -0.0286,  0.0250,  ..., -0.0030, -0.0234,  0.0449],\n",
      "        [ 0.0044, -0.0104, -0.0532,  ..., -0.0557, -0.0555, -0.0237]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0571, -0.0099,  ..., -0.0730,  0.0106,  0.0480],\n",
      "        [ 0.1118, -0.0330, -0.0142,  ..., -0.0578,  0.0061,  0.0703],\n",
      "        [ 0.0629, -0.0357,  0.0103,  ..., -0.0881,  0.0220,  0.0549],\n",
      "        ...,\n",
      "        [-0.0273,  0.0330,  0.0395,  ...,  0.0518, -0.0312, -0.0155],\n",
      "        [ 0.0037, -0.0286,  0.0250,  ..., -0.0030, -0.0234,  0.0449],\n",
      "        [ 0.0044, -0.0104, -0.0532,  ..., -0.0557, -0.0555, -0.0237]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0570, -0.0099,  ..., -0.0730,  0.0106,  0.0481],\n",
      "        [ 0.1118, -0.0330, -0.0142,  ..., -0.0578,  0.0061,  0.0704],\n",
      "        [ 0.0629, -0.0357,  0.0103,  ..., -0.0881,  0.0220,  0.0549],\n",
      "        ...,\n",
      "        [-0.0273,  0.0330,  0.0395,  ...,  0.0519, -0.0312, -0.0155],\n",
      "        [ 0.0037, -0.0286,  0.0250,  ..., -0.0030, -0.0233,  0.0449],\n",
      "        [ 0.0045, -0.0104, -0.0532,  ..., -0.0557, -0.0556, -0.0237]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0570, -0.0099,  ..., -0.0730,  0.0107,  0.0481],\n",
      "        [ 0.1118, -0.0330, -0.0142,  ..., -0.0578,  0.0061,  0.0704],\n",
      "        [ 0.0629, -0.0356,  0.0103,  ..., -0.0881,  0.0220,  0.0549],\n",
      "        ...,\n",
      "        [-0.0273,  0.0330,  0.0395,  ...,  0.0519, -0.0312, -0.0155],\n",
      "        [ 0.0037, -0.0286,  0.0250,  ..., -0.0030, -0.0233,  0.0449],\n",
      "        [ 0.0045, -0.0105, -0.0533,  ..., -0.0557, -0.0556, -0.0237]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0569, -0.0099,  ..., -0.0730,  0.0107,  0.0481],\n",
      "        [ 0.1118, -0.0330, -0.0142,  ..., -0.0578,  0.0061,  0.0704],\n",
      "        [ 0.0629, -0.0356,  0.0103,  ..., -0.0881,  0.0220,  0.0549],\n",
      "        ...,\n",
      "        [-0.0273,  0.0330,  0.0395,  ...,  0.0519, -0.0312, -0.0155],\n",
      "        [ 0.0037, -0.0286,  0.0250,  ..., -0.0030, -0.0233,  0.0448],\n",
      "        [ 0.0045, -0.0105, -0.0534,  ..., -0.0558, -0.0557, -0.0236]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0569, -0.0098,  ..., -0.0730,  0.0108,  0.0481],\n",
      "        [ 0.1118, -0.0330, -0.0142,  ..., -0.0578,  0.0061,  0.0704],\n",
      "        [ 0.0629, -0.0356,  0.0103,  ..., -0.0881,  0.0221,  0.0549],\n",
      "        ...,\n",
      "        [-0.0273,  0.0329,  0.0395,  ...,  0.0519, -0.0312, -0.0154],\n",
      "        [ 0.0037, -0.0286,  0.0250,  ..., -0.0030, -0.0232,  0.0448],\n",
      "        [ 0.0045, -0.0105, -0.0535,  ..., -0.0558, -0.0558, -0.0236]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0568, -0.0098,  ..., -0.0730,  0.0108,  0.0480],\n",
      "        [ 0.1118, -0.0329, -0.0141,  ..., -0.0578,  0.0061,  0.0704],\n",
      "        [ 0.0629, -0.0355,  0.0104,  ..., -0.0881,  0.0221,  0.0549],\n",
      "        ...,\n",
      "        [-0.0273,  0.0329,  0.0394,  ...,  0.0519, -0.0312, -0.0154],\n",
      "        [ 0.0036, -0.0286,  0.0250,  ..., -0.0030, -0.0232,  0.0448],\n",
      "        [ 0.0045, -0.0106, -0.0535,  ..., -0.0558, -0.0558, -0.0236]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0567, -0.0098,  ..., -0.0730,  0.0109,  0.0480],\n",
      "        [ 0.1119, -0.0329, -0.0141,  ..., -0.0578,  0.0062,  0.0704],\n",
      "        [ 0.0629, -0.0355,  0.0104,  ..., -0.0881,  0.0221,  0.0549],\n",
      "        ...,\n",
      "        [-0.0272,  0.0329,  0.0394,  ...,  0.0519, -0.0312, -0.0154],\n",
      "        [ 0.0036, -0.0286,  0.0250,  ..., -0.0030, -0.0232,  0.0448],\n",
      "        [ 0.0045, -0.0106, -0.0536,  ..., -0.0559, -0.0559, -0.0236]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0567, -0.0097,  ..., -0.0729,  0.0109,  0.0480],\n",
      "        [ 0.1119, -0.0328, -0.0141,  ..., -0.0578,  0.0062,  0.0704],\n",
      "        [ 0.0629, -0.0354,  0.0105,  ..., -0.0881,  0.0221,  0.0549],\n",
      "        ...,\n",
      "        [-0.0272,  0.0329,  0.0394,  ...,  0.0520, -0.0313, -0.0154],\n",
      "        [ 0.0036, -0.0286,  0.0250,  ..., -0.0030, -0.0232,  0.0448],\n",
      "        [ 0.0045, -0.0107, -0.0537,  ..., -0.0559, -0.0560, -0.0235]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0566, -0.0097,  ..., -0.0729,  0.0109,  0.0480],\n",
      "        [ 0.1119, -0.0328, -0.0140,  ..., -0.0578,  0.0062,  0.0704],\n",
      "        [ 0.0629, -0.0353,  0.0105,  ..., -0.0881,  0.0222,  0.0549],\n",
      "        ...,\n",
      "        [-0.0272,  0.0329,  0.0394,  ...,  0.0520, -0.0312, -0.0154],\n",
      "        [ 0.0036, -0.0286,  0.0250,  ..., -0.0030, -0.0232,  0.0447],\n",
      "        [ 0.0045, -0.0108, -0.0537,  ..., -0.0559, -0.0560, -0.0235]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0565, -0.0097,  ..., -0.0730,  0.0110,  0.0481],\n",
      "        [ 0.1119, -0.0327, -0.0140,  ..., -0.0578,  0.0063,  0.0705],\n",
      "        [ 0.0629, -0.0353,  0.0106,  ..., -0.0881,  0.0222,  0.0549],\n",
      "        ...,\n",
      "        [-0.0272,  0.0329,  0.0394,  ...,  0.0520, -0.0312, -0.0154],\n",
      "        [ 0.0036, -0.0286,  0.0250,  ..., -0.0030, -0.0232,  0.0447],\n",
      "        [ 0.0044, -0.0108, -0.0538,  ..., -0.0559, -0.0560, -0.0236]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0565, -0.0096,  ..., -0.0729,  0.0111,  0.0481],\n",
      "        [ 0.1119, -0.0327, -0.0139,  ..., -0.0578,  0.0063,  0.0705],\n",
      "        [ 0.0629, -0.0352,  0.0107,  ..., -0.0881,  0.0223,  0.0549],\n",
      "        ...,\n",
      "        [-0.0272,  0.0329,  0.0394,  ...,  0.0520, -0.0312, -0.0154],\n",
      "        [ 0.0036, -0.0286,  0.0250,  ..., -0.0030, -0.0231,  0.0447],\n",
      "        [ 0.0044, -0.0108, -0.0538,  ..., -0.0560, -0.0561, -0.0236]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0564, -0.0095,  ..., -0.0729,  0.0111,  0.0481],\n",
      "        [ 0.1119, -0.0327, -0.0138,  ..., -0.0578,  0.0064,  0.0705],\n",
      "        [ 0.0629, -0.0351,  0.0108,  ..., -0.0881,  0.0223,  0.0549],\n",
      "        ...,\n",
      "        [-0.0272,  0.0329,  0.0394,  ...,  0.0520, -0.0312, -0.0154],\n",
      "        [ 0.0036, -0.0286,  0.0250,  ..., -0.0030, -0.0231,  0.0447],\n",
      "        [ 0.0043, -0.0108, -0.0538,  ..., -0.0559, -0.0561, -0.0236]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0563, -0.0094,  ..., -0.0729,  0.0112,  0.0481],\n",
      "        [ 0.1120, -0.0326, -0.0137,  ..., -0.0578,  0.0065,  0.0705],\n",
      "        [ 0.0629, -0.0351,  0.0108,  ..., -0.0880,  0.0224,  0.0549],\n",
      "        ...,\n",
      "        [-0.0273,  0.0328,  0.0394,  ...,  0.0520, -0.0312, -0.0154],\n",
      "        [ 0.0036, -0.0286,  0.0249,  ..., -0.0030, -0.0231,  0.0446],\n",
      "        [ 0.0043, -0.0107, -0.0538,  ..., -0.0559, -0.0561, -0.0237]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0562, -0.0094,  ..., -0.0729,  0.0113,  0.0480],\n",
      "        [ 0.1120, -0.0325, -0.0136,  ..., -0.0578,  0.0066,  0.0705],\n",
      "        [ 0.0629, -0.0350,  0.0109,  ..., -0.0880,  0.0225,  0.0549],\n",
      "        ...,\n",
      "        [-0.0273,  0.0328,  0.0394,  ...,  0.0520, -0.0312, -0.0154],\n",
      "        [ 0.0036, -0.0286,  0.0249,  ..., -0.0030, -0.0230,  0.0446],\n",
      "        [ 0.0042, -0.0107, -0.0538,  ..., -0.0559, -0.0561, -0.0237]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0562, -0.0093,  ..., -0.0728,  0.0114,  0.0480],\n",
      "        [ 0.1120, -0.0325, -0.0135,  ..., -0.0578,  0.0067,  0.0705],\n",
      "        [ 0.0629, -0.0350,  0.0110,  ..., -0.0880,  0.0225,  0.0549],\n",
      "        ...,\n",
      "        [-0.0273,  0.0328,  0.0394,  ...,  0.0520, -0.0313, -0.0154],\n",
      "        [ 0.0036, -0.0286,  0.0249,  ..., -0.0030, -0.0230,  0.0445],\n",
      "        [ 0.0041, -0.0107, -0.0538,  ..., -0.0559, -0.0561, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0561, -0.0092,  ..., -0.0728,  0.0115,  0.0480],\n",
      "        [ 0.1120, -0.0324, -0.0135,  ..., -0.0578,  0.0068,  0.0705],\n",
      "        [ 0.0629, -0.0349,  0.0111,  ..., -0.0880,  0.0226,  0.0549],\n",
      "        ...,\n",
      "        [-0.0273,  0.0329,  0.0394,  ...,  0.0521, -0.0312, -0.0154],\n",
      "        [ 0.0036, -0.0285,  0.0249,  ..., -0.0030, -0.0230,  0.0445],\n",
      "        [ 0.0041, -0.0106, -0.0538,  ..., -0.0558, -0.0561, -0.0238]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0660, -0.0560, -0.0092,  ..., -0.0728,  0.0115,  0.0480],\n",
      "        [ 0.1121, -0.0324, -0.0134,  ..., -0.0578,  0.0068,  0.0705],\n",
      "        [ 0.0629, -0.0349,  0.0111,  ..., -0.0880,  0.0226,  0.0549],\n",
      "        ...,\n",
      "        [-0.0274,  0.0329,  0.0394,  ...,  0.0521, -0.0312, -0.0154],\n",
      "        [ 0.0037, -0.0286,  0.0248,  ..., -0.0030, -0.0230,  0.0445],\n",
      "        [ 0.0040, -0.0106, -0.0537,  ..., -0.0558, -0.0561, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0661, -0.0560, -0.0091,  ..., -0.0728,  0.0116,  0.0480],\n",
      "        [ 0.1121, -0.0323, -0.0133,  ..., -0.0578,  0.0069,  0.0705],\n",
      "        [ 0.0629, -0.0348,  0.0112,  ..., -0.0880,  0.0227,  0.0549],\n",
      "        ...,\n",
      "        [-0.0274,  0.0329,  0.0394,  ...,  0.0521, -0.0312, -0.0154],\n",
      "        [ 0.0037, -0.0286,  0.0248,  ..., -0.0030, -0.0230,  0.0445],\n",
      "        [ 0.0039, -0.0105, -0.0537,  ..., -0.0558, -0.0561, -0.0239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0661, -0.0559, -0.0090,  ..., -0.0728,  0.0116,  0.0480],\n",
      "        [ 0.1121, -0.0323, -0.0133,  ..., -0.0578,  0.0069,  0.0705],\n",
      "        [ 0.0629, -0.0348,  0.0112,  ..., -0.0880,  0.0227,  0.0549],\n",
      "        ...,\n",
      "        [-0.0274,  0.0329,  0.0394,  ...,  0.0521, -0.0313, -0.0154],\n",
      "        [ 0.0038, -0.0286,  0.0247,  ..., -0.0030, -0.0230,  0.0445],\n",
      "        [ 0.0039, -0.0105, -0.0537,  ..., -0.0557, -0.0561, -0.0240]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0661, -0.0559, -0.0090,  ..., -0.0728,  0.0117,  0.0480],\n",
      "        [ 0.1122, -0.0322, -0.0132,  ..., -0.0578,  0.0070,  0.0705],\n",
      "        [ 0.0630, -0.0348,  0.0112,  ..., -0.0880,  0.0227,  0.0549],\n",
      "        ...,\n",
      "        [-0.0275,  0.0329,  0.0395,  ...,  0.0522, -0.0313, -0.0154],\n",
      "        [ 0.0039, -0.0286,  0.0246,  ..., -0.0031, -0.0230,  0.0445],\n",
      "        [ 0.0038, -0.0105, -0.0537,  ..., -0.0558, -0.0561, -0.0240]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0661, -0.0558, -0.0090,  ..., -0.0728,  0.0117,  0.0480],\n",
      "        [ 0.1122, -0.0322, -0.0132,  ..., -0.0578,  0.0070,  0.0705],\n",
      "        [ 0.0630, -0.0347,  0.0112,  ..., -0.0880,  0.0228,  0.0549],\n",
      "        ...,\n",
      "        [-0.0275,  0.0329,  0.0395,  ...,  0.0522, -0.0313, -0.0154],\n",
      "        [ 0.0040, -0.0287,  0.0245,  ..., -0.0032, -0.0230,  0.0445],\n",
      "        [ 0.0038, -0.0105, -0.0537,  ..., -0.0558, -0.0561, -0.0240]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0661, -0.0558, -0.0089,  ..., -0.0728,  0.0118,  0.0480],\n",
      "        [ 0.1122, -0.0322, -0.0132,  ..., -0.0578,  0.0071,  0.0705],\n",
      "        [ 0.0630, -0.0347,  0.0112,  ..., -0.0880,  0.0228,  0.0549],\n",
      "        ...,\n",
      "        [-0.0276,  0.0329,  0.0395,  ...,  0.0522, -0.0313, -0.0154],\n",
      "        [ 0.0041, -0.0287,  0.0244,  ..., -0.0033, -0.0231,  0.0446],\n",
      "        [ 0.0038, -0.0105, -0.0537,  ..., -0.0558, -0.0562, -0.0241]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0662, -0.0558, -0.0089,  ..., -0.0728,  0.0118,  0.0480],\n",
      "        [ 0.1123, -0.0322, -0.0131,  ..., -0.0578,  0.0072,  0.0706],\n",
      "        [ 0.0630, -0.0347,  0.0113,  ..., -0.0880,  0.0229,  0.0549],\n",
      "        ...,\n",
      "        [-0.0276,  0.0329,  0.0396,  ...,  0.0522, -0.0313, -0.0154],\n",
      "        [ 0.0041, -0.0288,  0.0244,  ..., -0.0033, -0.0231,  0.0446],\n",
      "        [ 0.0037, -0.0105, -0.0537,  ..., -0.0558, -0.0561, -0.0241]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0662, -0.0558, -0.0089,  ..., -0.0728,  0.0119,  0.0480],\n",
      "        [ 0.1123, -0.0322, -0.0131,  ..., -0.0578,  0.0072,  0.0706],\n",
      "        [ 0.0631, -0.0347,  0.0113,  ..., -0.0880,  0.0230,  0.0549],\n",
      "        ...,\n",
      "        [-0.0277,  0.0329,  0.0396,  ...,  0.0523, -0.0313, -0.0154],\n",
      "        [ 0.0042, -0.0288,  0.0244,  ..., -0.0034, -0.0231,  0.0447],\n",
      "        [ 0.0037, -0.0105, -0.0536,  ..., -0.0558, -0.0561, -0.0242]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0662, -0.0558, -0.0089,  ..., -0.0729,  0.0119,  0.0480],\n",
      "        [ 0.1123, -0.0322, -0.0132,  ..., -0.0578,  0.0072,  0.0706],\n",
      "        [ 0.0631, -0.0347,  0.0113,  ..., -0.0880,  0.0230,  0.0549],\n",
      "        ...,\n",
      "        [-0.0277,  0.0329,  0.0396,  ...,  0.0523, -0.0313, -0.0154],\n",
      "        [ 0.0042, -0.0288,  0.0244,  ..., -0.0034, -0.0231,  0.0447],\n",
      "        [ 0.0037, -0.0105, -0.0536,  ..., -0.0558, -0.0561, -0.0242]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0662, -0.0558, -0.0089,  ..., -0.0729,  0.0119,  0.0479],\n",
      "        [ 0.1123, -0.0322, -0.0132,  ..., -0.0578,  0.0073,  0.0705],\n",
      "        [ 0.0631, -0.0347,  0.0113,  ..., -0.0880,  0.0231,  0.0549],\n",
      "        ...,\n",
      "        [-0.0278,  0.0329,  0.0397,  ...,  0.0523, -0.0313, -0.0154],\n",
      "        [ 0.0043, -0.0288,  0.0244,  ..., -0.0034, -0.0231,  0.0446],\n",
      "        [ 0.0036, -0.0105, -0.0535,  ..., -0.0558, -0.0561, -0.0243]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0662, -0.0558, -0.0089,  ..., -0.0729,  0.0120,  0.0479],\n",
      "        [ 0.1123, -0.0322, -0.0132,  ..., -0.0578,  0.0073,  0.0705],\n",
      "        [ 0.0631, -0.0347,  0.0113,  ..., -0.0880,  0.0231,  0.0549],\n",
      "        ...,\n",
      "        [-0.0278,  0.0329,  0.0397,  ...,  0.0523, -0.0313, -0.0154],\n",
      "        [ 0.0043, -0.0288,  0.0244,  ..., -0.0035, -0.0231,  0.0446],\n",
      "        [ 0.0036, -0.0105, -0.0535,  ..., -0.0558, -0.0562, -0.0243]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0662, -0.0558, -0.0090,  ..., -0.0729,  0.0120,  0.0479],\n",
      "        [ 0.1123, -0.0322, -0.0132,  ..., -0.0578,  0.0073,  0.0705],\n",
      "        [ 0.0631, -0.0346,  0.0112,  ..., -0.0880,  0.0231,  0.0549],\n",
      "        ...,\n",
      "        [-0.0278,  0.0329,  0.0397,  ...,  0.0523, -0.0313, -0.0155],\n",
      "        [ 0.0043, -0.0289,  0.0244,  ..., -0.0035, -0.0231,  0.0446],\n",
      "        [ 0.0036, -0.0105, -0.0534,  ..., -0.0558, -0.0562, -0.0244]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0662, -0.0558, -0.0090,  ..., -0.0729,  0.0120,  0.0479],\n",
      "        [ 0.1123, -0.0321, -0.0132,  ..., -0.0578,  0.0073,  0.0705],\n",
      "        [ 0.0631, -0.0346,  0.0112,  ..., -0.0880,  0.0232,  0.0549],\n",
      "        ...,\n",
      "        [-0.0279,  0.0329,  0.0397,  ...,  0.0523, -0.0314, -0.0155],\n",
      "        [ 0.0044, -0.0289,  0.0244,  ..., -0.0035, -0.0231,  0.0446],\n",
      "        [ 0.0036, -0.0106, -0.0534,  ..., -0.0558, -0.0562, -0.0244]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0662, -0.0558, -0.0090,  ..., -0.0729,  0.0120,  0.0479],\n",
      "        [ 0.1123, -0.0321, -0.0132,  ..., -0.0578,  0.0073,  0.0705],\n",
      "        [ 0.0631, -0.0346,  0.0112,  ..., -0.0880,  0.0232,  0.0549],\n",
      "        ...,\n",
      "        [-0.0279,  0.0330,  0.0397,  ...,  0.0524, -0.0314, -0.0155],\n",
      "        [ 0.0044, -0.0289,  0.0244,  ..., -0.0035, -0.0230,  0.0446],\n",
      "        [ 0.0036, -0.0106, -0.0534,  ..., -0.0559, -0.0562, -0.0245]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0662, -0.0558, -0.0090,  ..., -0.0729,  0.0119,  0.0479],\n",
      "        [ 0.1123, -0.0321, -0.0132,  ..., -0.0578,  0.0073,  0.0705],\n",
      "        [ 0.0631, -0.0346,  0.0112,  ..., -0.0880,  0.0232,  0.0549],\n",
      "        ...,\n",
      "        [-0.0279,  0.0330,  0.0397,  ...,  0.0524, -0.0314, -0.0155],\n",
      "        [ 0.0044, -0.0289,  0.0244,  ..., -0.0036, -0.0230,  0.0446],\n",
      "        [ 0.0036, -0.0106, -0.0534,  ..., -0.0559, -0.0562, -0.0245]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0558, -0.0090,  ..., -0.0729,  0.0119,  0.0479],\n",
      "        [ 0.1124, -0.0322, -0.0133,  ..., -0.0578,  0.0073,  0.0705],\n",
      "        [ 0.0631, -0.0346,  0.0112,  ..., -0.0880,  0.0232,  0.0549],\n",
      "        ...,\n",
      "        [-0.0279,  0.0330,  0.0397,  ...,  0.0524, -0.0314, -0.0155],\n",
      "        [ 0.0044, -0.0288,  0.0244,  ..., -0.0036, -0.0230,  0.0446],\n",
      "        [ 0.0035, -0.0106, -0.0534,  ..., -0.0559, -0.0562, -0.0246]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0559, -0.0091,  ..., -0.0729,  0.0119,  0.0479],\n",
      "        [ 0.1124, -0.0322, -0.0133,  ..., -0.0578,  0.0073,  0.0705],\n",
      "        [ 0.0631, -0.0347,  0.0112,  ..., -0.0880,  0.0232,  0.0549],\n",
      "        ...,\n",
      "        [-0.0280,  0.0330,  0.0397,  ...,  0.0524, -0.0314, -0.0155],\n",
      "        [ 0.0044, -0.0288,  0.0245,  ..., -0.0035, -0.0229,  0.0445],\n",
      "        [ 0.0035, -0.0106, -0.0534,  ..., -0.0559, -0.0562, -0.0246]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0559, -0.0091,  ..., -0.0729,  0.0119,  0.0479],\n",
      "        [ 0.1124, -0.0322, -0.0134,  ..., -0.0577,  0.0072,  0.0705],\n",
      "        [ 0.0631, -0.0347,  0.0111,  ..., -0.0880,  0.0232,  0.0549],\n",
      "        ...,\n",
      "        [-0.0280,  0.0330,  0.0397,  ...,  0.0524, -0.0315, -0.0155],\n",
      "        [ 0.0044, -0.0288,  0.0245,  ..., -0.0035, -0.0229,  0.0445],\n",
      "        [ 0.0034, -0.0106, -0.0534,  ..., -0.0559, -0.0561, -0.0247]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0560, -0.0091,  ..., -0.0729,  0.0119,  0.0479],\n",
      "        [ 0.1123, -0.0323, -0.0134,  ..., -0.0577,  0.0072,  0.0705],\n",
      "        [ 0.0631, -0.0348,  0.0111,  ..., -0.0879,  0.0232,  0.0549],\n",
      "        ...,\n",
      "        [-0.0280,  0.0330,  0.0397,  ...,  0.0524, -0.0315, -0.0155],\n",
      "        [ 0.0044, -0.0287,  0.0245,  ..., -0.0035, -0.0229,  0.0444],\n",
      "        [ 0.0033, -0.0105, -0.0534,  ..., -0.0559, -0.0561, -0.0248]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0560, -0.0092,  ..., -0.0729,  0.0119,  0.0479],\n",
      "        [ 0.1123, -0.0323, -0.0134,  ..., -0.0577,  0.0072,  0.0704],\n",
      "        [ 0.0631, -0.0348,  0.0111,  ..., -0.0880,  0.0232,  0.0549],\n",
      "        ...,\n",
      "        [-0.0280,  0.0330,  0.0397,  ...,  0.0524, -0.0315, -0.0155],\n",
      "        [ 0.0044, -0.0287,  0.0246,  ..., -0.0035, -0.0228,  0.0444],\n",
      "        [ 0.0032, -0.0105, -0.0533,  ..., -0.0559, -0.0561, -0.0248]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0561, -0.0092,  ..., -0.0729,  0.0119,  0.0479],\n",
      "        [ 0.1123, -0.0324, -0.0135,  ..., -0.0577,  0.0072,  0.0704],\n",
      "        [ 0.0631, -0.0349,  0.0111,  ..., -0.0879,  0.0231,  0.0549],\n",
      "        ...,\n",
      "        [-0.0280,  0.0330,  0.0397,  ...,  0.0525, -0.0315, -0.0155],\n",
      "        [ 0.0044, -0.0287,  0.0246,  ..., -0.0035, -0.0228,  0.0444],\n",
      "        [ 0.0032, -0.0104, -0.0533,  ..., -0.0558, -0.0561, -0.0249]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0562, -0.0092,  ..., -0.0729,  0.0118,  0.0478],\n",
      "        [ 0.1123, -0.0324, -0.0135,  ..., -0.0576,  0.0072,  0.0704],\n",
      "        [ 0.0631, -0.0350,  0.0111,  ..., -0.0880,  0.0231,  0.0549],\n",
      "        ...,\n",
      "        [-0.0280,  0.0330,  0.0397,  ...,  0.0525, -0.0315, -0.0155],\n",
      "        [ 0.0043, -0.0287,  0.0247,  ..., -0.0035, -0.0228,  0.0444],\n",
      "        [ 0.0031, -0.0104, -0.0533,  ..., -0.0558, -0.0560, -0.0249]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0562, -0.0092,  ..., -0.0729,  0.0118,  0.0478],\n",
      "        [ 0.1123, -0.0325, -0.0135,  ..., -0.0577,  0.0071,  0.0704],\n",
      "        [ 0.0631, -0.0350,  0.0110,  ..., -0.0880,  0.0231,  0.0549],\n",
      "        ...,\n",
      "        [-0.0280,  0.0331,  0.0397,  ...,  0.0525, -0.0315, -0.0154],\n",
      "        [ 0.0043, -0.0287,  0.0247,  ..., -0.0034, -0.0227,  0.0443],\n",
      "        [ 0.0030, -0.0103, -0.0532,  ..., -0.0557, -0.0560, -0.0250]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0563, -0.0092,  ..., -0.0729,  0.0118,  0.0478],\n",
      "        [ 0.1123, -0.0325, -0.0135,  ..., -0.0577,  0.0071,  0.0704],\n",
      "        [ 0.0631, -0.0351,  0.0111,  ..., -0.0880,  0.0231,  0.0548],\n",
      "        ...,\n",
      "        [-0.0280,  0.0331,  0.0397,  ...,  0.0525, -0.0316, -0.0154],\n",
      "        [ 0.0043, -0.0287,  0.0248,  ..., -0.0034, -0.0227,  0.0443],\n",
      "        [ 0.0029, -0.0102, -0.0532,  ..., -0.0556, -0.0559, -0.0251]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0563, -0.0091,  ..., -0.0730,  0.0118,  0.0478],\n",
      "        [ 0.1124, -0.0325, -0.0135,  ..., -0.0577,  0.0071,  0.0704],\n",
      "        [ 0.0632, -0.0351,  0.0111,  ..., -0.0880,  0.0231,  0.0549],\n",
      "        ...,\n",
      "        [-0.0280,  0.0331,  0.0397,  ...,  0.0525, -0.0316, -0.0154],\n",
      "        [ 0.0043, -0.0287,  0.0248,  ..., -0.0034, -0.0226,  0.0443],\n",
      "        [ 0.0028, -0.0101, -0.0531,  ..., -0.0556, -0.0558, -0.0251]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0564, -0.0091,  ..., -0.0730,  0.0118,  0.0478],\n",
      "        [ 0.1124, -0.0326, -0.0135,  ..., -0.0577,  0.0072,  0.0704],\n",
      "        [ 0.0632, -0.0351,  0.0111,  ..., -0.0880,  0.0231,  0.0549],\n",
      "        ...,\n",
      "        [-0.0279,  0.0331,  0.0397,  ...,  0.0525, -0.0316, -0.0153],\n",
      "        [ 0.0042, -0.0286,  0.0249,  ..., -0.0033, -0.0226,  0.0443],\n",
      "        [ 0.0027, -0.0100, -0.0531,  ..., -0.0555, -0.0558, -0.0252]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663, -0.0564, -0.0091,  ..., -0.0730,  0.0118,  0.0479],\n",
      "        [ 0.1124, -0.0326, -0.0135,  ..., -0.0577,  0.0072,  0.0704],\n",
      "        [ 0.0632, -0.0351,  0.0111,  ..., -0.0880,  0.0231,  0.0549],\n",
      "        ...,\n",
      "        [-0.0279,  0.0331,  0.0397,  ...,  0.0525, -0.0316, -0.0153],\n",
      "        [ 0.0042, -0.0286,  0.0249,  ..., -0.0033, -0.0226,  0.0443],\n",
      "        [ 0.0026, -0.0099, -0.0530,  ..., -0.0555, -0.0557, -0.0252]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0664, -0.0564, -0.0091,  ..., -0.0731,  0.0119,  0.0479],\n",
      "        [ 0.1125, -0.0326, -0.0134,  ..., -0.0578,  0.0072,  0.0705],\n",
      "        [ 0.0632, -0.0351,  0.0111,  ..., -0.0881,  0.0232,  0.0549],\n",
      "        ...,\n",
      "        [-0.0279,  0.0331,  0.0397,  ...,  0.0525, -0.0316, -0.0153],\n",
      "        [ 0.0042, -0.0286,  0.0250,  ..., -0.0033, -0.0225,  0.0443],\n",
      "        [ 0.0025, -0.0098, -0.0529,  ..., -0.0554, -0.0557, -0.0253]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0664, -0.0564, -0.0090,  ..., -0.0731,  0.0119,  0.0479],\n",
      "        [ 0.1125, -0.0326, -0.0134,  ..., -0.0578,  0.0072,  0.0705],\n",
      "        [ 0.0633, -0.0351,  0.0111,  ..., -0.0881,  0.0232,  0.0549],\n",
      "        ...,\n",
      "        [-0.0279,  0.0331,  0.0397,  ...,  0.0525, -0.0316, -0.0153],\n",
      "        [ 0.0042, -0.0286,  0.0250,  ..., -0.0033, -0.0225,  0.0443],\n",
      "        [ 0.0024, -0.0097, -0.0528,  ..., -0.0553, -0.0556, -0.0253]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0665, -0.0564, -0.0090,  ..., -0.0732,  0.0119,  0.0480],\n",
      "        [ 0.1126, -0.0326, -0.0134,  ..., -0.0579,  0.0073,  0.0706],\n",
      "        [ 0.0633, -0.0351,  0.0111,  ..., -0.0881,  0.0232,  0.0550],\n",
      "        ...,\n",
      "        [-0.0279,  0.0332,  0.0397,  ...,  0.0525, -0.0316, -0.0153],\n",
      "        [ 0.0042, -0.0286,  0.0250,  ..., -0.0033, -0.0225,  0.0443],\n",
      "        [ 0.0023, -0.0096, -0.0528,  ..., -0.0553, -0.0556, -0.0254]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0665, -0.0564, -0.0089,  ..., -0.0732,  0.0119,  0.0480],\n",
      "        [ 0.1126, -0.0326, -0.0134,  ..., -0.0579,  0.0073,  0.0706],\n",
      "        [ 0.0634, -0.0351,  0.0111,  ..., -0.0882,  0.0233,  0.0550],\n",
      "        ...,\n",
      "        [-0.0280,  0.0332,  0.0397,  ...,  0.0525, -0.0316, -0.0153],\n",
      "        [ 0.0042, -0.0286,  0.0250,  ..., -0.0032, -0.0225,  0.0444],\n",
      "        [ 0.0022, -0.0095, -0.0527,  ..., -0.0552, -0.0555, -0.0255]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0666, -0.0564, -0.0089,  ..., -0.0733,  0.0120,  0.0481],\n",
      "        [ 0.1127, -0.0326, -0.0134,  ..., -0.0580,  0.0074,  0.0707],\n",
      "        [ 0.0634, -0.0351,  0.0111,  ..., -0.0883,  0.0233,  0.0551],\n",
      "        ...,\n",
      "        [-0.0280,  0.0332,  0.0397,  ...,  0.0526, -0.0316, -0.0153],\n",
      "        [ 0.0042, -0.0286,  0.0251,  ..., -0.0032, -0.0225,  0.0444],\n",
      "        [ 0.0021, -0.0094, -0.0526,  ..., -0.0551, -0.0555, -0.0256]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0666, -0.0564, -0.0088,  ..., -0.0734,  0.0120,  0.0482],\n",
      "        [ 0.1127, -0.0326, -0.0133,  ..., -0.0580,  0.0074,  0.0708],\n",
      "        [ 0.0635, -0.0351,  0.0112,  ..., -0.0883,  0.0234,  0.0552],\n",
      "        ...,\n",
      "        [-0.0280,  0.0332,  0.0396,  ...,  0.0526, -0.0316, -0.0153],\n",
      "        [ 0.0041, -0.0286,  0.0251,  ..., -0.0032, -0.0225,  0.0444],\n",
      "        [ 0.0019, -0.0092, -0.0526,  ..., -0.0551, -0.0554, -0.0256]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0667, -0.0564, -0.0088,  ..., -0.0734,  0.0121,  0.0482],\n",
      "        [ 0.1127, -0.0325, -0.0132,  ..., -0.0581,  0.0075,  0.0708],\n",
      "        [ 0.0635, -0.0351,  0.0112,  ..., -0.0883,  0.0234,  0.0552],\n",
      "        ...,\n",
      "        [-0.0280,  0.0333,  0.0396,  ...,  0.0526, -0.0316, -0.0153],\n",
      "        [ 0.0041, -0.0285,  0.0251,  ..., -0.0031, -0.0225,  0.0445],\n",
      "        [ 0.0018, -0.0091, -0.0525,  ..., -0.0550, -0.0553, -0.0257]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0667, -0.0564, -0.0087,  ..., -0.0734,  0.0121,  0.0482],\n",
      "        [ 0.1127, -0.0325, -0.0131,  ..., -0.0581,  0.0075,  0.0708],\n",
      "        [ 0.0635, -0.0351,  0.0113,  ..., -0.0884,  0.0235,  0.0552],\n",
      "        ...,\n",
      "        [-0.0281,  0.0333,  0.0396,  ...,  0.0527, -0.0315, -0.0154],\n",
      "        [ 0.0041, -0.0285,  0.0251,  ..., -0.0031, -0.0225,  0.0445],\n",
      "        [ 0.0017, -0.0090, -0.0525,  ..., -0.0549, -0.0552, -0.0258]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0667, -0.0564, -0.0086,  ..., -0.0734,  0.0122,  0.0482],\n",
      "        [ 0.1127, -0.0325, -0.0130,  ..., -0.0581,  0.0076,  0.0708],\n",
      "        [ 0.0635, -0.0350,  0.0113,  ..., -0.0884,  0.0236,  0.0552],\n",
      "        ...,\n",
      "        [-0.0281,  0.0334,  0.0396,  ...,  0.0527, -0.0315, -0.0154],\n",
      "        [ 0.0041, -0.0285,  0.0252,  ..., -0.0030, -0.0225,  0.0445],\n",
      "        [ 0.0016, -0.0089, -0.0524,  ..., -0.0549, -0.0552, -0.0260]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0667, -0.0564, -0.0086,  ..., -0.0734,  0.0122,  0.0482],\n",
      "        [ 0.1128, -0.0324, -0.0130,  ..., -0.0581,  0.0077,  0.0708],\n",
      "        [ 0.0635, -0.0350,  0.0114,  ..., -0.0884,  0.0236,  0.0552],\n",
      "        ...,\n",
      "        [-0.0282,  0.0334,  0.0396,  ...,  0.0528, -0.0314, -0.0155],\n",
      "        [ 0.0041, -0.0285,  0.0252,  ..., -0.0030, -0.0225,  0.0445],\n",
      "        [ 0.0015, -0.0089, -0.0524,  ..., -0.0548, -0.0551, -0.0261]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0667, -0.0564, -0.0085,  ..., -0.0735,  0.0123,  0.0483],\n",
      "        [ 0.1128, -0.0324, -0.0129,  ..., -0.0582,  0.0078,  0.0709],\n",
      "        [ 0.0636, -0.0350,  0.0115,  ..., -0.0885,  0.0237,  0.0553],\n",
      "        ...,\n",
      "        [-0.0283,  0.0335,  0.0396,  ...,  0.0528, -0.0313, -0.0156],\n",
      "        [ 0.0041, -0.0285,  0.0252,  ..., -0.0030, -0.0224,  0.0445],\n",
      "        [ 0.0014, -0.0088, -0.0523,  ..., -0.0548, -0.0550, -0.0261]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0667, -0.0564, -0.0084,  ..., -0.0735,  0.0123,  0.0483],\n",
      "        [ 0.1128, -0.0324, -0.0128,  ..., -0.0582,  0.0079,  0.0709],\n",
      "        [ 0.0636, -0.0350,  0.0115,  ..., -0.0885,  0.0238,  0.0553],\n",
      "        ...,\n",
      "        [-0.0284,  0.0336,  0.0396,  ...,  0.0529, -0.0313, -0.0157],\n",
      "        [ 0.0040, -0.0284,  0.0253,  ..., -0.0029, -0.0224,  0.0444],\n",
      "        [ 0.0014, -0.0087, -0.0523,  ..., -0.0547, -0.0550, -0.0262]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0667, -0.0564, -0.0083,  ..., -0.0735,  0.0124,  0.0483],\n",
      "        [ 0.1128, -0.0324, -0.0127,  ..., -0.0583,  0.0080,  0.0709],\n",
      "        [ 0.0636, -0.0350,  0.0116,  ..., -0.0885,  0.0239,  0.0553],\n",
      "        ...,\n",
      "        [-0.0284,  0.0336,  0.0397,  ...,  0.0530, -0.0312, -0.0157],\n",
      "        [ 0.0040, -0.0284,  0.0253,  ..., -0.0028, -0.0223,  0.0444],\n",
      "        [ 0.0013, -0.0087, -0.0523,  ..., -0.0547, -0.0549, -0.0263]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0563, -0.0083,  ..., -0.0735,  0.0125,  0.0483],\n",
      "        [ 0.1129, -0.0323, -0.0126,  ..., -0.0583,  0.0081,  0.0710],\n",
      "        [ 0.0636, -0.0350,  0.0117,  ..., -0.0885,  0.0239,  0.0554],\n",
      "        ...,\n",
      "        [-0.0285,  0.0337,  0.0396,  ...,  0.0530, -0.0312, -0.0158],\n",
      "        [ 0.0040, -0.0284,  0.0254,  ..., -0.0028, -0.0223,  0.0444],\n",
      "        [ 0.0013, -0.0087, -0.0523,  ..., -0.0547, -0.0549, -0.0263]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0563, -0.0082,  ..., -0.0735,  0.0125,  0.0483],\n",
      "        [ 0.1129, -0.0322, -0.0125,  ..., -0.0583,  0.0081,  0.0710],\n",
      "        [ 0.0636, -0.0349,  0.0117,  ..., -0.0886,  0.0240,  0.0554],\n",
      "        ...,\n",
      "        [-0.0285,  0.0337,  0.0396,  ...,  0.0530, -0.0312, -0.0158],\n",
      "        [ 0.0040, -0.0283,  0.0254,  ..., -0.0028, -0.0223,  0.0444],\n",
      "        [ 0.0013, -0.0087, -0.0523,  ..., -0.0547, -0.0550, -0.0263]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0562, -0.0081,  ..., -0.0736,  0.0126,  0.0483],\n",
      "        [ 0.1129, -0.0322, -0.0125,  ..., -0.0583,  0.0082,  0.0710],\n",
      "        [ 0.0636, -0.0348,  0.0118,  ..., -0.0886,  0.0241,  0.0553],\n",
      "        ...,\n",
      "        [-0.0285,  0.0337,  0.0396,  ...,  0.0530, -0.0312, -0.0158],\n",
      "        [ 0.0040, -0.0283,  0.0254,  ..., -0.0027, -0.0222,  0.0444],\n",
      "        [ 0.0013, -0.0087, -0.0524,  ..., -0.0547, -0.0550, -0.0263]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0561, -0.0081,  ..., -0.0735,  0.0127,  0.0484],\n",
      "        [ 0.1129, -0.0321, -0.0124,  ..., -0.0583,  0.0083,  0.0710],\n",
      "        [ 0.0636, -0.0347,  0.0118,  ..., -0.0885,  0.0241,  0.0553],\n",
      "        ...,\n",
      "        [-0.0285,  0.0337,  0.0396,  ...,  0.0530, -0.0312, -0.0158],\n",
      "        [ 0.0039, -0.0282,  0.0254,  ..., -0.0027, -0.0222,  0.0443],\n",
      "        [ 0.0013, -0.0088, -0.0524,  ..., -0.0548, -0.0550, -0.0263]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0561, -0.0080,  ..., -0.0735,  0.0127,  0.0484],\n",
      "        [ 0.1129, -0.0320, -0.0123,  ..., -0.0583,  0.0084,  0.0710],\n",
      "        [ 0.0636, -0.0347,  0.0119,  ..., -0.0885,  0.0241,  0.0553],\n",
      "        ...,\n",
      "        [-0.0285,  0.0337,  0.0396,  ...,  0.0530, -0.0312, -0.0158],\n",
      "        [ 0.0040, -0.0282,  0.0254,  ..., -0.0027, -0.0222,  0.0443],\n",
      "        [ 0.0014, -0.0089, -0.0525,  ..., -0.0548, -0.0551, -0.0263]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0560, -0.0079,  ..., -0.0735,  0.0128,  0.0484],\n",
      "        [ 0.1129, -0.0319, -0.0123,  ..., -0.0583,  0.0084,  0.0710],\n",
      "        [ 0.0636, -0.0346,  0.0119,  ..., -0.0884,  0.0242,  0.0553],\n",
      "        ...,\n",
      "        [-0.0285,  0.0337,  0.0395,  ...,  0.0530, -0.0312, -0.0158],\n",
      "        [ 0.0040, -0.0282,  0.0254,  ..., -0.0027, -0.0222,  0.0444],\n",
      "        [ 0.0015, -0.0090, -0.0527,  ..., -0.0549, -0.0552, -0.0262]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0560, -0.0079,  ..., -0.0735,  0.0128,  0.0484],\n",
      "        [ 0.1129, -0.0318, -0.0122,  ..., -0.0583,  0.0085,  0.0710],\n",
      "        [ 0.0636, -0.0345,  0.0120,  ..., -0.0884,  0.0242,  0.0553],\n",
      "        ...,\n",
      "        [-0.0285,  0.0336,  0.0395,  ...,  0.0530, -0.0313, -0.0158],\n",
      "        [ 0.0040, -0.0282,  0.0253,  ..., -0.0027, -0.0222,  0.0444],\n",
      "        [ 0.0016, -0.0091, -0.0528,  ..., -0.0550, -0.0553, -0.0261]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0559, -0.0079,  ..., -0.0735,  0.0128,  0.0484],\n",
      "        [ 0.1129, -0.0318, -0.0122,  ..., -0.0582,  0.0085,  0.0710],\n",
      "        [ 0.0635, -0.0344,  0.0120,  ..., -0.0883,  0.0242,  0.0553],\n",
      "        ...,\n",
      "        [-0.0285,  0.0336,  0.0394,  ...,  0.0530, -0.0313, -0.0158],\n",
      "        [ 0.0040, -0.0282,  0.0253,  ..., -0.0027, -0.0222,  0.0444],\n",
      "        [ 0.0017, -0.0093, -0.0530,  ..., -0.0552, -0.0555, -0.0260]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0558, -0.0078,  ..., -0.0734,  0.0129,  0.0484],\n",
      "        [ 0.1129, -0.0317, -0.0121,  ..., -0.0582,  0.0086,  0.0710],\n",
      "        [ 0.0635, -0.0344,  0.0121,  ..., -0.0883,  0.0242,  0.0552],\n",
      "        ...,\n",
      "        [-0.0285,  0.0336,  0.0394,  ...,  0.0529, -0.0313, -0.0158],\n",
      "        [ 0.0041, -0.0282,  0.0252,  ..., -0.0027, -0.0223,  0.0445],\n",
      "        [ 0.0019, -0.0094, -0.0532,  ..., -0.0554, -0.0557, -0.0259]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0558, -0.0078,  ..., -0.0734,  0.0129,  0.0483],\n",
      "        [ 0.1129, -0.0316, -0.0121,  ..., -0.0582,  0.0086,  0.0710],\n",
      "        [ 0.0635, -0.0343,  0.0121,  ..., -0.0883,  0.0242,  0.0552],\n",
      "        ...,\n",
      "        [-0.0284,  0.0336,  0.0394,  ...,  0.0529, -0.0313, -0.0158],\n",
      "        [ 0.0041, -0.0282,  0.0252,  ..., -0.0028, -0.0223,  0.0445],\n",
      "        [ 0.0020, -0.0096, -0.0533,  ..., -0.0555, -0.0558, -0.0258]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0558, -0.0078,  ..., -0.0734,  0.0129,  0.0483],\n",
      "        [ 0.1129, -0.0316, -0.0121,  ..., -0.0582,  0.0086,  0.0710],\n",
      "        [ 0.0635, -0.0343,  0.0121,  ..., -0.0882,  0.0242,  0.0552],\n",
      "        ...,\n",
      "        [-0.0284,  0.0336,  0.0393,  ...,  0.0529, -0.0313, -0.0158],\n",
      "        [ 0.0042, -0.0283,  0.0251,  ..., -0.0028, -0.0224,  0.0445],\n",
      "        [ 0.0021, -0.0098, -0.0535,  ..., -0.0557, -0.0560, -0.0258]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0558, -0.0078,  ..., -0.0734,  0.0129,  0.0483],\n",
      "        [ 0.1129, -0.0316, -0.0121,  ..., -0.0582,  0.0086,  0.0710],\n",
      "        [ 0.0635, -0.0342,  0.0121,  ..., -0.0882,  0.0242,  0.0552],\n",
      "        ...,\n",
      "        [-0.0284,  0.0336,  0.0393,  ...,  0.0529, -0.0313, -0.0158],\n",
      "        [ 0.0042, -0.0283,  0.0251,  ..., -0.0029, -0.0225,  0.0446],\n",
      "        [ 0.0022, -0.0099, -0.0536,  ..., -0.0558, -0.0561, -0.0257]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0558, -0.0078,  ..., -0.0734,  0.0129,  0.0484],\n",
      "        [ 0.1129, -0.0316, -0.0121,  ..., -0.0582,  0.0086,  0.0710],\n",
      "        [ 0.0634, -0.0342,  0.0121,  ..., -0.0882,  0.0242,  0.0552],\n",
      "        ...,\n",
      "        [-0.0284,  0.0335,  0.0393,  ...,  0.0529, -0.0313, -0.0157],\n",
      "        [ 0.0043, -0.0284,  0.0251,  ..., -0.0030, -0.0225,  0.0446],\n",
      "        [ 0.0023, -0.0100, -0.0537,  ..., -0.0559, -0.0562, -0.0256]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0668, -0.0557, -0.0078,  ..., -0.0735,  0.0129,  0.0484],\n",
      "        [ 0.1129, -0.0315, -0.0121,  ..., -0.0582,  0.0087,  0.0710],\n",
      "        [ 0.0634, -0.0341,  0.0122,  ..., -0.0882,  0.0243,  0.0552],\n",
      "        ...,\n",
      "        [-0.0284,  0.0335,  0.0393,  ...,  0.0529, -0.0313, -0.0157],\n",
      "        [ 0.0043, -0.0284,  0.0250,  ..., -0.0030, -0.0225,  0.0446],\n",
      "        [ 0.0024, -0.0101, -0.0538,  ..., -0.0560, -0.0563, -0.0256]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "^C\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exiting from training early\n",
      "Traceback (most recent call last):\n",
      "  File \"train_synthetic.py\", line 787, in <module>\n",
      "    with open(os.path.join(args.work_dir, 'model.pt'), 'rb') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../evaluation/test-retrieval/20220113-163700/model.pt'\n"
     ]
    }
   ],
   "source": [
    "!bash run_retrieval.sh train --work_dir ../evaluation/test --lr 0.0001 --tgt_len 10 --eval_tgt_len 10 --mem_len 0 --num_mem_tokens 0 --device_ids 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1b56d8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[0, 2]\n",
      "Experiment dir : ../evaluation/tl10xl0mt0_lr3e-4_rlrp_as2-retrieval/20220112-235057\n",
      "====================================================================================================\n",
      "    - data : /home/admin/x-transformers/data\n",
      "    - dataset : retrieval\n",
      "    - n_layer : 4\n",
      "    - n_head : 4\n",
      "    - d_head : 64\n",
      "    - d_embed : 128\n",
      "    - d_model : 128\n",
      "    - d_inner : 256\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.0003\n",
      "    - mom : 0.0\n",
      "    - scheduler : dev_perf\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 1e-06\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 250000\n",
      "    - batch_size : 32\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 10\n",
      "    - eval_tgt_len : 10\n",
      "    - ext_len : 0\n",
      "    - mem_len : 0\n",
      "    - num_mem_tokens : 0\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : False\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : ../evaluation/tl10xl0mt0_lr3e-4_rlrp_as2-retrieval/20220112-235057\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : 50\n",
      "    - sample_softmax : -1\n",
      "    - patience : 3\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [0, 2]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : True\n",
      "    - read_mem_from_cache : True\n",
      "    - answer_size : 2\n",
      "    - tied : True\n",
      "    - ntokens : 37\n",
      "    - n_all_param : 926373\n",
      "    - n_nonemb_param : 921088\n",
      "====================================================================================================\n",
      "#params = 926373\n",
      "#non emb params = 921088\n",
      "| epoch   1 step      200 |    200 batches | lr 0.0003 | ms/batch 55.83508 | loss 1.62529 | ppl     5.080\n",
      "| epoch   1 step      400 |    400 batches | lr 0.0003 | ms/batch 38.80370 | loss 0.92868 | ppl     2.531\n",
      "| epoch   1 step      600 |    600 batches | lr 0.0003 | ms/batch 38.67261 | loss 0.81118 | ppl     2.251\n",
      "| epoch   1 step      800 |    800 batches | lr 0.0003 | ms/batch 38.06108 | loss 0.78737 | ppl     2.198\n",
      "| epoch   1 step     1000 |   1000 batches | lr 0.0003 | ms/batch 38.89196 | loss 0.76824 | ppl     2.156\n",
      "| epoch   1 step     1200 |   1200 batches | lr 0.0003 | ms/batch 38.89669 | loss 0.76518 | ppl     2.149\n",
      "| epoch   1 step     1400 |   1400 batches | lr 0.0003 | ms/batch 39.36184 | loss 0.75496 | ppl     2.128\n",
      "| epoch   1 step     1600 |   1600 batches | lr 0.0003 | ms/batch 38.40235 | loss 0.75369 | ppl     2.125\n",
      "| epoch   1 step     1800 |   1800 batches | lr 0.0003 | ms/batch 37.59125 | loss 0.75148 | ppl     2.120\n",
      "| epoch   1 step     2000 |   2000 batches | lr 0.0003 | ms/batch 38.68115 | loss 0.74765 | ppl     2.112\n",
      "| epoch   1 step     2200 |   2200 batches | lr 0.0003 | ms/batch 38.92409 | loss 0.74908 | ppl     2.115\n",
      "| epoch   1 step     2400 |   2400 batches | lr 0.0003 | ms/batch 39.32744 | loss 0.74158 | ppl     2.099\n",
      "| epoch   1 step     2600 |   2600 batches | lr 0.0003 | ms/batch 38.51231 | loss 0.74133 | ppl     2.099\n",
      "| epoch   1 step     2800 |   2800 batches | lr 0.0003 | ms/batch 38.52109 | loss 0.73738 | ppl     2.090\n",
      "| epoch   1 step     3000 |   3000 batches | lr 0.0003 | ms/batch 38.73821 | loss 0.73849 | ppl     2.093\n",
      "| epoch   1 step     3200 |   3200 batches | lr 0.0003 | ms/batch 39.62508 | loss 0.73653 | ppl     2.089\n",
      "| epoch   1 step     3400 |   3400 batches | lr 0.0003 | ms/batch 39.22730 | loss 0.73718 | ppl     2.090\n",
      "| epoch   1 step     3600 |   3600 batches | lr 0.0003 | ms/batch 38.48584 | loss 0.73150 | ppl     2.078\n",
      "| epoch   1 step     3800 |   3800 batches | lr 0.0003 | ms/batch 39.46649 | loss 0.73511 | ppl     2.086\n",
      "| epoch   1 step     4000 |   4000 batches | lr 0.0003 | ms/batch 38.59807 | loss 0.73194 | ppl     2.079\n",
      "|\n",
      "Source: [36  3 31  2 24  5 28  7 24 10]\n",
      "Target: [ 3 31  2 24  5 28  7 24 10  5]\n",
      "Teacher forcing: acc:0.6321875\n",
      "Preds:  [10  3 10  2 10  5 10  7 10  5]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step     4000 | time: 159.64s | valid loss 0.71772 | valid ppl    2.0498 | valid acc 0.632\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     4200 |   4200 batches | lr 0.0003 | ms/batch 44.16830 | loss 0.73226 | ppl     2.080\n",
      "| epoch   1 step     4400 |   4400 batches | lr 0.0003 | ms/batch 39.14157 | loss 0.72963 | ppl     2.074\n",
      "| epoch   1 step     4600 |   4600 batches | lr 0.0003 | ms/batch 40.64809 | loss 0.72844 | ppl     2.072\n",
      "| epoch   1 step     4800 |   4800 batches | lr 0.0003 | ms/batch 39.61196 | loss 0.73111 | ppl     2.077\n",
      "| epoch   1 step     5000 |   5000 batches | lr 0.0003 | ms/batch 40.99557 | loss 0.72809 | ppl     2.071\n",
      "| epoch   1 step     5200 |   5200 batches | lr 0.0003 | ms/batch 40.44276 | loss 0.72950 | ppl     2.074\n",
      "| epoch   1 step     5400 |   5400 batches | lr 0.0003 | ms/batch 39.14726 | loss 0.72766 | ppl     2.070\n",
      "| epoch   1 step     5600 |   5600 batches | lr 0.0003 | ms/batch 38.24384 | loss 0.72572 | ppl     2.066\n",
      "| epoch   1 step     5800 |   5800 batches | lr 0.0003 | ms/batch 39.31687 | loss 0.72578 | ppl     2.066\n",
      "| epoch   1 step     6000 |   6000 batches | lr 0.0003 | ms/batch 38.87245 | loss 0.72966 | ppl     2.074\n",
      "| epoch   1 step     6200 |   6200 batches | lr 0.0003 | ms/batch 38.80545 | loss 0.72587 | ppl     2.067\n",
      "| epoch   1 step     6400 |   6400 batches | lr 0.0003 | ms/batch 39.57503 | loss 0.72537 | ppl     2.065\n",
      "| epoch   1 step     6600 |   6600 batches | lr 0.0003 | ms/batch 39.55351 | loss 0.72404 | ppl     2.063\n",
      "| epoch   1 step     6800 |   6800 batches | lr 0.0003 | ms/batch 40.06822 | loss 0.72464 | ppl     2.064\n",
      "| epoch   1 step     7000 |   7000 batches | lr 0.0003 | ms/batch 39.42320 | loss 0.72822 | ppl     2.071\n",
      "| epoch   1 step     7200 |   7200 batches | lr 0.0003 | ms/batch 39.55436 | loss 0.72353 | ppl     2.062\n",
      "| epoch   1 step     7400 |   7400 batches | lr 0.0003 | ms/batch 39.08626 | loss 0.72378 | ppl     2.062\n",
      "| epoch   1 step     7600 |   7600 batches | lr 0.0003 | ms/batch 38.77110 | loss 0.72507 | ppl     2.065\n",
      "| epoch   1 step     7800 |   7800 batches | lr 0.0003 | ms/batch 39.94524 | loss 0.72416 | ppl     2.063\n",
      "| epoch   1 step     8000 |   8000 batches | lr 0.0003 | ms/batch 39.64004 | loss 0.72268 | ppl     2.060\n",
      "|\n",
      "Source: [26  9 21  1 25  4 15  3 26 10]\n",
      "Target: [ 9 21  1 25  4 15  3 26 10  9]\n",
      "Teacher forcing: acc:0.6334375\n",
      "Preds:  [10  9 10  9 10  4 10  3 10  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step     8000 | time: 158.99s | valid loss 0.70619 | valid ppl    2.0262 | valid acc 0.633\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     8200 |   8200 batches | lr 0.0003 | ms/batch 44.34876 | loss 0.72063 | ppl     2.056\n",
      "| epoch   1 step     8400 |   8400 batches | lr 0.0003 | ms/batch 39.38612 | loss 0.72503 | ppl     2.065\n",
      "| epoch   1 step     8600 |   8600 batches | lr 0.0003 | ms/batch 38.30704 | loss 0.72377 | ppl     2.062\n",
      "| epoch   1 step     8800 |   8800 batches | lr 0.0003 | ms/batch 40.69382 | loss 0.72060 | ppl     2.056\n",
      "| epoch   1 step     9000 |   9000 batches | lr 0.0003 | ms/batch 39.38017 | loss 0.72429 | ppl     2.063\n",
      "| epoch   1 step     9200 |   9200 batches | lr 0.0003 | ms/batch 40.31604 | loss 0.72724 | ppl     2.069\n",
      "| epoch   1 step     9400 |   9400 batches | lr 0.0003 | ms/batch 39.81326 | loss 0.72365 | ppl     2.062\n",
      "| epoch   1 step     9600 |   9600 batches | lr 0.0003 | ms/batch 38.02891 | loss 0.72140 | ppl     2.057\n",
      "| epoch   1 step     9800 |   9800 batches | lr 0.0003 | ms/batch 38.56333 | loss 0.72166 | ppl     2.058\n",
      "| epoch   1 step    10000 |  10000 batches | lr 0.0003 | ms/batch 38.79141 | loss 0.72237 | ppl     2.059\n",
      "| epoch   1 step    10200 |  10200 batches | lr 0.0003 | ms/batch 39.99344 | loss 0.72328 | ppl     2.061\n",
      "| epoch   1 step    10400 |  10400 batches | lr 0.0003 | ms/batch 38.98978 | loss 0.72809 | ppl     2.071\n",
      "| epoch   1 step    10600 |  10600 batches | lr 0.0003 | ms/batch 38.74152 | loss 0.72160 | ppl     2.058\n",
      "| epoch   1 step    10800 |  10800 batches | lr 0.0003 | ms/batch 39.70370 | loss 0.72123 | ppl     2.057\n",
      "| epoch   1 step    11000 |  11000 batches | lr 0.0003 | ms/batch 40.52795 | loss 0.72192 | ppl     2.058\n",
      "| epoch   1 step    11200 |  11200 batches | lr 0.0003 | ms/batch 38.71551 | loss 0.72125 | ppl     2.057\n",
      "| epoch   1 step    11400 |  11400 batches | lr 0.0003 | ms/batch 39.84112 | loss 0.72103 | ppl     2.057\n",
      "| epoch   1 step    11600 |  11600 batches | lr 0.0003 | ms/batch 39.08601 | loss 0.71993 | ppl     2.054\n",
      "| epoch   1 step    11800 |  11800 batches | lr 0.0003 | ms/batch 39.09304 | loss 0.72187 | ppl     2.058\n",
      "| epoch   1 step    12000 |  12000 batches | lr 0.0003 | ms/batch 39.91747 | loss 0.72158 | ppl     2.058\n",
      "|\n",
      "Source: [33  4 27  1 24  3 18  6 33 10]\n",
      "Target: [ 4 27  1 24  3 18  6 33 10  4]\n",
      "Teacher forcing: acc:0.635625\n",
      "Preds:  [10  4 10  1 10  1 10  6 10  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step    12000 | time: 158.39s | valid loss 0.70874 | valid ppl    2.0314 | valid acc 0.636\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    12200 |  12200 batches | lr 0.0003 | ms/batch 45.00131 | loss 0.72210 | ppl     2.059\n",
      "| epoch   1 step    12400 |  12400 batches | lr 0.0003 | ms/batch 39.78586 | loss 0.72088 | ppl     2.056\n",
      "| epoch   1 step    12600 |  12600 batches | lr 0.0003 | ms/batch 40.09907 | loss 0.72144 | ppl     2.057\n",
      "| epoch   1 step    12800 |  12800 batches | lr 0.0003 | ms/batch 40.09878 | loss 0.71744 | ppl     2.049\n",
      "| epoch   1 step    13000 |  13000 batches | lr 0.0003 | ms/batch 39.43865 | loss 0.71976 | ppl     2.054\n",
      "| epoch   1 step    13200 |  13200 batches | lr 0.0003 | ms/batch 39.92137 | loss 0.71975 | ppl     2.054\n",
      "| epoch   1 step    13400 |  13400 batches | lr 0.0003 | ms/batch 39.69638 | loss 0.72047 | ppl     2.055\n",
      "| epoch   1 step    13600 |  13600 batches | lr 0.0003 | ms/batch 40.23723 | loss 0.72605 | ppl     2.067\n",
      "| epoch   1 step    13800 |  13800 batches | lr 0.0003 | ms/batch 40.71126 | loss 0.71895 | ppl     2.052\n",
      "| epoch   1 step    14000 |  14000 batches | lr 0.0003 | ms/batch 40.23236 | loss 0.72055 | ppl     2.056\n",
      "| epoch   1 step    14200 |  14200 batches | lr 0.0003 | ms/batch 40.66683 | loss 0.72379 | ppl     2.062\n",
      "| epoch   1 step    14400 |  14400 batches | lr 0.0003 | ms/batch 40.17899 | loss 0.72337 | ppl     2.061\n",
      "| epoch   1 step    14600 |  14600 batches | lr 0.0003 | ms/batch 40.16977 | loss 0.72119 | ppl     2.057\n",
      "| epoch   1 step    14800 |  14800 batches | lr 0.0003 | ms/batch 40.71478 | loss 0.71685 | ppl     2.048\n",
      "| epoch   1 step    15000 |  15000 batches | lr 0.0003 | ms/batch 40.17347 | loss 0.71830 | ppl     2.051\n",
      "| epoch   1 step    15200 |  15200 batches | lr 0.0003 | ms/batch 40.16635 | loss 0.71818 | ppl     2.051\n",
      "| epoch   1 step    15400 |  15400 batches | lr 0.0003 | ms/batch 39.25238 | loss 0.71807 | ppl     2.050\n",
      "| epoch   1 step    15600 |  15600 batches | lr 0.0003 | ms/batch 39.21741 | loss 0.71763 | ppl     2.050\n",
      "| epoch   1 step    15800 |  15800 batches | lr 0.0003 | ms/batch 38.64384 | loss 0.71936 | ppl     2.053\n",
      "| epoch   1 step    16000 |  16000 batches | lr 0.0003 | ms/batch 39.76389 | loss 0.71899 | ppl     2.052\n",
      "|\n",
      "Source: [15  4 18  1 20  8 14  0 20 10]\n",
      "Target: [ 4 18  1 20  8 14  0 20 10  8]\n",
      "Teacher forcing: acc:0.6234375\n",
      "Preds:  [10  4 10  1 10  1 10  0 10  4]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   4 at step    16000 | time: 160.86s | valid loss 0.70817 | valid ppl    2.0303 | valid acc 0.623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    16200 |  16200 batches | lr 0.0003 | ms/batch 45.67241 | loss 0.71593 | ppl     2.046\n",
      "| epoch   1 step    16400 |  16400 batches | lr 0.0003 | ms/batch 39.85569 | loss 0.71725 | ppl     2.049\n",
      "| epoch   1 step    16600 |  16600 batches | lr 0.0003 | ms/batch 39.30039 | loss 0.71478 | ppl     2.044\n",
      "| epoch   1 step    16800 |  16800 batches | lr 0.0003 | ms/batch 39.15754 | loss 0.71811 | ppl     2.051\n",
      "| epoch   1 step    17000 |  17000 batches | lr 0.0003 | ms/batch 39.00635 | loss 0.71847 | ppl     2.051\n",
      "| epoch   1 step    17200 |  17200 batches | lr 0.0003 | ms/batch 40.09106 | loss 0.71645 | ppl     2.047\n",
      "| epoch   1 step    17400 |  17400 batches | lr 0.0003 | ms/batch 39.91080 | loss 0.71591 | ppl     2.046\n",
      "| epoch   1 step    17600 |  17600 batches | lr 0.0003 | ms/batch 39.82645 | loss 0.71549 | ppl     2.045\n",
      "| epoch   1 step    17800 |  17800 batches | lr 0.0003 | ms/batch 39.48836 | loss 0.71574 | ppl     2.046\n",
      "| epoch   1 step    18000 |  18000 batches | lr 0.0003 | ms/batch 39.83137 | loss 0.71812 | ppl     2.051\n",
      "| epoch   1 step    18200 |  18200 batches | lr 0.0003 | ms/batch 40.36928 | loss 0.71885 | ppl     2.052\n",
      "| epoch   1 step    18400 |  18400 batches | lr 0.0003 | ms/batch 39.65830 | loss 0.71617 | ppl     2.047\n",
      "| epoch   1 step    18600 |  18600 batches | lr 0.0003 | ms/batch 39.91307 | loss 0.71598 | ppl     2.046\n",
      "| epoch   1 step    18800 |  18800 batches | lr 0.0003 | ms/batch 40.03644 | loss 0.71748 | ppl     2.049\n",
      "| epoch   1 step    19000 |  19000 batches | lr 0.0003 | ms/batch 40.36503 | loss 0.71505 | ppl     2.044\n",
      "| epoch   1 step    19200 |  19200 batches | lr 0.0003 | ms/batch 40.17547 | loss 0.71382 | ppl     2.042\n",
      "| epoch   1 step    19400 |  19400 batches | lr 0.0003 | ms/batch 40.70471 | loss 0.71863 | ppl     2.052\n",
      "| epoch   1 step    19600 |  19600 batches | lr 0.0003 | ms/batch 40.72911 | loss 0.71705 | ppl     2.048\n",
      "| epoch   1 step    19800 |  19800 batches | lr 0.0003 | ms/batch 39.50183 | loss 0.71619 | ppl     2.047\n",
      "| epoch   1 step    20000 |  20000 batches | lr 0.0003 | ms/batch 39.18122 | loss 0.71621 | ppl     2.047\n",
      "|\n",
      "Source: [33  8 27  9 19  7 18  4 33 10]\n",
      "Target: [ 8 27  9 19  7 18  4 33 10  8]\n",
      "Teacher forcing: acc:0.6346875\n",
      "Preds:  [10  8 10  8 10  9 10  4 10  4]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   5 at step    20000 | time: 160.59s | valid loss 0.70025 | valid ppl    2.0143 | valid acc 0.635\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    20200 |  20200 batches | lr 0.0003 | ms/batch 45.92879 | loss 0.71431 | ppl     2.043\n",
      "| epoch   1 step    20400 |  20400 batches | lr 0.0003 | ms/batch 39.61779 | loss 0.71963 | ppl     2.054\n",
      "| epoch   1 step    20600 |  20600 batches | lr 0.0003 | ms/batch 39.09593 | loss 0.72193 | ppl     2.058\n",
      "| epoch   1 step    20800 |  20800 batches | lr 0.0003 | ms/batch 39.10480 | loss 0.71637 | ppl     2.047\n",
      "| epoch   1 step    21000 |  21000 batches | lr 0.0003 | ms/batch 39.24877 | loss 0.71568 | ppl     2.046\n",
      "| epoch   1 step    21200 |  21200 batches | lr 0.0003 | ms/batch 39.42591 | loss 0.71474 | ppl     2.044\n",
      "| epoch   1 step    21400 |  21400 batches | lr 0.0003 | ms/batch 39.56740 | loss 0.71445 | ppl     2.043\n",
      "| epoch   1 step    21600 |  21600 batches | lr 0.0003 | ms/batch 39.75213 | loss 0.71576 | ppl     2.046\n",
      "| epoch   1 step    21800 |  21800 batches | lr 0.0003 | ms/batch 39.74614 | loss 0.71656 | ppl     2.047\n",
      "| epoch   1 step    22000 |  22000 batches | lr 0.0003 | ms/batch 40.17555 | loss 0.71443 | ppl     2.043\n",
      "| epoch   1 step    22200 |  22200 batches | lr 0.0003 | ms/batch 39.98282 | loss 0.71361 | ppl     2.041\n",
      "| epoch   1 step    22400 |  22400 batches | lr 0.0003 | ms/batch 39.41036 | loss 0.71584 | ppl     2.046\n",
      "| epoch   1 step    22600 |  22600 batches | lr 0.0003 | ms/batch 39.36331 | loss 0.71481 | ppl     2.044\n",
      "| epoch   1 step    22800 |  22800 batches | lr 0.0003 | ms/batch 39.64530 | loss 0.71468 | ppl     2.044\n",
      "| epoch   1 step    23000 |  23000 batches | lr 0.0003 | ms/batch 40.79204 | loss 0.71391 | ppl     2.042\n",
      "| epoch   1 step    23200 |  23200 batches | lr 0.0003 | ms/batch 40.17725 | loss 0.71372 | ppl     2.042\n",
      "| epoch   1 step    23400 |  23400 batches | lr 0.0003 | ms/batch 40.62181 | loss 0.71431 | ppl     2.043\n",
      "| epoch   1 step    23600 |  23600 batches | lr 0.0003 | ms/batch 40.28181 | loss 0.71750 | ppl     2.049\n",
      "| epoch   1 step    23800 |  23800 batches | lr 0.0003 | ms/batch 40.22149 | loss 0.71558 | ppl     2.045\n",
      "| epoch   1 step    24000 |  24000 batches | lr 0.0003 | ms/batch 39.98828 | loss 0.71315 | ppl     2.040\n",
      "|\n",
      "Source: [24  7 33  3 12  5 26  0 12 10]\n",
      "Target: [ 7 33  3 12  5 26  0 12 10  5]\n",
      "Teacher forcing: acc:0.629375\n",
      "Preds:  [10  7 10  7 10  5 10  0 10  5]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   6 at step    24000 | time: 160.34s | valid loss 0.70189 | valid ppl    2.0176 | valid acc 0.629\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    24200 |  24200 batches | lr 0.0003 | ms/batch 46.36732 | loss 0.71362 | ppl     2.041\n",
      "| epoch   1 step    24400 |  24400 batches | lr 0.0003 | ms/batch 41.31971 | loss 0.71269 | ppl     2.039\n",
      "| epoch   1 step    24600 |  24600 batches | lr 0.0003 | ms/batch 39.09488 | loss 0.71326 | ppl     2.041\n",
      "| epoch   1 step    24800 |  24800 batches | lr 0.0003 | ms/batch 39.45959 | loss 0.71441 | ppl     2.043\n",
      "| epoch   1 step    31000 |  31000 batches | lr 0.0003 | ms/batch 41.55616 | loss 0.71317 | ppl     2.040\n",
      "| epoch   1 step    31200 |  31200 batches | lr 0.0003 | ms/batch 41.46528 | loss 0.71169 | ppl     2.037\n",
      "| epoch   1 step    31400 |  31400 batches | lr 0.0003 | ms/batch 41.68134 | loss 0.70942 | ppl     2.033\n",
      "| epoch   1 step    31600 |  31600 batches | lr 0.0003 | ms/batch 41.91075 | loss 0.71078 | ppl     2.036\n",
      "| epoch   1 step    31800 |  31800 batches | lr 0.0003 | ms/batch 41.53657 | loss 0.71614 | ppl     2.047\n",
      "| epoch   1 step    32000 |  32000 batches | lr 0.0003 | ms/batch 41.19007 | loss 0.71127 | ppl     2.037\n",
      "|\n",
      "Source: [32  9 13  7 20  4 30  3 32 10]\n",
      "Target: [ 9 13  7 20  4 30  3 32 10  9]\n",
      "Teacher forcing: acc:0.63625\n",
      "Preds:  [10  9 10  9 10  9 10  3 10  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   8 at step    32000 | time: 167.20s | valid loss 0.69979 | valid ppl    2.0133 | valid acc 0.636\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    32200 |  32200 batches | lr 0.0003 | ms/batch 46.86737 | loss 0.71454 | ppl     2.043\n",
      "| epoch   1 step    32400 |  32400 batches | lr 0.0003 | ms/batch 40.62668 | loss 0.71249 | ppl     2.039\n",
      "| epoch   1 step    32600 |  32600 batches | lr 0.0003 | ms/batch 39.80806 | loss 0.71173 | ppl     2.038\n",
      "| epoch   1 step    32800 |  32800 batches | lr 0.0003 | ms/batch 40.47695 | loss 0.71334 | ppl     2.041\n",
      "| epoch   1 step    33000 |  33000 batches | lr 0.0003 | ms/batch 40.47250 | loss 0.71137 | ppl     2.037\n",
      "| epoch   1 step    33200 |  33200 batches | lr 0.0003 | ms/batch 40.69743 | loss 0.71137 | ppl     2.037\n",
      "| epoch   1 step    33400 |  33400 batches | lr 0.0003 | ms/batch 40.14671 | loss 0.71170 | ppl     2.037\n",
      "| epoch   1 step    33600 |  33600 batches | lr 0.0003 | ms/batch 41.18402 | loss 0.71127 | ppl     2.037\n",
      "| epoch   1 step    33800 |  33800 batches | lr 0.0003 | ms/batch 41.26719 | loss 0.70963 | ppl     2.033\n",
      "| epoch   1 step    34000 |  34000 batches | lr 0.0003 | ms/batch 41.74125 | loss 0.70828 | ppl     2.030\n",
      "| epoch   1 step    34200 |  34200 batches | lr 0.0003 | ms/batch 40.75698 | loss 0.71053 | ppl     2.035\n",
      "| epoch   1 step    34400 |  34400 batches | lr 0.0003 | ms/batch 40.56774 | loss 0.71139 | ppl     2.037\n",
      "| epoch   1 step    34600 |  34600 batches | lr 0.0003 | ms/batch 39.76862 | loss 0.70855 | ppl     2.031\n",
      "| epoch   1 step    34800 |  34800 batches | lr 0.0003 | ms/batch 40.78260 | loss 0.71005 | ppl     2.034\n",
      "| epoch   1 step    35000 |  35000 batches | lr 0.0003 | ms/batch 40.72772 | loss 0.71029 | ppl     2.035\n",
      "| epoch   1 step    35200 |  35200 batches | lr 0.0003 | ms/batch 40.61520 | loss 0.70898 | ppl     2.032\n",
      "| epoch   1 step    35400 |  35400 batches | lr 0.0003 | ms/batch 40.10977 | loss 0.71010 | ppl     2.034\n",
      "| epoch   1 step    35600 |  35600 batches | lr 0.0003 | ms/batch 41.11571 | loss 0.71196 | ppl     2.038\n",
      "| epoch   1 step    35800 |  35800 batches | lr 0.0003 | ms/batch 40.61607 | loss 0.70998 | ppl     2.034\n",
      "| epoch   1 step    36000 |  36000 batches | lr 0.0003 | ms/batch 40.33380 | loss 0.70955 | ppl     2.033\n",
      "|\n",
      "Source: [13  6 23  9 14  8 15  5 15 10]\n",
      "Target: [ 6 23  9 14  8 15  5 15 10  5]\n",
      "Teacher forcing: acc:0.62625\n",
      "Preds:  [10  6 10  9 10  8 10  5 10  5]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   9 at step    36000 | time: 163.67s | valid loss 0.70076 | valid ppl    2.0153 | valid acc 0.626\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    36200 |  36200 batches | lr 0.0003 | ms/batch 46.00975 | loss 0.71057 | ppl     2.035\n",
      "| epoch   1 step    36400 |  36400 batches | lr 0.0003 | ms/batch 40.34484 | loss 0.70916 | ppl     2.032\n",
      "| epoch   1 step    36600 |  36600 batches | lr 0.0003 | ms/batch 40.71856 | loss 0.71043 | ppl     2.035\n",
      "| epoch   1 step    36800 |  36800 batches | lr 0.0003 | ms/batch 40.52578 | loss 0.71057 | ppl     2.035\n",
      "| epoch   1 step    37000 |  37000 batches | lr 0.0003 | ms/batch 40.61491 | loss 0.70669 | ppl     2.027\n",
      "| epoch   1 step    37200 |  37200 batches | lr 0.0003 | ms/batch 40.24040 | loss 0.71044 | ppl     2.035\n",
      "| epoch   1 step    37400 |  37400 batches | lr 0.0003 | ms/batch 40.41405 | loss 0.71225 | ppl     2.039\n",
      "| epoch   1 step    37600 |  37600 batches | lr 0.0003 | ms/batch 41.45972 | loss 0.70983 | ppl     2.034\n",
      "| epoch   1 step    37800 |  37800 batches | lr 0.0003 | ms/batch 40.70544 | loss 0.70805 | ppl     2.030\n",
      "| epoch   1 step    38000 |  38000 batches | lr 0.0003 | ms/batch 40.33976 | loss 0.71213 | ppl     2.038\n",
      "| epoch   1 step    38200 |  38200 batches | lr 0.0003 | ms/batch 40.74000 | loss 0.70958 | ppl     2.033\n",
      "| epoch   1 step    38400 |  38400 batches | lr 0.0003 | ms/batch 40.78238 | loss 0.71160 | ppl     2.037\n",
      "| epoch   1 step    38600 |  38600 batches | lr 0.0003 | ms/batch 40.71067 | loss 0.70949 | ppl     2.033\n",
      "| epoch   1 step    38800 |  38800 batches | lr 0.0003 | ms/batch 41.60723 | loss 0.71083 | ppl     2.036\n",
      "| epoch   1 step    39000 |  39000 batches | lr 0.0003 | ms/batch 39.55842 | loss 0.71099 | ppl     2.036\n",
      "| epoch   1 step    39200 |  39200 batches | lr 0.0003 | ms/batch 40.01639 | loss 0.70818 | ppl     2.030\n",
      "| epoch   1 step    39400 |  39400 batches | lr 0.0003 | ms/batch 40.01181 | loss 0.70881 | ppl     2.032\n",
      "| epoch   1 step    39600 |  39600 batches | lr 0.0003 | ms/batch 40.25148 | loss 0.71046 | ppl     2.035\n",
      "| epoch   1 step    39800 |  39800 batches | lr 0.0003 | ms/batch 40.20196 | loss 0.71152 | ppl     2.037\n",
      "| epoch   1 step    40000 |  40000 batches | lr 0.0003 | ms/batch 40.17492 | loss 0.70813 | ppl     2.030\n",
      "|\n",
      "Source: [32  5 36  3 24  4 33  8 32 10]\n",
      "Target: [ 5 36  3 24  4 33  8 32 10  5]\n",
      "Teacher forcing: acc:0.623125\n",
      "Preds:  [10  5 10  3 10  4 10 10 10  8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  10 at step    40000 | time: 163.07s | valid loss 0.70321 | valid ppl    2.0202 | valid acc 0.623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    40200 |  40200 batches | lr 0.0003 | ms/batch 46.39325 | loss 0.70894 | ppl     2.032\n",
      "| epoch   1 step    40400 |  40400 batches | lr 0.0003 | ms/batch 39.84888 | loss 0.71008 | ppl     2.034\n",
      "| epoch   1 step    40600 |  40600 batches | lr 0.0003 | ms/batch 40.34566 | loss 0.70829 | ppl     2.031\n",
      "| epoch   1 step    40800 |  40800 batches | lr 0.0003 | ms/batch 39.69411 | loss 0.70800 | ppl     2.030\n",
      "| epoch   1 step    41000 |  41000 batches | lr 0.0003 | ms/batch 40.12168 | loss 0.70930 | ppl     2.033\n",
      "| epoch   1 step    41200 |  41200 batches | lr 0.0003 | ms/batch 40.25601 | loss 0.70593 | ppl     2.026\n",
      "| epoch   1 step    41400 |  41400 batches | lr 0.0003 | ms/batch 39.29961 | loss 0.70932 | ppl     2.033\n",
      "| epoch   1 step    41600 |  41600 batches | lr 0.0003 | ms/batch 41.07440 | loss 0.70811 | ppl     2.030\n",
      "| epoch   1 step    41800 |  41800 batches | lr 0.0003 | ms/batch 40.72342 | loss 0.71077 | ppl     2.036\n",
      "| epoch   1 step    42000 |  42000 batches | lr 0.0003 | ms/batch 40.08219 | loss 0.70667 | ppl     2.027\n",
      "| epoch   1 step    42200 |  42200 batches | lr 0.0003 | ms/batch 40.76510 | loss 0.70813 | ppl     2.030\n",
      "| epoch   1 step    42400 |  42400 batches | lr 0.0003 | ms/batch 39.58845 | loss 0.70752 | ppl     2.029\n",
      "| epoch   1 step    42600 |  42600 batches | lr 0.0003 | ms/batch 39.92086 | loss 0.70787 | ppl     2.030\n",
      "| epoch   1 step    42800 |  42800 batches | lr 0.0003 | ms/batch 40.14368 | loss 0.71068 | ppl     2.035\n",
      "| epoch   1 step    43000 |  43000 batches | lr 0.0003 | ms/batch 40.94635 | loss 0.77180 | ppl     2.164\n",
      "| epoch   1 step    43200 |  43200 batches | lr 0.0003 | ms/batch 40.56410 | loss 0.74523 | ppl     2.107\n",
      "| epoch   1 step    43400 |  43400 batches | lr 0.0003 | ms/batch 40.91187 | loss 0.71164 | ppl     2.037\n",
      "| epoch   1 step    43600 |  43600 batches | lr 0.0003 | ms/batch 40.73925 | loss 0.70787 | ppl     2.030\n",
      "| epoch   1 step    43800 |  43800 batches | lr 0.0003 | ms/batch 39.97038 | loss 0.70802 | ppl     2.030\n",
      "| epoch   1 step    44000 |  44000 batches | lr 0.0003 | ms/batch 40.30897 | loss 0.70874 | ppl     2.031\n",
      "|\n",
      "Source: [20  4 27  3 28  7 11  2 28 10]\n",
      "Target: [ 4 27  3 28  7 11  2 28 10  7]\n",
      "Teacher forcing: acc:0.6203125\n",
      "Preds:  [10  4 10  5 10  3 10  0 10  4]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  11 at step    44000 | time: 162.32s | valid loss 0.70099 | valid ppl    2.0157 | valid acc 0.62\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    44200 |  44200 batches | lr 0.0003 | ms/batch 46.39268 | loss 0.70866 | ppl     2.031\n",
      "| epoch   1 step    44400 |  44400 batches | lr 0.0003 | ms/batch 39.84149 | loss 0.70787 | ppl     2.030\n",
      "| epoch   1 step    44600 |  44600 batches | lr 0.0003 | ms/batch 40.31422 | loss 0.70933 | ppl     2.033\n",
      "| epoch   1 step    44800 |  44800 batches | lr 0.0003 | ms/batch 40.67153 | loss 0.70892 | ppl     2.032\n",
      "| epoch   1 step    45000 |  45000 batches | lr 0.0003 | ms/batch 39.97346 | loss 0.70752 | ppl     2.029\n",
      "| epoch   1 step    45200 |  45200 batches | lr 0.0003 | ms/batch 38.99442 | loss 0.70673 | ppl     2.027\n",
      "| epoch   1 step    45400 |  45400 batches | lr 0.0003 | ms/batch 39.81899 | loss 0.70809 | ppl     2.030\n",
      "| epoch   1 step    45600 |  45600 batches | lr 0.0003 | ms/batch 39.21556 | loss 0.71004 | ppl     2.034\n",
      "| epoch   1 step    45800 |  45800 batches | lr 0.0003 | ms/batch 39.81759 | loss 0.70711 | ppl     2.028\n",
      "| epoch   1 step    46000 |  46000 batches | lr 0.0003 | ms/batch 40.23344 | loss 0.71461 | ppl     2.043\n",
      "| epoch   1 step    46200 |  46200 batches | lr 0.0003 | ms/batch 40.33163 | loss 0.70859 | ppl     2.031\n",
      "| epoch   1 step    46400 |  46400 batches | lr 0.0003 | ms/batch 40.24142 | loss 0.70729 | ppl     2.028\n",
      "| epoch   1 step    46600 |  46600 batches | lr 0.0003 | ms/batch 40.73733 | loss 0.70928 | ppl     2.033\n",
      "| epoch   1 step    46800 |  46800 batches | lr 0.0003 | ms/batch 39.68128 | loss 0.70628 | ppl     2.026\n",
      "| epoch   1 step    47000 |  47000 batches | lr 0.0003 | ms/batch 40.27471 | loss 0.70815 | ppl     2.030\n",
      "| epoch   1 step    47200 |  47200 batches | lr 0.0003 | ms/batch 39.18340 | loss 0.70698 | ppl     2.028\n",
      "| epoch   1 step    47400 |  47400 batches | lr 0.0003 | ms/batch 39.17164 | loss 0.70582 | ppl     2.026\n",
      "| epoch   1 step    47600 |  47600 batches | lr 0.0003 | ms/batch 39.62983 | loss 0.70715 | ppl     2.028\n",
      "| epoch   1 step    47800 |  47800 batches | lr 0.0003 | ms/batch 40.09499 | loss 0.70686 | ppl     2.028\n",
      "| epoch   1 step    48000 |  48000 batches | lr 0.0003 | ms/batch 39.94889 | loss 0.70676 | ppl     2.027\n",
      "|\n",
      "Source: [11  9 21  8 12  5 20  1 20 10]\n",
      "Target: [ 9 21  8 12  5 20  1 20 10  1]\n",
      "Teacher forcing: acc:0.626875\n",
      "Preds:  [10  9 10 10 10  5 10 10 10  5]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  12 at step    48000 | time: 160.99s | valid loss 0.69851 | valid ppl    2.0108 | valid acc 0.627\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    48200 |  48200 batches | lr 0.0003 | ms/batch 46.04566 | loss 0.70772 | ppl     2.029\n",
      "| epoch   1 step    48400 |  48400 batches | lr 0.0003 | ms/batch 40.01510 | loss 0.70688 | ppl     2.028\n",
      "| epoch   1 step    48600 |  48600 batches | lr 0.0003 | ms/batch 39.43898 | loss 0.70773 | ppl     2.029\n",
      "| epoch   1 step    48800 |  48800 batches | lr 0.0003 | ms/batch 39.89312 | loss 0.70804 | ppl     2.030\n",
      "| epoch   1 step    49000 |  49000 batches | lr 0.0003 | ms/batch 39.81614 | loss 0.70907 | ppl     2.032\n",
      "| epoch   1 step    49200 |  49200 batches | lr 0.0003 | ms/batch 39.48251 | loss 0.70841 | ppl     2.031\n",
      "| epoch   1 step    49400 |  49400 batches | lr 0.0003 | ms/batch 39.36289 | loss 0.70549 | ppl     2.025\n",
      "| epoch   1 step    49600 |  49600 batches | lr 0.0003 | ms/batch 39.17529 | loss 0.70646 | ppl     2.027\n",
      "| epoch   1 step    49800 |  49800 batches | lr 0.0003 | ms/batch 38.62687 | loss 0.70771 | ppl     2.029\n",
      "| epoch   1 step    50000 |  50000 batches | lr 0.0003 | ms/batch 39.70400 | loss 0.70697 | ppl     2.028\n",
      "| epoch   1 step    50200 |  50200 batches | lr 0.0003 | ms/batch 39.44903 | loss 0.70468 | ppl     2.023\n",
      "| epoch   1 step    50400 |  50400 batches | lr 0.0003 | ms/batch 39.50837 | loss 0.70725 | ppl     2.028\n",
      "| epoch   1 step    50600 |  50600 batches | lr 0.0003 | ms/batch 39.36273 | loss 0.70622 | ppl     2.026\n",
      "| epoch   1 step    50800 |  50800 batches | lr 0.0003 | ms/batch 39.87503 | loss 0.70734 | ppl     2.029\n",
      "| epoch   1 step    51000 |  51000 batches | lr 0.0003 | ms/batch 39.46623 | loss 0.70986 | ppl     2.034\n",
      "| epoch   1 step    51200 |  51200 batches | lr 0.0003 | ms/batch 39.65000 | loss 0.71129 | ppl     2.037\n",
      "| epoch   1 step    51400 |  51400 batches | lr 0.0003 | ms/batch 39.26657 | loss 0.71566 | ppl     2.046\n",
      "| epoch   1 step    51600 |  51600 batches | lr 0.0003 | ms/batch 39.89936 | loss 0.71049 | ppl     2.035\n",
      "| epoch   1 step    51800 |  51800 batches | lr 0.0003 | ms/batch 39.31284 | loss 0.70982 | ppl     2.034\n",
      "| epoch   1 step    52000 |  52000 batches | lr 0.0003 | ms/batch 40.94264 | loss 0.71008 | ppl     2.034\n",
      "|\n",
      "Source: [25  7 12  4 13  9 14  2 25 10]\n",
      "Target: [ 7 12  4 13  9 14  2 25 10  7]\n",
      "Teacher forcing: acc:0.624375\n",
      "Preds:  [10  7 10  4 10  9 10  2 10  4]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  13 at step    52000 | time: 159.56s | valid loss 0.70075 | valid ppl    2.0153 | valid acc 0.624\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    52200 |  52200 batches | lr 0.0003 | ms/batch 45.65806 | loss 0.71296 | ppl     2.040\n",
      "| epoch   1 step    52400 |  52400 batches | lr 0.0003 | ms/batch 40.35714 | loss 0.70881 | ppl     2.032\n",
      "| epoch   1 step    52600 |  52600 batches | lr 0.0003 | ms/batch 40.64096 | loss 0.70600 | ppl     2.026\n",
      "| epoch   1 step    52800 |  52800 batches | lr 0.0003 | ms/batch 39.21902 | loss 0.70638 | ppl     2.027\n",
      "| epoch   1 step    53000 |  53000 batches | lr 0.0003 | ms/batch 38.53688 | loss 0.70649 | ppl     2.027\n",
      "| epoch   1 step    53200 |  53200 batches | lr 0.0003 | ms/batch 39.92977 | loss 0.70773 | ppl     2.029\n",
      "| epoch   1 step    53400 |  53400 batches | lr 0.0003 | ms/batch 38.89708 | loss 0.70699 | ppl     2.028\n",
      "| epoch   1 step    53600 |  53600 batches | lr 0.0003 | ms/batch 40.03223 | loss 0.72121 | ppl     2.057\n",
      "| epoch   1 step    53800 |  53800 batches | lr 0.0003 | ms/batch 40.16738 | loss 0.71079 | ppl     2.036\n",
      "| epoch   1 step    54000 |  54000 batches | lr 0.0003 | ms/batch 40.02173 | loss 0.70601 | ppl     2.026\n",
      "| epoch   1 step    54200 |  54200 batches | lr 0.0003 | ms/batch 39.43340 | loss 0.70772 | ppl     2.029\n",
      "| epoch   1 step    54400 |  54400 batches | lr 0.0003 | ms/batch 39.87509 | loss 0.70684 | ppl     2.028\n",
      "| epoch   1 step    54600 |  54600 batches | lr 0.0003 | ms/batch 39.93975 | loss 0.70591 | ppl     2.026\n",
      "| epoch   1 step    54800 |  54800 batches | lr 0.0003 | ms/batch 39.70581 | loss 0.70705 | ppl     2.028\n",
      "| epoch   1 step    55000 |  55000 batches | lr 0.0003 | ms/batch 40.39142 | loss 0.70716 | ppl     2.028\n",
      "| epoch   1 step    55200 |  55200 batches | lr 0.0003 | ms/batch 39.71478 | loss 0.70704 | ppl     2.028\n",
      "| epoch   1 step    55400 |  55400 batches | lr 0.0003 | ms/batch 39.53864 | loss 0.70719 | ppl     2.028\n",
      "| epoch   1 step    55600 |  55600 batches | lr 0.0003 | ms/batch 39.99426 | loss 0.70738 | ppl     2.029\n",
      "| epoch   1 step    55800 |  55800 batches | lr 0.0003 | ms/batch 39.28533 | loss 0.70636 | ppl     2.027\n",
      "| epoch   1 step    56000 |  56000 batches | lr 0.0003 | ms/batch 40.07833 | loss 0.70759 | ppl     2.029\n",
      "|\n",
      "Source: [24  3 31  1 30  4 25  2 25 10]\n",
      "Target: [ 3 31  1 30  4 25  2 25 10  2]\n",
      "Teacher forcing: acc:0.6184375\n",
      "Preds:  [10  3 10  1 10  4 10  2 10  1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  14 at step    56000 | time: 160.27s | valid loss 0.70157 | valid ppl    2.0169 | valid acc 0.618\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    56200 |  56200 batches | lr 0.0003 | ms/batch 45.68601 | loss 0.70555 | ppl     2.025\n",
      "| epoch   1 step    56400 |  56400 batches | lr 0.0003 | ms/batch 39.86691 | loss 0.70716 | ppl     2.028\n",
      "| epoch   1 step    56600 |  56600 batches | lr 0.0003 | ms/batch 40.66513 | loss 0.70429 | ppl     2.022\n",
      "| epoch   1 step    56800 |  56800 batches | lr 0.0003 | ms/batch 40.12544 | loss 0.71255 | ppl     2.039\n",
      "| epoch   1 step    57000 |  57000 batches | lr 0.0003 | ms/batch 39.63689 | loss 0.71484 | ppl     2.044\n",
      "| epoch   1 step    57200 |  57200 batches | lr 0.0003 | ms/batch 39.98618 | loss 0.71532 | ppl     2.045\n",
      "| epoch   1 step    57400 |  57400 batches | lr 0.0003 | ms/batch 39.80010 | loss 0.71429 | ppl     2.043\n",
      "| epoch   1 step    57600 |  57600 batches | lr 0.0003 | ms/batch 39.83988 | loss 0.70643 | ppl     2.027\n",
      "| epoch   1 step    57800 |  57800 batches | lr 0.0003 | ms/batch 39.31538 | loss 0.70717 | ppl     2.028\n",
      "| epoch   1 step    58000 |  58000 batches | lr 0.0003 | ms/batch 40.05330 | loss 0.70782 | ppl     2.030\n",
      "| epoch   1 step    58200 |  58200 batches | lr 0.0003 | ms/batch 40.60394 | loss 0.70543 | ppl     2.025\n",
      "| epoch   1 step    58400 |  58400 batches | lr 0.0003 | ms/batch 40.05390 | loss 0.70497 | ppl     2.024\n",
      "| epoch   1 step    58600 |  58600 batches | lr 0.0003 | ms/batch 40.12604 | loss 0.70515 | ppl     2.024\n",
      "| epoch   1 step    58800 |  58800 batches | lr 0.0003 | ms/batch 39.94018 | loss 0.70575 | ppl     2.025\n",
      "| epoch   1 step    59000 |  59000 batches | lr 0.0003 | ms/batch 39.51530 | loss 0.70528 | ppl     2.024\n",
      "| epoch   1 step    59200 |  59200 batches | lr 0.0003 | ms/batch 39.79191 | loss 0.70644 | ppl     2.027\n",
      "| epoch   1 step    59400 |  59400 batches | lr 0.0003 | ms/batch 39.74100 | loss 0.70696 | ppl     2.028\n",
      "| epoch   1 step    59600 |  59600 batches | lr 0.0003 | ms/batch 39.16834 | loss 0.70449 | ppl     2.023\n",
      "| epoch   1 step    59800 |  59800 batches | lr 0.0003 | ms/batch 40.86542 | loss 0.70539 | ppl     2.025\n",
      "| epoch   1 step    60000 |  60000 batches | lr 0.0003 | ms/batch 40.48058 | loss 0.70554 | ppl     2.025\n",
      "|\n",
      "Source: [28  3 26  2 36  1 34  4 28 10]\n",
      "Target: [ 3 26  2 36  1 34  4 28 10  3]\n",
      "Teacher forcing: acc:0.6196875\n",
      "Preds:  [10  3 10  3 10  3 10  4 10  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  15 at step    60000 | time: 161.10s | valid loss 0.70206 | valid ppl    2.0179 | valid acc 0.62\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    60200 |  60200 batches | lr 0.0003 | ms/batch 47.04374 | loss 0.70579 | ppl     2.025\n",
      "| epoch   1 step    60400 |  60400 batches | lr 0.0003 | ms/batch 40.83041 | loss 0.70684 | ppl     2.028\n",
      "| epoch   1 step    60600 |  60600 batches | lr 0.0003 | ms/batch 40.72175 | loss 0.70534 | ppl     2.025\n",
      "| epoch   1 step    60800 |  60800 batches | lr 0.0003 | ms/batch 39.55722 | loss 0.70536 | ppl     2.025\n",
      "| epoch   1 step    61000 |  61000 batches | lr 0.0003 | ms/batch 40.20615 | loss 0.70610 | ppl     2.026\n",
      "| epoch   1 step    61200 |  61200 batches | lr 0.0003 | ms/batch 41.08698 | loss 0.71472 | ppl     2.044\n",
      "| epoch   1 step    61400 |  61400 batches | lr 0.0003 | ms/batch 40.22668 | loss 0.70628 | ppl     2.026\n",
      "| epoch   1 step    61600 |  61600 batches | lr 0.0003 | ms/batch 39.88084 | loss 0.70535 | ppl     2.025\n",
      "| epoch   1 step    61800 |  61800 batches | lr 0.0003 | ms/batch 40.62901 | loss 0.72880 | ppl     2.073\n",
      "| epoch   1 step    62000 |  62000 batches | lr 0.0003 | ms/batch 40.38570 | loss 0.71982 | ppl     2.054\n",
      "| epoch   1 step    62200 |  62200 batches | lr 0.0003 | ms/batch 41.25073 | loss 0.71139 | ppl     2.037\n",
      "| epoch   1 step    62400 |  62400 batches | lr 0.0003 | ms/batch 41.32624 | loss 0.70547 | ppl     2.025\n",
      "| epoch   1 step    62600 |  62600 batches | lr 0.0003 | ms/batch 41.12075 | loss 0.70669 | ppl     2.027\n",
      "| epoch   1 step    62800 |  62800 batches | lr 0.0003 | ms/batch 40.67942 | loss 0.70468 | ppl     2.023\n",
      "| epoch   1 step    63000 |  63000 batches | lr 0.0003 | ms/batch 40.96030 | loss 0.70392 | ppl     2.022\n",
      "| epoch   1 step    63200 |  63200 batches | lr 0.0003 | ms/batch 40.99469 | loss 0.70716 | ppl     2.028\n",
      "| epoch   1 step    63400 |  63400 batches | lr 0.0003 | ms/batch 40.51124 | loss 0.70564 | ppl     2.025\n",
      "| epoch   1 step    63600 |  63600 batches | lr 0.0003 | ms/batch 40.49906 | loss 0.70727 | ppl     2.028\n",
      "| epoch   1 step    63800 |  63800 batches | lr 0.0003 | ms/batch 40.07981 | loss 0.70596 | ppl     2.026\n",
      "| epoch   1 step    64000 |  64000 batches | lr 0.0003 | ms/batch 40.36029 | loss 0.70569 | ppl     2.025\n",
      "|\n",
      "Source: [17  0 32  2 25  4 35  1 35 10]\n",
      "Target: [ 0 32  2 25  4 35  1 35 10  1]\n",
      "Teacher forcing: acc:0.6253125\n",
      "Preds:  [10  0 10  0 10 10 10  0 10  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  16 at step    64000 | time: 163.69s | valid loss 0.69784 | valid ppl    2.0094 | valid acc 0.625\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    64200 |  64200 batches | lr 0.0003 | ms/batch 46.66055 | loss 0.70420 | ppl     2.022\n",
      "| epoch   1 step    64400 |  64400 batches | lr 0.0003 | ms/batch 40.79024 | loss 0.70679 | ppl     2.027\n",
      "| epoch   1 step    64600 |  64600 batches | lr 0.0003 | ms/batch 40.09231 | loss 0.70688 | ppl     2.028\n",
      "| epoch   1 step    64800 |  64800 batches | lr 0.0003 | ms/batch 40.82825 | loss 0.71231 | ppl     2.039\n",
      "| epoch   1 step    65000 |  65000 batches | lr 0.0003 | ms/batch 40.71584 | loss 0.71570 | ppl     2.046\n",
      "| epoch   1 step    65200 |  65200 batches | lr 0.0003 | ms/batch 40.57460 | loss 0.70396 | ppl     2.022\n",
      "| epoch   1 step    65400 |  65400 batches | lr 0.0003 | ms/batch 40.67705 | loss 0.70544 | ppl     2.025\n",
      "| epoch   1 step    65600 |  65600 batches | lr 0.0003 | ms/batch 41.37152 | loss 0.70387 | ppl     2.022\n",
      "| epoch   1 step    65800 |  65800 batches | lr 0.0003 | ms/batch 41.61659 | loss 0.70466 | ppl     2.023\n",
      "| epoch   1 step    66000 |  66000 batches | lr 0.0003 | ms/batch 42.51130 | loss 0.70732 | ppl     2.029\n",
      "| epoch   1 step    66200 |  66200 batches | lr 0.0003 | ms/batch 42.53297 | loss 0.70578 | ppl     2.025\n",
      "| epoch   1 step    66400 |  66400 batches | lr 0.0003 | ms/batch 41.41669 | loss 0.70533 | ppl     2.025\n",
      "| epoch   1 step    66600 |  66600 batches | lr 0.0003 | ms/batch 40.37216 | loss 0.70416 | ppl     2.022\n",
      "| epoch   1 step    66800 |  66800 batches | lr 0.0003 | ms/batch 40.11265 | loss 0.70417 | ppl     2.022\n",
      "| epoch   1 step    67000 |  67000 batches | lr 0.0003 | ms/batch 41.22384 | loss 0.70504 | ppl     2.024\n",
      "| epoch   1 step    67200 |  67200 batches | lr 0.0003 | ms/batch 40.97631 | loss 0.70402 | ppl     2.022\n",
      "| epoch   1 step    67400 |  67400 batches | lr 0.0003 | ms/batch 40.15242 | loss 0.70867 | ppl     2.031\n",
      "| epoch   1 step    67600 |  67600 batches | lr 0.0003 | ms/batch 40.41551 | loss 0.70585 | ppl     2.026\n",
      "| epoch   1 step    67800 |  67800 batches | lr 0.0003 | ms/batch 41.07031 | loss 0.70642 | ppl     2.027\n",
      "| epoch   1 step    68000 |  68000 batches | lr 0.0003 | ms/batch 40.96584 | loss 0.70415 | ppl     2.022\n",
      "|\n",
      "Source: [18  9 15  0 36  8 28  6 18 10]\n",
      "Target: [ 9 15  0 36  8 28  6 18 10  9]\n",
      "Teacher forcing: acc:0.6190625\n",
      "Preds:  [10  9 10  9 10 10 10  6 10  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  17 at step    68000 | time: 164.94s | valid loss 0.70137 | valid ppl    2.0165 | valid acc 0.619\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    68200 |  68200 batches | lr 0.0003 | ms/batch 47.23969 | loss 0.70417 | ppl     2.022\n",
      "| epoch   1 step    68400 |  68400 batches | lr 0.0003 | ms/batch 41.88003 | loss 0.70396 | ppl     2.022\n",
      "| epoch   1 step    68600 |  68600 batches | lr 0.0003 | ms/batch 42.68611 | loss 0.70394 | ppl     2.022\n",
      "| epoch   1 step    68800 |  68800 batches | lr 0.0003 | ms/batch 40.59557 | loss 0.70460 | ppl     2.023\n",
      "| epoch   1 step    69000 |  69000 batches | lr 0.0003 | ms/batch 41.56296 | loss 0.70759 | ppl     2.029\n",
      "| epoch   1 step    69200 |  69200 batches | lr 0.0003 | ms/batch 40.76278 | loss 0.70355 | ppl     2.021\n",
      "| epoch   1 step    69400 |  69400 batches | lr 0.0003 | ms/batch 42.07461 | loss 0.70445 | ppl     2.023\n",
      "| epoch   1 step    69600 |  69600 batches | lr 0.0003 | ms/batch 41.76916 | loss 0.70498 | ppl     2.024\n",
      "| epoch   1 step    69800 |  69800 batches | lr 0.0003 | ms/batch 42.11031 | loss 0.70572 | ppl     2.025\n",
      "| epoch   1 step    70000 |  70000 batches | lr 0.0003 | ms/batch 40.46871 | loss 0.70595 | ppl     2.026\n",
      "| epoch   1 step    70200 |  70200 batches | lr 0.0003 | ms/batch 40.57029 | loss 0.70701 | ppl     2.028\n",
      "| epoch   1 step    70400 |  70400 batches | lr 0.0003 | ms/batch 40.84511 | loss 0.70471 | ppl     2.023\n",
      "| epoch   1 step    70600 |  70600 batches | lr 0.0003 | ms/batch 41.03813 | loss 0.70442 | ppl     2.023\n",
      "| epoch   1 step    70800 |  70800 batches | lr 0.0003 | ms/batch 40.53276 | loss 0.70404 | ppl     2.022\n",
      "| epoch   1 step    71000 |  71000 batches | lr 0.0003 | ms/batch 41.57544 | loss 0.70565 | ppl     2.025\n",
      "| epoch   1 step    71200 |  71200 batches | lr 0.0003 | ms/batch 41.24313 | loss 0.70626 | ppl     2.026\n",
      "| epoch   1 step    71400 |  71400 batches | lr 0.0003 | ms/batch 41.57635 | loss 0.70449 | ppl     2.023\n",
      "| epoch   1 step    71600 |  71600 batches | lr 0.0003 | ms/batch 42.38463 | loss 0.70602 | ppl     2.026\n",
      "| epoch   1 step    71800 |  71800 batches | lr 0.0003 | ms/batch 40.87107 | loss 0.70636 | ppl     2.027\n",
      "| epoch   1 step    72000 |  72000 batches | lr 0.0003 | ms/batch 40.96177 | loss 0.70542 | ppl     2.025\n",
      "|\n",
      "Source: [20  2 18  5 24  6 14  9 18 10]\n",
      "Target: [ 2 18  5 24  6 14  9 18 10  5]\n",
      "Teacher forcing: acc:0.6315625\n",
      "Preds:  [10  0 10  5 10  6 10  6 10  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  18 at step    72000 | time: 166.56s | valid loss 0.69933 | valid ppl    2.0124 | valid acc 0.632\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    72200 |  72200 batches | lr 0.0003 | ms/batch 45.75445 | loss 0.70487 | ppl     2.024\n",
      "| epoch   1 step    72400 |  72400 batches | lr 0.0003 | ms/batch 39.36278 | loss 0.70345 | ppl     2.021\n",
      "| epoch   1 step    72600 |  72600 batches | lr 0.0003 | ms/batch 39.34005 | loss 0.70664 | ppl     2.027\n",
      "| epoch   1 step    72800 |  72800 batches | lr 0.0003 | ms/batch 39.66441 | loss 0.70301 | ppl     2.020\n",
      "| epoch   1 step    73000 |  73000 batches | lr 0.0003 | ms/batch 38.79708 | loss 0.70488 | ppl     2.024\n",
      "| epoch   1 step    73200 |  73200 batches | lr 0.0003 | ms/batch 40.08746 | loss 0.70401 | ppl     2.022\n",
      "| epoch   1 step    73400 |  73400 batches | lr 0.0003 | ms/batch 40.63112 | loss 0.70414 | ppl     2.022\n",
      "| epoch   1 step    73600 |  73600 batches | lr 0.0003 | ms/batch 39.96005 | loss 0.70360 | ppl     2.021\n",
      "| epoch   1 step    73800 |  73800 batches | lr 0.0003 | ms/batch 40.69149 | loss 0.70497 | ppl     2.024\n",
      "| epoch   1 step    74000 |  74000 batches | lr 0.0003 | ms/batch 40.07027 | loss 0.70526 | ppl     2.024\n",
      "| epoch   1 step    74200 |  74200 batches | lr 0.0003 | ms/batch 39.84247 | loss 0.70456 | ppl     2.023\n",
      "| epoch   1 step    74400 |  74400 batches | lr 0.0003 | ms/batch 38.40183 | loss 0.70438 | ppl     2.023\n",
      "| epoch   1 step    74600 |  74600 batches | lr 0.0003 | ms/batch 39.82825 | loss 0.70271 | ppl     2.019\n",
      "| epoch   1 step    74800 |  74800 batches | lr 0.0003 | ms/batch 39.90062 | loss 0.70422 | ppl     2.022\n",
      "| epoch   1 step    75000 |  75000 batches | lr 0.0003 | ms/batch 39.73319 | loss 0.70359 | ppl     2.021\n",
      "| epoch   1 step    75200 |  75200 batches | lr 0.0003 | ms/batch 39.19685 | loss 0.80196 | ppl     2.230\n",
      "| epoch   1 step    75400 |  75400 batches | lr 0.0003 | ms/batch 40.56347 | loss 0.73141 | ppl     2.078\n",
      "| epoch   1 step    75600 |  75600 batches | lr 0.0003 | ms/batch 41.52640 | loss 0.70483 | ppl     2.023\n",
      "| epoch   1 step    75800 |  75800 batches | lr 0.0003 | ms/batch 39.82802 | loss 0.70316 | ppl     2.020\n",
      "| epoch   1 step    76000 |  76000 batches | lr 0.0003 | ms/batch 39.35437 | loss 0.70473 | ppl     2.023\n",
      "maslina\n",
      "|\n",
      "Source: [26  0 30  3 28  4 15  9 30 10]\n",
      "Target: [ 0 30  3 28  4 15  9 30 10  3]\n",
      "Teacher forcing: acc:0.6176658163265306\n",
      "Preds:  [10  0 10 10 10  3 10  3 10  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  19 at step    76000 | time: 160.46s | valid loss 0.70043 | valid ppl    2.0146 | valid acc 0.618\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    76200 |  76200 batches | lr 0.0003 | ms/batch 44.99836 | loss 0.70450 | ppl     2.023\n",
      "| epoch   1 step    76400 |  76400 batches | lr 0.0003 | ms/batch 40.74252 | loss 0.70324 | ppl     2.020\n",
      "| epoch   1 step    76600 |  76600 batches | lr 0.0003 | ms/batch 40.58621 | loss 0.70433 | ppl     2.022\n",
      "| epoch   1 step    76800 |  76800 batches | lr 0.0003 | ms/batch 40.12434 | loss 0.70323 | ppl     2.020\n",
      "| epoch   1 step    77000 |  77000 batches | lr 0.0003 | ms/batch 41.13896 | loss 0.70463 | ppl     2.023\n",
      "| epoch   1 step    77200 |  77200 batches | lr 0.0003 | ms/batch 42.40489 | loss 0.70482 | ppl     2.023\n",
      "| epoch   1 step    77400 |  77400 batches | lr 0.0003 | ms/batch 40.68761 | loss 0.70192 | ppl     2.018\n",
      "| epoch   1 step    77600 |  77600 batches | lr 0.0003 | ms/batch 40.38440 | loss 0.70510 | ppl     2.024\n",
      "| epoch   1 step    77800 |  77800 batches | lr 0.0003 | ms/batch 41.30323 | loss 0.70470 | ppl     2.023\n",
      "| epoch   1 step    78000 |  78000 batches | lr 0.0003 | ms/batch 40.73261 | loss 0.70397 | ppl     2.022\n",
      "| epoch   1 step    78200 |  78200 batches | lr 0.0003 | ms/batch 40.92227 | loss 0.70295 | ppl     2.020\n",
      "| epoch   1 step    78400 |  78400 batches | lr 0.0003 | ms/batch 40.72835 | loss 0.70472 | ppl     2.023\n",
      "| epoch   1 step    78600 |  78600 batches | lr 0.0003 | ms/batch 40.61367 | loss 0.70267 | ppl     2.019\n",
      "| epoch   1 step    78800 |  78800 batches | lr 0.0003 | ms/batch 41.31746 | loss 0.70396 | ppl     2.022\n",
      "| epoch   1 step    79000 |  79000 batches | lr 0.0003 | ms/batch 41.36812 | loss 0.70437 | ppl     2.023\n",
      "| epoch   1 step    79200 |  79200 batches | lr 0.0003 | ms/batch 41.39247 | loss 0.70424 | ppl     2.022\n",
      "| epoch   1 step    79400 |  79400 batches | lr 0.0003 | ms/batch 40.75892 | loss 0.70484 | ppl     2.024\n",
      "| epoch   1 step    79600 |  79600 batches | lr 0.0003 | ms/batch 40.72407 | loss 0.70325 | ppl     2.020\n",
      "| epoch   1 step    79800 |  79800 batches | lr 0.0003 | ms/batch 39.84058 | loss 0.70447 | ppl     2.023\n",
      "| epoch   1 step    80000 |  80000 batches | lr 0.0003 | ms/batch 40.52950 | loss 0.70400 | ppl     2.022\n",
      "|\n",
      "Source: [20  3 35  7 36  6 25  8 20 10]\n",
      "Target: [ 3 35  7 36  6 25  8 20 10  3]\n",
      "Teacher forcing: acc:0.6203125\n",
      "Preds:  [10 10 10  3 10  7 10 10 10  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  20 at step    80000 | time: 164.31s | valid loss 0.69907 | valid ppl    2.0119 | valid acc 0.62\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    80200 |  80200 batches | lr 0.00015 | ms/batch 47.94245 | loss 0.70369 | ppl     2.021\n",
      "| epoch   1 step    80400 |  80400 batches | lr 0.00015 | ms/batch 40.78690 | loss 0.70131 | ppl     2.016\n",
      "| epoch   1 step    80600 |  80600 batches | lr 0.00015 | ms/batch 41.21576 | loss 0.70000 | ppl     2.014\n",
      "| epoch   1 step    80800 |  80800 batches | lr 0.00015 | ms/batch 40.58404 | loss 0.69984 | ppl     2.013\n",
      "| epoch   1 step    81000 |  81000 batches | lr 0.00015 | ms/batch 40.98068 | loss 0.70185 | ppl     2.017\n",
      "| epoch   1 step    81200 |  81200 batches | lr 0.00015 | ms/batch 41.18202 | loss 0.69972 | ppl     2.013\n",
      "| epoch   1 step    81400 |  81400 batches | lr 0.00015 | ms/batch 41.28468 | loss 0.70488 | ppl     2.024\n",
      "| epoch   1 step    81600 |  81600 batches | lr 0.00015 | ms/batch 40.51067 | loss 0.70094 | ppl     2.016\n",
      "| epoch   1 step    81800 |  81800 batches | lr 0.00015 | ms/batch 40.78317 | loss 0.69933 | ppl     2.012\n",
      "| epoch   1 step    82000 |  82000 batches | lr 0.00015 | ms/batch 40.58748 | loss 0.70138 | ppl     2.017\n",
      "| epoch   1 step    82200 |  82200 batches | lr 0.00015 | ms/batch 41.55203 | loss 0.69921 | ppl     2.012\n",
      "| epoch   1 step    82400 |  82400 batches | lr 0.00015 | ms/batch 41.19580 | loss 0.70149 | ppl     2.017\n",
      "| epoch   1 step    82600 |  82600 batches | lr 0.00015 | ms/batch 41.76155 | loss 0.70058 | ppl     2.015\n",
      "| epoch   1 step    82800 |  82800 batches | lr 0.00015 | ms/batch 41.15418 | loss 0.70055 | ppl     2.015\n",
      "| epoch   1 step    83000 |  83000 batches | lr 0.00015 | ms/batch 40.97255 | loss 0.70660 | ppl     2.027\n",
      "| epoch   1 step    83200 |  83200 batches | lr 0.00015 | ms/batch 40.87710 | loss 0.70477 | ppl     2.023\n",
      "| epoch   1 step    83400 |  83400 batches | lr 0.00015 | ms/batch 40.84790 | loss 0.70249 | ppl     2.019\n",
      "| epoch   1 step    83600 |  83600 batches | lr 0.00015 | ms/batch 40.53758 | loss 0.70324 | ppl     2.020\n",
      "| epoch   1 step    83800 |  83800 batches | lr 0.00015 | ms/batch 41.48292 | loss 0.69957 | ppl     2.013\n",
      "| epoch   1 step    84000 |  84000 batches | lr 0.00015 | ms/batch 41.26841 | loss 0.70042 | ppl     2.015\n",
      "|\n",
      "Source: [12  2 14  6 16  3 29  4 29 10]\n",
      "Target: [ 2 14  6 16  3 29  4 29 10  4]\n",
      "Teacher forcing: acc:0.6240625\n",
      "Preds:  [10  2 10  2 10  6 10  4 10  4]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  21 at step    84000 | time: 165.57s | valid loss 0.69564 | valid ppl    2.0050 | valid acc 0.624\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    84200 |  84200 batches | lr 0.00015 | ms/batch 48.73104 | loss 0.71395 | ppl     2.042\n",
      "| epoch   1 step    84400 |  84400 batches | lr 0.00015 | ms/batch 41.00233 | loss 0.69991 | ppl     2.014\n",
      "| epoch   1 step    84600 |  84600 batches | lr 0.00015 | ms/batch 41.23973 | loss 0.70093 | ppl     2.016\n",
      "| epoch   1 step    84800 |  84800 batches | lr 0.00015 | ms/batch 41.72897 | loss 0.70129 | ppl     2.016\n",
      "| epoch   1 step    85000 |  85000 batches | lr 0.00015 | ms/batch 41.41549 | loss 0.69964 | ppl     2.013\n",
      "| epoch   1 step    85200 |  85200 batches | lr 0.00015 | ms/batch 41.01675 | loss 0.69922 | ppl     2.012\n",
      "| epoch   1 step    85400 |  85400 batches | lr 0.00015 | ms/batch 40.63177 | loss 0.69879 | ppl     2.011\n",
      "| epoch   1 step    85600 |  85600 batches | lr 0.00015 | ms/batch 41.09689 | loss 0.70092 | ppl     2.016\n",
      "| epoch   1 step    85800 |  85800 batches | lr 0.00015 | ms/batch 41.22797 | loss 0.69904 | ppl     2.012\n",
      "| epoch   1 step    86000 |  86000 batches | lr 0.00015 | ms/batch 41.26869 | loss 0.70045 | ppl     2.015\n",
      "| epoch   1 step    86200 |  86200 batches | lr 0.00015 | ms/batch 40.78001 | loss 0.69983 | ppl     2.013\n",
      "| epoch   1 step    86400 |  86400 batches | lr 0.00015 | ms/batch 39.98359 | loss 0.69840 | ppl     2.011\n",
      "| epoch   1 step    86600 |  86600 batches | lr 0.00015 | ms/batch 40.64736 | loss 0.70034 | ppl     2.014\n",
      "| epoch   1 step    86800 |  86800 batches | lr 0.00015 | ms/batch 40.68949 | loss 0.69960 | ppl     2.013\n",
      "| epoch   1 step    87000 |  87000 batches | lr 0.00015 | ms/batch 40.61424 | loss 0.70089 | ppl     2.016\n",
      "| epoch   1 step    87200 |  87200 batches | lr 0.00015 | ms/batch 40.39345 | loss 0.70269 | ppl     2.019\n",
      "| epoch   1 step    87400 |  87400 batches | lr 0.00015 | ms/batch 40.76206 | loss 0.70023 | ppl     2.014\n",
      "| epoch   1 step    87600 |  87600 batches | lr 0.00015 | ms/batch 39.92963 | loss 0.69971 | ppl     2.013\n",
      "| epoch   1 step    87800 |  87800 batches | lr 0.00015 | ms/batch 41.04507 | loss 0.70946 | ppl     2.033\n",
      "| epoch   1 step    88000 |  88000 batches | lr 0.00015 | ms/batch 40.89186 | loss 0.69973 | ppl     2.013\n",
      "|\n",
      "Source: [33  1 27  2 19  5 29  0 29 10]\n",
      "Target: [ 1 27  2 19  5 29  0 29 10  0]\n",
      "Teacher forcing: acc:0.6225\n",
      "Preds:  [10  4 10  2 10  5 10  2 10  2]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  22 at step    88000 | time: 164.89s | valid loss 0.69685 | valid ppl    2.0074 | valid acc 0.623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    88200 |  88200 batches | lr 0.00015 | ms/batch 46.31718 | loss 0.70093 | ppl     2.016\n",
      "| epoch   1 step    88400 |  88400 batches | lr 0.00015 | ms/batch 39.57994 | loss 0.70053 | ppl     2.015\n",
      "| epoch   1 step    88600 |  88600 batches | lr 0.00015 | ms/batch 40.55801 | loss 0.70123 | ppl     2.016\n",
      "| epoch   1 step    88800 |  88800 batches | lr 0.00015 | ms/batch 40.50940 | loss 0.70032 | ppl     2.014\n",
      "| epoch   1 step    89000 |  89000 batches | lr 0.00015 | ms/batch 40.26453 | loss 0.70026 | ppl     2.014\n",
      "| epoch   1 step    89200 |  89200 batches | lr 0.00015 | ms/batch 40.87251 | loss 0.69899 | ppl     2.012\n",
      "| epoch   1 step    89400 |  89400 batches | lr 0.00015 | ms/batch 39.59972 | loss 0.69962 | ppl     2.013\n",
      "| epoch   1 step    89600 |  89600 batches | lr 0.00015 | ms/batch 40.24111 | loss 0.69907 | ppl     2.012\n",
      "| epoch   1 step    89800 |  89800 batches | lr 0.00015 | ms/batch 39.81269 | loss 0.70035 | ppl     2.014\n",
      "| epoch   1 step    90000 |  90000 batches | lr 0.00015 | ms/batch 41.40611 | loss 0.69951 | ppl     2.013\n",
      "| epoch   1 step    90200 |  90200 batches | lr 0.00015 | ms/batch 40.40629 | loss 0.70023 | ppl     2.014\n",
      "| epoch   1 step    90400 |  90400 batches | lr 0.00015 | ms/batch 40.37341 | loss 0.69959 | ppl     2.013\n",
      "| epoch   1 step    90600 |  90600 batches | lr 0.00015 | ms/batch 40.33492 | loss 0.69867 | ppl     2.011\n",
      "| epoch   1 step    90800 |  90800 batches | lr 0.00015 | ms/batch 40.01790 | loss 0.70054 | ppl     2.015\n",
      "| epoch   1 step    91000 |  91000 batches | lr 0.00015 | ms/batch 39.71494 | loss 0.69937 | ppl     2.012\n",
      "| epoch   1 step    91200 |  91200 batches | lr 0.00015 | ms/batch 40.37436 | loss 0.69985 | ppl     2.013\n",
      "| epoch   1 step    91400 |  91400 batches | lr 0.00015 | ms/batch 40.57174 | loss 0.69983 | ppl     2.013\n",
      "| epoch   1 step    91600 |  91600 batches | lr 0.00015 | ms/batch 40.00828 | loss 0.69975 | ppl     2.013\n",
      "| epoch   1 step    91800 |  91800 batches | lr 0.00015 | ms/batch 40.40161 | loss 0.69911 | ppl     2.012\n",
      "| epoch   1 step    92000 |  92000 batches | lr 0.00015 | ms/batch 40.49856 | loss 0.69935 | ppl     2.012\n",
      "|\n",
      "Source: [11  6 21  3 15  0 12  1 15 10]\n",
      "Target: [ 6 21  3 15  0 12  1 15 10  0]\n",
      "Teacher forcing: acc:0.61625\n",
      "Preds:  [10  6 10  5 10  0 10  1 10  1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  23 at step    92000 | time: 162.42s | valid loss 0.69957 | valid ppl    2.0129 | valid acc 0.616\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    92200 |  92200 batches | lr 0.00015 | ms/batch 45.77998 | loss 0.70007 | ppl     2.014\n",
      "| epoch   1 step    92400 |  92400 batches | lr 0.00015 | ms/batch 39.52681 | loss 0.70114 | ppl     2.016\n",
      "| epoch   1 step    92600 |  92600 batches | lr 0.00015 | ms/batch 40.11916 | loss 0.70249 | ppl     2.019\n",
      "| epoch   1 step    92800 |  92800 batches | lr 0.00015 | ms/batch 39.46631 | loss 0.70611 | ppl     2.026\n",
      "| epoch   1 step    93000 |  93000 batches | lr 0.00015 | ms/batch 39.60001 | loss 0.70240 | ppl     2.019\n",
      "| epoch   1 step    93200 |  93200 batches | lr 0.00015 | ms/batch 38.77967 | loss 0.70043 | ppl     2.015\n",
      "| epoch   1 step    93400 |  93400 batches | lr 0.00015 | ms/batch 40.14448 | loss 0.70138 | ppl     2.017\n",
      "| epoch   1 step    93600 |  93600 batches | lr 0.00015 | ms/batch 40.48905 | loss 0.69919 | ppl     2.012\n",
      "| epoch   1 step    93800 |  93800 batches | lr 0.00015 | ms/batch 39.27495 | loss 0.69948 | ppl     2.013\n",
      "| epoch   1 step    94000 |  94000 batches | lr 0.00015 | ms/batch 39.85306 | loss 0.69954 | ppl     2.013\n",
      "| epoch   1 step    94200 |  94200 batches | lr 0.00015 | ms/batch 39.69640 | loss 0.69803 | ppl     2.010\n",
      "| epoch   1 step    94400 |  94400 batches | lr 0.00015 | ms/batch 40.29590 | loss 0.70502 | ppl     2.024\n",
      "| epoch   1 step    94600 |  94600 batches | lr 0.00015 | ms/batch 40.44791 | loss 0.70028 | ppl     2.014\n",
      "| epoch   1 step    94800 |  94800 batches | lr 0.00015 | ms/batch 40.14485 | loss 0.70029 | ppl     2.014\n",
      "| epoch   1 step    95000 |  95000 batches | lr 0.00015 | ms/batch 39.54419 | loss 0.70000 | ppl     2.014\n",
      "| epoch   1 step    95200 |  95200 batches | lr 0.00015 | ms/batch 39.87841 | loss 0.70065 | ppl     2.015\n",
      "| epoch   1 step    95400 |  95400 batches | lr 0.00015 | ms/batch 38.98846 | loss 0.69956 | ppl     2.013\n",
      "| epoch   1 step    95600 |  95600 batches | lr 0.00015 | ms/batch 39.85152 | loss 0.69780 | ppl     2.009\n",
      "| epoch   1 step    95800 |  95800 batches | lr 0.00015 | ms/batch 39.40541 | loss 0.69956 | ppl     2.013\n",
      "| epoch   1 step    96000 |  96000 batches | lr 0.00015 | ms/batch 39.72179 | loss 0.70177 | ppl     2.017\n",
      "|\n",
      "Source: [35  2 18  1 28  8 33  6 18 10]\n",
      "Target: [ 2 18  1 28  8 33  6 18 10  1]\n",
      "Teacher forcing: acc:0.621875\n",
      "Preds:  [10  2 10  2 10 10 10  8 10  8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  24 at step    96000 | time: 160.22s | valid loss 0.69819 | valid ppl    2.0101 | valid acc 0.622\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    96200 |  96200 batches | lr 0.00015 | ms/batch 45.23241 | loss 0.69952 | ppl     2.013\n",
      "| epoch   1 step    96400 |  96400 batches | lr 0.00015 | ms/batch 39.19583 | loss 0.70039 | ppl     2.015\n",
      "| epoch   1 step    96600 |  96600 batches | lr 0.00015 | ms/batch 39.50022 | loss 0.70246 | ppl     2.019\n",
      "| epoch   1 step    96800 |  96800 batches | lr 0.00015 | ms/batch 39.27133 | loss 0.69952 | ppl     2.013\n",
      "| epoch   1 step    97000 |  97000 batches | lr 0.00015 | ms/batch 40.11116 | loss 0.69893 | ppl     2.012\n",
      "| epoch   1 step    97200 |  97200 batches | lr 0.00015 | ms/batch 40.53092 | loss 0.70145 | ppl     2.017\n",
      "| epoch   1 step    97400 |  97400 batches | lr 0.00015 | ms/batch 40.60269 | loss 0.69831 | ppl     2.010\n",
      "| epoch   1 step    97600 |  97600 batches | lr 0.00015 | ms/batch 40.42721 | loss 0.70016 | ppl     2.014\n",
      "| epoch   1 step    97800 |  97800 batches | lr 0.00015 | ms/batch 40.54385 | loss 0.69921 | ppl     2.012\n",
      "| epoch   1 step    98000 |  98000 batches | lr 0.00015 | ms/batch 39.90958 | loss 0.70061 | ppl     2.015\n",
      "| epoch   1 step    98200 |  98200 batches | lr 0.00015 | ms/batch 39.70834 | loss 0.70561 | ppl     2.025\n",
      "| epoch   1 step    98400 |  98400 batches | lr 0.00015 | ms/batch 41.19411 | loss 0.70057 | ppl     2.015\n",
      "| epoch   1 step    98600 |  98600 batches | lr 0.00015 | ms/batch 40.07406 | loss 0.70111 | ppl     2.016\n",
      "| epoch   1 step    98800 |  98800 batches | lr 0.00015 | ms/batch 40.71529 | loss 0.69900 | ppl     2.012\n",
      "| epoch   1 step    99000 |  99000 batches | lr 0.00015 | ms/batch 41.11779 | loss 0.70054 | ppl     2.015\n",
      "| epoch   1 step    99200 |  99200 batches | lr 0.00015 | ms/batch 39.53412 | loss 0.69985 | ppl     2.013\n",
      "| epoch   1 step    99400 |  99400 batches | lr 0.00015 | ms/batch 39.76253 | loss 0.70017 | ppl     2.014\n",
      "| epoch   1 step    99600 |  99600 batches | lr 0.00015 | ms/batch 39.40401 | loss 0.70152 | ppl     2.017\n",
      "| epoch   1 step    99800 |  99800 batches | lr 0.00015 | ms/batch 40.75062 | loss 0.70070 | ppl     2.015\n",
      "| epoch   1 step   100000 | 100000 batches | lr 0.00015 | ms/batch 39.24157 | loss 0.69942 | ppl     2.013\n",
      "|\n",
      "Source: [34  1 30  9 12  6 18  4 18 10]\n",
      "Target: [ 1 30  9 12  6 18  4 18 10  4]\n",
      "Teacher forcing: acc:0.624375\n",
      "Preds:  [10  1 10  9 10  9 10  4 10  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  25 at step   100000 | time: 161.33s | valid loss 0.69722 | valid ppl    2.0082 | valid acc 0.624\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   100200 | 100200 batches | lr 7.5e-05 | ms/batch 45.45272 | loss 0.69785 | ppl     2.009\n",
      "| epoch   1 step   100400 | 100400 batches | lr 7.5e-05 | ms/batch 40.18922 | loss 0.69865 | ppl     2.011\n",
      "| epoch   1 step   100600 | 100600 batches | lr 7.5e-05 | ms/batch 41.01603 | loss 0.69775 | ppl     2.009\n",
      "| epoch   1 step   100800 | 100800 batches | lr 7.5e-05 | ms/batch 40.60850 | loss 0.69794 | ppl     2.010\n",
      "| epoch   1 step   101000 | 101000 batches | lr 7.5e-05 | ms/batch 40.61856 | loss 0.69896 | ppl     2.012\n",
      "| epoch   1 step   101200 | 101200 batches | lr 7.5e-05 | ms/batch 42.19005 | loss 0.69811 | ppl     2.010\n",
      "| epoch   1 step   101400 | 101400 batches | lr 7.5e-05 | ms/batch 41.03859 | loss 0.69780 | ppl     2.009\n",
      "| epoch   1 step   101600 | 101600 batches | lr 7.5e-05 | ms/batch 40.64634 | loss 0.69696 | ppl     2.008\n",
      "| epoch   1 step   101800 | 101800 batches | lr 7.5e-05 | ms/batch 40.53592 | loss 0.69782 | ppl     2.009\n",
      "| epoch   1 step   102000 | 102000 batches | lr 7.5e-05 | ms/batch 40.90562 | loss 0.69781 | ppl     2.009\n",
      "| epoch   1 step   102200 | 102200 batches | lr 7.5e-05 | ms/batch 40.73445 | loss 0.69810 | ppl     2.010\n",
      "| epoch   1 step   102400 | 102400 batches | lr 7.5e-05 | ms/batch 40.84988 | loss 0.69726 | ppl     2.008\n",
      "| epoch   1 step   102600 | 102600 batches | lr 7.5e-05 | ms/batch 39.89184 | loss 0.69804 | ppl     2.010\n",
      "| epoch   1 step   102800 | 102800 batches | lr 7.5e-05 | ms/batch 39.98863 | loss 0.69716 | ppl     2.008\n",
      "| epoch   1 step   103000 | 103000 batches | lr 7.5e-05 | ms/batch 39.59223 | loss 0.69773 | ppl     2.009\n",
      "| epoch   1 step   103200 | 103200 batches | lr 7.5e-05 | ms/batch 40.02532 | loss 0.69815 | ppl     2.010\n",
      "| epoch   1 step   103400 | 103400 batches | lr 7.5e-05 | ms/batch 40.22876 | loss 0.69878 | ppl     2.011\n",
      "| epoch   1 step   103600 | 103600 batches | lr 7.5e-05 | ms/batch 41.01997 | loss 0.69795 | ppl     2.010\n",
      "| epoch   1 step   103800 | 103800 batches | lr 7.5e-05 | ms/batch 39.96629 | loss 0.69883 | ppl     2.011\n",
      "| epoch   1 step   104000 | 104000 batches | lr 7.5e-05 | ms/batch 39.83923 | loss 0.69812 | ppl     2.010\n",
      "|\n",
      "Source: [33  0 34  5 25  3 12  9 25 10]\n",
      "Target: [ 0 34  5 25  3 12  9 25 10  3]\n",
      "Teacher forcing: acc:0.6403125\n",
      "Preds:  [10  0 10  5 10  5 10  3 10  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  26 at step   104000 | time: 163.07s | valid loss 0.69271 | valid ppl    1.9991 | valid acc 0.64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   104200 | 104200 batches | lr 7.5e-05 | ms/batch 46.68190 | loss 0.69781 | ppl     2.009\n",
      "| epoch   1 step   104400 | 104400 batches | lr 7.5e-05 | ms/batch 40.03194 | loss 0.69752 | ppl     2.009\n",
      "| epoch   1 step   104600 | 104600 batches | lr 7.5e-05 | ms/batch 40.21146 | loss 0.69728 | ppl     2.008\n",
      "| epoch   1 step   104800 | 104800 batches | lr 7.5e-05 | ms/batch 40.48078 | loss 0.69668 | ppl     2.007\n",
      "| epoch   1 step   105000 | 105000 batches | lr 7.5e-05 | ms/batch 40.65230 | loss 0.69695 | ppl     2.008\n",
      "| epoch   1 step   105200 | 105200 batches | lr 7.5e-05 | ms/batch 41.02780 | loss 0.69652 | ppl     2.007\n",
      "| epoch   1 step   105400 | 105400 batches | lr 7.5e-05 | ms/batch 41.49590 | loss 0.69813 | ppl     2.010\n",
      "| epoch   1 step   105600 | 105600 batches | lr 7.5e-05 | ms/batch 40.77499 | loss 0.69708 | ppl     2.008\n",
      "| epoch   1 step   105800 | 105800 batches | lr 7.5e-05 | ms/batch 41.09055 | loss 0.69700 | ppl     2.008\n",
      "| epoch   1 step   106000 | 106000 batches | lr 7.5e-05 | ms/batch 40.39058 | loss 0.69934 | ppl     2.012\n",
      "| epoch   1 step   106200 | 106200 batches | lr 7.5e-05 | ms/batch 40.48721 | loss 0.69892 | ppl     2.012\n",
      "| epoch   1 step   106400 | 106400 batches | lr 7.5e-05 | ms/batch 41.02344 | loss 0.69743 | ppl     2.009\n",
      "| epoch   1 step   106600 | 106600 batches | lr 7.5e-05 | ms/batch 41.37552 | loss 0.69730 | ppl     2.008\n",
      "| epoch   1 step   106800 | 106800 batches | lr 7.5e-05 | ms/batch 40.34756 | loss 0.69715 | ppl     2.008\n",
      "| epoch   1 step   107000 | 107000 batches | lr 7.5e-05 | ms/batch 40.61511 | loss 0.69727 | ppl     2.008\n",
      "| epoch   1 step   107200 | 107200 batches | lr 7.5e-05 | ms/batch 40.36712 | loss 0.69757 | ppl     2.009\n",
      "| epoch   1 step   107400 | 107400 batches | lr 7.5e-05 | ms/batch 40.58977 | loss 0.69828 | ppl     2.010\n",
      "| epoch   1 step   107600 | 107600 batches | lr 7.5e-05 | ms/batch 40.30316 | loss 0.69688 | ppl     2.007\n",
      "| epoch   1 step   107800 | 107800 batches | lr 7.5e-05 | ms/batch 40.62339 | loss 0.69907 | ppl     2.012\n",
      "| epoch   1 step   108000 | 108000 batches | lr 7.5e-05 | ms/batch 41.57239 | loss 0.69764 | ppl     2.009\n",
      "|\n",
      "Source: [18  3 11  4 24  1 31  9 11 10]\n",
      "Target: [ 3 11  4 24  1 31  9 11 10  4]\n",
      "Teacher forcing: acc:0.620625\n",
      "Preds:  [10  3 10  3 10  1 10  9 10  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  27 at step   108000 | time: 164.04s | valid loss 0.69451 | valid ppl    2.0027 | valid acc 0.621\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   108200 | 108200 batches | lr 7.5e-05 | ms/batch 47.30569 | loss 0.69548 | ppl     2.005\n",
      "| epoch   1 step   108400 | 108400 batches | lr 7.5e-05 | ms/batch 39.97643 | loss 0.69639 | ppl     2.006\n",
      "| epoch   1 step   108600 | 108600 batches | lr 7.5e-05 | ms/batch 41.36089 | loss 0.70008 | ppl     2.014\n",
      "| epoch   1 step   108800 | 108800 batches | lr 7.5e-05 | ms/batch 40.66410 | loss 0.69785 | ppl     2.009\n",
      "| epoch   1 step   109000 | 109000 batches | lr 7.5e-05 | ms/batch 40.25205 | loss 0.69796 | ppl     2.010\n",
      "| epoch   1 step   109200 | 109200 batches | lr 7.5e-05 | ms/batch 40.98960 | loss 0.69751 | ppl     2.009\n",
      "| epoch   1 step   109400 | 109400 batches | lr 7.5e-05 | ms/batch 39.56927 | loss 0.69864 | ppl     2.011\n",
      "| epoch   1 step   109600 | 109600 batches | lr 7.5e-05 | ms/batch 40.14009 | loss 0.69561 | ppl     2.005\n",
      "| epoch   1 step   109800 | 109800 batches | lr 7.5e-05 | ms/batch 41.03894 | loss 0.69601 | ppl     2.006\n",
      "| epoch   1 step   110000 | 110000 batches | lr 7.5e-05 | ms/batch 39.53414 | loss 0.69787 | ppl     2.009\n",
      "| epoch   1 step   110200 | 110200 batches | lr 7.5e-05 | ms/batch 41.42164 | loss 0.69797 | ppl     2.010\n",
      "| epoch   1 step   110400 | 110400 batches | lr 7.5e-05 | ms/batch 40.63918 | loss 0.69664 | ppl     2.007\n",
      "| epoch   1 step   110600 | 110600 batches | lr 7.5e-05 | ms/batch 41.54088 | loss 0.69817 | ppl     2.010\n",
      "| epoch   1 step   110800 | 110800 batches | lr 7.5e-05 | ms/batch 40.96019 | loss 0.69691 | ppl     2.008\n",
      "| epoch   1 step   111000 | 111000 batches | lr 7.5e-05 | ms/batch 41.47944 | loss 0.69767 | ppl     2.009\n",
      "| epoch   1 step   111200 | 111200 batches | lr 7.5e-05 | ms/batch 41.34367 | loss 0.69772 | ppl     2.009\n",
      "| epoch   1 step   111400 | 111400 batches | lr 7.5e-05 | ms/batch 41.87316 | loss 0.69715 | ppl     2.008\n",
      "| epoch   1 step   111600 | 111600 batches | lr 7.5e-05 | ms/batch 42.12627 | loss 0.69785 | ppl     2.009\n",
      "| epoch   1 step   111800 | 111800 batches | lr 7.5e-05 | ms/batch 40.70118 | loss 0.69680 | ppl     2.007\n",
      "| epoch   1 step   112000 | 112000 batches | lr 7.5e-05 | ms/batch 41.36282 | loss 0.69823 | ppl     2.010\n",
      "|\n",
      "Source: [19  1 17  2 30  5 18  7 18 10]\n",
      "Target: [ 1 17  2 30  5 18  7 18 10  7]\n",
      "Teacher forcing: acc:0.6275\n",
      "Preds:  [10  1 10  2 10  5 10  1 10  1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  28 at step   112000 | time: 164.81s | valid loss 0.69483 | valid ppl    2.0034 | valid acc 0.627\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   112200 | 112200 batches | lr 7.5e-05 | ms/batch 47.48116 | loss 0.69720 | ppl     2.008\n",
      "| epoch   1 step   112400 | 112400 batches | lr 7.5e-05 | ms/batch 41.35287 | loss 0.69755 | ppl     2.009\n",
      "| epoch   1 step   112600 | 112600 batches | lr 7.5e-05 | ms/batch 40.82164 | loss 0.69668 | ppl     2.007\n",
      "| epoch   1 step   112800 | 112800 batches | lr 7.5e-05 | ms/batch 41.32532 | loss 0.69759 | ppl     2.009\n",
      "| epoch   1 step   113000 | 113000 batches | lr 7.5e-05 | ms/batch 41.05692 | loss 0.69659 | ppl     2.007\n",
      "| epoch   1 step   113200 | 113200 batches | lr 7.5e-05 | ms/batch 40.61773 | loss 0.69691 | ppl     2.008\n",
      "| epoch   1 step   113400 | 113400 batches | lr 7.5e-05 | ms/batch 40.04845 | loss 0.69782 | ppl     2.009\n",
      "| epoch   1 step   113600 | 113600 batches | lr 7.5e-05 | ms/batch 40.42114 | loss 0.69810 | ppl     2.010\n",
      "| epoch   1 step   113800 | 113800 batches | lr 7.5e-05 | ms/batch 40.21163 | loss 0.69740 | ppl     2.009\n",
      "| epoch   1 step   114000 | 114000 batches | lr 7.5e-05 | ms/batch 40.22222 | loss 0.69816 | ppl     2.010\n",
      "| epoch   1 step   114200 | 114200 batches | lr 7.5e-05 | ms/batch 39.49889 | loss 0.69659 | ppl     2.007\n",
      "| epoch   1 step   114400 | 114400 batches | lr 7.5e-05 | ms/batch 39.67838 | loss 0.69735 | ppl     2.008\n",
      "| epoch   1 step   114600 | 114600 batches | lr 7.5e-05 | ms/batch 40.07272 | loss 0.69850 | ppl     2.011\n",
      "| epoch   1 step   114800 | 114800 batches | lr 7.5e-05 | ms/batch 39.91977 | loss 0.69754 | ppl     2.009\n",
      "| epoch   1 step   115000 | 115000 batches | lr 7.5e-05 | ms/batch 40.26094 | loss 0.70452 | ppl     2.023\n",
      "| epoch   1 step   115200 | 115200 batches | lr 7.5e-05 | ms/batch 40.64766 | loss 0.69660 | ppl     2.007\n",
      "| epoch   1 step   115400 | 115400 batches | lr 7.5e-05 | ms/batch 40.28930 | loss 0.69684 | ppl     2.007\n",
      "| epoch   1 step   115600 | 115600 batches | lr 7.5e-05 | ms/batch 39.99810 | loss 0.69701 | ppl     2.008\n",
      "| epoch   1 step   115800 | 115800 batches | lr 7.5e-05 | ms/batch 41.01852 | loss 0.69654 | ppl     2.007\n",
      "| epoch   1 step   116000 | 116000 batches | lr 7.5e-05 | ms/batch 41.04450 | loss 0.69741 | ppl     2.009\n",
      "|\n",
      "Source: [12  2 20  4 36  8 14  7 14 10]\n",
      "Target: [ 2 20  4 36  8 14  7 14 10  7]\n",
      "Teacher forcing: acc:0.6196875\n",
      "Preds:  [10  2 10  2 10 10 10  8 10  8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  29 at step   116000 | time: 163.18s | valid loss 0.69554 | valid ppl    2.0048 | valid acc 0.62\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   116200 | 116200 batches | lr 7.5e-05 | ms/batch 47.40099 | loss 0.69673 | ppl     2.007\n",
      "| epoch   1 step   116400 | 116400 batches | lr 7.5e-05 | ms/batch 41.41538 | loss 0.69742 | ppl     2.009\n",
      "| epoch   1 step   116600 | 116600 batches | lr 7.5e-05 | ms/batch 40.38460 | loss 0.69728 | ppl     2.008\n",
      "| epoch   1 step   116800 | 116800 batches | lr 7.5e-05 | ms/batch 40.44610 | loss 0.69840 | ppl     2.011\n",
      "| epoch   1 step   117000 | 117000 batches | lr 7.5e-05 | ms/batch 41.34841 | loss 0.69631 | ppl     2.006\n",
      "| epoch   1 step   117200 | 117200 batches | lr 7.5e-05 | ms/batch 40.01956 | loss 0.69626 | ppl     2.006\n",
      "| epoch   1 step   117400 | 117400 batches | lr 7.5e-05 | ms/batch 40.33800 | loss 0.69757 | ppl     2.009\n",
      "| epoch   1 step   117600 | 117600 batches | lr 7.5e-05 | ms/batch 40.80299 | loss 0.69654 | ppl     2.007\n",
      "| epoch   1 step   117800 | 117800 batches | lr 7.5e-05 | ms/batch 40.69293 | loss 0.69671 | ppl     2.007\n",
      "| epoch   1 step   118000 | 118000 batches | lr 7.5e-05 | ms/batch 40.43710 | loss 0.69661 | ppl     2.007\n",
      "| epoch   1 step   118200 | 118200 batches | lr 7.5e-05 | ms/batch 40.54484 | loss 0.69750 | ppl     2.009\n",
      "| epoch   1 step   118400 | 118400 batches | lr 7.5e-05 | ms/batch 40.40000 | loss 0.69671 | ppl     2.007\n",
      "| epoch   1 step   118600 | 118600 batches | lr 7.5e-05 | ms/batch 40.54219 | loss 0.69721 | ppl     2.008\n",
      "| epoch   1 step   118800 | 118800 batches | lr 7.5e-05 | ms/batch 41.75685 | loss 0.69629 | ppl     2.006\n",
      "| epoch   1 step   119000 | 119000 batches | lr 7.5e-05 | ms/batch 41.03108 | loss 0.69755 | ppl     2.009\n",
      "| epoch   1 step   119200 | 119200 batches | lr 7.5e-05 | ms/batch 40.96805 | loss 0.69576 | ppl     2.005\n",
      "| epoch   1 step   119400 | 119400 batches | lr 7.5e-05 | ms/batch 41.40518 | loss 0.69728 | ppl     2.008\n",
      "| epoch   1 step   119600 | 119600 batches | lr 7.5e-05 | ms/batch 41.22615 | loss 0.69760 | ppl     2.009\n",
      "| epoch   1 step   119800 | 119800 batches | lr 7.5e-05 | ms/batch 40.71176 | loss 0.69778 | ppl     2.009\n",
      "| epoch   1 step   120000 | 120000 batches | lr 7.5e-05 | ms/batch 41.05044 | loss 0.69676 | ppl     2.007\n",
      "|\n",
      "Source: [25  5 29  7 15  2 33  0 29 10]\n",
      "Target: [ 5 29  7 15  2 33  0 29 10  7]\n",
      "Teacher forcing: acc:0.6284375\n",
      "Preds:  [10  5 10  5 10  2 10  0 10  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  30 at step   120000 | time: 164.63s | valid loss 0.69514 | valid ppl    2.0040 | valid acc 0.628\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   120200 | 120200 batches | lr 3.75e-05 | ms/batch 46.38821 | loss 0.69596 | ppl     2.006\n",
      "| epoch   1 step   120400 | 120400 batches | lr 3.75e-05 | ms/batch 40.27410 | loss 0.69621 | ppl     2.006\n",
      "| epoch   1 step   120600 | 120600 batches | lr 3.75e-05 | ms/batch 40.51392 | loss 0.69576 | ppl     2.005\n",
      "| epoch   1 step   120800 | 120800 batches | lr 3.75e-05 | ms/batch 40.26295 | loss 0.69559 | ppl     2.005\n",
      "| epoch   1 step   121000 | 121000 batches | lr 3.75e-05 | ms/batch 40.10374 | loss 0.69676 | ppl     2.007\n",
      "| epoch   1 step   121200 | 121200 batches | lr 3.75e-05 | ms/batch 39.50232 | loss 0.69731 | ppl     2.008\n",
      "| epoch   1 step   121400 | 121400 batches | lr 3.75e-05 | ms/batch 39.62680 | loss 0.69674 | ppl     2.007\n",
      "| epoch   1 step   121600 | 121600 batches | lr 3.75e-05 | ms/batch 40.29462 | loss 0.69671 | ppl     2.007\n",
      "| epoch   1 step   121800 | 121800 batches | lr 3.75e-05 | ms/batch 39.59226 | loss 0.69533 | ppl     2.004\n",
      "| epoch   1 step   122000 | 122000 batches | lr 3.75e-05 | ms/batch 40.71499 | loss 0.69598 | ppl     2.006\n",
      "| epoch   1 step   122200 | 122200 batches | lr 3.75e-05 | ms/batch 40.39841 | loss 0.69547 | ppl     2.005\n",
      "| epoch   1 step   122400 | 122400 batches | lr 3.75e-05 | ms/batch 40.67112 | loss 0.69650 | ppl     2.007\n",
      "| epoch   1 step   122600 | 122600 batches | lr 3.75e-05 | ms/batch 41.32713 | loss 0.69693 | ppl     2.008\n",
      "| epoch   1 step   122800 | 122800 batches | lr 3.75e-05 | ms/batch 40.72083 | loss 0.69621 | ppl     2.006\n",
      "| epoch   1 step   123000 | 123000 batches | lr 3.75e-05 | ms/batch 41.71375 | loss 0.69614 | ppl     2.006\n",
      "| epoch   1 step   123200 | 123200 batches | lr 3.75e-05 | ms/batch 41.34303 | loss 0.69544 | ppl     2.005\n",
      "| epoch   1 step   123400 | 123400 batches | lr 3.75e-05 | ms/batch 41.76578 | loss 0.69603 | ppl     2.006\n",
      "| epoch   1 step   123600 | 123600 batches | lr 3.75e-05 | ms/batch 42.94264 | loss 0.69608 | ppl     2.006\n",
      "| epoch   1 step   123800 | 123800 batches | lr 3.75e-05 | ms/batch 41.94066 | loss 0.69592 | ppl     2.006\n",
      "| epoch   1 step   124000 | 124000 batches | lr 3.75e-05 | ms/batch 41.18340 | loss 0.69590 | ppl     2.006\n",
      "maslina\n",
      "|\n",
      "Source: [11  2 19  9 35  3 27  1 35 10]\n",
      "Target: [ 2 19  9 35  3 27  1 35 10  3]\n",
      "Teacher forcing: acc:0.6275510204081632\n",
      "Preds:  [10  2 10  9 10  3 10  1 10  1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  31 at step   124000 | time: 164.25s | valid loss 0.69521 | valid ppl    2.0041 | valid acc 0.628\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   124200 | 124200 batches | lr 3.75e-05 | ms/batch 48.01500 | loss 0.69760 | ppl     2.009\n",
      "| epoch   1 step   124400 | 124400 batches | lr 3.75e-05 | ms/batch 41.89751 | loss 0.69563 | ppl     2.005\n",
      "| epoch   1 step   124600 | 124600 batches | lr 3.75e-05 | ms/batch 42.27455 | loss 0.69571 | ppl     2.005\n",
      "| epoch   1 step   124800 | 124800 batches | lr 3.75e-05 | ms/batch 42.23586 | loss 0.69592 | ppl     2.006\n",
      "| epoch   1 step   125000 | 125000 batches | lr 3.75e-05 | ms/batch 41.28620 | loss 0.69547 | ppl     2.005\n",
      "| epoch   1 step   125200 | 125200 batches | lr 3.75e-05 | ms/batch 41.59795 | loss 0.69518 | ppl     2.004\n",
      "| epoch   1 step   125400 | 125400 batches | lr 3.75e-05 | ms/batch 42.49286 | loss 0.69716 | ppl     2.008\n",
      "| epoch   1 step   125600 | 125600 batches | lr 3.75e-05 | ms/batch 43.52609 | loss 0.69513 | ppl     2.004\n",
      "| epoch   1 step   125800 | 125800 batches | lr 3.75e-05 | ms/batch 42.46029 | loss 0.69566 | ppl     2.005\n",
      "| epoch   1 step   126000 | 126000 batches | lr 3.75e-05 | ms/batch 41.15129 | loss 0.69590 | ppl     2.006\n",
      "| epoch   1 step   126200 | 126200 batches | lr 3.75e-05 | ms/batch 41.29016 | loss 0.69554 | ppl     2.005\n",
      "| epoch   1 step   126400 | 126400 batches | lr 3.75e-05 | ms/batch 41.23323 | loss 0.69589 | ppl     2.005\n",
      "| epoch   1 step   126600 | 126600 batches | lr 3.75e-05 | ms/batch 42.18057 | loss 0.69569 | ppl     2.005\n",
      "| epoch   1 step   126800 | 126800 batches | lr 3.75e-05 | ms/batch 41.92210 | loss 0.69516 | ppl     2.004\n",
      "| epoch   1 step   127000 | 127000 batches | lr 3.75e-05 | ms/batch 42.49319 | loss 0.69513 | ppl     2.004\n",
      "| epoch   1 step   127200 | 127200 batches | lr 3.75e-05 | ms/batch 42.59042 | loss 0.69603 | ppl     2.006\n",
      "| epoch   1 step   127400 | 127400 batches | lr 3.75e-05 | ms/batch 41.08302 | loss 0.69525 | ppl     2.004\n",
      "| epoch   1 step   127600 | 127600 batches | lr 3.75e-05 | ms/batch 41.04993 | loss 0.69689 | ppl     2.007\n",
      "| epoch   1 step   127800 | 127800 batches | lr 3.75e-05 | ms/batch 41.18669 | loss 0.69527 | ppl     2.004\n",
      "| epoch   1 step   128000 | 128000 batches | lr 3.75e-05 | ms/batch 40.94046 | loss 0.69513 | ppl     2.004\n",
      "|\n",
      "Source: [32  0 31  3 20  8 15  5 15 10]\n",
      "Target: [ 0 31  3 20  8 15  5 15 10  5]\n",
      "Teacher forcing: acc:0.6284375\n",
      "Preds:  [10  0 10  3 10 10 10  3 10  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  32 at step   128000 | time: 168.64s | valid loss 0.69560 | valid ppl    2.0049 | valid acc 0.628\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   128200 | 128200 batches | lr 3.75e-05 | ms/batch 47.44393 | loss 0.69553 | ppl     2.005\n",
      "| epoch   1 step   128400 | 128400 batches | lr 3.75e-05 | ms/batch 42.16230 | loss 0.69658 | ppl     2.007\n",
      "| epoch   1 step   128600 | 128600 batches | lr 3.75e-05 | ms/batch 41.16699 | loss 0.69559 | ppl     2.005\n",
      "| epoch   1 step   128800 | 128800 batches | lr 3.75e-05 | ms/batch 42.56389 | loss 0.69682 | ppl     2.007\n",
      "| epoch   1 step   129000 | 129000 batches | lr 3.75e-05 | ms/batch 42.20867 | loss 0.69658 | ppl     2.007\n",
      "| epoch   1 step   129200 | 129200 batches | lr 3.75e-05 | ms/batch 42.23552 | loss 0.69553 | ppl     2.005\n",
      "| epoch   1 step   129400 | 129400 batches | lr 3.75e-05 | ms/batch 41.28895 | loss 0.69538 | ppl     2.004\n",
      "| epoch   1 step   129600 | 129600 batches | lr 3.75e-05 | ms/batch 41.24069 | loss 0.69583 | ppl     2.005\n",
      "| epoch   1 step   129800 | 129800 batches | lr 3.75e-05 | ms/batch 40.77825 | loss 0.69516 | ppl     2.004\n",
      "| epoch   1 step   130000 | 130000 batches | lr 3.75e-05 | ms/batch 42.01109 | loss 0.69668 | ppl     2.007\n",
      "| epoch   1 step   130200 | 130200 batches | lr 3.75e-05 | ms/batch 40.44202 | loss 0.69411 | ppl     2.002\n",
      "| epoch   1 step   130400 | 130400 batches | lr 3.75e-05 | ms/batch 41.18800 | loss 0.69543 | ppl     2.005\n",
      "| epoch   1 step   130600 | 130600 batches | lr 3.75e-05 | ms/batch 41.96758 | loss 0.69506 | ppl     2.004\n",
      "| epoch   1 step   130800 | 130800 batches | lr 3.75e-05 | ms/batch 41.23548 | loss 0.69590 | ppl     2.006\n",
      "| epoch   1 step   131000 | 131000 batches | lr 3.75e-05 | ms/batch 42.53451 | loss 0.69675 | ppl     2.007\n",
      "| epoch   1 step   131200 | 131200 batches | lr 3.75e-05 | ms/batch 43.73197 | loss 0.69486 | ppl     2.003\n",
      "| epoch   1 step   131400 | 131400 batches | lr 3.75e-05 | ms/batch 44.18409 | loss 0.69567 | ppl     2.005\n",
      "| epoch   1 step   131600 | 131600 batches | lr 3.75e-05 | ms/batch 44.73391 | loss 0.69466 | ppl     2.003\n",
      "| epoch   1 step   131800 | 131800 batches | lr 3.75e-05 | ms/batch 45.45536 | loss 0.69682 | ppl     2.007\n",
      "| epoch   1 step   132000 | 132000 batches | lr 3.75e-05 | ms/batch 44.45736 | loss 0.69631 | ppl     2.006\n",
      "|\n",
      "Source: [36  9 20  0 21  7 23  5 23 10]\n",
      "Target: [ 9 20  0 21  7 23  5 23 10  5]\n",
      "Teacher forcing: acc:0.6203125\n",
      "Preds:  [10  9 10  9 10  9 10  7 10  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  33 at step   132000 | time: 170.69s | valid loss 0.69521 | valid ppl    2.0041 | valid acc 0.62\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   132200 | 132200 batches | lr 3.75e-05 | ms/batch 51.08447 | loss 0.69555 | ppl     2.005\n",
      "| epoch   1 step   132400 | 132400 batches | lr 3.75e-05 | ms/batch 44.16086 | loss 0.69503 | ppl     2.004\n",
      "| epoch   1 step   132600 | 132600 batches | lr 3.75e-05 | ms/batch 44.66151 | loss 0.69583 | ppl     2.005\n",
      "| epoch   1 step   132800 | 132800 batches | lr 3.75e-05 | ms/batch 43.64123 | loss 0.69549 | ppl     2.005\n",
      "| epoch   1 step   133000 | 133000 batches | lr 3.75e-05 | ms/batch 42.80897 | loss 0.69536 | ppl     2.004\n",
      "| epoch   1 step   133200 | 133200 batches | lr 3.75e-05 | ms/batch 43.19022 | loss 0.69480 | ppl     2.003\n",
      "| epoch   1 step   133400 | 133400 batches | lr 3.75e-05 | ms/batch 41.15382 | loss 0.69542 | ppl     2.005\n",
      "| epoch   1 step   133600 | 133600 batches | lr 3.75e-05 | ms/batch 41.72060 | loss 0.69451 | ppl     2.003\n",
      "| epoch   1 step   133800 | 133800 batches | lr 3.75e-05 | ms/batch 41.25764 | loss 0.69564 | ppl     2.005\n",
      "| epoch   1 step   134000 | 134000 batches | lr 3.75e-05 | ms/batch 41.03274 | loss 0.69640 | ppl     2.007\n",
      "| epoch   1 step   134200 | 134200 batches | lr 3.75e-05 | ms/batch 41.26955 | loss 0.69525 | ppl     2.004\n",
      "| epoch   1 step   134400 | 134400 batches | lr 3.75e-05 | ms/batch 41.09182 | loss 0.69686 | ppl     2.007\n",
      "| epoch   1 step   134600 | 134600 batches | lr 3.75e-05 | ms/batch 41.82092 | loss 0.69846 | ppl     2.011\n",
      "| epoch   1 step   134800 | 134800 batches | lr 3.75e-05 | ms/batch 43.16663 | loss 0.69603 | ppl     2.006\n",
      "| epoch   1 step   135000 | 135000 batches | lr 3.75e-05 | ms/batch 43.62196 | loss 0.69600 | ppl     2.006\n",
      "| epoch   1 step   135200 | 135200 batches | lr 3.75e-05 | ms/batch 43.94045 | loss 0.69584 | ppl     2.005\n",
      "| epoch   1 step   135400 | 135400 batches | lr 3.75e-05 | ms/batch 43.49285 | loss 0.69521 | ppl     2.004\n",
      "| epoch   1 step   135600 | 135600 batches | lr 3.75e-05 | ms/batch 43.94694 | loss 0.69528 | ppl     2.004\n",
      "| epoch   1 step   135800 | 135800 batches | lr 3.75e-05 | ms/batch 43.75101 | loss 0.69492 | ppl     2.004\n",
      "| epoch   1 step   136000 | 136000 batches | lr 3.75e-05 | ms/batch 44.34586 | loss 0.69545 | ppl     2.005\n",
      "|\n",
      "Source: [21  2 34  4 17  5 36  0 21 10]\n",
      "Target: [ 2 34  4 17  5 36  0 21 10  2]\n",
      "Teacher forcing: acc:0.6228125\n",
      "Preds:  [10  2 10  4 10  4 10  0 10  4]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  34 at step   136000 | time: 173.05s | valid loss 0.69542 | valid ppl    2.0045 | valid acc 0.623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   136200 | 136200 batches | lr 1.87e-05 | ms/batch 50.25199 | loss 0.69479 | ppl     2.003\n",
      "| epoch   1 step   136400 | 136400 batches | lr 1.87e-05 | ms/batch 44.27392 | loss 0.69423 | ppl     2.002\n",
      "| epoch   1 step   136600 | 136600 batches | lr 1.87e-05 | ms/batch 43.57045 | loss 0.69521 | ppl     2.004\n",
      "| epoch   1 step   136800 | 136800 batches | lr 1.87e-05 | ms/batch 43.35952 | loss 0.69528 | ppl     2.004\n",
      "| epoch   1 step   137000 | 137000 batches | lr 1.87e-05 | ms/batch 43.95803 | loss 0.69579 | ppl     2.005\n",
      "| epoch   1 step   137200 | 137200 batches | lr 1.87e-05 | ms/batch 43.29149 | loss 0.69516 | ppl     2.004\n",
      "| epoch   1 step   137400 | 137400 batches | lr 1.87e-05 | ms/batch 43.85331 | loss 0.69462 | ppl     2.003\n",
      "| epoch   1 step   137600 | 137600 batches | lr 1.87e-05 | ms/batch 43.97432 | loss 0.69270 | ppl     1.999\n",
      "| epoch   1 step   137800 | 137800 batches | lr 1.87e-05 | ms/batch 43.48168 | loss 0.69573 | ppl     2.005\n",
      "| epoch   1 step   138000 | 138000 batches | lr 1.87e-05 | ms/batch 42.59827 | loss 0.69379 | ppl     2.001\n",
      "| epoch   1 step   138200 | 138200 batches | lr 1.87e-05 | ms/batch 42.82394 | loss 0.69467 | ppl     2.003\n",
      "| epoch   1 step   138400 | 138400 batches | lr 1.87e-05 | ms/batch 43.35905 | loss 0.69384 | ppl     2.001\n",
      "| epoch   1 step   138600 | 138600 batches | lr 1.87e-05 | ms/batch 43.50904 | loss 0.69445 | ppl     2.003\n",
      "| epoch   1 step   138800 | 138800 batches | lr 1.87e-05 | ms/batch 44.12552 | loss 0.69391 | ppl     2.002\n",
      "| epoch   1 step   139000 | 139000 batches | lr 1.87e-05 | ms/batch 42.16315 | loss 0.69457 | ppl     2.003\n",
      "| epoch   1 step   139200 | 139200 batches | lr 1.87e-05 | ms/batch 40.85337 | loss 0.69502 | ppl     2.004\n",
      "| epoch   1 step   139400 | 139400 batches | lr 1.87e-05 | ms/batch 41.06917 | loss 0.69537 | ppl     2.004\n",
      "| epoch   1 step   139600 | 139600 batches | lr 1.87e-05 | ms/batch 40.83483 | loss 0.69449 | ppl     2.003\n",
      "| epoch   1 step   139800 | 139800 batches | lr 1.87e-05 | ms/batch 41.19447 | loss 0.69471 | ppl     2.003\n",
      "| epoch   1 step   140000 | 140000 batches | lr 1.87e-05 | ms/batch 41.93945 | loss 0.69502 | ppl     2.004\n",
      "|\n",
      "Source: [11  5 12  0 29  9 20  2 11 10]\n",
      "Target: [ 5 12  0 29  9 20  2 11 10  5]\n",
      "Teacher forcing: acc:0.621875\n",
      "Preds:  [10  5 10  0 10  5 10  2 10  2]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  35 at step   140000 | time: 172.78s | valid loss 0.69627 | valid ppl    2.0063 | valid acc 0.622\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   140200 | 140200 batches | lr 1.87e-05 | ms/batch 46.98585 | loss 0.69523 | ppl     2.004\n",
      "| epoch   1 step   140400 | 140400 batches | lr 1.87e-05 | ms/batch 41.46471 | loss 0.69537 | ppl     2.004\n",
      "| epoch   1 step   140600 | 140600 batches | lr 1.87e-05 | ms/batch 41.33160 | loss 0.69306 | ppl     2.000\n",
      "| epoch   1 step   140800 | 140800 batches | lr 1.87e-05 | ms/batch 41.15381 | loss 0.69416 | ppl     2.002\n",
      "| epoch   1 step   141000 | 141000 batches | lr 1.87e-05 | ms/batch 40.72937 | loss 0.69477 | ppl     2.003\n",
      "| epoch   1 step   141200 | 141200 batches | lr 1.87e-05 | ms/batch 39.98167 | loss 0.69551 | ppl     2.005\n",
      "| epoch   1 step   141400 | 141400 batches | lr 1.87e-05 | ms/batch 40.68912 | loss 0.69472 | ppl     2.003\n",
      "| epoch   1 step   141600 | 141600 batches | lr 1.87e-05 | ms/batch 40.86484 | loss 0.69548 | ppl     2.005\n",
      "| epoch   1 step   141800 | 141800 batches | lr 1.87e-05 | ms/batch 41.19091 | loss 0.69318 | ppl     2.000\n",
      "| epoch   1 step   142000 | 142000 batches | lr 1.87e-05 | ms/batch 40.32456 | loss 0.69524 | ppl     2.004\n",
      "| epoch   1 step   142200 | 142200 batches | lr 1.87e-05 | ms/batch 42.08940 | loss 0.69582 | ppl     2.005\n",
      "| epoch   1 step   142400 | 142400 batches | lr 1.87e-05 | ms/batch 41.92957 | loss 0.69443 | ppl     2.003\n",
      "| epoch   1 step   142600 | 142600 batches | lr 1.87e-05 | ms/batch 42.80140 | loss 0.69493 | ppl     2.004\n",
      "| epoch   1 step   142800 | 142800 batches | lr 1.87e-05 | ms/batch 44.04030 | loss 0.69515 | ppl     2.004\n",
      "| epoch   1 step   143000 | 143000 batches | lr 1.87e-05 | ms/batch 43.80490 | loss 0.69476 | ppl     2.003\n",
      "| epoch   1 step   143200 | 143200 batches | lr 1.87e-05 | ms/batch 43.56179 | loss 0.69435 | ppl     2.002\n",
      "| epoch   1 step   143400 | 143400 batches | lr 1.87e-05 | ms/batch 43.37416 | loss 0.69560 | ppl     2.005\n",
      "| epoch   1 step   143600 | 143600 batches | lr 1.87e-05 | ms/batch 44.16483 | loss 0.69505 | ppl     2.004\n",
      "| epoch   1 step   143800 | 143800 batches | lr 1.87e-05 | ms/batch 43.35509 | loss 0.69476 | ppl     2.003\n",
      "| epoch   1 step   144000 | 144000 batches | lr 1.87e-05 | ms/batch 43.95350 | loss 0.69617 | ppl     2.006\n",
      "|\n",
      "Source: [27  8 24  3 28  2 13  0 13 10]\n",
      "Target: [ 8 24  3 28  2 13  0 13 10  0]\n",
      "Teacher forcing: acc:0.6221875\n",
      "Preds:  [10 10 10  3 10  3 10  0 10  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  36 at step   144000 | time: 169.64s | valid loss 0.69545 | valid ppl    2.0046 | valid acc 0.622\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   144200 | 144200 batches | lr 1.87e-05 | ms/batch 50.49757 | loss 0.69484 | ppl     2.003\n",
      "| epoch   1 step   144400 | 144400 batches | lr 1.87e-05 | ms/batch 42.56986 | loss 0.69496 | ppl     2.004\n",
      "| epoch   1 step   144600 | 144600 batches | lr 1.87e-05 | ms/batch 42.41297 | loss 0.69375 | ppl     2.001\n",
      "| epoch   1 step   144800 | 144800 batches | lr 1.87e-05 | ms/batch 41.99867 | loss 0.69477 | ppl     2.003\n",
      "| epoch   1 step   145000 | 145000 batches | lr 1.87e-05 | ms/batch 42.00519 | loss 0.69390 | ppl     2.002\n",
      "| epoch   1 step   145200 | 145200 batches | lr 1.87e-05 | ms/batch 41.50095 | loss 0.69519 | ppl     2.004\n",
      "| epoch   1 step   145400 | 145400 batches | lr 1.87e-05 | ms/batch 42.39549 | loss 0.69565 | ppl     2.005\n",
      "| epoch   1 step   145600 | 145600 batches | lr 1.87e-05 | ms/batch 41.82173 | loss 0.69448 | ppl     2.003\n",
      "| epoch   1 step   145800 | 145800 batches | lr 1.87e-05 | ms/batch 41.81380 | loss 0.69362 | ppl     2.001\n",
      "| epoch   1 step   146000 | 146000 batches | lr 1.87e-05 | ms/batch 41.90382 | loss 0.69426 | ppl     2.002\n",
      "| epoch   1 step   146200 | 146200 batches | lr 1.87e-05 | ms/batch 41.28786 | loss 0.69441 | ppl     2.003\n",
      "| epoch   1 step   146400 | 146400 batches | lr 1.87e-05 | ms/batch 41.73323 | loss 0.69557 | ppl     2.005\n",
      "| epoch   1 step   146600 | 146600 batches | lr 1.87e-05 | ms/batch 41.81313 | loss 0.69465 | ppl     2.003\n",
      "| epoch   1 step   146800 | 146800 batches | lr 1.87e-05 | ms/batch 41.64037 | loss 0.69325 | ppl     2.000\n",
      "| epoch   1 step   147000 | 147000 batches | lr 1.87e-05 | ms/batch 41.03040 | loss 0.69468 | ppl     2.003\n",
      "| epoch   1 step   147200 | 147200 batches | lr 1.87e-05 | ms/batch 41.63108 | loss 0.69486 | ppl     2.003\n",
      "| epoch   1 step   147400 | 147400 batches | lr 1.87e-05 | ms/batch 42.04419 | loss 0.69396 | ppl     2.002\n",
      "| epoch   1 step   147600 | 147600 batches | lr 1.87e-05 | ms/batch 42.06234 | loss 0.69489 | ppl     2.003\n",
      "| epoch   1 step   147800 | 147800 batches | lr 1.87e-05 | ms/batch 42.11162 | loss 0.69511 | ppl     2.004\n",
      "| epoch   1 step   148000 | 148000 batches | lr 1.87e-05 | ms/batch 42.32481 | loss 0.69412 | ppl     2.002\n",
      "|\n",
      "Source: [12  7 36  2 13  0 27  9 12 10]\n",
      "Target: [ 7 36  2 13  0 27  9 12 10  7]\n",
      "Teacher forcing: acc:0.6246875\n",
      "Preds:  [10  1 10  2 10  2 10  7 10  2]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  37 at step   148000 | time: 169.24s | valid loss 0.69533 | valid ppl    2.0044 | valid acc 0.625\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   148200 | 148200 batches | lr 1.87e-05 | ms/batch 47.44152 | loss 0.69436 | ppl     2.002\n",
      "| epoch   1 step   148400 | 148400 batches | lr 1.87e-05 | ms/batch 41.56194 | loss 0.69374 | ppl     2.001\n",
      "| epoch   1 step   148600 | 148600 batches | lr 1.87e-05 | ms/batch 40.97784 | loss 0.69334 | ppl     2.000\n",
      "| epoch   1 step   148800 | 148800 batches | lr 1.87e-05 | ms/batch 41.77871 | loss 0.69422 | ppl     2.002\n",
      "| epoch   1 step   149000 | 149000 batches | lr 1.87e-05 | ms/batch 41.77184 | loss 0.69437 | ppl     2.002\n",
      "| epoch   1 step   149200 | 149200 batches | lr 1.87e-05 | ms/batch 42.35716 | loss 0.69538 | ppl     2.004\n",
      "| epoch   1 step   149400 | 149400 batches | lr 1.87e-05 | ms/batch 42.28139 | loss 0.69362 | ppl     2.001\n",
      "| epoch   1 step   149600 | 149600 batches | lr 1.87e-05 | ms/batch 41.29849 | loss 0.69520 | ppl     2.004\n",
      "| epoch   1 step   149800 | 149800 batches | lr 1.87e-05 | ms/batch 42.55579 | loss 0.69455 | ppl     2.003\n",
      "| epoch   1 step   150000 | 150000 batches | lr 1.87e-05 | ms/batch 41.55716 | loss 0.69312 | ppl     2.000\n",
      "| epoch   1 step   150200 | 150200 batches | lr 1.87e-05 | ms/batch 42.08690 | loss 0.69504 | ppl     2.004\n",
      "| epoch   1 step   150400 | 150400 batches | lr 1.87e-05 | ms/batch 41.35185 | loss 0.69358 | ppl     2.001\n",
      "| epoch   1 step   150600 | 150600 batches | lr 1.87e-05 | ms/batch 41.50648 | loss 0.69500 | ppl     2.004\n",
      "| epoch   1 step   150800 | 150800 batches | lr 1.87e-05 | ms/batch 41.70648 | loss 0.69516 | ppl     2.004\n",
      "| epoch   1 step   151000 | 151000 batches | lr 1.87e-05 | ms/batch 42.35003 | loss 0.69429 | ppl     2.002\n",
      "| epoch   1 step   151200 | 151200 batches | lr 1.87e-05 | ms/batch 41.51655 | loss 0.69397 | ppl     2.002\n",
      "| epoch   1 step   151400 | 151400 batches | lr 1.87e-05 | ms/batch 41.66121 | loss 0.69456 | ppl     2.003\n",
      "| epoch   1 step   151600 | 151600 batches | lr 1.87e-05 | ms/batch 41.57030 | loss 0.69411 | ppl     2.002\n",
      "| epoch   1 step   151800 | 151800 batches | lr 1.87e-05 | ms/batch 41.49899 | loss 0.69489 | ppl     2.003\n",
      "| epoch   1 step   152000 | 152000 batches | lr 1.87e-05 | ms/batch 41.04992 | loss 0.69312 | ppl     2.000\n",
      "|\n",
      "Source: [29  5 20  6 11  2 14  3 14 10]\n",
      "Target: [ 5 20  6 11  2 14  3 14 10  3]\n",
      "Teacher forcing: acc:0.6159375\n",
      "Preds:  [10  5 10  5 10  2 10  3 10  5]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  38 at step   152000 | time: 167.98s | valid loss 0.69759 | valid ppl    2.0089 | valid acc 0.616\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   152200 | 152200 batches | lr 9.37e-06 | ms/batch 46.26361 | loss 0.69432 | ppl     2.002\n",
      "| epoch   1 step   152400 | 152400 batches | lr 9.37e-06 | ms/batch 40.57857 | loss 0.69361 | ppl     2.001\n",
      "| epoch   1 step   152600 | 152600 batches | lr 9.37e-06 | ms/batch 41.76499 | loss 0.69507 | ppl     2.004\n",
      "| epoch   1 step   152800 | 152800 batches | lr 9.37e-06 | ms/batch 40.90200 | loss 0.69471 | ppl     2.003\n",
      "| epoch   1 step   153000 | 153000 batches | lr 9.37e-06 | ms/batch 40.59578 | loss 0.69473 | ppl     2.003\n",
      "| epoch   1 step   153200 | 153200 batches | lr 9.37e-06 | ms/batch 41.20679 | loss 0.69392 | ppl     2.002\n",
      "| epoch   1 step   153600 | 153600 batches | lr 9.37e-06 | ms/batch 39.37094 | loss 0.69306 | ppl     2.000\n",
      "| epoch   1 step   153800 | 153800 batches | lr 9.37e-06 | ms/batch 40.79384 | loss 0.69276 | ppl     1.999\n",
      "| epoch   1 step   154000 | 154000 batches | lr 9.37e-06 | ms/batch 40.89907 | loss 0.69500 | ppl     2.004\n",
      "| epoch   1 step   154200 | 154200 batches | lr 9.37e-06 | ms/batch 41.14552 | loss 0.69299 | ppl     2.000\n",
      "| epoch   1 step   154400 | 154400 batches | lr 9.37e-06 | ms/batch 42.17019 | loss 0.69467 | ppl     2.003\n",
      "| epoch   1 step   154600 | 154600 batches | lr 9.37e-06 | ms/batch 41.51552 | loss 0.69346 | ppl     2.001\n",
      "| epoch   1 step   154800 | 154800 batches | lr 9.37e-06 | ms/batch 41.48745 | loss 0.69398 | ppl     2.002\n",
      "| epoch   1 step   155000 | 155000 batches | lr 9.37e-06 | ms/batch 40.05482 | loss 0.69481 | ppl     2.003\n",
      "| epoch   1 step   155200 | 155200 batches | lr 9.37e-06 | ms/batch 40.32137 | loss 0.69290 | ppl     2.000\n",
      "| epoch   1 step   155400 | 155400 batches | lr 9.37e-06 | ms/batch 41.43580 | loss 0.69527 | ppl     2.004\n",
      "| epoch   1 step   155600 | 155600 batches | lr 9.37e-06 | ms/batch 40.45133 | loss 0.69310 | ppl     2.000\n",
      "| epoch   1 step   155800 | 155800 batches | lr 9.37e-06 | ms/batch 40.35358 | loss 0.69479 | ppl     2.003\n",
      "| epoch   1 step   156000 | 156000 batches | lr 9.37e-06 | ms/batch 40.78233 | loss 0.69434 | ppl     2.002\n",
      "|\n",
      "Source: [12  6 36  0 32  1 34  8 34 10]\n",
      "Target: [ 6 36  0 32  1 34  8 34 10  8]\n",
      "Teacher forcing: acc:0.6221875\n",
      "Preds:  [10  6 10  0 10  1 10  6 10  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  39 at step   156000 | time: 164.39s | valid loss 0.69676 | valid ppl    2.0072 | valid acc 0.622\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   156200 | 156200 batches | lr 9.37e-06 | ms/batch 47.47993 | loss 0.69198 | ppl     1.998\n",
      "| epoch   1 step   156400 | 156400 batches | lr 9.37e-06 | ms/batch 41.67863 | loss 0.69430 | ppl     2.002\n",
      "| epoch   1 step   156600 | 156600 batches | lr 9.37e-06 | ms/batch 41.67542 | loss 0.69497 | ppl     2.004\n",
      "| epoch   1 step   156800 | 156800 batches | lr 9.37e-06 | ms/batch 41.66365 | loss 0.69478 | ppl     2.003\n",
      "| epoch   1 step   157000 | 157000 batches | lr 9.37e-06 | ms/batch 42.37107 | loss 0.69277 | ppl     1.999\n",
      "| epoch   1 step   157200 | 157200 batches | lr 9.37e-06 | ms/batch 41.90993 | loss 0.69486 | ppl     2.003\n",
      "| epoch   1 step   157400 | 157400 batches | lr 9.37e-06 | ms/batch 41.11092 | loss 0.69417 | ppl     2.002\n",
      "| epoch   1 step   157600 | 157600 batches | lr 9.37e-06 | ms/batch 41.10252 | loss 0.69411 | ppl     2.002\n",
      "| epoch   1 step   157800 | 157800 batches | lr 9.37e-06 | ms/batch 41.32534 | loss 0.69391 | ppl     2.002\n",
      "| epoch   1 step   158000 | 158000 batches | lr 9.37e-06 | ms/batch 41.44818 | loss 0.69467 | ppl     2.003\n",
      "| epoch   1 step   158200 | 158200 batches | lr 9.37e-06 | ms/batch 41.21232 | loss 0.69319 | ppl     2.000\n",
      "| epoch   1 step   158400 | 158400 batches | lr 9.37e-06 | ms/batch 41.30021 | loss 0.69384 | ppl     2.001\n",
      "| epoch   1 step   158600 | 158600 batches | lr 9.37e-06 | ms/batch 41.43302 | loss 0.69416 | ppl     2.002\n",
      "| epoch   1 step   158800 | 158800 batches | lr 9.37e-06 | ms/batch 40.87507 | loss 0.69326 | ppl     2.000\n",
      "| epoch   1 step   159000 | 159000 batches | lr 9.37e-06 | ms/batch 40.83830 | loss 0.69456 | ppl     2.003\n",
      "| epoch   1 step   159200 | 159200 batches | lr 9.37e-06 | ms/batch 40.66146 | loss 0.69421 | ppl     2.002\n",
      "| epoch   1 step   159400 | 159400 batches | lr 9.37e-06 | ms/batch 40.84798 | loss 0.69408 | ppl     2.002\n",
      "| epoch   1 step   159600 | 159600 batches | lr 9.37e-06 | ms/batch 41.67287 | loss 0.69403 | ppl     2.002\n",
      "| epoch   1 step   159800 | 159800 batches | lr 9.37e-06 | ms/batch 41.65525 | loss 0.69385 | ppl     2.001\n",
      "| epoch   1 step   160000 | 160000 batches | lr 9.37e-06 | ms/batch 41.92275 | loss 0.69382 | ppl     2.001\n",
      "|\n",
      "Source: [21  4 24  8 13  0 31  6 21 10]\n",
      "Target: [ 4 24  8 13  0 31  6 21 10  4]\n",
      "Teacher forcing: acc:0.623125\n",
      "Preds:  [10  4 10 10 10  0 10  8 10  8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  40 at step   160000 | time: 166.89s | valid loss 0.69586 | valid ppl    2.0054 | valid acc 0.623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   160200 | 160200 batches | lr 9.37e-06 | ms/batch 47.95604 | loss 0.69343 | ppl     2.001\n",
      "| epoch   1 step   160400 | 160400 batches | lr 9.37e-06 | ms/batch 41.67093 | loss 0.69350 | ppl     2.001\n",
      "| epoch   1 step   160800 | 160800 batches | lr 9.37e-06 | ms/batch 41.25732 | loss 0.69347 | ppl     2.001\n",
      "| epoch   1 step   161000 | 161000 batches | lr 9.37e-06 | ms/batch 42.52218 | loss 0.69447 | ppl     2.003\n",
      "| epoch   1 step   161200 | 161200 batches | lr 9.37e-06 | ms/batch 41.09265 | loss 0.69309 | ppl     2.000\n",
      "| epoch   1 step   161400 | 161400 batches | lr 9.37e-06 | ms/batch 41.72512 | loss 0.69255 | ppl     1.999\n",
      "| epoch   1 step   161600 | 161600 batches | lr 9.37e-06 | ms/batch 41.57746 | loss 0.69367 | ppl     2.001\n",
      "| epoch   1 step   161800 | 161800 batches | lr 9.37e-06 | ms/batch 41.85009 | loss 0.69357 | ppl     2.001\n",
      "| epoch   1 step   162000 | 162000 batches | lr 9.37e-06 | ms/batch 41.59512 | loss 0.69518 | ppl     2.004\n",
      "| epoch   1 step   162200 | 162200 batches | lr 9.37e-06 | ms/batch 41.94719 | loss 0.69378 | ppl     2.001\n",
      "| epoch   1 step   162400 | 162400 batches | lr 9.37e-06 | ms/batch 41.41522 | loss 0.69259 | ppl     1.999\n",
      "| epoch   1 step   162600 | 162600 batches | lr 9.37e-06 | ms/batch 42.00683 | loss 0.69257 | ppl     1.999\n",
      "| epoch   1 step   162800 | 162800 batches | lr 9.37e-06 | ms/batch 42.40266 | loss 0.69515 | ppl     2.004\n",
      "| epoch   1 step   163000 | 163000 batches | lr 9.37e-06 | ms/batch 41.31488 | loss 0.69400 | ppl     2.002\n",
      "| epoch   1 step   163200 | 163200 batches | lr 9.37e-06 | ms/batch 41.37053 | loss 0.69381 | ppl     2.001\n",
      "| epoch   1 step   163400 | 163400 batches | lr 9.37e-06 | ms/batch 41.76189 | loss 0.69414 | ppl     2.002\n",
      "| epoch   1 step   163600 | 163600 batches | lr 9.37e-06 | ms/batch 41.25211 | loss 0.69460 | ppl     2.003\n",
      "| epoch   1 step   163800 | 163800 batches | lr 9.37e-06 | ms/batch 41.52928 | loss 0.69358 | ppl     2.001\n",
      "| epoch   1 step   164000 | 164000 batches | lr 9.37e-06 | ms/batch 41.38667 | loss 0.69311 | ppl     2.000\n",
      "|\n",
      "Source: [22  9 31  0 14  3 12  2 22 10]\n",
      "Target: [ 9 31  0 14  3 12  2 22 10  9]\n",
      "Teacher forcing: acc:0.6278125\n",
      "Preds:  [10  9 10  9 10  9 10  9 10  2]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  41 at step   164000 | time: 167.76s | valid loss 0.69613 | valid ppl    2.0060 | valid acc 0.628\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   164200 | 164200 batches | lr 9.37e-06 | ms/batch 47.74338 | loss 0.69404 | ppl     2.002\n",
      "| epoch   1 step   164400 | 164400 batches | lr 9.37e-06 | ms/batch 41.43790 | loss 0.69398 | ppl     2.002\n",
      "| epoch   1 step   164600 | 164600 batches | lr 9.37e-06 | ms/batch 41.97192 | loss 0.69302 | ppl     2.000\n",
      "| epoch   1 step   164800 | 164800 batches | lr 9.37e-06 | ms/batch 41.08679 | loss 0.69322 | ppl     2.000\n",
      "| epoch   1 step   165000 | 165000 batches | lr 9.37e-06 | ms/batch 41.56068 | loss 0.69377 | ppl     2.001\n",
      "| epoch   1 step   165200 | 165200 batches | lr 9.37e-06 | ms/batch 41.52090 | loss 0.69496 | ppl     2.004\n",
      "| epoch   1 step   165400 | 165400 batches | lr 9.37e-06 | ms/batch 40.48778 | loss 0.69382 | ppl     2.001\n",
      "| epoch   1 step   165600 | 165600 batches | lr 9.37e-06 | ms/batch 41.90303 | loss 0.69408 | ppl     2.002\n",
      "| epoch   1 step   165800 | 165800 batches | lr 9.37e-06 | ms/batch 40.86250 | loss 0.69491 | ppl     2.004\n",
      "| epoch   1 step   166000 | 166000 batches | lr 9.37e-06 | ms/batch 40.87169 | loss 0.69357 | ppl     2.001\n",
      "| epoch   1 step   166200 | 166200 batches | lr 9.37e-06 | ms/batch 41.67847 | loss 0.69393 | ppl     2.002\n",
      "| epoch   1 step   166400 | 166400 batches | lr 9.37e-06 | ms/batch 40.99202 | loss 0.69389 | ppl     2.001\n",
      "| epoch   1 step   166600 | 166600 batches | lr 9.37e-06 | ms/batch 40.79586 | loss 0.69478 | ppl     2.003\n",
      "| epoch   1 step   166800 | 166800 batches | lr 9.37e-06 | ms/batch 41.66740 | loss 0.69311 | ppl     2.000\n",
      "| epoch   1 step   167000 | 167000 batches | lr 9.37e-06 | ms/batch 41.90789 | loss 0.69419 | ppl     2.002\n",
      "| epoch   1 step   167200 | 167200 batches | lr 9.37e-06 | ms/batch 42.46041 | loss 0.69432 | ppl     2.002\n",
      "| epoch   1 step   167400 | 167400 batches | lr 9.37e-06 | ms/batch 41.59518 | loss 0.69444 | ppl     2.003\n",
      "| epoch   1 step   167600 | 167600 batches | lr 9.37e-06 | ms/batch 41.76414 | loss 0.69418 | ppl     2.002\n",
      "| epoch   1 step   167800 | 167800 batches | lr 9.37e-06 | ms/batch 41.49388 | loss 0.69336 | ppl     2.000\n",
      "| epoch   1 step   168000 | 168000 batches | lr 9.37e-06 | ms/batch 41.94705 | loss 0.69431 | ppl     2.002\n",
      "|\n",
      "Source: [18  7 32  1 36  2 21  8 18 10]\n",
      "Target: [ 7 32  1 36  2 21  8 18 10  7]\n",
      "Teacher forcing: acc:0.6259375\n",
      "Preds:  [10  1 10  1 10  2 10 10 10  2]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  42 at step   168000 | time: 167.16s | valid loss 0.69482 | valid ppl    2.0034 | valid acc 0.626\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   168200 | 168200 batches | lr 4.69e-06 | ms/batch 47.72608 | loss 0.69377 | ppl     2.001\n",
      "| epoch   1 step   168400 | 168400 batches | lr 4.69e-06 | ms/batch 41.41093 | loss 0.69253 | ppl     1.999\n",
      "| epoch   1 step   168600 | 168600 batches | lr 4.69e-06 | ms/batch 41.06954 | loss 0.69431 | ppl     2.002\n",
      "| epoch   1 step   168800 | 168800 batches | lr 4.69e-06 | ms/batch 40.82040 | loss 0.69276 | ppl     1.999\n",
      "| epoch   1 step   169000 | 169000 batches | lr 4.69e-06 | ms/batch 40.88497 | loss 0.69619 | ppl     2.006\n",
      "| epoch   1 step   169200 | 169200 batches | lr 4.69e-06 | ms/batch 41.58144 | loss 0.69435 | ppl     2.002\n",
      "| epoch   1 step   169400 | 169400 batches | lr 4.69e-06 | ms/batch 41.64154 | loss 0.69443 | ppl     2.003\n",
      "| epoch   1 step   169600 | 169600 batches | lr 4.69e-06 | ms/batch 41.29810 | loss 0.69637 | ppl     2.006\n",
      "| epoch   1 step   169800 | 169800 batches | lr 4.69e-06 | ms/batch 40.96042 | loss 0.69361 | ppl     2.001\n",
      "| epoch   1 step   170000 | 170000 batches | lr 4.69e-06 | ms/batch 40.92605 | loss 0.69356 | ppl     2.001\n",
      "| epoch   1 step   170200 | 170200 batches | lr 4.69e-06 | ms/batch 41.46725 | loss 0.69405 | ppl     2.002\n",
      "| epoch   1 step   170400 | 170400 batches | lr 4.69e-06 | ms/batch 41.15462 | loss 0.69297 | ppl     2.000\n",
      "| epoch   1 step   170600 | 170600 batches | lr 4.69e-06 | ms/batch 41.18988 | loss 0.69312 | ppl     2.000\n",
      "| epoch   1 step   170800 | 170800 batches | lr 4.69e-06 | ms/batch 40.30220 | loss 0.69295 | ppl     2.000\n",
      "| epoch   1 step   171000 | 171000 batches | lr 4.69e-06 | ms/batch 40.67748 | loss 0.69459 | ppl     2.003\n",
      "| epoch   1 step   171200 | 171200 batches | lr 4.69e-06 | ms/batch 41.72983 | loss 0.69336 | ppl     2.000\n",
      "| epoch   1 step   171400 | 171400 batches | lr 4.69e-06 | ms/batch 40.89776 | loss 0.69340 | ppl     2.001\n",
      "| epoch   1 step   171600 | 171600 batches | lr 4.69e-06 | ms/batch 40.88510 | loss 0.69226 | ppl     1.998\n",
      "| epoch   1 step   171800 | 171800 batches | lr 4.69e-06 | ms/batch 41.11598 | loss 0.69292 | ppl     2.000\n",
      "| epoch   1 step   172000 | 172000 batches | lr 4.69e-06 | ms/batch 40.53719 | loss 0.69359 | ppl     2.001\n",
      "maslina\n",
      "|\n",
      "Source: [15  5 12  3 16  2 29  0 16 10]\n",
      "Target: [ 5 12  3 16  2 29  0 16 10  2]\n",
      "Teacher forcing: acc:0.6253188775510204\n",
      "Preds:  [10  5 10  3 10  2 10  0 10  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  43 at step   172000 | time: 165.61s | valid loss 0.69626 | valid ppl    2.0062 | valid acc 0.625\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   172200 | 172200 batches | lr 4.69e-06 | ms/batch 47.76009 | loss 0.69531 | ppl     2.004\n",
      "| epoch   1 step   172400 | 172400 batches | lr 4.69e-06 | ms/batch 41.27112 | loss 0.69443 | ppl     2.003\n",
      "| epoch   1 step   172600 | 172600 batches | lr 4.69e-06 | ms/batch 40.41342 | loss 0.69342 | ppl     2.001\n",
      "| epoch   1 step   172800 | 172800 batches | lr 4.69e-06 | ms/batch 40.01976 | loss 0.69482 | ppl     2.003\n",
      "| epoch   1 step   173000 | 173000 batches | lr 4.69e-06 | ms/batch 40.16298 | loss 0.69363 | ppl     2.001\n",
      "| epoch   1 step   173200 | 173200 batches | lr 4.69e-06 | ms/batch 39.76652 | loss 0.69329 | ppl     2.000\n",
      "| epoch   1 step   173400 | 173400 batches | lr 4.69e-06 | ms/batch 39.58204 | loss 0.69347 | ppl     2.001\n",
      "| epoch   1 step   173600 | 173600 batches | lr 4.69e-06 | ms/batch 40.06496 | loss 0.69436 | ppl     2.002\n",
      "| epoch   1 step   173800 | 173800 batches | lr 4.69e-06 | ms/batch 41.08895 | loss 0.69395 | ppl     2.002\n",
      "| epoch   1 step   174000 | 174000 batches | lr 4.69e-06 | ms/batch 40.91159 | loss 0.69289 | ppl     1.999\n",
      "| epoch   1 step   174200 | 174200 batches | lr 4.69e-06 | ms/batch 41.76198 | loss 0.69398 | ppl     2.002\n",
      "| epoch   1 step   174400 | 174400 batches | lr 4.69e-06 | ms/batch 40.97315 | loss 0.69419 | ppl     2.002\n",
      "| epoch   1 step   174600 | 174600 batches | lr 4.69e-06 | ms/batch 41.02222 | loss 0.69257 | ppl     1.999\n",
      "| epoch   1 step   174800 | 174800 batches | lr 4.69e-06 | ms/batch 40.82012 | loss 0.69515 | ppl     2.004\n",
      "| epoch   1 step   175000 | 175000 batches | lr 4.69e-06 | ms/batch 41.26168 | loss 0.69362 | ppl     2.001\n",
      "| epoch   1 step   175200 | 175200 batches | lr 4.69e-06 | ms/batch 41.21472 | loss 0.69403 | ppl     2.002\n",
      "| epoch   1 step   175400 | 175400 batches | lr 4.69e-06 | ms/batch 41.16568 | loss 0.69412 | ppl     2.002\n",
      "| epoch   1 step   175600 | 175600 batches | lr 4.69e-06 | ms/batch 40.52204 | loss 0.69300 | ppl     2.000\n",
      "| epoch   1 step   175800 | 175800 batches | lr 4.69e-06 | ms/batch 39.86747 | loss 0.69252 | ppl     1.999\n",
      "| epoch   1 step   176000 | 176000 batches | lr 4.69e-06 | ms/batch 40.35582 | loss 0.69282 | ppl     1.999\n",
      "|\n",
      "Source: [30  6 17  9 12  8 33  5 33 10]\n",
      "Target: [ 6 17  9 12  8 33  5 33 10  5]\n",
      "Teacher forcing: acc:0.6203125\n",
      "Preds:  [10  6 10  6 10 10 10  6 10  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  44 at step   176000 | time: 164.02s | valid loss 0.69609 | valid ppl    2.0059 | valid acc 0.62\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   176200 | 176200 batches | lr 4.69e-06 | ms/batch 46.15548 | loss 0.69313 | ppl     2.000\n",
      "| epoch   1 step   176400 | 176400 batches | lr 4.69e-06 | ms/batch 40.09858 | loss 0.69377 | ppl     2.001\n",
      "| epoch   1 step   176600 | 176600 batches | lr 4.69e-06 | ms/batch 39.77924 | loss 0.69307 | ppl     2.000\n",
      "| epoch   1 step   176800 | 176800 batches | lr 4.69e-06 | ms/batch 41.30420 | loss 0.69314 | ppl     2.000\n",
      "| epoch   1 step   177000 | 177000 batches | lr 4.69e-06 | ms/batch 41.07455 | loss 0.69274 | ppl     1.999\n",
      "| epoch   1 step   177200 | 177200 batches | lr 4.69e-06 | ms/batch 41.03669 | loss 0.69399 | ppl     2.002\n",
      "| epoch   1 step   177400 | 177400 batches | lr 4.69e-06 | ms/batch 41.58286 | loss 0.69305 | ppl     2.000\n",
      "| epoch   1 step   177600 | 177600 batches | lr 4.69e-06 | ms/batch 41.94092 | loss 0.69357 | ppl     2.001\n",
      "| epoch   1 step   177800 | 177800 batches | lr 4.69e-06 | ms/batch 42.21371 | loss 0.69287 | ppl     1.999\n",
      "| epoch   1 step   178000 | 178000 batches | lr 4.69e-06 | ms/batch 42.38037 | loss 0.69196 | ppl     1.998\n",
      "| epoch   1 step   178200 | 178200 batches | lr 4.69e-06 | ms/batch 41.98590 | loss 0.69156 | ppl     1.997\n",
      "| epoch   1 step   178400 | 178400 batches | lr 4.69e-06 | ms/batch 41.93038 | loss 0.69392 | ppl     2.002\n",
      "| epoch   1 step   178600 | 178600 batches | lr 4.69e-06 | ms/batch 41.75359 | loss 0.69192 | ppl     1.998\n",
      "| epoch   1 step   178800 | 178800 batches | lr 4.69e-06 | ms/batch 41.50297 | loss 0.69456 | ppl     2.003\n",
      "| epoch   1 step   179000 | 179000 batches | lr 4.69e-06 | ms/batch 42.48789 | loss 0.69429 | ppl     2.002\n",
      "| epoch   1 step   179200 | 179200 batches | lr 4.69e-06 | ms/batch 41.26082 | loss 0.69422 | ppl     2.002\n",
      "| epoch   1 step   179400 | 179400 batches | lr 4.69e-06 | ms/batch 41.58006 | loss 0.69357 | ppl     2.001\n",
      "| epoch   1 step   179600 | 179600 batches | lr 4.69e-06 | ms/batch 40.49094 | loss 0.69314 | ppl     2.000\n",
      "| epoch   1 step   179800 | 179800 batches | lr 4.69e-06 | ms/batch 41.08892 | loss 0.69250 | ppl     1.999\n",
      "| epoch   1 step   180000 | 180000 batches | lr 4.69e-06 | ms/batch 41.26673 | loss 0.69315 | ppl     2.000\n",
      "|\n",
      "Source: [27  0 18  4 13  7 29  6 13 10]\n",
      "Target: [ 0 18  4 13  7 29  6 13 10  7]\n",
      "Teacher forcing: acc:0.62875\n",
      "Preds:  [10  0 10  4 10  0 10  7 10  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  45 at step   180000 | time: 166.63s | valid loss 0.69652 | valid ppl    2.0068 | valid acc 0.629\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   180200 | 180200 batches | lr 4.69e-06 | ms/batch 47.73346 | loss 0.69301 | ppl     2.000\n",
      "| epoch   1 step   180400 | 180400 batches | lr 4.69e-06 | ms/batch 41.15364 | loss 0.69409 | ppl     2.002\n",
      "| epoch   1 step   180600 | 180600 batches | lr 4.69e-06 | ms/batch 42.15017 | loss 0.69213 | ppl     1.998\n",
      "| epoch   1 step   180800 | 180800 batches | lr 4.69e-06 | ms/batch 41.38876 | loss 0.69393 | ppl     2.002\n",
      "| epoch   1 step   181000 | 181000 batches | lr 4.69e-06 | ms/batch 41.44764 | loss 0.69322 | ppl     2.000\n",
      "| epoch   1 step   181200 | 181200 batches | lr 4.69e-06 | ms/batch 42.02193 | loss 0.69358 | ppl     2.001\n",
      "| epoch   1 step   181400 | 181400 batches | lr 4.69e-06 | ms/batch 41.59008 | loss 0.69327 | ppl     2.000\n",
      "| epoch   1 step   181600 | 181600 batches | lr 4.69e-06 | ms/batch 42.55602 | loss 0.69439 | ppl     2.002\n",
      "| epoch   1 step   181800 | 181800 batches | lr 4.69e-06 | ms/batch 40.83761 | loss 0.69423 | ppl     2.002\n",
      "| epoch   1 step   182000 | 182000 batches | lr 4.69e-06 | ms/batch 42.12978 | loss 0.69265 | ppl     1.999\n",
      "| epoch   1 step   182200 | 182200 batches | lr 4.69e-06 | ms/batch 41.49312 | loss 0.69397 | ppl     2.002\n",
      "| epoch   1 step   182400 | 182400 batches | lr 4.69e-06 | ms/batch 41.28794 | loss 0.69284 | ppl     1.999\n",
      "| epoch   1 step   182600 | 182600 batches | lr 4.69e-06 | ms/batch 40.64888 | loss 0.69286 | ppl     1.999\n",
      "| epoch   1 step   182800 | 182800 batches | lr 4.69e-06 | ms/batch 41.88184 | loss 0.69285 | ppl     1.999\n",
      "| epoch   1 step   183000 | 183000 batches | lr 4.69e-06 | ms/batch 39.75239 | loss 0.69422 | ppl     2.002\n",
      "| epoch   1 step   183200 | 183200 batches | lr 4.69e-06 | ms/batch 39.84057 | loss 0.69110 | ppl     1.996\n",
      "| epoch   1 step   183400 | 183400 batches | lr 4.69e-06 | ms/batch 40.21098 | loss 0.69309 | ppl     2.000\n",
      "| epoch   1 step   183600 | 183600 batches | lr 4.69e-06 | ms/batch 40.40220 | loss 0.69442 | ppl     2.003\n",
      "| epoch   1 step   183800 | 183800 batches | lr 4.69e-06 | ms/batch 40.85959 | loss 0.69459 | ppl     2.003\n",
      "| epoch   1 step   184000 | 184000 batches | lr 4.69e-06 | ms/batch 40.32853 | loss 0.69413 | ppl     2.002\n",
      "|\n",
      "Source: [25  7 15  3 34  2 14  1 14 10]\n",
      "Target: [ 7 15  3 34  2 14  1 14 10  1]\n",
      "Teacher forcing: acc:0.62\n",
      "Preds:  [10  1 10  3 10  2 10  7 10  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  46 at step   184000 | time: 165.91s | valid loss 0.69610 | valid ppl    2.0059 | valid acc 0.62\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   184200 | 184200 batches | lr 2.34e-06 | ms/batch 46.71422 | loss 0.69389 | ppl     2.001\n",
      "| epoch   1 step   184400 | 184400 batches | lr 2.34e-06 | ms/batch 40.55199 | loss 0.69140 | ppl     1.997\n",
      "| epoch   1 step   184600 | 184600 batches | lr 2.34e-06 | ms/batch 41.22095 | loss 0.69414 | ppl     2.002\n",
      "| epoch   1 step   184800 | 184800 batches | lr 2.34e-06 | ms/batch 40.94029 | loss 0.69394 | ppl     2.002\n",
      "| epoch   1 step   185000 | 185000 batches | lr 2.34e-06 | ms/batch 40.82820 | loss 0.69372 | ppl     2.001\n",
      "| epoch   1 step   185200 | 185200 batches | lr 2.34e-06 | ms/batch 40.24078 | loss 0.69367 | ppl     2.001\n",
      "| epoch   1 step   185400 | 185400 batches | lr 2.34e-06 | ms/batch 40.74818 | loss 0.69383 | ppl     2.001\n",
      "| epoch   1 step   185600 | 185600 batches | lr 2.34e-06 | ms/batch 40.89052 | loss 0.69263 | ppl     1.999\n",
      "| epoch   1 step   185800 | 185800 batches | lr 2.34e-06 | ms/batch 40.58340 | loss 0.69307 | ppl     2.000\n",
      "| epoch   1 step   186000 | 186000 batches | lr 2.34e-06 | ms/batch 39.79077 | loss 0.69321 | ppl     2.000\n",
      "| epoch   1 step   186200 | 186200 batches | lr 2.34e-06 | ms/batch 41.35290 | loss 0.69365 | ppl     2.001\n",
      "| epoch   1 step   186400 | 186400 batches | lr 2.34e-06 | ms/batch 41.05681 | loss 0.69234 | ppl     1.998\n",
      "| epoch   1 step   186600 | 186600 batches | lr 2.34e-06 | ms/batch 40.38355 | loss 0.69338 | ppl     2.000\n",
      "| epoch   1 step   186800 | 186800 batches | lr 2.34e-06 | ms/batch 40.17938 | loss 0.69441 | ppl     2.003\n",
      "| epoch   1 step   187000 | 187000 batches | lr 2.34e-06 | ms/batch 39.82685 | loss 0.69363 | ppl     2.001\n",
      "| epoch   1 step   187200 | 187200 batches | lr 2.34e-06 | ms/batch 40.53690 | loss 0.69363 | ppl     2.001\n",
      "| epoch   1 step   187400 | 187400 batches | lr 2.34e-06 | ms/batch 39.50808 | loss 0.69433 | ppl     2.002\n",
      "| epoch   1 step   187600 | 187600 batches | lr 2.34e-06 | ms/batch 39.77568 | loss 0.69199 | ppl     1.998\n",
      "| epoch   1 step   187800 | 187800 batches | lr 2.34e-06 | ms/batch 40.02577 | loss 0.69387 | ppl     2.001\n",
      "| epoch   1 step   188000 | 188000 batches | lr 2.34e-06 | ms/batch 40.19007 | loss 0.69381 | ppl     2.001\n",
      "|\n",
      "Source: [33  1 35  8 31  0 36  4 36 10]\n",
      "Target: [ 1 35  8 31  0 36  4 36 10  4]\n",
      "Teacher forcing: acc:0.6275\n",
      "Preds:  [10  4 10 10 10  1 10  4 10  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  47 at step   188000 | time: 163.11s | valid loss 0.69645 | valid ppl    2.0066 | valid acc 0.627\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   188200 | 188200 batches | lr 2.34e-06 | ms/batch 46.22057 | loss 0.69231 | ppl     1.998\n",
      "| epoch   1 step   188400 | 188400 batches | lr 2.34e-06 | ms/batch 41.04105 | loss 0.69419 | ppl     2.002\n",
      "| epoch   1 step   188600 | 188600 batches | lr 2.34e-06 | ms/batch 41.31725 | loss 0.69228 | ppl     1.998\n",
      "| epoch   1 step   188800 | 188800 batches | lr 2.34e-06 | ms/batch 40.31958 | loss 0.69229 | ppl     1.998\n",
      "| epoch   1 step   189000 | 189000 batches | lr 2.34e-06 | ms/batch 40.89404 | loss 0.69198 | ppl     1.998\n",
      "| epoch   1 step   189200 | 189200 batches | lr 2.34e-06 | ms/batch 41.22451 | loss 0.69259 | ppl     1.999\n",
      "| epoch   1 step   189400 | 189400 batches | lr 2.34e-06 | ms/batch 41.40186 | loss 0.69401 | ppl     2.002\n",
      "| epoch   1 step   189600 | 189600 batches | lr 2.34e-06 | ms/batch 41.78142 | loss 0.69422 | ppl     2.002\n",
      "| epoch   1 step   189800 | 189800 batches | lr 2.34e-06 | ms/batch 40.54364 | loss 0.69419 | ppl     2.002\n",
      "| epoch   1 step   190000 | 190000 batches | lr 2.34e-06 | ms/batch 41.00775 | loss 0.69367 | ppl     2.001\n",
      "| epoch   1 step   190200 | 190200 batches | lr 2.34e-06 | ms/batch 40.79733 | loss 0.69315 | ppl     2.000\n",
      "| epoch   1 step   190400 | 190400 batches | lr 2.34e-06 | ms/batch 42.01709 | loss 0.69509 | ppl     2.004\n",
      "| epoch   1 step   190600 | 190600 batches | lr 2.34e-06 | ms/batch 41.09573 | loss 0.69378 | ppl     2.001\n",
      "| epoch   1 step   190800 | 190800 batches | lr 2.34e-06 | ms/batch 41.41001 | loss 0.69293 | ppl     2.000\n",
      "| epoch   1 step   191000 | 191000 batches | lr 2.34e-06 | ms/batch 40.90207 | loss 0.69338 | ppl     2.000\n",
      "| epoch   1 step   191200 | 191200 batches | lr 2.34e-06 | ms/batch 40.89318 | loss 0.69435 | ppl     2.002\n",
      "| epoch   1 step   191400 | 191400 batches | lr 2.34e-06 | ms/batch 41.69791 | loss 0.69364 | ppl     2.001\n",
      "| epoch   1 step   191600 | 191600 batches | lr 2.34e-06 | ms/batch 41.43925 | loss 0.69351 | ppl     2.001\n",
      "| epoch   1 step   191800 | 191800 batches | lr 2.34e-06 | ms/batch 41.11432 | loss 0.69332 | ppl     2.000\n",
      "| epoch   1 step   192000 | 192000 batches | lr 2.34e-06 | ms/batch 41.07714 | loss 0.69399 | ppl     2.002\n",
      "|\n",
      "Source: [34  8 32  9 19  1 33  2 33 10]\n",
      "Target: [ 8 32  9 19  1 33  2 33 10  2]\n",
      "Teacher forcing: acc:0.625\n",
      "Preds:  [10 10 10  9 10 10 10  2 10  8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  48 at step   192000 | time: 165.63s | valid loss 0.69554 | valid ppl    2.0048 | valid acc 0.625\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   192200 | 192200 batches | lr 2.34e-06 | ms/batch 46.41295 | loss 0.69169 | ppl     1.997\n",
      "| epoch   1 step   192400 | 192400 batches | lr 2.34e-06 | ms/batch 41.61816 | loss 0.69443 | ppl     2.003\n",
      "| epoch   1 step   192600 | 192600 batches | lr 2.34e-06 | ms/batch 41.91896 | loss 0.69391 | ppl     2.002\n",
      "| epoch   1 step   192800 | 192800 batches | lr 2.34e-06 | ms/batch 41.94218 | loss 0.69499 | ppl     2.004\n",
      "| epoch   1 step   193000 | 193000 batches | lr 2.34e-06 | ms/batch 41.51335 | loss 0.69280 | ppl     1.999\n",
      "| epoch   1 step   193200 | 193200 batches | lr 2.34e-06 | ms/batch 41.56836 | loss 0.69208 | ppl     1.998\n",
      "| epoch   1 step   193400 | 193400 batches | lr 2.34e-06 | ms/batch 40.87982 | loss 0.69322 | ppl     2.000\n",
      "| epoch   1 step   193600 | 193600 batches | lr 2.34e-06 | ms/batch 39.90379 | loss 0.69371 | ppl     2.001\n",
      "| epoch   1 step   193800 | 193800 batches | lr 2.34e-06 | ms/batch 40.98436 | loss 0.69289 | ppl     1.999\n",
      "| epoch   1 step   194000 | 194000 batches | lr 2.34e-06 | ms/batch 40.88834 | loss 0.69394 | ppl     2.002\n",
      "| epoch   1 step   194200 | 194200 batches | lr 2.34e-06 | ms/batch 40.59061 | loss 0.69452 | ppl     2.003\n",
      "| epoch   1 step   194400 | 194400 batches | lr 2.34e-06 | ms/batch 40.95796 | loss 0.69348 | ppl     2.001\n",
      "| epoch   1 step   194600 | 194600 batches | lr 2.34e-06 | ms/batch 40.60683 | loss 0.69321 | ppl     2.000\n",
      "| epoch   1 step   194800 | 194800 batches | lr 2.34e-06 | ms/batch 40.88160 | loss 0.69274 | ppl     1.999\n",
      "| epoch   1 step   195000 | 195000 batches | lr 2.34e-06 | ms/batch 41.16856 | loss 0.69271 | ppl     1.999\n",
      "| epoch   1 step   195200 | 195200 batches | lr 2.34e-06 | ms/batch 40.65855 | loss 0.69383 | ppl     2.001\n",
      "| epoch   1 step   195400 | 195400 batches | lr 2.34e-06 | ms/batch 41.43591 | loss 0.69190 | ppl     1.997\n",
      "| epoch   1 step   195600 | 195600 batches | lr 2.34e-06 | ms/batch 41.00811 | loss 0.69336 | ppl     2.000\n",
      "| epoch   1 step   195800 | 195800 batches | lr 2.34e-06 | ms/batch 40.88916 | loss 0.69277 | ppl     1.999\n",
      "| epoch   1 step   196000 | 196000 batches | lr 2.34e-06 | ms/batch 40.89888 | loss 0.69261 | ppl     1.999\n",
      "|\n",
      "Source: [16  3 27  7 21  4 12  9 27 10]\n",
      "Target: [ 3 27  7 21  4 12  9 27 10  7]\n",
      "Teacher forcing: acc:0.628125\n",
      "Preds:  [10  3 10  3 10  7 10  7 10  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  49 at step   196000 | time: 165.33s | valid loss 0.69548 | valid ppl    2.0047 | valid acc 0.628\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   196200 | 196200 batches | lr 2.34e-06 | ms/batch 45.74710 | loss 0.69234 | ppl     1.998\n",
      "| epoch   1 step   196400 | 196400 batches | lr 2.34e-06 | ms/batch 40.65796 | loss 0.69293 | ppl     2.000\n",
      "| epoch   1 step   196600 | 196600 batches | lr 2.34e-06 | ms/batch 40.59367 | loss 0.69325 | ppl     2.000\n",
      "| epoch   1 step   196800 | 196800 batches | lr 2.34e-06 | ms/batch 40.04167 | loss 0.69306 | ppl     2.000\n",
      "| epoch   1 step   197000 | 197000 batches | lr 2.34e-06 | ms/batch 40.54833 | loss 0.69289 | ppl     1.999\n",
      "| epoch   1 step   197200 | 197200 batches | lr 2.34e-06 | ms/batch 40.06182 | loss 0.69441 | ppl     2.003\n",
      "| epoch   1 step   197400 | 197400 batches | lr 2.34e-06 | ms/batch 40.30154 | loss 0.69395 | ppl     2.002\n",
      "| epoch   1 step   197600 | 197600 batches | lr 2.34e-06 | ms/batch 40.04405 | loss 0.69330 | ppl     2.000\n",
      "| epoch   1 step   197800 | 197800 batches | lr 2.34e-06 | ms/batch 39.63952 | loss 0.69281 | ppl     1.999\n",
      "| epoch   1 step   198000 | 198000 batches | lr 2.34e-06 | ms/batch 39.89413 | loss 0.69232 | ppl     1.998\n",
      "| epoch   1 step   198200 | 198200 batches | lr 2.34e-06 | ms/batch 40.27122 | loss 0.69221 | ppl     1.998\n",
      "| epoch   1 step   198400 | 198400 batches | lr 2.34e-06 | ms/batch 40.13474 | loss 0.69283 | ppl     1.999\n",
      "| epoch   1 step   198600 | 198600 batches | lr 2.34e-06 | ms/batch 39.49733 | loss 0.69439 | ppl     2.002\n",
      "| epoch   1 step   198800 | 198800 batches | lr 2.34e-06 | ms/batch 39.77030 | loss 0.69390 | ppl     2.001\n",
      "| epoch   1 step   199000 | 199000 batches | lr 2.34e-06 | ms/batch 39.86112 | loss 0.69294 | ppl     2.000\n",
      "| epoch   1 step   199200 | 199200 batches | lr 2.34e-06 | ms/batch 39.27384 | loss 0.69307 | ppl     2.000\n",
      "| epoch   1 step   199400 | 199400 batches | lr 2.34e-06 | ms/batch 40.14282 | loss 0.69357 | ppl     2.001\n",
      "| epoch   1 step   199600 | 199600 batches | lr 2.34e-06 | ms/batch 40.84620 | loss 0.69274 | ppl     1.999\n",
      "| epoch   1 step   199800 | 199800 batches | lr 2.34e-06 | ms/batch 39.50011 | loss 0.69312 | ppl     2.000\n",
      "| epoch   1 step   200000 | 200000 batches | lr 2.34e-06 | ms/batch 40.08594 | loss 0.69368 | ppl     2.001\n",
      "|\n",
      "Source: [20  8 23  0 16  4 33  6 23 10]\n",
      "Target: [ 8 23  0 16  4 33  6 23 10  0]\n",
      "Teacher forcing: acc:0.615625\n",
      "Preds:  [10  5 10  8 10  4 10  8 10  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  50 at step   200000 | time: 161.36s | valid loss 0.69699 | valid ppl    2.0077 | valid acc 0.616\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   200200 | 200200 batches | lr 1.17e-06 | ms/batch 46.27323 | loss 0.69327 | ppl     2.000\n",
      "| epoch   1 step   200400 | 200400 batches | lr 1.17e-06 | ms/batch 39.93469 | loss 0.69361 | ppl     2.001\n",
      "| epoch   1 step   200600 | 200600 batches | lr 1.17e-06 | ms/batch 39.93844 | loss 0.69422 | ppl     2.002\n",
      "| epoch   1 step   200800 | 200800 batches | lr 1.17e-06 | ms/batch 39.60792 | loss 0.69233 | ppl     1.998\n",
      "| epoch   1 step   201000 | 201000 batches | lr 1.17e-06 | ms/batch 41.26819 | loss 0.69281 | ppl     1.999\n",
      "| epoch   1 step   201200 | 201200 batches | lr 1.17e-06 | ms/batch 39.71072 | loss 0.69366 | ppl     2.001\n",
      "| epoch   1 step   201400 | 201400 batches | lr 1.17e-06 | ms/batch 40.47301 | loss 0.69432 | ppl     2.002\n",
      "| epoch   1 step   201600 | 201600 batches | lr 1.17e-06 | ms/batch 40.62460 | loss 0.69259 | ppl     1.999\n",
      "| epoch   1 step   201800 | 201800 batches | lr 1.17e-06 | ms/batch 40.12087 | loss 0.69372 | ppl     2.001\n",
      "| epoch   1 step   202000 | 202000 batches | lr 1.17e-06 | ms/batch 40.17247 | loss 0.69213 | ppl     1.998\n",
      "| epoch   1 step   202200 | 202200 batches | lr 1.17e-06 | ms/batch 40.34931 | loss 0.69328 | ppl     2.000\n",
      "| epoch   1 step   202400 | 202400 batches | lr 1.17e-06 | ms/batch 40.36332 | loss 0.69363 | ppl     2.001\n",
      "| epoch   1 step   202600 | 202600 batches | lr 1.17e-06 | ms/batch 40.00696 | loss 0.69366 | ppl     2.001\n",
      "| epoch   1 step   202800 | 202800 batches | lr 1.17e-06 | ms/batch 40.01075 | loss 0.69382 | ppl     2.001\n",
      "| epoch   1 step   203000 | 203000 batches | lr 1.17e-06 | ms/batch 39.98536 | loss 0.69310 | ppl     2.000\n",
      "| epoch   1 step   203200 | 203200 batches | lr 1.17e-06 | ms/batch 40.43858 | loss 0.69350 | ppl     2.001\n",
      "| epoch   1 step   203400 | 203400 batches | lr 1.17e-06 | ms/batch 39.50218 | loss 0.69438 | ppl     2.002\n",
      "| epoch   1 step   203600 | 203600 batches | lr 1.17e-06 | ms/batch 40.78486 | loss 0.69322 | ppl     2.000\n",
      "| epoch   1 step   203800 | 203800 batches | lr 1.17e-06 | ms/batch 40.54434 | loss 0.69425 | ppl     2.002\n",
      "| epoch   1 step   204000 | 204000 batches | lr 1.17e-06 | ms/batch 40.21733 | loss 0.69337 | ppl     2.000\n",
      "|\n",
      "Source: [20  5 35  6 12  4 32  2 32 10]\n",
      "Target: [ 5 35  6 12  4 32  2 32 10  2]\n",
      "Teacher forcing: acc:0.6309375\n",
      "Preds:  [10  5 10  6 10  4 10  2 10  2]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  51 at step   204000 | time: 162.08s | valid loss 0.69541 | valid ppl    2.0045 | valid acc 0.631\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   204200 | 204200 batches | lr 1.17e-06 | ms/batch 46.60840 | loss 0.69309 | ppl     2.000\n",
      "| epoch   1 step   204400 | 204400 batches | lr 1.17e-06 | ms/batch 41.03244 | loss 0.69316 | ppl     2.000\n",
      "| epoch   1 step   204600 | 204600 batches | lr 1.17e-06 | ms/batch 40.75872 | loss 0.69429 | ppl     2.002\n",
      "| epoch   1 step   204800 | 204800 batches | lr 1.17e-06 | ms/batch 40.37853 | loss 0.69466 | ppl     2.003\n",
      "| epoch   1 step   205000 | 205000 batches | lr 1.17e-06 | ms/batch 40.77506 | loss 0.69423 | ppl     2.002\n",
      "| epoch   1 step   205200 | 205200 batches | lr 1.17e-06 | ms/batch 40.56895 | loss 0.69181 | ppl     1.997\n",
      "| epoch   1 step   205400 | 205400 batches | lr 1.17e-06 | ms/batch 40.68845 | loss 0.69342 | ppl     2.001\n",
      "| epoch   1 step   205600 | 205600 batches | lr 1.17e-06 | ms/batch 40.40485 | loss 0.69370 | ppl     2.001\n",
      "| epoch   1 step   205800 | 205800 batches | lr 1.17e-06 | ms/batch 41.02458 | loss 0.69544 | ppl     2.005\n",
      "| epoch   1 step   206000 | 206000 batches | lr 1.17e-06 | ms/batch 40.26910 | loss 0.69315 | ppl     2.000\n",
      "| epoch   1 step   206200 | 206200 batches | lr 1.17e-06 | ms/batch 40.60898 | loss 0.69126 | ppl     1.996\n",
      "| epoch   1 step   206400 | 206400 batches | lr 1.17e-06 | ms/batch 41.01421 | loss 0.69259 | ppl     1.999\n",
      "| epoch   1 step   206600 | 206600 batches | lr 1.17e-06 | ms/batch 39.62865 | loss 0.69374 | ppl     2.001\n",
      "| epoch   1 step   206800 | 206800 batches | lr 1.17e-06 | ms/batch 39.88019 | loss 0.69272 | ppl     1.999\n",
      "| epoch   1 step   207000 | 207000 batches | lr 1.17e-06 | ms/batch 40.22400 | loss 0.69346 | ppl     2.001\n",
      "| epoch   1 step   207200 | 207200 batches | lr 1.17e-06 | ms/batch 40.61722 | loss 0.69457 | ppl     2.003\n",
      "| epoch   1 step   207400 | 207400 batches | lr 1.17e-06 | ms/batch 41.09636 | loss 0.69399 | ppl     2.002\n",
      "| epoch   1 step   207600 | 207600 batches | lr 1.17e-06 | ms/batch 40.08285 | loss 0.69368 | ppl     2.001\n",
      "| epoch   1 step   207800 | 207800 batches | lr 1.17e-06 | ms/batch 40.33792 | loss 0.69295 | ppl     2.000\n",
      "| epoch   1 step   208000 | 208000 batches | lr 1.17e-06 | ms/batch 39.44603 | loss 0.69243 | ppl     1.999\n",
      "|\n",
      "Source: [30  6 18  7 35  8 16  9 30 10]\n",
      "Target: [ 6 18  7 35  8 16  9 30 10  6]\n",
      "Teacher forcing: acc:0.6196875\n",
      "Preds:  [10  6 10  6 10 10 10  7 10  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  52 at step   208000 | time: 163.14s | valid loss 0.69669 | valid ppl    2.0071 | valid acc 0.62\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   208200 | 208200 batches | lr 1.17e-06 | ms/batch 46.14898 | loss 0.69320 | ppl     2.000\n",
      "| epoch   1 step   208400 | 208400 batches | lr 1.17e-06 | ms/batch 40.02968 | loss 0.69297 | ppl     2.000\n",
      "| epoch   1 step   208600 | 208600 batches | lr 1.17e-06 | ms/batch 40.83464 | loss 0.69324 | ppl     2.000\n",
      "| epoch   1 step   208800 | 208800 batches | lr 1.17e-06 | ms/batch 40.26273 | loss 0.69236 | ppl     1.998\n",
      "| epoch   1 step   209000 | 209000 batches | lr 1.17e-06 | ms/batch 40.80600 | loss 0.69368 | ppl     2.001\n",
      "| epoch   1 step   209200 | 209200 batches | lr 1.17e-06 | ms/batch 40.76874 | loss 0.69258 | ppl     1.999\n",
      "| epoch   1 step   209400 | 209400 batches | lr 1.17e-06 | ms/batch 40.73119 | loss 0.69278 | ppl     1.999\n",
      "| epoch   1 step   209600 | 209600 batches | lr 1.17e-06 | ms/batch 39.72674 | loss 0.69414 | ppl     2.002\n",
      "| epoch   1 step   209800 | 209800 batches | lr 1.17e-06 | ms/batch 40.51705 | loss 0.69342 | ppl     2.001\n",
      "| epoch   1 step   210000 | 210000 batches | lr 1.17e-06 | ms/batch 40.24799 | loss 0.69409 | ppl     2.002\n",
      "| epoch   1 step   210200 | 210200 batches | lr 1.17e-06 | ms/batch 39.61194 | loss 0.69251 | ppl     1.999\n",
      "| epoch   1 step   210400 | 210400 batches | lr 1.17e-06 | ms/batch 39.71841 | loss 0.69311 | ppl     2.000\n",
      "| epoch   1 step   210600 | 210600 batches | lr 1.17e-06 | ms/batch 39.92730 | loss 0.69386 | ppl     2.001\n",
      "| epoch   1 step   210800 | 210800 batches | lr 1.17e-06 | ms/batch 40.34298 | loss 0.69340 | ppl     2.001\n",
      "| epoch   1 step   211000 | 211000 batches | lr 1.17e-06 | ms/batch 40.82684 | loss 0.69271 | ppl     1.999\n",
      "| epoch   1 step   211200 | 211200 batches | lr 1.17e-06 | ms/batch 39.74196 | loss 0.69335 | ppl     2.000\n",
      "| epoch   1 step   211400 | 211400 batches | lr 1.17e-06 | ms/batch 40.49971 | loss 0.69235 | ppl     1.998\n",
      "| epoch   1 step   211600 | 211600 batches | lr 1.17e-06 | ms/batch 40.31150 | loss 0.69347 | ppl     2.001\n",
      "| epoch   1 step   211800 | 211800 batches | lr 1.17e-06 | ms/batch 39.08178 | loss 0.69259 | ppl     1.999\n",
      "| epoch   1 step   212000 | 212000 batches | lr 1.17e-06 | ms/batch 40.23929 | loss 0.69468 | ppl     2.003\n",
      "|\n",
      "Source: [18  6 28  9 33  2 11  8 28 10]\n",
      "Target: [ 6 28  9 33  2 11  8 28 10  9]\n",
      "Teacher forcing: acc:0.625625\n",
      "Preds:  [10  6 10  9 10  2 10 10 10  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  53 at step   212000 | time: 162.04s | valid loss 0.69639 | valid ppl    2.0065 | valid acc 0.626\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   212200 | 212200 batches | lr 1.17e-06 | ms/batch 46.44921 | loss 0.69440 | ppl     2.003\n",
      "| epoch   1 step   212400 | 212400 batches | lr 1.17e-06 | ms/batch 40.47235 | loss 0.69331 | ppl     2.000\n",
      "| epoch   1 step   212600 | 212600 batches | lr 1.17e-06 | ms/batch 40.56398 | loss 0.69214 | ppl     1.998\n",
      "| epoch   1 step   212800 | 212800 batches | lr 1.17e-06 | ms/batch 40.32758 | loss 0.69422 | ppl     2.002\n",
      "| epoch   1 step   213000 | 213000 batches | lr 1.17e-06 | ms/batch 40.12558 | loss 0.69382 | ppl     2.001\n",
      "| epoch   1 step   213200 | 213200 batches | lr 1.17e-06 | ms/batch 40.78464 | loss 0.69324 | ppl     2.000\n",
      "| epoch   1 step   213400 | 213400 batches | lr 1.17e-06 | ms/batch 40.27837 | loss 0.69461 | ppl     2.003\n",
      "| epoch   1 step   213600 | 213600 batches | lr 1.17e-06 | ms/batch 39.68202 | loss 0.69399 | ppl     2.002\n",
      "| epoch   1 step   213800 | 213800 batches | lr 1.17e-06 | ms/batch 40.26182 | loss 0.69184 | ppl     1.997\n",
      "| epoch   1 step   214000 | 214000 batches | lr 1.17e-06 | ms/batch 40.12905 | loss 0.69324 | ppl     2.000\n",
      "| epoch   1 step   214200 | 214200 batches | lr 1.17e-06 | ms/batch 39.93000 | loss 0.69286 | ppl     1.999\n",
      "| epoch   1 step   214400 | 214400 batches | lr 1.17e-06 | ms/batch 40.21149 | loss 0.69320 | ppl     2.000\n",
      "| epoch   1 step   214600 | 214600 batches | lr 1.17e-06 | ms/batch 40.28214 | loss 0.69246 | ppl     1.999\n",
      "| epoch   1 step   214800 | 214800 batches | lr 1.17e-06 | ms/batch 39.74322 | loss 0.69356 | ppl     2.001\n",
      "| epoch   1 step   215000 | 215000 batches | lr 1.17e-06 | ms/batch 40.69459 | loss 0.69251 | ppl     1.999\n",
      "| epoch   1 step   215200 | 215200 batches | lr 1.17e-06 | ms/batch 40.57392 | loss 0.69160 | ppl     1.997\n",
      "| epoch   1 step   215400 | 215400 batches | lr 1.17e-06 | ms/batch 39.59329 | loss 0.69259 | ppl     1.999\n",
      "| epoch   1 step   215600 | 215600 batches | lr 1.17e-06 | ms/batch 39.48476 | loss 0.69353 | ppl     2.001\n",
      "| epoch   1 step   215800 | 215800 batches | lr 1.17e-06 | ms/batch 39.44176 | loss 0.69353 | ppl     2.001\n",
      "| epoch   1 step   216000 | 216000 batches | lr 1.17e-06 | ms/batch 39.86455 | loss 0.69491 | ppl     2.004\n",
      "|\n",
      "Source: [15  8 16  6 20  0 33  7 33 10]\n",
      "Target: [ 8 16  6 20  0 33  7 33 10  7]\n",
      "Teacher forcing: acc:0.625625\n",
      "Preds:  [10  5 10  8 10  6 10  6 10  0]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  54 at step   216000 | time: 161.78s | valid loss 0.69575 | valid ppl    2.0052 | valid acc 0.626\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   216200 | 216200 batches | lr 1e-06 | ms/batch 45.37054 | loss 0.69212 | ppl     1.998\n",
      "| epoch   1 step   216400 | 216400 batches | lr 1e-06 | ms/batch 40.04486 | loss 0.69243 | ppl     1.999\n",
      "| epoch   1 step   216600 | 216600 batches | lr 1e-06 | ms/batch 39.39418 | loss 0.69384 | ppl     2.001\n",
      "| epoch   1 step   216800 | 216800 batches | lr 1e-06 | ms/batch 40.42396 | loss 0.69290 | ppl     2.000\n",
      "| epoch   1 step   217000 | 217000 batches | lr 1e-06 | ms/batch 40.75327 | loss 0.69388 | ppl     2.001\n",
      "| epoch   1 step   217200 | 217200 batches | lr 1e-06 | ms/batch 39.31703 | loss 0.69250 | ppl     1.999\n",
      "| epoch   1 step   217400 | 217400 batches | lr 1e-06 | ms/batch 39.80218 | loss 0.69293 | ppl     2.000\n",
      "| epoch   1 step   217600 | 217600 batches | lr 1e-06 | ms/batch 39.56947 | loss 0.69224 | ppl     1.998\n",
      "| epoch   1 step   217800 | 217800 batches | lr 1e-06 | ms/batch 39.82276 | loss 0.69276 | ppl     1.999\n",
      "| epoch   1 step   218000 | 218000 batches | lr 1e-06 | ms/batch 38.73141 | loss 0.69263 | ppl     1.999\n",
      "| epoch   1 step   218200 | 218200 batches | lr 1e-06 | ms/batch 39.57750 | loss 0.69325 | ppl     2.000\n",
      "| epoch   1 step   218400 | 218400 batches | lr 1e-06 | ms/batch 39.21542 | loss 0.69263 | ppl     1.999\n",
      "| epoch   1 step   218600 | 218600 batches | lr 1e-06 | ms/batch 39.69433 | loss 0.69287 | ppl     1.999\n",
      "| epoch   1 step   218800 | 218800 batches | lr 1e-06 | ms/batch 40.69328 | loss 0.69288 | ppl     1.999\n",
      "| epoch   1 step   219000 | 219000 batches | lr 1e-06 | ms/batch 39.86617 | loss 0.69395 | ppl     2.002\n",
      "| epoch   1 step   219200 | 219200 batches | lr 1e-06 | ms/batch 40.02143 | loss 0.69338 | ppl     2.000\n",
      "| epoch   1 step   219400 | 219400 batches | lr 1e-06 | ms/batch 40.19401 | loss 0.69268 | ppl     1.999\n",
      "| epoch   1 step   219600 | 219600 batches | lr 1e-06 | ms/batch 40.53915 | loss 0.69350 | ppl     2.001\n",
      "| epoch   1 step   219800 | 219800 batches | lr 1e-06 | ms/batch 39.56076 | loss 0.69223 | ppl     1.998\n",
      "| epoch   1 step   220000 | 220000 batches | lr 1e-06 | ms/batch 39.72590 | loss 0.69382 | ppl     2.001\n",
      "|\n",
      "Source: [25  5 14  1 33  7 36  4 25 10]\n",
      "Target: [ 5 14  1 33  7 36  4 25 10  5]\n",
      "Teacher forcing: acc:0.626875\n",
      "Preds:  [10  5 10  4 10  1 10  4 10  1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  55 at step   220000 | time: 160.49s | valid loss 0.69553 | valid ppl    2.0048 | valid acc 0.627\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   220200 | 220200 batches | lr 1e-06 | ms/batch 47.06564 | loss 0.69464 | ppl     2.003\n",
      "| epoch   1 step   220400 | 220400 batches | lr 1e-06 | ms/batch 39.67170 | loss 0.69358 | ppl     2.001\n",
      "| epoch   1 step   220600 | 220600 batches | lr 1e-06 | ms/batch 40.28174 | loss 0.69364 | ppl     2.001\n",
      "| epoch   1 step   220800 | 220800 batches | lr 1e-06 | ms/batch 39.64207 | loss 0.69351 | ppl     2.001\n",
      "| epoch   1 step   221000 | 221000 batches | lr 1e-06 | ms/batch 40.15390 | loss 0.69257 | ppl     1.999\n",
      "| epoch   1 step   221200 | 221200 batches | lr 1e-06 | ms/batch 40.43926 | loss 0.69360 | ppl     2.001\n",
      "| epoch   1 step   221400 | 221400 batches | lr 1e-06 | ms/batch 39.85688 | loss 0.69389 | ppl     2.001\n",
      "| epoch   1 step   221600 | 221600 batches | lr 1e-06 | ms/batch 40.64368 | loss 0.69244 | ppl     1.999\n",
      "| epoch   1 step   221800 | 221800 batches | lr 1e-06 | ms/batch 39.88058 | loss 0.69351 | ppl     2.001\n",
      "| epoch   1 step   222000 | 222000 batches | lr 1e-06 | ms/batch 40.83627 | loss 0.69281 | ppl     1.999\n",
      "| epoch   1 step   222200 | 222200 batches | lr 1e-06 | ms/batch 39.98180 | loss 0.69329 | ppl     2.000\n",
      "| epoch   1 step   222400 | 222400 batches | lr 1e-06 | ms/batch 40.71600 | loss 0.69508 | ppl     2.004\n",
      "| epoch   1 step   222600 | 222600 batches | lr 1e-06 | ms/batch 40.63871 | loss 0.69265 | ppl     1.999\n",
      "| epoch   1 step   222800 | 222800 batches | lr 1e-06 | ms/batch 40.30658 | loss 0.69391 | ppl     2.002\n",
      "| epoch   1 step   223000 | 223000 batches | lr 1e-06 | ms/batch 40.22159 | loss 0.69310 | ppl     2.000\n",
      "| epoch   1 step   223200 | 223200 batches | lr 1e-06 | ms/batch 39.70030 | loss 0.69217 | ppl     1.998\n",
      "| epoch   1 step   223400 | 223400 batches | lr 1e-06 | ms/batch 39.65326 | loss 0.69351 | ppl     2.001\n",
      "| epoch   1 step   223600 | 223600 batches | lr 1e-06 | ms/batch 39.92661 | loss 0.69317 | ppl     2.000\n",
      "| epoch   1 step   223800 | 223800 batches | lr 1e-06 | ms/batch 39.99672 | loss 0.69270 | ppl     1.999\n",
      "| epoch   1 step   224000 | 224000 batches | lr 1e-06 | ms/batch 39.61665 | loss 0.69306 | ppl     2.000\n",
      "maslina\n",
      "|\n",
      "Source: [23  9 20  1 29  5 14  3 20 10]\n",
      "Target: [ 9 20  1 29  5 14  3 20 10  1]\n",
      "Teacher forcing: acc:0.6160714285714286\n",
      "Preds:  [10  9 10  4 10  5 10  5 10  5]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  56 at step   224000 | time: 161.78s | valid loss 0.69706 | valid ppl    2.0078 | valid acc 0.616\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   224200 | 224200 batches | lr 1e-06 | ms/batch 45.45458 | loss 0.69361 | ppl     2.001\n",
      "| epoch   1 step   224400 | 224400 batches | lr 1e-06 | ms/batch 40.24983 | loss 0.69238 | ppl     1.998\n",
      "| epoch   1 step   224600 | 224600 batches | lr 1e-06 | ms/batch 40.34668 | loss 0.69408 | ppl     2.002\n",
      "| epoch   1 step   224800 | 224800 batches | lr 1e-06 | ms/batch 39.85559 | loss 0.69310 | ppl     2.000\n",
      "| epoch   1 step   225000 | 225000 batches | lr 1e-06 | ms/batch 40.58292 | loss 0.69363 | ppl     2.001\n",
      "| epoch   1 step   225200 | 225200 batches | lr 1e-06 | ms/batch 41.04893 | loss 0.69318 | ppl     2.000\n",
      "| epoch   1 step   225400 | 225400 batches | lr 1e-06 | ms/batch 40.44345 | loss 0.69332 | ppl     2.000\n",
      "| epoch   1 step   225600 | 225600 batches | lr 1e-06 | ms/batch 39.65119 | loss 0.69338 | ppl     2.000\n",
      "| epoch   1 step   225800 | 225800 batches | lr 1e-06 | ms/batch 39.90031 | loss 0.69216 | ppl     1.998\n",
      "| epoch   1 step   226000 | 226000 batches | lr 1e-06 | ms/batch 40.56169 | loss 0.69320 | ppl     2.000\n",
      "| epoch   1 step   226200 | 226200 batches | lr 1e-06 | ms/batch 39.69618 | loss 0.69313 | ppl     2.000\n",
      "| epoch   1 step   226400 | 226400 batches | lr 1e-06 | ms/batch 39.99123 | loss 0.69218 | ppl     1.998\n",
      "| epoch   1 step   226600 | 226600 batches | lr 1e-06 | ms/batch 40.64734 | loss 0.69291 | ppl     2.000\n",
      "| epoch   1 step   226800 | 226800 batches | lr 1e-06 | ms/batch 40.59423 | loss 0.69229 | ppl     1.998\n",
      "| epoch   1 step   227000 | 227000 batches | lr 1e-06 | ms/batch 39.94133 | loss 0.69259 | ppl     1.999\n",
      "| epoch   1 step   227200 | 227200 batches | lr 1e-06 | ms/batch 39.90362 | loss 0.69368 | ppl     2.001\n",
      "| epoch   1 step   227400 | 227400 batches | lr 1e-06 | ms/batch 40.81661 | loss 0.69392 | ppl     2.002\n",
      "| epoch   1 step   227600 | 227600 batches | lr 1e-06 | ms/batch 40.66909 | loss 0.69368 | ppl     2.001\n",
      "| epoch   1 step   227800 | 227800 batches | lr 1e-06 | ms/batch 40.20263 | loss 0.69241 | ppl     1.999\n",
      "| epoch   1 step   228000 | 228000 batches | lr 1e-06 | ms/batch 40.55857 | loss 0.69265 | ppl     1.999\n",
      "|\n",
      "Source: [33  2 15  7 32  3 24  8 32 10]\n",
      "Target: [ 2 15  7 32  3 24  8 32 10  3]\n",
      "Teacher forcing: acc:0.625\n",
      "Preds:  [10  2 10  7 10  3 10 10 10  8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  57 at step   228000 | time: 162.29s | valid loss 0.69530 | valid ppl    2.0043 | valid acc 0.625\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   228200 | 228200 batches | lr 1e-06 | ms/batch 46.71286 | loss 0.69379 | ppl     2.001\n",
      "| epoch   1 step   228400 | 228400 batches | lr 1e-06 | ms/batch 40.50426 | loss 0.69394 | ppl     2.002\n",
      "| epoch   1 step   228600 | 228600 batches | lr 1e-06 | ms/batch 40.31782 | loss 0.69222 | ppl     1.998\n",
      "| epoch   1 step   228800 | 228800 batches | lr 1e-06 | ms/batch 40.64668 | loss 0.69443 | ppl     2.003\n",
      "| epoch   1 step   229000 | 229000 batches | lr 1e-06 | ms/batch 41.10282 | loss 0.69409 | ppl     2.002\n",
      "| epoch   1 step   229200 | 229200 batches | lr 1e-06 | ms/batch 39.96041 | loss 0.69398 | ppl     2.002\n",
      "| epoch   1 step   229400 | 229400 batches | lr 1e-06 | ms/batch 39.80740 | loss 0.69411 | ppl     2.002\n",
      "| epoch   1 step   229600 | 229600 batches | lr 1e-06 | ms/batch 40.59290 | loss 0.69218 | ppl     1.998\n",
      "| epoch   1 step   229800 | 229800 batches | lr 1e-06 | ms/batch 40.85405 | loss 0.69291 | ppl     2.000\n",
      "| epoch   1 step   230000 | 230000 batches | lr 1e-06 | ms/batch 39.89794 | loss 0.69371 | ppl     2.001\n",
      "| epoch   1 step   230200 | 230200 batches | lr 1e-06 | ms/batch 40.69072 | loss 0.69226 | ppl     1.998\n",
      "| epoch   1 step   230400 | 230400 batches | lr 1e-06 | ms/batch 41.60538 | loss 0.69191 | ppl     1.998\n",
      "| epoch   1 step   230600 | 230600 batches | lr 1e-06 | ms/batch 39.88195 | loss 0.69272 | ppl     1.999\n",
      "| epoch   1 step   230800 | 230800 batches | lr 1e-06 | ms/batch 40.46210 | loss 0.69469 | ppl     2.003\n",
      "| epoch   1 step   231000 | 231000 batches | lr 1e-06 | ms/batch 40.36833 | loss 0.69403 | ppl     2.002\n",
      "| epoch   1 step   231200 | 231200 batches | lr 1e-06 | ms/batch 40.21418 | loss 0.69213 | ppl     1.998\n",
      "| epoch   1 step   231400 | 231400 batches | lr 1e-06 | ms/batch 39.72014 | loss 0.69207 | ppl     1.998\n",
      "| epoch   1 step   231600 | 231600 batches | lr 1e-06 | ms/batch 39.97099 | loss 0.69313 | ppl     2.000\n",
      "| epoch   1 step   231800 | 231800 batches | lr 1e-06 | ms/batch 40.07329 | loss 0.69425 | ppl     2.002\n",
      "| epoch   1 step   232000 | 232000 batches | lr 1e-06 | ms/batch 40.11816 | loss 0.69300 | ppl     2.000\n",
      "|\n",
      "Source: [26  1 29  9 20  0 24  8 29 10]\n",
      "Target: [ 1 29  9 20  0 24  8 29 10  9]\n",
      "Teacher forcing: acc:0.6175\n",
      "Preds:  [10  4 10  9 10  0 10 10 10  9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  58 at step   232000 | time: 162.73s | valid loss 0.69830 | valid ppl    2.0103 | valid acc 0.618\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   232200 | 232200 batches | lr 1e-06 | ms/batch 46.15056 | loss 0.69354 | ppl     2.001\n",
      "| epoch   1 step   232400 | 232400 batches | lr 1e-06 | ms/batch 40.45132 | loss 0.69353 | ppl     2.001\n",
      "| epoch   1 step   232600 | 232600 batches | lr 1e-06 | ms/batch 39.96669 | loss 0.69414 | ppl     2.002\n",
      "| epoch   1 step   232800 | 232800 batches | lr 1e-06 | ms/batch 40.41874 | loss 0.69311 | ppl     2.000\n",
      "| epoch   1 step   233000 | 233000 batches | lr 1e-06 | ms/batch 40.03601 | loss 0.69346 | ppl     2.001\n",
      "| epoch   1 step   233200 | 233200 batches | lr 1e-06 | ms/batch 39.75816 | loss 0.69291 | ppl     2.000\n",
      "| epoch   1 step   233400 | 233400 batches | lr 1e-06 | ms/batch 40.08788 | loss 0.69348 | ppl     2.001\n",
      "| epoch   1 step   233600 | 233600 batches | lr 1e-06 | ms/batch 40.46428 | loss 0.69358 | ppl     2.001\n",
      "| epoch   1 step   233800 | 233800 batches | lr 1e-06 | ms/batch 39.29903 | loss 0.69209 | ppl     1.998\n",
      "| epoch   1 step   234000 | 234000 batches | lr 1e-06 | ms/batch 40.58375 | loss 0.69329 | ppl     2.000\n",
      "| epoch   1 step   234200 | 234200 batches | lr 1e-06 | ms/batch 39.69473 | loss 0.69355 | ppl     2.001\n",
      "| epoch   1 step   234400 | 234400 batches | lr 1e-06 | ms/batch 39.59815 | loss 0.69318 | ppl     2.000\n",
      "| epoch   1 step   234600 | 234600 batches | lr 1e-06 | ms/batch 39.13523 | loss 0.69318 | ppl     2.000\n",
      "| epoch   1 step   234800 | 234800 batches | lr 1e-06 | ms/batch 39.75007 | loss 0.69424 | ppl     2.002\n",
      "| epoch   1 step   235000 | 235000 batches | lr 1e-06 | ms/batch 40.00126 | loss 0.69273 | ppl     1.999\n",
      "| epoch   1 step   235200 | 235200 batches | lr 1e-06 | ms/batch 40.33623 | loss 0.69241 | ppl     1.999\n",
      "| epoch   1 step   235400 | 235400 batches | lr 1e-06 | ms/batch 40.42295 | loss 0.69213 | ppl     1.998\n",
      "| epoch   1 step   235600 | 235600 batches | lr 1e-06 | ms/batch 39.35791 | loss 0.69250 | ppl     1.999\n",
      "| epoch   1 step   235800 | 235800 batches | lr 1e-06 | ms/batch 39.60349 | loss 0.69233 | ppl     1.998\n",
      "| epoch   1 step   236000 | 236000 batches | lr 1e-06 | ms/batch 39.80853 | loss 0.69363 | ppl     2.001\n",
      "|\n",
      "Source: [21  6 16  3 23  1 24  9 23 10]\n",
      "Target: [ 6 16  3 23  1 24  9 23 10  1]\n",
      "Teacher forcing: acc:0.62375\n",
      "Preds:  [10  6 10  6 10  1 10  6 10  6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  59 at step   236000 | time: 160.92s | valid loss 0.69665 | valid ppl    2.0070 | valid acc 0.624\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   236200 | 236200 batches | lr 1e-06 | ms/batch 45.49815 | loss 0.69251 | ppl     1.999\n",
      "| epoch   1 step   236400 | 236400 batches | lr 1e-06 | ms/batch 40.66606 | loss 0.69267 | ppl     1.999\n",
      "| epoch   1 step   236600 | 236600 batches | lr 1e-06 | ms/batch 40.52334 | loss 0.69259 | ppl     1.999\n",
      "| epoch   1 step   236800 | 236800 batches | lr 1e-06 | ms/batch 39.77796 | loss 0.69288 | ppl     1.999\n",
      "| epoch   1 step   237000 | 237000 batches | lr 1e-06 | ms/batch 41.01866 | loss 0.69389 | ppl     2.001\n",
      "| epoch   1 step   237200 | 237200 batches | lr 1e-06 | ms/batch 40.64235 | loss 0.69225 | ppl     1.998\n",
      "| epoch   1 step   237400 | 237400 batches | lr 1e-06 | ms/batch 40.92796 | loss 0.69206 | ppl     1.998\n",
      "| epoch   1 step   237600 | 237600 batches | lr 1e-06 | ms/batch 40.91862 | loss 0.69215 | ppl     1.998\n",
      "| epoch   1 step   237800 | 237800 batches | lr 1e-06 | ms/batch 40.02572 | loss 0.69406 | ppl     2.002\n",
      "| epoch   1 step   238000 | 238000 batches | lr 1e-06 | ms/batch 39.99283 | loss 0.69385 | ppl     2.001\n",
      "| epoch   1 step   238200 | 238200 batches | lr 1e-06 | ms/batch 40.23071 | loss 0.69233 | ppl     1.998\n",
      "| epoch   1 step   238400 | 238400 batches | lr 1e-06 | ms/batch 40.51912 | loss 0.69302 | ppl     2.000\n",
      "| epoch   1 step   238600 | 238600 batches | lr 1e-06 | ms/batch 40.50459 | loss 0.69387 | ppl     2.001\n",
      "| epoch   1 step   238800 | 238800 batches | lr 1e-06 | ms/batch 40.86283 | loss 0.69273 | ppl     1.999\n",
      "| epoch   1 step   239000 | 239000 batches | lr 1e-06 | ms/batch 39.08976 | loss 0.69233 | ppl     1.998\n",
      "| epoch   1 step   239200 | 239200 batches | lr 1e-06 | ms/batch 39.34532 | loss 0.69240 | ppl     1.998\n",
      "| epoch   1 step   239400 | 239400 batches | lr 1e-06 | ms/batch 40.34130 | loss 0.69318 | ppl     2.000\n",
      "| epoch   1 step   239600 | 239600 batches | lr 1e-06 | ms/batch 40.68730 | loss 0.69380 | ppl     2.001\n",
      "| epoch   1 step   239800 | 239800 batches | lr 1e-06 | ms/batch 39.13743 | loss 0.69260 | ppl     1.999\n",
      "| epoch   1 step   240000 | 240000 batches | lr 1e-06 | ms/batch 40.85027 | loss 0.69333 | ppl     2.000\n",
      "|\n",
      "Source: [36  7 27  5 34  3 30  1 36 10]\n",
      "Target: [ 7 27  5 34  3 30  1 36 10  7]\n",
      "Teacher forcing: acc:0.62875\n",
      "Preds:  [10  1 10  5 10  5 10  7 10  3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  60 at step   240000 | time: 162.37s | valid loss 0.69465 | valid ppl    2.0030 | valid acc 0.629\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   240200 | 240200 batches | lr 1e-06 | ms/batch 45.88003 | loss 0.69102 | ppl     1.996\n",
      "| epoch   1 step   240400 | 240400 batches | lr 1e-06 | ms/batch 40.59188 | loss 0.69409 | ppl     2.002\n",
      "| epoch   1 step   240600 | 240600 batches | lr 1e-06 | ms/batch 39.85689 | loss 0.69280 | ppl     1.999\n",
      "| epoch   1 step   240800 | 240800 batches | lr 1e-06 | ms/batch 39.43469 | loss 0.69394 | ppl     2.002\n",
      "| epoch   1 step   241000 | 241000 batches | lr 1e-06 | ms/batch 40.00914 | loss 0.69380 | ppl     2.001\n",
      "| epoch   1 step   241200 | 241200 batches | lr 1e-06 | ms/batch 40.54274 | loss 0.69334 | ppl     2.000\n",
      "| epoch   1 step   241400 | 241400 batches | lr 1e-06 | ms/batch 40.93370 | loss 0.69321 | ppl     2.000\n",
      "| epoch   1 step   241600 | 241600 batches | lr 1e-06 | ms/batch 40.74549 | loss 0.69330 | ppl     2.000\n",
      "| epoch   1 step   241800 | 241800 batches | lr 1e-06 | ms/batch 40.75807 | loss 0.69311 | ppl     2.000\n",
      "| epoch   1 step   242000 | 242000 batches | lr 1e-06 | ms/batch 41.34852 | loss 0.69396 | ppl     2.002\n",
      "| epoch   1 step   242200 | 242200 batches | lr 1e-06 | ms/batch 39.74931 | loss 0.69306 | ppl     2.000\n",
      "| epoch   1 step   242400 | 242400 batches | lr 1e-06 | ms/batch 40.81931 | loss 0.69342 | ppl     2.001\n",
      "| epoch   1 step   242600 | 242600 batches | lr 1e-06 | ms/batch 41.23301 | loss 0.69214 | ppl     1.998\n",
      "| epoch   1 step   242800 | 242800 batches | lr 1e-06 | ms/batch 40.63554 | loss 0.69179 | ppl     1.997\n",
      "| epoch   1 step   243000 | 243000 batches | lr 1e-06 | ms/batch 40.41491 | loss 0.69317 | ppl     2.000\n",
      "| epoch   1 step   243200 | 243200 batches | lr 1e-06 | ms/batch 39.83995 | loss 0.69309 | ppl     2.000\n",
      "| epoch   1 step   243400 | 243400 batches | lr 1e-06 | ms/batch 40.46892 | loss 0.69291 | ppl     2.000\n",
      "| epoch   1 step   243600 | 243600 batches | lr 1e-06 | ms/batch 40.86402 | loss 0.69394 | ppl     2.002\n",
      "| epoch   1 step   243800 | 243800 batches | lr 1e-06 | ms/batch 41.04468 | loss 0.69245 | ppl     1.999\n",
      "| epoch   1 step   244000 | 244000 batches | lr 1e-06 | ms/batch 41.07058 | loss 0.69433 | ppl     2.002\n",
      "|\n",
      "Source: [29  8 12  2 20  4 19  7 29 10]\n",
      "Target: [ 8 12  2 20  4 19  7 29 10  8]\n",
      "Teacher forcing: acc:0.6284375\n",
      "Preds:  [10  5 10  2 10  2 10  7 10  7]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  61 at step   244000 | time: 163.24s | valid loss 0.69563 | valid ppl    2.0050 | valid acc 0.628\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   244200 | 244200 batches | lr 1e-06 | ms/batch 46.09121 | loss 0.69321 | ppl     2.000\n",
      "| epoch   1 step   244400 | 244400 batches | lr 1e-06 | ms/batch 41.13611 | loss 0.69309 | ppl     2.000\n",
      "| epoch   1 step   244600 | 244600 batches | lr 1e-06 | ms/batch 40.53982 | loss 0.69330 | ppl     2.000\n",
      "| epoch   1 step   244800 | 244800 batches | lr 1e-06 | ms/batch 40.54567 | loss 0.69292 | ppl     2.000\n",
      "| epoch   1 step   245000 | 245000 batches | lr 1e-06 | ms/batch 39.89026 | loss 0.69216 | ppl     1.998\n",
      "| epoch   1 step   245200 | 245200 batches | lr 1e-06 | ms/batch 40.37748 | loss 0.69317 | ppl     2.000\n",
      "| epoch   1 step   245400 | 245400 batches | lr 1e-06 | ms/batch 40.91517 | loss 0.69306 | ppl     2.000\n",
      "| epoch   1 step   245600 | 245600 batches | lr 1e-06 | ms/batch 40.43846 | loss 0.69310 | ppl     2.000\n",
      "| epoch   1 step   245800 | 245800 batches | lr 1e-06 | ms/batch 39.76561 | loss 0.69180 | ppl     1.997\n",
      "| epoch   1 step   246000 | 246000 batches | lr 1e-06 | ms/batch 40.13328 | loss 0.69403 | ppl     2.002\n",
      "| epoch   1 step   246200 | 246200 batches | lr 1e-06 | ms/batch 40.60585 | loss 0.69278 | ppl     1.999\n",
      "| epoch   1 step   246400 | 246400 batches | lr 1e-06 | ms/batch 41.06932 | loss 0.69278 | ppl     1.999\n",
      "| epoch   1 step   246600 | 246600 batches | lr 1e-06 | ms/batch 39.97133 | loss 0.69187 | ppl     1.997\n",
      "| epoch   1 step   246800 | 246800 batches | lr 1e-06 | ms/batch 40.60821 | loss 0.69293 | ppl     2.000\n",
      "| epoch   1 step   247000 | 247000 batches | lr 1e-06 | ms/batch 39.77686 | loss 0.69362 | ppl     2.001\n",
      "| epoch   1 step   247200 | 247200 batches | lr 1e-06 | ms/batch 39.48971 | loss 0.69325 | ppl     2.000\n",
      "| epoch   1 step   247400 | 247400 batches | lr 1e-06 | ms/batch 40.21793 | loss 0.69323 | ppl     2.000\n",
      "| epoch   1 step   247600 | 247600 batches | lr 1e-06 | ms/batch 40.99626 | loss 0.69225 | ppl     1.998\n",
      "| epoch   1 step   247800 | 247800 batches | lr 1e-06 | ms/batch 41.05234 | loss 0.69368 | ppl     2.001\n",
      "| epoch   1 step   248000 | 248000 batches | lr 1e-06 | ms/batch 41.92488 | loss 0.69185 | ppl     1.997\n",
      "|\n",
      "Source: [34  4 31  7 30  8 18  6 34 10]\n",
      "Target: [ 4 31  7 30  8 18  6 34 10  4]\n",
      "Teacher forcing: acc:0.6175\n",
      "Preds:  [10  4 10  0 10 10 10  8 10  8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  62 at step   248000 | time: 163.12s | valid loss 0.69748 | valid ppl    2.0087 | valid acc 0.618\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   248200 | 248200 batches | lr 1e-06 | ms/batch 47.54982 | loss 0.69282 | ppl     1.999\n",
      "| epoch   1 step   248400 | 248400 batches | lr 1e-06 | ms/batch 42.31220 | loss 0.69389 | ppl     2.001\n",
      "| epoch   1 step   248600 | 248600 batches | lr 1e-06 | ms/batch 41.51631 | loss 0.69210 | ppl     1.998\n",
      "| epoch   1 step   248800 | 248800 batches | lr 1e-06 | ms/batch 40.88459 | loss 0.69353 | ppl     2.001\n",
      "| epoch   1 step   249000 | 249000 batches | lr 1e-06 | ms/batch 40.86391 | loss 0.69356 | ppl     2.001\n",
      "| epoch   1 step   249200 | 249200 batches | lr 1e-06 | ms/batch 41.19638 | loss 0.69227 | ppl     1.998\n",
      "| epoch   1 step   249400 | 249400 batches | lr 1e-06 | ms/batch 42.22043 | loss 0.69301 | ppl     2.000\n",
      "| epoch   1 step   249600 | 249600 batches | lr 1e-06 | ms/batch 40.36232 | loss 0.69161 | ppl     1.997\n",
      "| epoch   1 step   249800 | 249800 batches | lr 1e-06 | ms/batch 41.33050 | loss 0.69228 | ppl     1.998\n",
      "| epoch   1 step   250000 | 250000 batches | lr 1e-06 | ms/batch 41.17542 | loss 0.69246 | ppl     1.999\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "|\n",
      "Source: [14  1 16  4 30  2 13  7 30 10]\n",
      "Target: [ 1 16  4 30  2 13  7 30 10  2]\n",
      "Teacher forcing: acc:0.6309375\n",
      "Preds:  [10  4 10  4 10 10 10  1 10  2]\n",
      "\n",
      "====================================================================================================\n",
      "| End of training | test loss 0.69561 | test ppl   2.00493\n",
      " | test acc 0.631\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "!bash run_retrieval.sh train --work_dir ../evaluation/tl10xl0mt0_lr3e-4_rlrp_as2 --answer_size 2 --lr 0.0003 --tgt_len 10 --eval_tgt_len 10 --mem_len 0 --num_mem_tokens 0 --device_ids 0 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4cc555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[0, 2]\n",
      "Experiment dir : ../evaluation/tl10xl0mt0_l2h2_lr3e-4_rlrp_as1-retrieval/20220113-101945\n",
      "====================================================================================================\n",
      "    - data : /home/admin/x-transformers/data\n",
      "    - dataset : retrieval\n",
      "    - n_layer : 2\n",
      "    - n_head : 2\n",
      "    - d_head : 64\n",
      "    - d_embed : 128\n",
      "    - d_model : 128\n",
      "    - d_inner : 256\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.0003\n",
      "    - mom : 0.0\n",
      "    - scheduler : dev_perf\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 1e-06\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 250000\n",
      "    - batch_size : 32\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 10\n",
      "    - eval_tgt_len : 10\n",
      "    - ext_len : 0\n",
      "    - mem_len : 0\n",
      "    - num_mem_tokens : 0\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : False\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : ../evaluation/tl10xl0mt0_l2h2_lr3e-4_rlrp_as1-retrieval/20220113-101945\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : 50\n",
      "    - sample_softmax : -1\n",
      "    - patience : 3\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [0, 2]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : True\n",
      "    - read_mem_from_cache : True\n",
      "    - answer_size : 1\n",
      "    - tied : True\n",
      "    - ntokens : 37\n",
      "    - n_all_param : 301733\n",
      "    - n_nonemb_param : 296704\n",
      "====================================================================================================\n",
      "#params = 301733\n",
      "#non emb params = 296704\n",
      "| epoch   1 step      200 |    200 batches | lr 0.0003 | ms/batch 36.19106 | loss 2.28241 | ppl     9.800\n",
      "| epoch   1 step      400 |    400 batches | lr 0.0003 | ms/batch 20.84359 | loss 1.70467 | ppl     5.500\n",
      "| epoch   1 step      600 |    600 batches | lr 0.0003 | ms/batch 20.71970 | loss 1.61302 | ppl     5.018\n",
      "| epoch   1 step      800 |    800 batches | lr 0.0003 | ms/batch 20.47344 | loss 1.57327 | ppl     4.822\n",
      "| epoch   1 step     1000 |   1000 batches | lr 0.0003 | ms/batch 21.21388 | loss 1.55611 | ppl     4.740\n",
      "| epoch   1 step     1200 |   1200 batches | lr 0.0003 | ms/batch 20.99420 | loss 1.54722 | ppl     4.698\n",
      "| epoch   1 step     1400 |   1400 batches | lr 0.0003 | ms/batch 20.22676 | loss 1.53119 | ppl     4.624\n",
      "| epoch   1 step     1600 |   1600 batches | lr 0.0003 | ms/batch 20.64193 | loss 1.53061 | ppl     4.621\n",
      "| epoch   1 step     1800 |   1800 batches | lr 0.0003 | ms/batch 20.28532 | loss 1.51816 | ppl     4.564\n",
      "| epoch   1 step     2000 |   2000 batches | lr 0.0003 | ms/batch 20.19951 | loss 1.51729 | ppl     4.560\n",
      "| epoch   1 step     2200 |   2200 batches | lr 0.0003 | ms/batch 20.82358 | loss 1.50717 | ppl     4.514\n",
      "| epoch   1 step     2400 |   2400 batches | lr 0.0003 | ms/batch 20.63767 | loss 1.50537 | ppl     4.506\n",
      "| epoch   1 step     2600 |   2600 batches | lr 0.0003 | ms/batch 20.15867 | loss 1.50030 | ppl     4.483\n",
      "| epoch   1 step     2800 |   2800 batches | lr 0.0003 | ms/batch 20.64392 | loss 1.48115 | ppl     4.398\n",
      "| epoch   1 step     3000 |   3000 batches | lr 0.0003 | ms/batch 20.66234 | loss 1.48703 | ppl     4.424\n",
      "| epoch   1 step     3200 |   3200 batches | lr 0.0003 | ms/batch 20.17140 | loss 1.47835 | ppl     4.386\n",
      "| epoch   1 step     3400 |   3400 batches | lr 0.0003 | ms/batch 19.94759 | loss 1.47723 | ppl     4.381\n",
      "| epoch   1 step     3600 |   3600 batches | lr 0.0003 | ms/batch 20.61949 | loss 1.47076 | ppl     4.353\n",
      "| epoch   1 step     3800 |   3800 batches | lr 0.0003 | ms/batch 21.06361 | loss 1.47422 | ppl     4.368\n",
      "| epoch   1 step     4000 |   4000 batches | lr 0.0003 | ms/batch 21.12077 | loss 1.46633 | ppl     4.333\n",
      "|\n",
      "Source: [36  3 31  2 24  5 28  7 24 10]\n",
      "Target: [ 3 31  2 24  5 28  7 24 10  5]\n",
      "Teacher forcing: acc:0.2625\n",
      "Preds:  [6 3 6 2 3 5 3 7 3 3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step     4000 | time: 86.15s | valid loss 1.43096 | valid ppl    4.1827 | valid acc 0.263\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     4200 |   4200 batches | lr 0.0003 | ms/batch 24.67963 | loss 1.46776 | ppl     4.340\n",
      "| epoch   1 step     4400 |   4400 batches | lr 0.0003 | ms/batch 20.88235 | loss 1.47312 | ppl     4.363\n",
      "| epoch   1 step     4600 |   4600 batches | lr 0.0003 | ms/batch 20.87266 | loss 1.45748 | ppl     4.295\n",
      "| epoch   1 step     4800 |   4800 batches | lr 0.0003 | ms/batch 21.41709 | loss 1.45872 | ppl     4.300\n",
      "| epoch   1 step     5000 |   5000 batches | lr 0.0003 | ms/batch 21.20314 | loss 1.45399 | ppl     4.280\n",
      "| epoch   1 step     5200 |   5200 batches | lr 0.0003 | ms/batch 20.62580 | loss 1.45910 | ppl     4.302\n",
      "| epoch   1 step     5400 |   5400 batches | lr 0.0003 | ms/batch 21.42569 | loss 1.45945 | ppl     4.304\n",
      "| epoch   1 step     5600 |   5600 batches | lr 0.0003 | ms/batch 20.81664 | loss 1.45427 | ppl     4.281\n",
      "| epoch   1 step     5800 |   5800 batches | lr 0.0003 | ms/batch 20.64554 | loss 1.45128 | ppl     4.269\n",
      "| epoch   1 step     6000 |   6000 batches | lr 0.0003 | ms/batch 20.92719 | loss 1.45599 | ppl     4.289\n",
      "| epoch   1 step     6200 |   6200 batches | lr 0.0003 | ms/batch 21.21239 | loss 1.44645 | ppl     4.248\n",
      "| epoch   1 step     6400 |   6400 batches | lr 0.0003 | ms/batch 20.75486 | loss 1.45470 | ppl     4.283\n",
      "| epoch   1 step     6600 |   6600 batches | lr 0.0003 | ms/batch 20.82398 | loss 1.44734 | ppl     4.252\n",
      "| epoch   1 step     6800 |   6800 batches | lr 0.0003 | ms/batch 20.85513 | loss 1.45008 | ppl     4.263\n",
      "| epoch   1 step     7000 |   7000 batches | lr 0.0003 | ms/batch 21.00034 | loss 1.44775 | ppl     4.254\n",
      "| epoch   1 step     7200 |   7200 batches | lr 0.0003 | ms/batch 21.02966 | loss 1.44099 | ppl     4.225\n",
      "| epoch   1 step     7400 |   7400 batches | lr 0.0003 | ms/batch 21.41358 | loss 1.44310 | ppl     4.234\n",
      "| epoch   1 step     7600 |   7600 batches | lr 0.0003 | ms/batch 20.89563 | loss 1.44427 | ppl     4.239\n",
      "| epoch   1 step     7800 |   7800 batches | lr 0.0003 | ms/batch 21.30499 | loss 1.44276 | ppl     4.232\n",
      "| epoch   1 step     8000 |   8000 batches | lr 0.0003 | ms/batch 20.50925 | loss 1.44470 | ppl     4.241\n",
      "|\n",
      "Source: [26  9 21  1 25  4 15  3 26 10]\n",
      "Target: [ 9 21  1 25  4 15  3 26 10  9]\n",
      "Teacher forcing: acc:0.273125\n",
      "Preds:  [6 9 4 1 4 4 1 3 4 3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step     8000 | time: 84.65s | valid loss 1.40668 | valid ppl    4.0824 | valid acc 0.273\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     8200 |   8200 batches | lr 0.0003 | ms/batch 23.82574 | loss 1.44538 | ppl     4.243\n",
      "| epoch   1 step     8400 |   8400 batches | lr 0.0003 | ms/batch 20.34855 | loss 1.44278 | ppl     4.232\n",
      "| epoch   1 step     8600 |   8600 batches | lr 0.0003 | ms/batch 20.32819 | loss 1.44241 | ppl     4.231\n",
      "| epoch   1 step     8800 |   8800 batches | lr 0.0003 | ms/batch 21.17106 | loss 1.44633 | ppl     4.247\n",
      "| epoch   1 step     9000 |   9000 batches | lr 0.0003 | ms/batch 21.11057 | loss 1.44088 | ppl     4.224\n",
      "| epoch   1 step     9200 |   9200 batches | lr 0.0003 | ms/batch 20.82389 | loss 1.44123 | ppl     4.226\n",
      "| epoch   1 step     9400 |   9400 batches | lr 0.0003 | ms/batch 21.47135 | loss 1.44141 | ppl     4.227\n",
      "| epoch   1 step     9600 |   9600 batches | lr 0.0003 | ms/batch 21.16361 | loss 1.44256 | ppl     4.232\n",
      "| epoch   1 step     9800 |   9800 batches | lr 0.0003 | ms/batch 20.97150 | loss 1.43773 | ppl     4.211\n",
      "| epoch   1 step    10000 |  10000 batches | lr 0.0003 | ms/batch 20.84988 | loss 1.44237 | ppl     4.231\n",
      "| epoch   1 step    10200 |  10200 batches | lr 0.0003 | ms/batch 21.60511 | loss 1.43616 | ppl     4.205\n",
      "| epoch   1 step    10400 |  10400 batches | lr 0.0003 | ms/batch 21.19636 | loss 1.43967 | ppl     4.219\n",
      "| epoch   1 step    10600 |  10600 batches | lr 0.0003 | ms/batch 20.57633 | loss 1.44045 | ppl     4.223\n",
      "| epoch   1 step    10800 |  10800 batches | lr 0.0003 | ms/batch 20.98681 | loss 1.43657 | ppl     4.206\n",
      "| epoch   1 step    11000 |  11000 batches | lr 0.0003 | ms/batch 20.35950 | loss 1.44043 | ppl     4.222\n",
      "| epoch   1 step    11200 |  11200 batches | lr 0.0003 | ms/batch 20.41571 | loss 1.43553 | ppl     4.202\n",
      "| epoch   1 step    11400 |  11400 batches | lr 0.0003 | ms/batch 20.86744 | loss 1.43504 | ppl     4.200\n",
      "| epoch   1 step    11600 |  11600 batches | lr 0.0003 | ms/batch 20.83447 | loss 1.44123 | ppl     4.226\n",
      "| epoch   1 step    11800 |  11800 batches | lr 0.0003 | ms/batch 21.00885 | loss 1.43333 | ppl     4.193\n",
      "| epoch   1 step    12000 |  12000 batches | lr 0.0003 | ms/batch 20.47263 | loss 1.42651 | ppl     4.164\n",
      "|\n",
      "Source: [33  4 27  1 24  3 18  6 33 10]\n",
      "Target: [ 4 27  1 24  3 18  6 33 10  4]\n",
      "Teacher forcing: acc:0.26625\n",
      "Preds:  [3 4 3 1 3 3 3 6 3 3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step    12000 | time: 84.07s | valid loss 1.40978 | valid ppl    4.0950 | valid acc 0.266\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    12200 |  12200 batches | lr 0.0003 | ms/batch 23.95299 | loss 1.43123 | ppl     4.184\n",
      "| epoch   1 step    12400 |  12400 batches | lr 0.0003 | ms/batch 20.71954 | loss 1.43281 | ppl     4.190\n",
      "| epoch   1 step    12600 |  12600 batches | lr 0.0003 | ms/batch 20.79547 | loss 1.43418 | ppl     4.196\n",
      "| epoch   1 step    12800 |  12800 batches | lr 0.0003 | ms/batch 21.28034 | loss 1.43272 | ppl     4.190\n",
      "| epoch   1 step    13000 |  13000 batches | lr 0.0003 | ms/batch 21.33137 | loss 1.42784 | ppl     4.170\n",
      "| epoch   1 step    13200 |  13200 batches | lr 0.0003 | ms/batch 21.18124 | loss 1.43503 | ppl     4.200\n",
      "| epoch   1 step    13400 |  13400 batches | lr 0.0003 | ms/batch 20.88835 | loss 1.43655 | ppl     4.206\n",
      "| epoch   1 step    13600 |  13600 batches | lr 0.0003 | ms/batch 21.18722 | loss 1.42911 | ppl     4.175\n",
      "| epoch   1 step    13800 |  13800 batches | lr 0.0003 | ms/batch 21.40103 | loss 1.43150 | ppl     4.185\n",
      "| epoch   1 step    14000 |  14000 batches | lr 0.0003 | ms/batch 21.27182 | loss 1.43204 | ppl     4.187\n",
      "| epoch   1 step    14200 |  14200 batches | lr 0.0003 | ms/batch 21.32653 | loss 1.42805 | ppl     4.171\n",
      "| epoch   1 step    14400 |  14400 batches | lr 0.0003 | ms/batch 21.24114 | loss 1.43112 | ppl     4.183\n",
      "| epoch   1 step    14600 |  14600 batches | lr 0.0003 | ms/batch 20.62960 | loss 1.43310 | ppl     4.192\n",
      "| epoch   1 step    14800 |  14800 batches | lr 0.0003 | ms/batch 21.23935 | loss 1.43681 | ppl     4.207\n",
      "| epoch   1 step    15000 |  15000 batches | lr 0.0003 | ms/batch 21.04590 | loss 1.43465 | ppl     4.198\n",
      "| epoch   1 step    15200 |  15200 batches | lr 0.0003 | ms/batch 20.71992 | loss 1.42959 | ppl     4.177\n",
      "| epoch   1 step    15400 |  15400 batches | lr 0.0003 | ms/batch 21.11650 | loss 1.42944 | ppl     4.176\n",
      "| epoch   1 step    15600 |  15600 batches | lr 0.0003 | ms/batch 20.80710 | loss 1.43203 | ppl     4.187\n",
      "| epoch   1 step    15800 |  15800 batches | lr 0.0003 | ms/batch 20.94575 | loss 1.42952 | ppl     4.177\n",
      "| epoch   1 step    16000 |  16000 batches | lr 0.0003 | ms/batch 21.02709 | loss 1.42977 | ppl     4.178\n",
      "|\n",
      "Source: [15  4 18  1 20  8 14  0 20 10]\n",
      "Target: [ 4 18  1 20  8 14  0 20 10  8]\n",
      "Teacher forcing: acc:0.253125\n",
      "Preds:  [3 4 3 1 3 8 3 1 3 4]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   4 at step    16000 | time: 84.83s | valid loss 1.40637 | valid ppl    4.0811 | valid acc 0.253\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    16200 |  16200 batches | lr 0.0003 | ms/batch 24.07051 | loss 1.42249 | ppl     4.147\n",
      "| epoch   1 step    16400 |  16400 batches | lr 0.0003 | ms/batch 20.86275 | loss 1.43137 | ppl     4.184\n",
      "| epoch   1 step    16600 |  16600 batches | lr 0.0003 | ms/batch 20.81558 | loss 1.42930 | ppl     4.176\n",
      "| epoch   1 step    16800 |  16800 batches | lr 0.0003 | ms/batch 20.56108 | loss 1.42842 | ppl     4.172\n",
      "| epoch   1 step    17000 |  17000 batches | lr 0.0003 | ms/batch 20.86529 | loss 1.42992 | ppl     4.178\n",
      "| epoch   1 step    17200 |  17200 batches | lr 0.0003 | ms/batch 21.18147 | loss 1.42791 | ppl     4.170\n",
      "| epoch   1 step    17400 |  17400 batches | lr 0.0003 | ms/batch 20.97002 | loss 1.43024 | ppl     4.180\n",
      "| epoch   1 step    17600 |  17600 batches | lr 0.0003 | ms/batch 20.80732 | loss 1.42604 | ppl     4.162\n",
      "| epoch   1 step    17800 |  17800 batches | lr 0.0003 | ms/batch 20.99708 | loss 1.42837 | ppl     4.172\n",
      "| epoch   1 step    18000 |  18000 batches | lr 0.0003 | ms/batch 21.02739 | loss 1.42763 | ppl     4.169\n",
      "| epoch   1 step    18200 |  18200 batches | lr 0.0003 | ms/batch 20.55171 | loss 1.42850 | ppl     4.172\n",
      "| epoch   1 step    18400 |  18400 batches | lr 0.0003 | ms/batch 21.36178 | loss 1.42560 | ppl     4.160\n",
      "| epoch   1 step    18600 |  18600 batches | lr 0.0003 | ms/batch 21.11231 | loss 1.42911 | ppl     4.175\n",
      "| epoch   1 step    18800 |  18800 batches | lr 0.0003 | ms/batch 21.61208 | loss 1.42574 | ppl     4.161\n",
      "| epoch   1 step    19000 |  19000 batches | lr 0.0003 | ms/batch 20.96578 | loss 1.42547 | ppl     4.160\n",
      "| epoch   1 step    19200 |  19200 batches | lr 0.0003 | ms/batch 20.55488 | loss 1.42449 | ppl     4.156\n",
      "| epoch   1 step    19400 |  19400 batches | lr 0.0003 | ms/batch 20.92753 | loss 1.42669 | ppl     4.165\n",
      "| epoch   1 step    19600 |  19600 batches | lr 0.0003 | ms/batch 20.95801 | loss 1.42197 | ppl     4.145\n",
      "| epoch   1 step    19800 |  19800 batches | lr 0.0003 | ms/batch 20.41293 | loss 1.42855 | ppl     4.173\n",
      "| epoch   1 step    20000 |  20000 batches | lr 0.0003 | ms/batch 20.41756 | loss 1.42353 | ppl     4.152\n",
      "|\n",
      "Source: [33  8 27  9 19  7 18  4 33 10]\n",
      "Target: [ 8 27  9 19  7 18  4 33 10  8]\n",
      "Teacher forcing: acc:0.27625\n",
      "Preds:  [3 8 8 9 8 7 8 4 8 4]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   5 at step    20000 | time: 84.18s | valid loss 1.39487 | valid ppl    4.0345 | valid acc 0.276\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    20200 |  20200 batches | lr 0.0003 | ms/batch 24.28427 | loss 1.42103 | ppl     4.141\n",
      "| epoch   1 step    20400 |  20400 batches | lr 0.0003 | ms/batch 21.21946 | loss 1.42211 | ppl     4.146\n",
      "| epoch   1 step    20600 |  20600 batches | lr 0.0003 | ms/batch 21.26526 | loss 1.42631 | ppl     4.163\n",
      "| epoch   1 step    20800 |  20800 batches | lr 0.0003 | ms/batch 20.93423 | loss 1.42308 | ppl     4.150\n",
      "| epoch   1 step    21000 |  21000 batches | lr 0.0003 | ms/batch 20.90855 | loss 1.42397 | ppl     4.154\n",
      "| epoch   1 step    21200 |  21200 batches | lr 0.0003 | ms/batch 21.04325 | loss 1.42321 | ppl     4.150\n",
      "| epoch   1 step    21400 |  21400 batches | lr 0.0003 | ms/batch 20.99411 | loss 1.42257 | ppl     4.148\n",
      "| epoch   1 step    21600 |  21600 batches | lr 0.0003 | ms/batch 20.69679 | loss 1.42707 | ppl     4.166\n",
      "| epoch   1 step    21800 |  21800 batches | lr 0.0003 | ms/batch 20.92983 | loss 1.42084 | ppl     4.141\n",
      "| epoch   1 step    22000 |  22000 batches | lr 0.0003 | ms/batch 21.02587 | loss 1.42643 | ppl     4.164\n",
      "| epoch   1 step    22200 |  22200 batches | lr 0.0003 | ms/batch 22.09960 | loss 1.41986 | ppl     4.137\n",
      "| epoch   1 step    22400 |  22400 batches | lr 0.0003 | ms/batch 20.48122 | loss 1.42110 | ppl     4.142\n",
      "| epoch   1 step    22600 |  22600 batches | lr 0.0003 | ms/batch 21.15529 | loss 1.42074 | ppl     4.140\n",
      "| epoch   1 step    22800 |  22800 batches | lr 0.0003 | ms/batch 20.22915 | loss 1.42342 | ppl     4.151\n",
      "| epoch   1 step    23000 |  23000 batches | lr 0.0003 | ms/batch 20.24853 | loss 1.42162 | ppl     4.144\n",
      "| epoch   1 step    23200 |  23200 batches | lr 0.0003 | ms/batch 20.75392 | loss 1.42113 | ppl     4.142\n",
      "| epoch   1 step    23400 |  23400 batches | lr 0.0003 | ms/batch 21.15751 | loss 1.42194 | ppl     4.145\n",
      "| epoch   1 step    23600 |  23600 batches | lr 0.0003 | ms/batch 21.17842 | loss 1.42371 | ppl     4.152\n",
      "| epoch   1 step    23800 |  23800 batches | lr 0.0003 | ms/batch 21.19907 | loss 1.42118 | ppl     4.142\n",
      "| epoch   1 step    24000 |  24000 batches | lr 0.0003 | ms/batch 21.20970 | loss 1.42489 | ppl     4.157\n",
      "|\n",
      "Source: [24  7 33  3 12  5 26  0 12 10]\n",
      "Target: [ 7 33  3 12  5 26  0 12 10  5]\n",
      "Teacher forcing: acc:0.246875\n",
      "Preds:  [3 7 3 3 3 5 3 0 3 5]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   6 at step    24000 | time: 84.61s | valid loss 1.39752 | valid ppl    4.0451 | valid acc 0.247\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    24200 |  24200 batches | lr 0.0003 | ms/batch 24.13107 | loss 1.41872 | ppl     4.132\n",
      "| epoch   1 step    24400 |  24400 batches | lr 0.0003 | ms/batch 20.67707 | loss 1.42027 | ppl     4.138\n",
      "| epoch   1 step    24600 |  24600 batches | lr 0.0003 | ms/batch 20.83458 | loss 1.41623 | ppl     4.122\n",
      "| epoch   1 step    24800 |  24800 batches | lr 0.0003 | ms/batch 21.02281 | loss 1.42042 | ppl     4.139\n",
      "| epoch   1 step    25000 |  25000 batches | lr 0.0003 | ms/batch 20.86112 | loss 1.41939 | ppl     4.135\n",
      "| epoch   1 step    25200 |  25200 batches | lr 0.0003 | ms/batch 20.40405 | loss 1.41998 | ppl     4.137\n",
      "| epoch   1 step    25400 |  25400 batches | lr 0.0003 | ms/batch 21.05688 | loss 1.41781 | ppl     4.128\n",
      "| epoch   1 step    25600 |  25600 batches | lr 0.0003 | ms/batch 20.77260 | loss 1.41633 | ppl     4.122\n",
      "| epoch   1 step    25800 |  25800 batches | lr 0.0003 | ms/batch 20.99102 | loss 1.41814 | ppl     4.129\n",
      "| epoch   1 step    26000 |  26000 batches | lr 0.0003 | ms/batch 20.62699 | loss 1.41797 | ppl     4.129\n",
      "| epoch   1 step    26200 |  26200 batches | lr 0.0003 | ms/batch 20.44523 | loss 1.41727 | ppl     4.126\n",
      "| epoch   1 step    26400 |  26400 batches | lr 0.0003 | ms/batch 20.89193 | loss 1.42323 | ppl     4.151\n",
      "| epoch   1 step    26600 |  26600 batches | lr 0.0003 | ms/batch 20.62260 | loss 1.41667 | ppl     4.123\n",
      "| epoch   1 step    26800 |  26800 batches | lr 0.0003 | ms/batch 20.71927 | loss 1.42122 | ppl     4.142\n",
      "| epoch   1 step    27000 |  27000 batches | lr 0.0003 | ms/batch 20.84751 | loss 1.41584 | ppl     4.120\n",
      "| epoch   1 step    27200 |  27200 batches | lr 0.0003 | ms/batch 20.30801 | loss 1.42101 | ppl     4.141\n",
      "| epoch   1 step    27400 |  27400 batches | lr 0.0003 | ms/batch 20.65222 | loss 1.41245 | ppl     4.106\n",
      "| epoch   1 step    27600 |  27600 batches | lr 0.0003 | ms/batch 20.64464 | loss 1.41511 | ppl     4.117\n",
      "| epoch   1 step    27800 |  27800 batches | lr 0.0003 | ms/batch 21.14664 | loss 1.41611 | ppl     4.121\n",
      "| epoch   1 step    28000 |  28000 batches | lr 0.0003 | ms/batch 20.42356 | loss 1.42039 | ppl     4.139\n",
      "maslina\n",
      "|\n",
      "Source: [28  3 32  4 36  5 26  2 36 10]\n",
      "Target: [ 3 32  4 36  5 26  2 36 10  5]\n",
      "Teacher forcing: acc:0.24043367346938777\n",
      "Preds:  [4 3 3 4 4 5 4 2 3 3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   7 at step    28000 | time: 83.58s | valid loss 1.40693 | valid ppl    4.0834 | valid acc 0.24\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    28200 |  28200 batches | lr 0.0003 | ms/batch 23.25556 | loss 1.41497 | ppl     4.116\n",
      "| epoch   1 step    28400 |  28400 batches | lr 0.0003 | ms/batch 20.14480 | loss 1.41579 | ppl     4.120\n",
      "| epoch   1 step    28600 |  28600 batches | lr 0.0003 | ms/batch 20.96847 | loss 1.41305 | ppl     4.108\n",
      "| epoch   1 step    28800 |  28800 batches | lr 0.0003 | ms/batch 20.96666 | loss 1.41667 | ppl     4.123\n",
      "| epoch   1 step    29000 |  29000 batches | lr 0.0003 | ms/batch 21.82179 | loss 1.41648 | ppl     4.123\n",
      "| epoch   1 step    29200 |  29200 batches | lr 0.0003 | ms/batch 21.26789 | loss 1.41723 | ppl     4.126\n",
      "| epoch   1 step    29400 |  29400 batches | lr 0.0003 | ms/batch 21.44727 | loss 1.42068 | ppl     4.140\n",
      "| epoch   1 step    29600 |  29600 batches | lr 0.0003 | ms/batch 21.43538 | loss 1.41628 | ppl     4.122\n",
      "| epoch   1 step    29800 |  29800 batches | lr 0.0003 | ms/batch 21.45066 | loss 1.41246 | ppl     4.106\n",
      "| epoch   1 step    30000 |  30000 batches | lr 0.0003 | ms/batch 20.78921 | loss 1.41844 | ppl     4.131\n",
      "| epoch   1 step    30200 |  30200 batches | lr 0.0003 | ms/batch 21.38713 | loss 1.41669 | ppl     4.123\n",
      "| epoch   1 step    30400 |  30400 batches | lr 0.0003 | ms/batch 21.32068 | loss 1.41557 | ppl     4.119\n",
      "| epoch   1 step    30600 |  30600 batches | lr 0.0003 | ms/batch 21.08341 | loss 1.41628 | ppl     4.122\n",
      "| epoch   1 step    30800 |  30800 batches | lr 0.0003 | ms/batch 21.04985 | loss 1.41428 | ppl     4.114\n",
      "| epoch   1 step    31000 |  31000 batches | lr 0.0003 | ms/batch 21.76275 | loss 1.42881 | ppl     4.174\n",
      "| epoch   1 step    31200 |  31200 batches | lr 0.0003 | ms/batch 21.20538 | loss 1.41932 | ppl     4.134\n",
      "| epoch   1 step    31400 |  31400 batches | lr 0.0003 | ms/batch 21.97583 | loss 1.41613 | ppl     4.121\n",
      "| epoch   1 step    31600 |  31600 batches | lr 0.0003 | ms/batch 21.80066 | loss 1.41501 | ppl     4.117\n",
      "| epoch   1 step    31800 |  31800 batches | lr 0.0003 | ms/batch 21.85475 | loss 1.41401 | ppl     4.112\n",
      "| epoch   1 step    32000 |  32000 batches | lr 0.0003 | ms/batch 21.40025 | loss 1.41939 | ppl     4.135\n",
      "|\n",
      "Source: [32  9 13  7 20  4 30  3 32 10]\n",
      "Target: [ 9 13  7 20  4 30  3 32 10  9]\n",
      "Teacher forcing: acc:0.25375\n",
      "Preds:  [2 9 2 7 2 4 2 2 2 9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   8 at step    32000 | time: 85.72s | valid loss 1.39502 | valid ppl    4.0351 | valid acc 0.254\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    32200 |  32200 batches | lr 0.0003 | ms/batch 24.71221 | loss 1.41071 | ppl     4.099\n",
      "| epoch   1 step    32400 |  32400 batches | lr 0.0003 | ms/batch 21.42463 | loss 1.41485 | ppl     4.116\n",
      "| epoch   1 step    32600 |  32600 batches | lr 0.0003 | ms/batch 22.24261 | loss 1.41435 | ppl     4.114\n",
      "| epoch   1 step    32800 |  32800 batches | lr 0.0003 | ms/batch 21.98387 | loss 1.41643 | ppl     4.122\n",
      "| epoch   1 step    33000 |  33000 batches | lr 0.0003 | ms/batch 21.42471 | loss 1.41689 | ppl     4.124\n",
      "| epoch   1 step    33200 |  33200 batches | lr 0.0003 | ms/batch 21.27902 | loss 1.41252 | ppl     4.106\n",
      "| epoch   1 step    33400 |  33400 batches | lr 0.0003 | ms/batch 21.48754 | loss 1.41851 | ppl     4.131\n",
      "| epoch   1 step    33600 |  33600 batches | lr 0.0003 | ms/batch 21.51242 | loss 1.41260 | ppl     4.107\n",
      "| epoch   1 step    33800 |  33800 batches | lr 0.0003 | ms/batch 22.35480 | loss 1.41443 | ppl     4.114\n",
      "| epoch   1 step    34000 |  34000 batches | lr 0.0003 | ms/batch 21.42705 | loss 1.41171 | ppl     4.103\n",
      "| epoch   1 step    34200 |  34200 batches | lr 0.0003 | ms/batch 20.80424 | loss 1.41658 | ppl     4.123\n",
      "| epoch   1 step    34400 |  34400 batches | lr 0.0003 | ms/batch 21.29805 | loss 1.41247 | ppl     4.106\n",
      "| epoch   1 step    34600 |  34600 batches | lr 0.0003 | ms/batch 21.08911 | loss 1.41389 | ppl     4.112\n",
      "| epoch   1 step    34800 |  34800 batches | lr 0.0003 | ms/batch 20.98129 | loss 1.41515 | ppl     4.117\n",
      "| epoch   1 step    35000 |  35000 batches | lr 0.0003 | ms/batch 21.88674 | loss 1.41188 | ppl     4.104\n",
      "| epoch   1 step    35200 |  35200 batches | lr 0.0003 | ms/batch 21.22119 | loss 1.41550 | ppl     4.119\n",
      "| epoch   1 step    35400 |  35400 batches | lr 0.0003 | ms/batch 21.03161 | loss 1.41515 | ppl     4.117\n",
      "| epoch   1 step    35600 |  35600 batches | lr 0.0003 | ms/batch 20.67646 | loss 1.41103 | ppl     4.100\n",
      "| epoch   1 step    35800 |  35800 batches | lr 0.0003 | ms/batch 21.36849 | loss 1.41143 | ppl     4.102\n",
      "| epoch   1 step    36000 |  36000 batches | lr 0.0003 | ms/batch 21.09367 | loss 1.42809 | ppl     4.171\n",
      "|\n",
      "Source: [13  6 23  9 14  8 15  5 15 10]\n",
      "Target: [ 6 23  9 14  8 15  5 15 10  5]\n",
      "Teacher forcing: acc:0.24\n",
      "Preds:  [4 6 6 6 6 8 8 2 8 5]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   9 at step    36000 | time: 86.28s | valid loss 1.40023 | valid ppl    4.0561 | valid acc 0.24\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    36200 |  36200 batches | lr 0.00015 | ms/batch 24.96654 | loss 1.41141 | ppl     4.102\n",
      "| epoch   1 step    36400 |  36400 batches | lr 0.00015 | ms/batch 21.61367 | loss 1.40800 | ppl     4.088\n",
      "| epoch   1 step    36600 |  36600 batches | lr 0.00015 | ms/batch 21.66024 | loss 1.40598 | ppl     4.080\n",
      "| epoch   1 step    36800 |  36800 batches | lr 0.00015 | ms/batch 21.84898 | loss 1.40637 | ppl     4.081\n",
      "| epoch   1 step    37000 |  37000 batches | lr 0.00015 | ms/batch 21.96032 | loss 1.40253 | ppl     4.065\n",
      "| epoch   1 step    37200 |  37200 batches | lr 0.00015 | ms/batch 21.06213 | loss 1.40452 | ppl     4.074\n",
      "| epoch   1 step    37400 |  37400 batches | lr 0.00015 | ms/batch 21.65528 | loss 1.40727 | ppl     4.085\n",
      "| epoch   1 step    37600 |  37600 batches | lr 0.00015 | ms/batch 21.36184 | loss 1.40618 | ppl     4.080\n",
      "| epoch   1 step    37800 |  37800 batches | lr 0.00015 | ms/batch 21.74046 | loss 1.40483 | ppl     4.075\n",
      "| epoch   1 step    38000 |  38000 batches | lr 0.00015 | ms/batch 20.71937 | loss 1.40286 | ppl     4.067\n",
      "| epoch   1 step    38200 |  38200 batches | lr 0.00015 | ms/batch 21.00597 | loss 1.40479 | ppl     4.075\n",
      "| epoch   1 step    38400 |  38400 batches | lr 0.00015 | ms/batch 21.69967 | loss 1.40189 | ppl     4.063\n",
      "| epoch   1 step    38600 |  38600 batches | lr 0.00015 | ms/batch 21.30510 | loss 1.40370 | ppl     4.070\n",
      "| epoch   1 step    38800 |  38800 batches | lr 0.00015 | ms/batch 21.14748 | loss 1.40458 | ppl     4.074\n",
      "| epoch   1 step    39000 |  39000 batches | lr 0.00015 | ms/batch 22.04139 | loss 1.40364 | ppl     4.070\n",
      "| epoch   1 step    39200 |  39200 batches | lr 0.00015 | ms/batch 20.74078 | loss 1.40326 | ppl     4.068\n",
      "| epoch   1 step    39400 |  39400 batches | lr 0.00015 | ms/batch 21.19171 | loss 1.40294 | ppl     4.067\n",
      "| epoch   1 step    39600 |  39600 batches | lr 0.00015 | ms/batch 21.10679 | loss 1.40539 | ppl     4.077\n",
      "| epoch   1 step    39800 |  39800 batches | lr 0.00015 | ms/batch 20.55622 | loss 1.40429 | ppl     4.073\n",
      "| epoch   1 step    40000 |  40000 batches | lr 0.00015 | ms/batch 20.58538 | loss 1.40449 | ppl     4.073\n",
      "|\n",
      "Source: [32  5 36  3 24  4 33  8 32 10]\n",
      "Target: [ 5 36  3 24  4 33  8 32 10  5]\n",
      "Teacher forcing: acc:0.24125\n",
      "Preds:  [2 3 5 2 3 4 4 8 8 8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  10 at step    40000 | time: 85.99s | valid loss 1.39376 | valid ppl    4.0300 | valid acc 0.241\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    40200 |  40200 batches | lr 0.00015 | ms/batch 24.80205 | loss 1.40136 | ppl     4.061\n",
      "| epoch   1 step    40400 |  40400 batches | lr 0.00015 | ms/batch 21.03720 | loss 1.40299 | ppl     4.067\n",
      "| epoch   1 step    40600 |  40600 batches | lr 0.00015 | ms/batch 20.69690 | loss 1.40475 | ppl     4.075\n",
      "| epoch   1 step    40800 |  40800 batches | lr 0.00015 | ms/batch 21.64334 | loss 1.40228 | ppl     4.064\n",
      "| epoch   1 step    41000 |  41000 batches | lr 0.00015 | ms/batch 21.78922 | loss 1.40419 | ppl     4.072\n",
      "| epoch   1 step    41200 |  41200 batches | lr 0.00015 | ms/batch 21.42690 | loss 1.40286 | ppl     4.067\n",
      "| epoch   1 step    41400 |  41400 batches | lr 0.00015 | ms/batch 21.56164 | loss 1.40564 | ppl     4.078\n",
      "| epoch   1 step    41600 |  41600 batches | lr 0.00015 | ms/batch 20.98364 | loss 1.40322 | ppl     4.068\n",
      "| epoch   1 step    41800 |  41800 batches | lr 0.00015 | ms/batch 20.94874 | loss 1.40499 | ppl     4.075\n",
      "| epoch   1 step    42000 |  42000 batches | lr 0.00015 | ms/batch 20.77670 | loss 1.40067 | ppl     4.058\n",
      "| epoch   1 step    42200 |  42200 batches | lr 0.00015 | ms/batch 20.78544 | loss 1.40223 | ppl     4.064\n",
      "| epoch   1 step    42400 |  42400 batches | lr 0.00015 | ms/batch 21.14386 | loss 1.40287 | ppl     4.067\n",
      "| epoch   1 step    42600 |  42600 batches | lr 0.00015 | ms/batch 20.48179 | loss 1.40346 | ppl     4.069\n",
      "| epoch   1 step    42800 |  42800 batches | lr 0.00015 | ms/batch 21.11217 | loss 1.40486 | ppl     4.075\n",
      "| epoch   1 step    43000 |  43000 batches | lr 0.00015 | ms/batch 21.21936 | loss 1.40462 | ppl     4.074\n",
      "| epoch   1 step    43200 |  43200 batches | lr 0.00015 | ms/batch 22.28067 | loss 1.40223 | ppl     4.064\n",
      "| epoch   1 step    43400 |  43400 batches | lr 0.00015 | ms/batch 20.68954 | loss 1.40033 | ppl     4.057\n",
      "| epoch   1 step    43600 |  43600 batches | lr 0.00015 | ms/batch 20.73158 | loss 1.40248 | ppl     4.065\n",
      "| epoch   1 step    43800 |  43800 batches | lr 0.00015 | ms/batch 21.27609 | loss 1.40427 | ppl     4.073\n",
      "| epoch   1 step    44000 |  44000 batches | lr 0.00015 | ms/batch 20.45007 | loss 1.40305 | ppl     4.068\n",
      "|\n",
      "Source: [20  4 27  3 28  7 11  2 28 10]\n",
      "Target: [ 4 27  3 28  7 11  2 28 10  7]\n",
      "Teacher forcing: acc:0.24125\n",
      "Preds:  [4 4 4 3 3 7 3 2 3 4]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  11 at step    44000 | time: 85.15s | valid loss 1.39458 | valid ppl    4.0333 | valid acc 0.241\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    44200 |  44200 batches | lr 0.00015 | ms/batch 24.31261 | loss 1.40072 | ppl     4.058\n",
      "| epoch   1 step    44400 |  44400 batches | lr 0.00015 | ms/batch 21.69571 | loss 1.40310 | ppl     4.068\n",
      "| epoch   1 step    44600 |  44600 batches | lr 0.00015 | ms/batch 21.87762 | loss 1.40309 | ppl     4.068\n",
      "| epoch   1 step    44800 |  44800 batches | lr 0.00015 | ms/batch 20.82055 | loss 1.40344 | ppl     4.069\n",
      "| epoch   1 step    45000 |  45000 batches | lr 0.00015 | ms/batch 20.77123 | loss 1.40298 | ppl     4.067\n",
      "| epoch   1 step    45200 |  45200 batches | lr 0.00015 | ms/batch 21.06974 | loss 1.40254 | ppl     4.066\n",
      "| epoch   1 step    45400 |  45400 batches | lr 0.00015 | ms/batch 21.53999 | loss 1.40402 | ppl     4.072\n",
      "| epoch   1 step    45600 |  45600 batches | lr 0.00015 | ms/batch 20.60003 | loss 1.40579 | ppl     4.079\n",
      "| epoch   1 step    45800 |  45800 batches | lr 0.00015 | ms/batch 21.38321 | loss 1.40441 | ppl     4.073\n",
      "| epoch   1 step    46000 |  46000 batches | lr 0.00015 | ms/batch 21.14740 | loss 1.40273 | ppl     4.066\n",
      "| epoch   1 step    46200 |  46200 batches | lr 0.00015 | ms/batch 21.76342 | loss 1.40247 | ppl     4.065\n",
      "| epoch   1 step    46400 |  46400 batches | lr 0.00015 | ms/batch 20.89442 | loss 1.39856 | ppl     4.049\n",
      "| epoch   1 step    46600 |  46600 batches | lr 0.00015 | ms/batch 20.92605 | loss 1.40290 | ppl     4.067\n",
      "| epoch   1 step    46800 |  46800 batches | lr 0.00015 | ms/batch 21.43049 | loss 1.40014 | ppl     4.056\n",
      "| epoch   1 step    47000 |  47000 batches | lr 0.00015 | ms/batch 22.22882 | loss 1.40305 | ppl     4.068\n",
      "| epoch   1 step    47200 |  47200 batches | lr 0.00015 | ms/batch 21.44077 | loss 1.40161 | ppl     4.062\n",
      "| epoch   1 step    47400 |  47400 batches | lr 0.00015 | ms/batch 20.91016 | loss 1.40128 | ppl     4.060\n",
      "| epoch   1 step    47600 |  47600 batches | lr 0.00015 | ms/batch 20.94452 | loss 1.40324 | ppl     4.068\n",
      "| epoch   1 step    47800 |  47800 batches | lr 0.00015 | ms/batch 21.13653 | loss 1.40020 | ppl     4.056\n",
      "| epoch   1 step    48000 |  48000 batches | lr 0.00015 | ms/batch 21.59218 | loss 1.40214 | ppl     4.064\n",
      "|\n",
      "Source: [11  9 21  8 12  5 20  1 20 10]\n",
      "Target: [ 9 21  8 12  5 20  1 20 10  1]\n",
      "Teacher forcing: acc:0.25\n",
      "Preds:  [4 9 9 4 8 5 5 4 5 5]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  12 at step    48000 | time: 85.73s | valid loss 1.39274 | valid ppl    4.0259 | valid acc 0.25\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    48200 |  48200 batches | lr 0.00015 | ms/batch 25.15315 | loss 1.40355 | ppl     4.070\n",
      "| epoch   1 step    48400 |  48400 batches | lr 0.00015 | ms/batch 21.02681 | loss 1.40068 | ppl     4.058\n",
      "| epoch   1 step    48600 |  48600 batches | lr 0.00015 | ms/batch 20.71796 | loss 1.40325 | ppl     4.068\n",
      "| epoch   1 step    48800 |  48800 batches | lr 0.00015 | ms/batch 20.70261 | loss 1.40384 | ppl     4.071\n",
      "| epoch   1 step    49000 |  49000 batches | lr 0.00015 | ms/batch 21.30249 | loss 1.40379 | ppl     4.071\n",
      "| epoch   1 step    49200 |  49200 batches | lr 0.00015 | ms/batch 21.43820 | loss 1.40496 | ppl     4.075\n",
      "| epoch   1 step    49400 |  49400 batches | lr 0.00015 | ms/batch 20.85498 | loss 1.40117 | ppl     4.060\n",
      "| epoch   1 step    49600 |  49600 batches | lr 0.00015 | ms/batch 20.44744 | loss 1.40120 | ppl     4.060\n",
      "| epoch   1 step    49800 |  49800 batches | lr 0.00015 | ms/batch 21.14332 | loss 1.40455 | ppl     4.074\n",
      "| epoch   1 step    50000 |  50000 batches | lr 0.00015 | ms/batch 20.10314 | loss 1.40007 | ppl     4.055\n",
      "| epoch   1 step    50200 |  50200 batches | lr 0.00015 | ms/batch 21.03392 | loss 1.40032 | ppl     4.056\n",
      "| epoch   1 step    50400 |  50400 batches | lr 0.00015 | ms/batch 20.38793 | loss 1.40212 | ppl     4.064\n",
      "| epoch   1 step    50600 |  50600 batches | lr 0.00015 | ms/batch 20.33138 | loss 1.40119 | ppl     4.060\n",
      "| epoch   1 step    50800 |  50800 batches | lr 0.00015 | ms/batch 20.39605 | loss 1.40408 | ppl     4.072\n",
      "| epoch   1 step    51000 |  51000 batches | lr 0.00015 | ms/batch 20.72366 | loss 1.40354 | ppl     4.070\n",
      "| epoch   1 step    51200 |  51200 batches | lr 0.00015 | ms/batch 20.49041 | loss 1.40007 | ppl     4.055\n",
      "| epoch   1 step    51400 |  51400 batches | lr 0.00015 | ms/batch 20.66013 | loss 1.40140 | ppl     4.061\n",
      "| epoch   1 step    51600 |  51600 batches | lr 0.00015 | ms/batch 20.84733 | loss 1.40200 | ppl     4.063\n",
      "| epoch   1 step    51800 |  51800 batches | lr 0.00015 | ms/batch 20.53025 | loss 1.40185 | ppl     4.063\n",
      "| epoch   1 step    52000 |  52000 batches | lr 0.00015 | ms/batch 20.69341 | loss 1.40020 | ppl     4.056\n",
      "|\n",
      "Source: [25  7 12  4 13  9 14  2 25 10]\n",
      "Target: [ 7 12  4 13  9 14  2 25 10  7]\n",
      "Teacher forcing: acc:0.248125\n",
      "Preds:  [2 7 7 7 7 4 7 4 7 2]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  13 at step    52000 | time: 83.74s | valid loss 1.39719 | valid ppl    4.0438 | valid acc 0.248\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    52200 |  52200 batches | lr 0.00015 | ms/batch 23.92203 | loss 1.40385 | ppl     4.071\n",
      "| epoch   1 step    52400 |  52400 batches | lr 0.00015 | ms/batch 20.66893 | loss 1.40064 | ppl     4.058\n",
      "| epoch   1 step    52600 |  52600 batches | lr 0.00015 | ms/batch 21.09239 | loss 1.40050 | ppl     4.057\n",
      "| epoch   1 step    52800 |  52800 batches | lr 0.00015 | ms/batch 21.18555 | loss 1.40053 | ppl     4.057\n",
      "| epoch   1 step    53000 |  53000 batches | lr 0.00015 | ms/batch 21.25492 | loss 1.40418 | ppl     4.072\n",
      "| epoch   1 step    53200 |  53200 batches | lr 0.00015 | ms/batch 20.59911 | loss 1.39996 | ppl     4.055\n",
      "| epoch   1 step    53400 |  53400 batches | lr 0.00015 | ms/batch 20.51660 | loss 1.40179 | ppl     4.062\n",
      "| epoch   1 step    53600 |  53600 batches | lr 0.00015 | ms/batch 21.29971 | loss 1.39940 | ppl     4.053\n",
      "| epoch   1 step    53800 |  53800 batches | lr 0.00015 | ms/batch 20.83656 | loss 1.40216 | ppl     4.064\n",
      "| epoch   1 step    54000 |  54000 batches | lr 0.00015 | ms/batch 20.97402 | loss 1.40280 | ppl     4.067\n",
      "| epoch   1 step    54200 |  54200 batches | lr 0.00015 | ms/batch 21.90475 | loss 1.40076 | ppl     4.058\n",
      "| epoch   1 step    54400 |  54400 batches | lr 0.00015 | ms/batch 21.22902 | loss 1.40332 | ppl     4.069\n",
      "| epoch   1 step    54600 |  54600 batches | lr 0.00015 | ms/batch 21.04370 | loss 1.40403 | ppl     4.072\n",
      "| epoch   1 step    54800 |  54800 batches | lr 0.00015 | ms/batch 20.80507 | loss 1.40469 | ppl     4.074\n",
      "| epoch   1 step    55000 |  55000 batches | lr 0.00015 | ms/batch 20.61215 | loss 1.40249 | ppl     4.065\n",
      "| epoch   1 step    55200 |  55200 batches | lr 0.00015 | ms/batch 20.96240 | loss 1.40253 | ppl     4.065\n",
      "| epoch   1 step    55400 |  55400 batches | lr 0.00015 | ms/batch 21.04814 | loss 1.39909 | ppl     4.052\n",
      "| epoch   1 step    55600 |  55600 batches | lr 0.00015 | ms/batch 20.83412 | loss 1.40119 | ppl     4.060\n",
      "| epoch   1 step    55800 |  55800 batches | lr 0.00015 | ms/batch 20.61027 | loss 1.40157 | ppl     4.062\n",
      "| epoch   1 step    56000 |  56000 batches | lr 0.00015 | ms/batch 21.20220 | loss 1.40201 | ppl     4.063\n",
      "|\n",
      "Source: [24  3 31  1 30  4 25  2 25 10]\n",
      "Target: [ 3 31  1 30  4 25  2 25 10  2]\n",
      "Teacher forcing: acc:0.238125\n",
      "Preds:  [1 3 3 1 1 1 4 1 4 1]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  14 at step    56000 | time: 84.53s | valid loss 1.39236 | valid ppl    4.0243 | valid acc 0.238\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    56200 |  56200 batches | lr 0.00015 | ms/batch 24.14739 | loss 1.39947 | ppl     4.053\n",
      "| epoch   1 step    56400 |  56400 batches | lr 0.00015 | ms/batch 20.93229 | loss 1.40044 | ppl     4.057\n",
      "| epoch   1 step    56600 |  56600 batches | lr 0.00015 | ms/batch 20.73651 | loss 1.40036 | ppl     4.057\n",
      "| epoch   1 step    56800 |  56800 batches | lr 0.00015 | ms/batch 21.06414 | loss 1.40092 | ppl     4.059\n",
      "| epoch   1 step    57000 |  57000 batches | lr 0.00015 | ms/batch 21.80242 | loss 1.40180 | ppl     4.062\n",
      "| epoch   1 step    57200 |  57200 batches | lr 0.00015 | ms/batch 21.39241 | loss 1.40156 | ppl     4.062\n",
      "| epoch   1 step    57400 |  57400 batches | lr 0.00015 | ms/batch 21.59179 | loss 1.40214 | ppl     4.064\n",
      "| epoch   1 step    57600 |  57600 batches | lr 0.00015 | ms/batch 21.00592 | loss 1.39906 | ppl     4.051\n",
      "| epoch   1 step    57800 |  57800 batches | lr 0.00015 | ms/batch 21.45118 | loss 1.40175 | ppl     4.062\n",
      "| epoch   1 step    58000 |  58000 batches | lr 0.00015 | ms/batch 20.91260 | loss 1.39937 | ppl     4.053\n",
      "| epoch   1 step    58200 |  58200 batches | lr 0.00015 | ms/batch 20.72902 | loss 1.40074 | ppl     4.058\n",
      "| epoch   1 step    58400 |  58400 batches | lr 0.00015 | ms/batch 21.18568 | loss 1.40402 | ppl     4.072\n",
      "| epoch   1 step    58600 |  58600 batches | lr 0.00015 | ms/batch 21.48836 | loss 1.39846 | ppl     4.049\n",
      "| epoch   1 step    58800 |  58800 batches | lr 0.00015 | ms/batch 21.01558 | loss 1.39975 | ppl     4.054\n",
      "| epoch   1 step    59000 |  59000 batches | lr 0.00015 | ms/batch 20.76317 | loss 1.39958 | ppl     4.053\n",
      "| epoch   1 step    59200 |  59200 batches | lr 0.00015 | ms/batch 20.96539 | loss 1.40302 | ppl     4.067\n",
      "| epoch   1 step    59400 |  59400 batches | lr 0.00015 | ms/batch 21.29752 | loss 1.40034 | ppl     4.057\n",
      "| epoch   1 step    59600 |  59600 batches | lr 0.00015 | ms/batch 20.76476 | loss 1.39879 | ppl     4.050\n",
      "| epoch   1 step    59800 |  59800 batches | lr 0.00015 | ms/batch 21.10416 | loss 1.40130 | ppl     4.060\n",
      "| epoch   1 step    60000 |  60000 batches | lr 0.00015 | ms/batch 21.27293 | loss 1.39926 | ppl     4.052\n",
      "|\n",
      "Source: [28  3 26  2 36  1 34  4 28 10]\n",
      "Target: [ 3 26  2 36  1 34  4 28 10  3]\n",
      "Teacher forcing: acc:0.235625\n",
      "Preds:  [2 3 3 2 3 1 3 4 3 2]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  15 at step    60000 | time: 85.12s | valid loss 1.39319 | valid ppl    4.0277 | valid acc 0.236\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    60200 |  60200 batches | lr 0.00015 | ms/batch 24.21407 | loss 1.40283 | ppl     4.067\n",
      "| epoch   1 step    60400 |  60400 batches | lr 0.00015 | ms/batch 21.44233 | loss 1.40132 | ppl     4.061\n",
      "| epoch   1 step    60600 |  60600 batches | lr 0.00015 | ms/batch 21.83000 | loss 1.40119 | ppl     4.060\n",
      "| epoch   1 step    60800 |  60800 batches | lr 0.00015 | ms/batch 21.22567 | loss 1.40178 | ppl     4.062\n",
      "| epoch   1 step    61000 |  61000 batches | lr 0.00015 | ms/batch 21.70710 | loss 1.40237 | ppl     4.065\n",
      "| epoch   1 step    61200 |  61200 batches | lr 0.00015 | ms/batch 21.85048 | loss 1.40160 | ppl     4.062\n",
      "| epoch   1 step    61400 |  61400 batches | lr 0.00015 | ms/batch 21.70557 | loss 1.40135 | ppl     4.061\n",
      "| epoch   1 step    61600 |  61600 batches | lr 0.00015 | ms/batch 21.78518 | loss 1.40090 | ppl     4.059\n",
      "| epoch   1 step    61800 |  61800 batches | lr 0.00015 | ms/batch 21.50108 | loss 1.39908 | ppl     4.051\n",
      "| epoch   1 step    62000 |  62000 batches | lr 0.00015 | ms/batch 21.08316 | loss 1.40009 | ppl     4.056\n",
      "| epoch   1 step    62200 |  62200 batches | lr 0.00015 | ms/batch 21.01963 | loss 1.40012 | ppl     4.056\n",
      "| epoch   1 step    62400 |  62400 batches | lr 0.00015 | ms/batch 21.79130 | loss 1.40196 | ppl     4.063\n",
      "| epoch   1 step    62600 |  62600 batches | lr 0.00015 | ms/batch 21.58213 | loss 1.40176 | ppl     4.062\n",
      "| epoch   1 step    62800 |  62800 batches | lr 0.00015 | ms/batch 20.79262 | loss 1.39933 | ppl     4.053\n",
      "| epoch   1 step    63000 |  63000 batches | lr 0.00015 | ms/batch 20.89912 | loss 1.39903 | ppl     4.051\n",
      "| epoch   1 step    63200 |  63200 batches | lr 0.00015 | ms/batch 20.76494 | loss 1.40086 | ppl     4.059\n",
      "| epoch   1 step    63400 |  63400 batches | lr 0.00015 | ms/batch 21.56691 | loss 1.40055 | ppl     4.057\n",
      "| epoch   1 step    63600 |  63600 batches | lr 0.00015 | ms/batch 21.19041 | loss 1.39917 | ppl     4.052\n",
      "| epoch   1 step    63800 |  63800 batches | lr 0.00015 | ms/batch 21.11339 | loss 1.39833 | ppl     4.048\n",
      "| epoch   1 step    64000 |  64000 batches | lr 0.00015 | ms/batch 21.70174 | loss 1.39975 | ppl     4.054\n",
      "|\n",
      "Source: [17  0 32  2 25  4 35  1 35 10]\n",
      "Target: [ 0 32  2 25  4 35  1 35 10  1]\n",
      "Teacher forcing: acc:0.250625\n",
      "Preds:  [4 0 4 2 2 4 2 4 1 4]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  16 at step    64000 | time: 86.16s | valid loss 1.39058 | valid ppl    4.0172 | valid acc 0.251\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    64200 |  64200 batches | lr 0.00015 | ms/batch 24.99848 | loss 1.39922 | ppl     4.052\n",
      "| epoch   1 step    64400 |  64400 batches | lr 0.00015 | ms/batch 21.40753 | loss 1.39881 | ppl     4.050\n",
      "| epoch   1 step    64600 |  64600 batches | lr 0.00015 | ms/batch 20.71641 | loss 1.40105 | ppl     4.059\n",
      "| epoch   1 step    64800 |  64800 batches | lr 0.00015 | ms/batch 21.07440 | loss 1.40020 | ppl     4.056\n",
      "| epoch   1 step    65000 |  65000 batches | lr 0.00015 | ms/batch 21.23904 | loss 1.39913 | ppl     4.052\n",
      "| epoch   1 step    65200 |  65200 batches | lr 0.00015 | ms/batch 21.14905 | loss 1.39724 | ppl     4.044\n",
      "| epoch   1 step    65400 |  65400 batches | lr 0.00015 | ms/batch 20.99227 | loss 1.39832 | ppl     4.048\n",
      "| epoch   1 step    65600 |  65600 batches | lr 0.00015 | ms/batch 21.02897 | loss 1.40168 | ppl     4.062\n",
      "| epoch   1 step    65800 |  65800 batches | lr 0.00015 | ms/batch 21.53567 | loss 1.39701 | ppl     4.043\n",
      "| epoch   1 step    66000 |  66000 batches | lr 0.00015 | ms/batch 21.15503 | loss 1.40033 | ppl     4.057\n",
      "| epoch   1 step    66200 |  66200 batches | lr 0.00015 | ms/batch 20.75134 | loss 1.39727 | ppl     4.044\n",
      "| epoch   1 step    66400 |  66400 batches | lr 0.00015 | ms/batch 21.12156 | loss 1.39973 | ppl     4.054\n",
      "| epoch   1 step    66600 |  66600 batches | lr 0.00015 | ms/batch 20.80972 | loss 1.40083 | ppl     4.059\n",
      "| epoch   1 step    66800 |  66800 batches | lr 0.00015 | ms/batch 21.35018 | loss 1.39944 | ppl     4.053\n",
      "| epoch   1 step    67000 |  67000 batches | lr 0.00015 | ms/batch 21.28522 | loss 1.39913 | ppl     4.052\n",
      "| epoch   1 step    67200 |  67200 batches | lr 0.00015 | ms/batch 21.84340 | loss 1.39772 | ppl     4.046\n",
      "| epoch   1 step    67400 |  67400 batches | lr 0.00015 | ms/batch 21.60618 | loss 1.40015 | ppl     4.056\n",
      "| epoch   1 step    67600 |  67600 batches | lr 0.00015 | ms/batch 20.80246 | loss 1.40105 | ppl     4.059\n",
      "| epoch   1 step    67800 |  67800 batches | lr 0.00015 | ms/batch 21.71037 | loss 1.40203 | ppl     4.063\n",
      "| epoch   1 step    68000 |  68000 batches | lr 0.00015 | ms/batch 22.15874 | loss 1.39981 | ppl     4.054\n",
      "|\n",
      "Source: [18  9 15  0 36  8 28  6 18 10]\n",
      "Target: [ 9 15  0 36  8 28  6 18 10  9]\n",
      "Teacher forcing: acc:0.24375\n",
      "Preds:  [4 9 9 5 0 4 4 6 6 8]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  17 at step    68000 | time: 85.85s | valid loss 1.39274 | valid ppl    4.0259 | valid acc 0.244\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    68200 |  68200 batches | lr 0.00015 | ms/batch 25.34773 | loss 1.39896 | ppl     4.051\n",
      "| epoch   1 step    68400 |  68400 batches | lr 0.00015 | ms/batch 21.08186 | loss 1.39526 | ppl     4.036\n",
      "| epoch   1 step    68600 |  68600 batches | lr 0.00015 | ms/batch 21.51524 | loss 1.40076 | ppl     4.058\n",
      "| epoch   1 step    68800 |  68800 batches | lr 0.00015 | ms/batch 20.79678 | loss 1.40025 | ppl     4.056\n",
      "| epoch   1 step    69000 |  69000 batches | lr 0.00015 | ms/batch 21.64385 | loss 1.40103 | ppl     4.059\n",
      "| epoch   1 step    69200 |  69200 batches | lr 0.00015 | ms/batch 21.19768 | loss 1.39655 | ppl     4.041\n",
      "| epoch   1 step    69400 |  69400 batches | lr 0.00015 | ms/batch 21.16570 | loss 1.39883 | ppl     4.050\n",
      "| epoch   1 step    69600 |  69600 batches | lr 0.00015 | ms/batch 21.29478 | loss 1.40028 | ppl     4.056\n",
      "| epoch   1 step    69800 |  69800 batches | lr 0.00015 | ms/batch 20.60559 | loss 1.40176 | ppl     4.062\n",
      "| epoch   1 step    70000 |  70000 batches | lr 0.00015 | ms/batch 21.37982 | loss 1.40044 | ppl     4.057\n",
      "| epoch   1 step    70200 |  70200 batches | lr 0.00015 | ms/batch 21.78771 | loss 1.39860 | ppl     4.050\n",
      "| epoch   1 step    70400 |  70400 batches | lr 0.00015 | ms/batch 21.26021 | loss 1.39722 | ppl     4.044\n",
      "| epoch   1 step    70600 |  70600 batches | lr 0.00015 | ms/batch 20.76249 | loss 1.40153 | ppl     4.061\n",
      "| epoch   1 step    70800 |  70800 batches | lr 0.00015 | ms/batch 21.27981 | loss 1.39871 | ppl     4.050\n",
      "| epoch   1 step    71000 |  71000 batches | lr 0.00015 | ms/batch 21.11757 | loss 1.40069 | ppl     4.058\n",
      "| epoch   1 step    71200 |  71200 batches | lr 0.00015 | ms/batch 21.25040 | loss 1.39922 | ppl     4.052\n",
      "| epoch   1 step    71400 |  71400 batches | lr 0.00015 | ms/batch 20.91263 | loss 1.39877 | ppl     4.050\n",
      "| epoch   1 step    71600 |  71600 batches | lr 0.00015 | ms/batch 21.72903 | loss 1.39996 | ppl     4.055\n",
      "| epoch   1 step    71800 |  71800 batches | lr 0.00015 | ms/batch 21.85777 | loss 1.39843 | ppl     4.049\n",
      "| epoch   1 step    72000 |  72000 batches | lr 0.00015 | ms/batch 21.47524 | loss 1.39982 | ppl     4.054\n",
      "|\n",
      "Source: [20  2 18  5 24  6 14  9 18 10]\n",
      "Target: [ 2 18  5 24  6 14  9 18 10  5]\n",
      "Teacher forcing: acc:0.265625\n",
      "Preds:  [4 2 2 4 5 6 5 6 6 9]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  18 at step    72000 | time: 85.77s | valid loss 1.39203 | valid ppl    4.0230 | valid acc 0.266\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    72200 |  72200 batches | lr 0.00015 | ms/batch 24.07432 | loss 1.39629 | ppl     4.040\n",
      "| epoch   1 step    72400 |  72400 batches | lr 0.00015 | ms/batch 21.35940 | loss 1.39725 | ppl     4.044\n",
      "| epoch   1 step    72600 |  72600 batches | lr 0.00015 | ms/batch 20.90451 | loss 1.39920 | ppl     4.052\n",
      "| epoch   1 step    72800 |  72800 batches | lr 0.00015 | ms/batch 20.78448 | loss 1.39829 | ppl     4.048\n",
      "| epoch   1 step    73000 |  73000 batches | lr 0.00015 | ms/batch 21.22340 | loss 1.39901 | ppl     4.051\n",
      "| epoch   1 step    73200 |  73200 batches | lr 0.00015 | ms/batch 21.12756 | loss 1.39891 | ppl     4.051\n",
      "| epoch   1 step    73400 |  73400 batches | lr 0.00015 | ms/batch 20.99352 | loss 1.39933 | ppl     4.052\n",
      "| epoch   1 step    73600 |  73600 batches | lr 0.00015 | ms/batch 21.23044 | loss 1.39972 | ppl     4.054\n",
      "| epoch   1 step    73800 |  73800 batches | lr 0.00015 | ms/batch 21.23127 | loss 1.39875 | ppl     4.050\n",
      "| epoch   1 step    74000 |  74000 batches | lr 0.00015 | ms/batch 21.30845 | loss 1.40255 | ppl     4.066\n",
      "| epoch   1 step    74200 |  74200 batches | lr 0.00015 | ms/batch 21.34885 | loss 1.40159 | ppl     4.062\n",
      "| epoch   1 step    74400 |  74400 batches | lr 0.00015 | ms/batch 20.96646 | loss 1.39648 | ppl     4.041\n",
      "| epoch   1 step    74600 |  74600 batches | lr 0.00015 | ms/batch 21.30456 | loss 1.39759 | ppl     4.045\n",
      "| epoch   1 step    74800 |  74800 batches | lr 0.00015 | ms/batch 21.39408 | loss 1.39895 | ppl     4.051\n",
      "| epoch   1 step    75000 |  75000 batches | lr 0.00015 | ms/batch 21.10221 | loss 1.39516 | ppl     4.036\n",
      "| epoch   1 step    75200 |  75200 batches | lr 0.00015 | ms/batch 21.57060 | loss 1.39858 | ppl     4.049\n",
      "| epoch   1 step    75400 |  75400 batches | lr 0.00015 | ms/batch 20.82791 | loss 1.40069 | ppl     4.058\n",
      "| epoch   1 step    75600 |  75600 batches | lr 0.00015 | ms/batch 21.42739 | loss 1.40078 | ppl     4.058\n",
      "| epoch   1 step    75800 |  75800 batches | lr 0.00015 | ms/batch 20.89434 | loss 1.40700 | ppl     4.084\n",
      "| epoch   1 step    76000 |  76000 batches | lr 0.00015 | ms/batch 21.79900 | loss 1.40464 | ppl     4.074\n",
      "maslina\n",
      "|\n",
      "Source: [26  0 30  3 28  4 15  9 30 10]\n",
      "Target: [ 0 30  3 28  4 15  9 30 10  3]\n",
      "Teacher forcing: acc:0.2423469387755102\n",
      "Preds:  [4 4 0 3 0 4 4 4 4 3]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  19 at step    76000 | time: 85.38s | valid loss 1.39732 | valid ppl    4.0444 | valid acc 0.242\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    76200 |  76200 batches | lr 0.00015 | ms/batch 24.53834 | loss 1.39930 | ppl     4.052\n",
      "| epoch   1 step    76400 |  76400 batches | lr 0.00015 | ms/batch 21.18280 | loss 1.39745 | ppl     4.045\n",
      "| epoch   1 step    76600 |  76600 batches | lr 0.00015 | ms/batch 20.80752 | loss 1.39930 | ppl     4.052\n",
      "| epoch   1 step    76800 |  76800 batches | lr 0.00015 | ms/batch 20.74750 | loss 1.39997 | ppl     4.055\n",
      "| epoch   1 step    77000 |  77000 batches | lr 0.00015 | ms/batch 21.25798 | loss 1.40093 | ppl     4.059\n",
      "| epoch   1 step    77200 |  77200 batches | lr 0.00015 | ms/batch 21.41375 | loss 1.40000 | ppl     4.055\n",
      "| epoch   1 step    77400 |  77400 batches | lr 0.00015 | ms/batch 21.23307 | loss 1.39808 | ppl     4.047\n",
      "| epoch   1 step    77600 |  77600 batches | lr 0.00015 | ms/batch 20.88308 | loss 1.40093 | ppl     4.059\n",
      "| epoch   1 step    77800 |  77800 batches | lr 0.00015 | ms/batch 20.88061 | loss 1.39811 | ppl     4.048\n",
      "| epoch   1 step    78000 |  78000 batches | lr 0.00015 | ms/batch 21.00991 | loss 1.40064 | ppl     4.058\n",
      "| epoch   1 step    78200 |  78200 batches | lr 0.00015 | ms/batch 21.32185 | loss 1.39685 | ppl     4.042\n",
      "| epoch   1 step    78400 |  78400 batches | lr 0.00015 | ms/batch 20.55422 | loss 1.39817 | ppl     4.048\n",
      "| epoch   1 step    78600 |  78600 batches | lr 0.00015 | ms/batch 20.69090 | loss 1.39706 | ppl     4.043\n",
      "| epoch   1 step    78800 |  78800 batches | lr 0.00015 | ms/batch 20.92628 | loss 1.39822 | ppl     4.048\n",
      "| epoch   1 step    79000 |  79000 batches | lr 0.00015 | ms/batch 21.60371 | loss 1.40043 | ppl     4.057\n",
      "| epoch   1 step    79200 |  79200 batches | lr 0.00015 | ms/batch 20.54992 | loss 1.39980 | ppl     4.054\n",
      "| epoch   1 step    79400 |  79400 batches | lr 0.00015 | ms/batch 20.81350 | loss 1.39692 | ppl     4.043\n",
      "| epoch   1 step    79600 |  79600 batches | lr 0.00015 | ms/batch 20.21204 | loss 1.39855 | ppl     4.049\n",
      "| epoch   1 step    79800 |  79800 batches | lr 0.00015 | ms/batch 20.23768 | loss 1.39705 | ppl     4.043\n",
      "| epoch   1 step    80000 |  80000 batches | lr 0.00015 | ms/batch 20.36437 | loss 1.39839 | ppl     4.049\n",
      "|\n",
      "Source: [20  3 35  7 36  6 25  8 20 10]\n",
      "Target: [ 3 35  7 36  6 25  8 20 10  3]\n",
      "Teacher forcing: acc:0.261875\n",
      "Preds:  [4 3 3 7 7 6 7 8 3 6]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  20 at step    80000 | time: 84.24s | valid loss 1.38864 | valid ppl    4.0094 | valid acc 0.262\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    80200 |  80200 batches | lr 0.00015 | ms/batch 24.30257 | loss 1.40125 | ppl     4.060\n",
      "| epoch   1 step    80400 |  80400 batches | lr 0.00015 | ms/batch 20.59855 | loss 1.40049 | ppl     4.057\n",
      "| epoch   1 step    80600 |  80600 batches | lr 0.00015 | ms/batch 20.66233 | loss 1.39722 | ppl     4.044\n",
      "| epoch   1 step    80800 |  80800 batches | lr 0.00015 | ms/batch 20.55825 | loss 1.39669 | ppl     4.042\n",
      "| epoch   1 step    81000 |  81000 batches | lr 0.00015 | ms/batch 20.70421 | loss 1.40071 | ppl     4.058\n",
      "| epoch   1 step    81200 |  81200 batches | lr 0.00015 | ms/batch 20.49743 | loss 1.40027 | ppl     4.056\n",
      "| epoch   1 step    81400 |  81400 batches | lr 0.00015 | ms/batch 20.57880 | loss 1.39933 | ppl     4.052\n",
      "| epoch   1 step    81600 |  81600 batches | lr 0.00015 | ms/batch 20.79382 | loss 1.40003 | ppl     4.055\n",
      "| epoch   1 step    81800 |  81800 batches | lr 0.00015 | ms/batch 21.59996 | loss 1.39685 | ppl     4.042\n",
      "| epoch   1 step    82000 |  82000 batches | lr 0.00015 | ms/batch 21.43102 | loss 1.39764 | ppl     4.046\n"
     ]
    }
   ],
   "source": [
    "!bash run_retrieval.sh train --work_dir ../evaluation/tl10xl0mt0_l2h2_lr3e-4_rlrp_as1 --n_layer 2 --n_head 2 --answer_size 1 --lr 0.0003 --tgt_len 10 --eval_tgt_len 10 --mem_len 0 --num_mem_tokens 0 --device_ids 0 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2099e2a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[0, 2]\n",
      "Experiment dir : ../evaluation/tl12xl0mt12_deeper_lr1e-4_rlrp-reverse/20220111-165124\n",
      "====================================================================================================\n",
      "    - data : /home/admin/x-transformers/data24\n",
      "    - dataset : reverse\n",
      "    - n_layer : 6\n",
      "    - n_head : 3\n",
      "    - d_head : 64\n",
      "    - d_embed : 128\n",
      "    - d_model : 128\n",
      "    - d_inner : 256\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.0001\n",
      "    - mom : 0.0\n",
      "    - scheduler : dev_perf\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 0.0\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 250000\n",
      "    - batch_size : 32\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 12\n",
      "    - eval_tgt_len : 12\n",
      "    - ext_len : 0\n",
      "    - mem_len : 0\n",
      "    - num_mem_tokens : 12\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : False\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : ../evaluation/tl12xl0mt12_deeper_lr1e-4_rlrp-reverse/20220111-165124\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : 50\n",
      "    - sample_softmax : -1\n",
      "    - patience : 5\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [0, 2]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : True\n",
      "    - read_mem_from_cache : True\n",
      "    - answer_size : 24\n",
      "    - tied : True\n",
      "    - ntokens : 12\n",
      "    - n_all_param : 1139340\n",
      "    - n_nonemb_param : 1135872\n",
      "====================================================================================================\n",
      "#params = 1139340\n",
      "#non emb params = 1135872\n",
      "| epoch   1 step      200 |    200 batches | lr 0.0001 | ms/batch 231.22954 | loss 2.34959 | ppl    10.481\n",
      "| epoch   1 step      400 |    400 batches | lr 0.0001 | ms/batch 215.22606 | loss 2.30949 | ppl    10.069\n",
      "| epoch   1 step      600 |    600 batches | lr 0.0001 | ms/batch 219.64101 | loss 2.30709 | ppl    10.045\n",
      "| epoch   1 step      800 |    800 batches | lr 0.0001 | ms/batch 215.95480 | loss 2.30613 | ppl    10.036\n",
      "| epoch   1 step     1000 |   1000 batches | lr 0.0001 | ms/batch 216.35310 | loss 2.30549 | ppl    10.029\n",
      "| epoch   1 step     1200 |   1200 batches | lr 0.0001 | ms/batch 217.85137 | loss 2.30495 | ppl    10.024\n",
      "| epoch   1 step     1400 |   1400 batches | lr 0.0001 | ms/batch 218.16331 | loss 2.30477 | ppl    10.022\n",
      "| epoch   1 step     1600 |   1600 batches | lr 0.0001 | ms/batch 213.06253 | loss 2.30452 | ppl    10.019\n",
      "| epoch   1 step     1800 |   1800 batches | lr 0.0001 | ms/batch 215.88551 | loss 2.30441 | ppl    10.018\n",
      "| epoch   1 step     2000 |   2000 batches | lr 0.0001 | ms/batch 218.09066 | loss 2.30428 | ppl    10.017\n",
      "| epoch   1 step     2200 |   2200 batches | lr 0.0001 | ms/batch 216.07061 | loss 2.30400 | ppl    10.014\n",
      "| epoch   1 step     2400 |   2400 batches | lr 0.0001 | ms/batch 220.56785 | loss 2.30371 | ppl    10.011\n",
      "| epoch   1 step     2600 |   2600 batches | lr 0.0001 | ms/batch 214.11929 | loss 2.30355 | ppl    10.010\n",
      "| epoch   1 step     2800 |   2800 batches | lr 0.0001 | ms/batch 216.37570 | loss 2.30370 | ppl    10.011\n",
      "| epoch   1 step     3000 |   3000 batches | lr 0.0001 | ms/batch 216.64577 | loss 2.30357 | ppl    10.010\n",
      "| epoch   1 step     3200 |   3200 batches | lr 0.0001 | ms/batch 218.20493 | loss 2.30338 | ppl    10.008\n",
      "| epoch   1 step     3400 |   3400 batches | lr 0.0001 | ms/batch 216.57620 | loss 2.30341 | ppl    10.008\n",
      "| epoch   1 step     3600 |   3600 batches | lr 0.0001 | ms/batch 216.23083 | loss 2.30333 | ppl    10.007\n",
      "| epoch   1 step     3800 |   3800 batches | lr 0.0001 | ms/batch 216.87392 | loss 2.30314 | ppl    10.006\n",
      "| epoch   1 step     4000 |   4000 batches | lr 0.0001 | ms/batch 218.75409 | loss 2.30313 | ppl    10.005\n",
      "|\n",
      "Source: [ 4  9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10\n",
      "  1 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9]\n",
      "Target: [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9  4]\n",
      "Teacher forcing: acc:0.10208333333333333\n",
      "Preds:  [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "No teacher forcing: acc:0.10208333333333333\n",
      "Preds:  [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step     4000 | time: 887.44s | valid loss 2.30279 | valid ppl   10.0021 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     4200 |   4200 batches | lr 0.0001 | ms/batch 306.31188 | loss 2.30316 | ppl    10.006\n",
      "| epoch   1 step     4400 |   4400 batches | lr 0.0001 | ms/batch 220.50820 | loss 2.30309 | ppl    10.005\n",
      "| epoch   1 step     4600 |   4600 batches | lr 0.0001 | ms/batch 221.77755 | loss 2.30305 | ppl    10.005\n",
      "| epoch   1 step     4800 |   4800 batches | lr 0.0001 | ms/batch 221.04608 | loss 2.30315 | ppl    10.006\n",
      "| epoch   1 step     5000 |   5000 batches | lr 0.0001 | ms/batch 217.52555 | loss 2.30301 | ppl    10.004\n",
      "| epoch   1 step     5200 |   5200 batches | lr 0.0001 | ms/batch 224.65335 | loss 2.30288 | ppl    10.003\n",
      "| epoch   1 step     5400 |   5400 batches | lr 0.0001 | ms/batch 220.58355 | loss 2.30297 | ppl    10.004\n",
      "| epoch   1 step     5600 |   5600 batches | lr 0.0001 | ms/batch 218.95249 | loss 2.30308 | ppl    10.005\n",
      "| epoch   1 step     5800 |   5800 batches | lr 0.0001 | ms/batch 216.27889 | loss 2.30284 | ppl    10.003\n",
      "| epoch   1 step     6000 |   6000 batches | lr 0.0001 | ms/batch 221.95867 | loss 2.30286 | ppl    10.003\n",
      "| epoch   1 step     6200 |   6200 batches | lr 0.0001 | ms/batch 221.76820 | loss 2.30294 | ppl    10.004\n",
      "| epoch   1 step     6400 |   6400 batches | lr 0.0001 | ms/batch 223.67334 | loss 2.30278 | ppl    10.002\n",
      "| epoch   1 step     6600 |   6600 batches | lr 0.0001 | ms/batch 222.53880 | loss 2.30282 | ppl    10.002\n",
      "| epoch   1 step     6800 |   6800 batches | lr 0.0001 | ms/batch 221.34493 | loss 2.30289 | ppl    10.003\n",
      "| epoch   1 step     7000 |   7000 batches | lr 0.0001 | ms/batch 219.50869 | loss 2.30283 | ppl    10.002\n",
      "| epoch   1 step     7200 |   7200 batches | lr 0.0001 | ms/batch 221.97492 | loss 2.30286 | ppl    10.003\n",
      "| epoch   1 step     7400 |   7400 batches | lr 0.0001 | ms/batch 219.51500 | loss 2.30284 | ppl    10.003\n",
      "| epoch   1 step     7600 |   7600 batches | lr 0.0001 | ms/batch 223.28499 | loss 2.30282 | ppl    10.002\n",
      "| epoch   1 step     7800 |   7800 batches | lr 0.0001 | ms/batch 218.63453 | loss 2.30292 | ppl    10.003\n",
      "| epoch   1 step     8000 |   8000 batches | lr 0.0001 | ms/batch 220.28692 | loss 2.30281 | ppl    10.002\n",
      "maslina\n",
      "|\n",
      "Source: [ 4 10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8\n",
      "  1  8  5  8  2  9 10 10  8  2  6  6  5  8 10  2  7  9  3  6  6  5  5 10]\n",
      "Target: [10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8  1\n",
      "  8  5  8  2  9 10 10  8  2  6  6  5  8 10  2  7  9  3  6  6  5  5 10  4]\n",
      "Teacher forcing: acc:0.10212053571428571\n",
      "Preds:  [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5]\n",
      "\n",
      "No teacher forcing: acc:0.10212053571428571\n",
      "Preds:  [10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8  1\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step     8000 | time: 900.13s | valid loss 2.30265 | valid ppl   10.0007 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     8200 |   8200 batches | lr 0.0001 | ms/batch 305.82782 | loss 2.30283 | ppl    10.002\n",
      "| epoch   1 step     8400 |   8400 batches | lr 0.0001 | ms/batch 220.84853 | loss 2.30278 | ppl    10.002\n",
      "| epoch   1 step     8600 |   8600 batches | lr 0.0001 | ms/batch 223.08609 | loss 2.30279 | ppl    10.002\n",
      "| epoch   1 step     8800 |   8800 batches | lr 0.0001 | ms/batch 220.96297 | loss 2.30278 | ppl    10.002\n",
      "| epoch   1 step     9000 |   9000 batches | lr 0.0001 | ms/batch 219.21357 | loss 2.30272 | ppl    10.001\n",
      "| epoch   1 step     9200 |   9200 batches | lr 0.0001 | ms/batch 221.35987 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step     9400 |   9400 batches | lr 0.0001 | ms/batch 221.32741 | loss 2.30276 | ppl    10.002\n",
      "| epoch   1 step     9600 |   9600 batches | lr 0.0001 | ms/batch 220.93921 | loss 2.30277 | ppl    10.002\n",
      "| epoch   1 step     9800 |   9800 batches | lr 0.0001 | ms/batch 218.50967 | loss 2.30275 | ppl    10.002\n",
      "| epoch   1 step    10000 |  10000 batches | lr 0.0001 | ms/batch 220.79642 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    10200 |  10200 batches | lr 0.0001 | ms/batch 220.47030 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    10400 |  10400 batches | lr 0.0001 | ms/batch 220.93622 | loss 2.30272 | ppl    10.001\n",
      "| epoch   1 step    10600 |  10600 batches | lr 0.0001 | ms/batch 221.37741 | loss 2.30276 | ppl    10.002\n",
      "| epoch   1 step    10800 |  10800 batches | lr 0.0001 | ms/batch 225.64179 | loss 2.30273 | ppl    10.001\n",
      "| epoch   1 step    11000 |  11000 batches | lr 0.0001 | ms/batch 221.00520 | loss 2.30278 | ppl    10.002\n",
      "| epoch   1 step    11200 |  11200 batches | lr 0.0001 | ms/batch 219.34540 | loss 2.30277 | ppl    10.002\n",
      "| epoch   1 step    11400 |  11400 batches | lr 0.0001 | ms/batch 219.10021 | loss 2.30272 | ppl    10.001\n",
      "| epoch   1 step    11600 |  11600 batches | lr 0.0001 | ms/batch 218.28295 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    11800 |  11800 batches | lr 0.0001 | ms/batch 219.72631 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    12000 |  12000 batches | lr 0.0001 | ms/batch 218.86798 | loss 2.30271 | ppl    10.001\n",
      "|\n",
      "Source: [ 9  7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3\n",
      "  1  3  3  4 11  6  5  9  9  8  3  7  6  2  9 10 10 11  4 11  3  3 10  7]\n",
      "Target: [ 7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3  1\n",
      "  3  3  4 11  6  5  9  9  8  3  7  6  2  9 10 10 11  4 11  3  3 10  7  9]\n",
      "Teacher forcing: acc:0.098125\n",
      "Preds:  [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7]\n",
      "\n",
      "No teacher forcing: acc:0.098125\n",
      "Preds:  [ 7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3  1\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step    12000 | time: 900.18s | valid loss 2.30262 | valid ppl   10.0003 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    12200 |  12200 batches | lr 0.0001 | ms/batch 311.00323 | loss 2.30272 | ppl    10.001\n",
      "| epoch   1 step    12400 |  12400 batches | lr 0.0001 | ms/batch 218.07062 | loss 2.30272 | ppl    10.001\n",
      "| epoch   1 step    12600 |  12600 batches | lr 0.0001 | ms/batch 217.68528 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    12800 |  12800 batches | lr 0.0001 | ms/batch 219.42305 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    13000 |  13000 batches | lr 0.0001 | ms/batch 218.23404 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    13200 |  13200 batches | lr 0.0001 | ms/batch 220.00454 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    13400 |  13400 batches | lr 0.0001 | ms/batch 219.96786 | loss 2.30272 | ppl    10.001\n",
      "| epoch   1 step    13600 |  13600 batches | lr 0.0001 | ms/batch 223.50750 | loss 2.30270 | ppl    10.001\n",
      "| epoch   1 step    13800 |  13800 batches | lr 0.0001 | ms/batch 220.57564 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    14000 |  14000 batches | lr 0.0001 | ms/batch 221.25061 | loss 2.30270 | ppl    10.001\n",
      "| epoch   1 step    14200 |  14200 batches | lr 0.0001 | ms/batch 227.12128 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    14400 |  14400 batches | lr 0.0001 | ms/batch 223.92723 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    14600 |  14600 batches | lr 0.0001 | ms/batch 223.96918 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    14800 |  14800 batches | lr 0.0001 | ms/batch 233.81877 | loss 2.30270 | ppl    10.001\n",
      "| epoch   1 step    15000 |  15000 batches | lr 0.0001 | ms/batch 233.30349 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    15200 |  15200 batches | lr 0.0001 | ms/batch 224.54464 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    15400 |  15400 batches | lr 0.0001 | ms/batch 219.78621 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    15600 |  15600 batches | lr 0.0001 | ms/batch 218.34553 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    15800 |  15800 batches | lr 0.0001 | ms/batch 223.64315 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    16000 |  16000 batches | lr 0.0001 | ms/batch 219.81163 | loss 2.30273 | ppl    10.001\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9\n",
      "  1  9  6  4  6  2  6 10 10 10  8  8  3  9  7 11  2 11  3  3  9  4  6  3]\n",
      "Target: [ 3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9  1\n",
      "  9  6  4  6  2  6 10 10 10  8  8  3  9  7 11  2 11  3  3  9  4  6  3  7]\n",
      "Teacher forcing: acc:0.09909119897959184\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.09909119897959184\n",
      "Preds:  [ 3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   4 at step    16000 | time: 907.04s | valid loss 2.30268 | valid ppl   10.0010 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    16200 |  16200 batches | lr 0.0001 | ms/batch 307.44020 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    16400 |  16400 batches | lr 0.0001 | ms/batch 227.05452 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    16600 |  16600 batches | lr 0.0001 | ms/batch 222.10943 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    16800 |  16800 batches | lr 0.0001 | ms/batch 223.05999 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    17000 |  17000 batches | lr 0.0001 | ms/batch 219.55197 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    17200 |  17200 batches | lr 0.0001 | ms/batch 221.24571 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    17400 |  17400 batches | lr 0.0001 | ms/batch 221.57712 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    17600 |  17600 batches | lr 0.0001 | ms/batch 220.09569 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    17800 |  17800 batches | lr 0.0001 | ms/batch 220.32274 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    18000 |  18000 batches | lr 0.0001 | ms/batch 218.14287 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    18200 |  18200 batches | lr 0.0001 | ms/batch 219.94160 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    18400 |  18400 batches | lr 0.0001 | ms/batch 220.49862 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    18600 |  18600 batches | lr 0.0001 | ms/batch 219.97289 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    18800 |  18800 batches | lr 0.0001 | ms/batch 219.73691 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    19000 |  19000 batches | lr 0.0001 | ms/batch 221.29619 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    19200 |  19200 batches | lr 0.0001 | ms/batch 223.18235 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    19400 |  19400 batches | lr 0.0001 | ms/batch 223.69981 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    19600 |  19600 batches | lr 0.0001 | ms/batch 223.77968 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    19800 |  19800 batches | lr 0.0001 | ms/batch 225.04961 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    20000 |  20000 batches | lr 0.0001 | ms/batch 226.59437 | loss 2.30266 | ppl    10.001\n",
      "|\n",
      "Source: [ 2  3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10\n",
      "  1 10  8  3  2  6  6  3  3  6  5 10  8  4  2  3  4  6  2 11  7  9  9  3]\n",
      "Target: [ 3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10  1\n",
      " 10  8  3  2  6  6  3  3  6  5 10  8  4  2  3  4  6  2 11  7  9  9  3  2]\n",
      "Teacher forcing: acc:0.09794270833333334\n",
      "Preds:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9]\n",
      "\n",
      "No teacher forcing: acc:0.09794270833333334\n",
      "Preds:  [ 3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10  1\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   5 at step    20000 | time: 905.60s | valid loss 2.30265 | valid ppl   10.0006 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    20200 |  20200 batches | lr 0.0001 | ms/batch 312.87982 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    20400 |  20400 batches | lr 0.0001 | ms/batch 225.65463 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    20600 |  20600 batches | lr 0.0001 | ms/batch 224.83760 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    20800 |  20800 batches | lr 0.0001 | ms/batch 223.82899 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    21000 |  21000 batches | lr 0.0001 | ms/batch 223.74481 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    21200 |  21200 batches | lr 0.0001 | ms/batch 221.64772 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    21400 |  21400 batches | lr 0.0001 | ms/batch 223.02603 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    21600 |  21600 batches | lr 0.0001 | ms/batch 219.96881 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    21800 |  21800 batches | lr 0.0001 | ms/batch 223.34577 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    22000 |  22000 batches | lr 0.0001 | ms/batch 225.84555 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    22200 |  22200 batches | lr 0.0001 | ms/batch 223.83682 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    22400 |  22400 batches | lr 0.0001 | ms/batch 222.81242 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    22600 |  22600 batches | lr 0.0001 | ms/batch 221.55886 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    22800 |  22800 batches | lr 0.0001 | ms/batch 223.99980 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    23000 |  23000 batches | lr 0.0001 | ms/batch 223.00606 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    23200 |  23200 batches | lr 0.0001 | ms/batch 224.89690 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    23400 |  23400 batches | lr 0.0001 | ms/batch 228.55305 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    23600 |  23600 batches | lr 0.0001 | ms/batch 225.30614 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    23800 |  23800 batches | lr 0.0001 | ms/batch 225.82489 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    24000 |  24000 batches | lr 0.0001 | ms/batch 225.52609 | loss 2.30264 | ppl    10.001\n",
      "|\n",
      "Source: [ 2  3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3\n",
      "  1  3  3 10  2  5  6  4  2 11 10  6  6  6  7  6  4  7  8  7  2  2 11  3]\n",
      "Target: [ 3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3  1\n",
      "  3  3 10  2  5  6  4  2 11 10  6  6  6  7  6  4  7  8  7  2  2 11  3  2]\n",
      "Teacher forcing: acc:0.10182291666666667\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10182291666666667\n",
      "Preds:  [ 3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   6 at step    24000 | time: 913.93s | valid loss 2.30260 | valid ppl   10.0001 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    24200 |  24200 batches | lr 0.0001 | ms/batch 308.73071 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    24400 |  24400 batches | lr 0.0001 | ms/batch 222.13960 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    24600 |  24600 batches | lr 0.0001 | ms/batch 223.28577 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    24800 |  24800 batches | lr 0.0001 | ms/batch 221.54273 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    25000 |  25000 batches | lr 0.0001 | ms/batch 222.09263 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    25200 |  25200 batches | lr 0.0001 | ms/batch 221.69780 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    25400 |  25400 batches | lr 0.0001 | ms/batch 220.03692 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    25600 |  25600 batches | lr 0.0001 | ms/batch 221.87767 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    25800 |  25800 batches | lr 0.0001 | ms/batch 221.53766 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    26000 |  26000 batches | lr 0.0001 | ms/batch 219.45159 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    26200 |  26200 batches | lr 0.0001 | ms/batch 224.81429 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    26400 |  26400 batches | lr 0.0001 | ms/batch 223.25187 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    26600 |  26600 batches | lr 0.0001 | ms/batch 221.17183 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    26800 |  26800 batches | lr 0.0001 | ms/batch 222.78844 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    27000 |  27000 batches | lr 0.0001 | ms/batch 230.92799 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    27200 |  27200 batches | lr 0.0001 | ms/batch 238.56935 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    27400 |  27400 batches | lr 0.0001 | ms/batch 238.85820 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    27600 |  27600 batches | lr 0.0001 | ms/batch 243.12156 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    27800 |  27800 batches | lr 0.0001 | ms/batch 237.78813 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    28000 |  28000 batches | lr 0.0001 | ms/batch 242.00374 | loss 2.30266 | ppl    10.001\n",
      "maslina\n",
      "|\n",
      "Source: [ 9 10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7\n",
      "  1  7 11  3  6 10 11  5  3  9  2 11  9  7  4 10  6  8  4  4  3  4  9 10]\n",
      "Target: [10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7  1\n",
      "  7 11  3  6 10 11  5  3  9  2 11  9  7  4 10  6  8  4  4  3  4  9 10  9]\n",
      "Teacher forcing: acc:0.0975233843537415\n",
      "Preds:  [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "No teacher forcing: acc:0.0975233843537415\n",
      "Preds:  [10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7  1\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   7 at step    28000 | time: 926.33s | valid loss 2.30261 | valid ppl   10.0002 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    28200 |  28200 batches | lr 5e-05 | ms/batch 333.22465 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    28400 |  28400 batches | lr 5e-05 | ms/batch 240.00026 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    28600 |  28600 batches | lr 5e-05 | ms/batch 242.65074 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    28800 |  28800 batches | lr 5e-05 | ms/batch 245.82380 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    29000 |  29000 batches | lr 5e-05 | ms/batch 238.75050 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    29200 |  29200 batches | lr 5e-05 | ms/batch 238.89224 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    29400 |  29400 batches | lr 5e-05 | ms/batch 239.83489 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    29600 |  29600 batches | lr 5e-05 | ms/batch 238.75000 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    29800 |  29800 batches | lr 5e-05 | ms/batch 237.85046 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    30000 |  30000 batches | lr 5e-05 | ms/batch 246.52360 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    30200 |  30200 batches | lr 5e-05 | ms/batch 241.86694 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    30400 |  30400 batches | lr 5e-05 | ms/batch 241.99485 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    30600 |  30600 batches | lr 5e-05 | ms/batch 239.47137 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    30800 |  30800 batches | lr 5e-05 | ms/batch 240.49619 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    31000 |  31000 batches | lr 5e-05 | ms/batch 240.16480 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    31200 |  31200 batches | lr 5e-05 | ms/batch 238.67944 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    31400 |  31400 batches | lr 5e-05 | ms/batch 243.42033 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    31600 |  31600 batches | lr 5e-05 | ms/batch 237.38357 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    31800 |  31800 batches | lr 5e-05 | ms/batch 239.33889 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    32000 |  32000 batches | lr 5e-05 | ms/batch 239.00679 | loss 2.30262 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8\n",
      "  1  8  2  8  3  3 11  4  4 10  5  2  3  7  3  3  5  7  6  6  8  6  5  5]\n",
      "Target: [ 5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8  1\n",
      "  8  2  8  3  3 11  4  4 10  5  2  3  7  3  3  5  7  6  6  8  6  5  5  2]\n",
      "Teacher forcing: acc:0.101015625\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.101015625\n",
      "Preds:  [ 5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   8 at step    32000 | time: 981.60s | valid loss 2.30266 | valid ppl   10.0007 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    32200 |  32200 batches | lr 5e-05 | ms/batch 337.33743 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    32400 |  32400 batches | lr 5e-05 | ms/batch 243.00839 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    32600 |  32600 batches | lr 5e-05 | ms/batch 243.26829 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    32800 |  32800 batches | lr 5e-05 | ms/batch 240.57279 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    33000 |  33000 batches | lr 5e-05 | ms/batch 239.97608 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    33200 |  33200 batches | lr 5e-05 | ms/batch 238.54284 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    33400 |  33400 batches | lr 5e-05 | ms/batch 246.88770 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    33600 |  33600 batches | lr 5e-05 | ms/batch 249.30737 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    33800 |  33800 batches | lr 5e-05 | ms/batch 244.02331 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    34000 |  34000 batches | lr 5e-05 | ms/batch 238.10706 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    34200 |  34200 batches | lr 5e-05 | ms/batch 246.04997 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    34400 |  34400 batches | lr 5e-05 | ms/batch 241.89888 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    34600 |  34600 batches | lr 5e-05 | ms/batch 240.50170 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    34800 |  34800 batches | lr 5e-05 | ms/batch 240.28601 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    35000 |  35000 batches | lr 5e-05 | ms/batch 239.40072 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    35200 |  35200 batches | lr 5e-05 | ms/batch 239.24888 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    35400 |  35400 batches | lr 5e-05 | ms/batch 237.66240 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    35600 |  35600 batches | lr 5e-05 | ms/batch 236.17291 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    35800 |  35800 batches | lr 5e-05 | ms/batch 237.97464 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    36000 |  36000 batches | lr 5e-05 | ms/batch 235.72621 | loss 2.30263 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 5  3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3\n",
      "  1  3  6  5  9  4  5 11  4  7  7  2  4  3  5  9 11  7  8 11 11 10 10  3]\n",
      "Target: [ 3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3  1\n",
      "  3  6  5  9  4  5 11  4  7  7  2  4  3  5  9 11  7  8 11 11 10 10  3  5]\n",
      "Teacher forcing: acc:0.10036670918367346\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10036670918367346\n",
      "Preds:  [ 3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   9 at step    36000 | time: 982.73s | valid loss 2.30264 | valid ppl   10.0005 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    36200 |  36200 batches | lr 5e-05 | ms/batch 332.54599 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    36400 |  36400 batches | lr 5e-05 | ms/batch 241.44686 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    36600 |  36600 batches | lr 5e-05 | ms/batch 237.21942 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    36800 |  36800 batches | lr 5e-05 | ms/batch 238.15869 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    37000 |  37000 batches | lr 5e-05 | ms/batch 238.99173 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    37200 |  37200 batches | lr 5e-05 | ms/batch 236.92320 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    37400 |  37400 batches | lr 5e-05 | ms/batch 238.78550 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    37600 |  37600 batches | lr 5e-05 | ms/batch 237.88299 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    37800 |  37800 batches | lr 5e-05 | ms/batch 246.27816 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    38000 |  38000 batches | lr 5e-05 | ms/batch 239.18443 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    38200 |  38200 batches | lr 5e-05 | ms/batch 263.71328 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    38400 |  38400 batches | lr 5e-05 | ms/batch 242.61135 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    38600 |  38600 batches | lr 5e-05 | ms/batch 242.34037 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    38800 |  38800 batches | lr 5e-05 | ms/batch 240.96493 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    39000 |  39000 batches | lr 5e-05 | ms/batch 241.99895 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    39200 |  39200 batches | lr 5e-05 | ms/batch 237.51424 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    39400 |  39400 batches | lr 5e-05 | ms/batch 236.85055 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    39600 |  39600 batches | lr 5e-05 | ms/batch 237.49393 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    39800 |  39800 batches | lr 5e-05 | ms/batch 237.41112 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    40000 |  40000 batches | lr 5e-05 | ms/batch 241.69398 | loss 2.30261 | ppl    10.000\n",
      "|\n",
      "Source: [ 4 11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2\n",
      "  1  2  3  8  7  8 10  7 11  7  2  5  6 10  8  2 11  9  3  8 11  7  9 11]\n",
      "Target: [11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2  1\n",
      "  2  3  8  7  8 10  7 11  7  2  5  6 10  8  2 11  9  3  8 11  7  9 11  4]\n",
      "Teacher forcing: acc:0.09830729166666667\n",
      "Preds:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9]\n",
      "\n",
      "No teacher forcing: acc:0.09830729166666667\n",
      "Preds:  [11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2  1\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  10 at step    40000 | time: 982.01s | valid loss 2.30266 | valid ppl   10.0008 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    40200 |  40200 batches | lr 5e-05 | ms/batch 336.24316 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    40400 |  40400 batches | lr 5e-05 | ms/batch 241.36237 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    40600 |  40600 batches | lr 5e-05 | ms/batch 239.90292 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    40800 |  40800 batches | lr 5e-05 | ms/batch 239.59695 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    41000 |  41000 batches | lr 5e-05 | ms/batch 241.56505 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    41200 |  41200 batches | lr 5e-05 | ms/batch 241.60790 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    41400 |  41400 batches | lr 5e-05 | ms/batch 239.75761 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    41600 |  41600 batches | lr 5e-05 | ms/batch 243.02506 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    41800 |  41800 batches | lr 5e-05 | ms/batch 240.30650 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    42000 |  42000 batches | lr 5e-05 | ms/batch 239.11089 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    42200 |  42200 batches | lr 5e-05 | ms/batch 240.72138 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    42400 |  42400 batches | lr 5e-05 | ms/batch 238.60163 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    42600 |  42600 batches | lr 5e-05 | ms/batch 240.43384 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    42800 |  42800 batches | lr 5e-05 | ms/batch 240.81766 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    43000 |  43000 batches | lr 5e-05 | ms/batch 244.57753 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    43200 |  43200 batches | lr 5e-05 | ms/batch 243.52908 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    43400 |  43400 batches | lr 5e-05 | ms/batch 241.79267 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    43600 |  43600 batches | lr 5e-05 | ms/batch 240.62878 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    43800 |  43800 batches | lr 5e-05 | ms/batch 240.60186 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    44000 |  44000 batches | lr 5e-05 | ms/batch 238.92945 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 8  3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9\n",
      "  1  9  7  9 10  3  7  9  6  4  4  4  8  8  4  5  9  7  8  6  5  8  6  3]\n",
      "Target: [ 3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9  1\n",
      "  9  7  9 10  3  7  9  6  4  4  4  8  8  4  5  9  7  8  6  5  8  6  3  8]\n",
      "Teacher forcing: acc:0.098671875\n",
      "Preds:  [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "No teacher forcing: acc:0.098671875\n",
      "Preds:  [ 3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9  1\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  11 at step    44000 | time: 982.89s | valid loss 2.30266 | valid ppl   10.0008 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    44200 |  44200 batches | lr 5e-05 | ms/batch 340.29033 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    44400 |  44400 batches | lr 5e-05 | ms/batch 239.62453 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    44600 |  44600 batches | lr 5e-05 | ms/batch 240.95116 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    44800 |  44800 batches | lr 5e-05 | ms/batch 243.19756 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    45000 |  45000 batches | lr 5e-05 | ms/batch 240.47657 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    45200 |  45200 batches | lr 5e-05 | ms/batch 245.25979 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    45400 |  45400 batches | lr 5e-05 | ms/batch 245.21304 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    45600 |  45600 batches | lr 5e-05 | ms/batch 243.29769 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    45800 |  45800 batches | lr 5e-05 | ms/batch 243.34748 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    46000 |  46000 batches | lr 5e-05 | ms/batch 239.95662 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    46200 |  46200 batches | lr 5e-05 | ms/batch 240.07513 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    46400 |  46400 batches | lr 5e-05 | ms/batch 240.38448 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    46600 |  46600 batches | lr 5e-05 | ms/batch 239.01536 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    46800 |  46800 batches | lr 5e-05 | ms/batch 242.70745 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    47000 |  47000 batches | lr 5e-05 | ms/batch 240.91843 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    47200 |  47200 batches | lr 5e-05 | ms/batch 240.69454 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    47400 |  47400 batches | lr 5e-05 | ms/batch 238.52735 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    47600 |  47600 batches | lr 5e-05 | ms/batch 239.30325 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    47800 |  47800 batches | lr 5e-05 | ms/batch 256.62660 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    48000 |  48000 batches | lr 5e-05 | ms/batch 241.90071 | loss 2.30261 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [11 10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4\n",
      "  1  4 10  4  4  3 11  9  7  8  9  7 10  7  8  3  3  6 10 11  7  7  5 10]\n",
      "Target: [10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4  1\n",
      "  4 10  4  4  3 11  9  7  8  9  7 10  7  8  3  3  6 10 11  7  7  5 10 11]\n",
      "Teacher forcing: acc:0.1017750850340136\n",
      "Preds:  [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  6 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "No teacher forcing: acc:0.1017750850340136\n",
      "Preds:  [10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4  1\n",
      "  6 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  12 at step    48000 | time: 988.89s | valid loss 2.30260 | valid ppl   10.0001 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    48200 |  48200 batches | lr 5e-05 | ms/batch 360.62986 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    48400 |  48400 batches | lr 5e-05 | ms/batch 241.29307 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    48600 |  48600 batches | lr 5e-05 | ms/batch 238.15367 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    48800 |  48800 batches | lr 5e-05 | ms/batch 237.59704 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    49000 |  49000 batches | lr 5e-05 | ms/batch 241.49994 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    49200 |  49200 batches | lr 5e-05 | ms/batch 245.30682 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    49400 |  49400 batches | lr 5e-05 | ms/batch 238.56631 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    49600 |  49600 batches | lr 5e-05 | ms/batch 238.99972 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    49800 |  49800 batches | lr 5e-05 | ms/batch 242.25257 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    50000 |  50000 batches | lr 5e-05 | ms/batch 240.04046 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    50200 |  50200 batches | lr 5e-05 | ms/batch 242.67838 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    50400 |  50400 batches | lr 5e-05 | ms/batch 247.55730 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    50600 |  50600 batches | lr 5e-05 | ms/batch 245.67839 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    50800 |  50800 batches | lr 5e-05 | ms/batch 237.57886 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    51000 |  51000 batches | lr 5e-05 | ms/batch 239.79978 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    51200 |  51200 batches | lr 5e-05 | ms/batch 241.82274 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    51400 |  51400 batches | lr 5e-05 | ms/batch 242.06998 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    51600 |  51600 batches | lr 5e-05 | ms/batch 245.95225 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    51800 |  51800 batches | lr 5e-05 | ms/batch 244.70244 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    52000 |  52000 batches | lr 5e-05 | ms/batch 240.29676 | loss 2.30263 | ppl    10.000\n",
      "|\n",
      "Source: [ 4  7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10\n",
      "  1 10 10  7  6  8 10  9  4  6 11 11 11  9  2  4  2  6 10 11  8  6  3  7]\n",
      "Target: [ 7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10  1\n",
      " 10 10  7  6  8 10  9  4  6 11 11 11  9  2  4  2  6 10 11  8  6  3  7  4]\n",
      "Teacher forcing: acc:0.103203125\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.103203125\n",
      "Preds:  [ 7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  13 at step    52000 | time: 989.87s | valid loss 2.30253 | valid ppl    9.9995 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    52200 |  52200 batches | lr 5e-05 | ms/batch 358.92346 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    52400 |  52400 batches | lr 5e-05 | ms/batch 247.55284 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    52600 |  52600 batches | lr 5e-05 | ms/batch 238.46927 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    52800 |  52800 batches | lr 5e-05 | ms/batch 240.70183 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    53000 |  53000 batches | lr 5e-05 | ms/batch 247.18571 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    53200 |  53200 batches | lr 5e-05 | ms/batch 249.84792 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    53400 |  53400 batches | lr 5e-05 | ms/batch 238.56918 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    53600 |  53600 batches | lr 5e-05 | ms/batch 240.55724 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    53800 |  53800 batches | lr 5e-05 | ms/batch 243.79672 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    54000 |  54000 batches | lr 5e-05 | ms/batch 241.96089 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    54200 |  54200 batches | lr 5e-05 | ms/batch 237.85035 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    54400 |  54400 batches | lr 5e-05 | ms/batch 246.97306 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    54600 |  54600 batches | lr 5e-05 | ms/batch 240.76945 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    54800 |  54800 batches | lr 5e-05 | ms/batch 238.31531 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    55000 |  55000 batches | lr 5e-05 | ms/batch 238.58828 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    55200 |  55200 batches | lr 5e-05 | ms/batch 240.10709 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    55400 |  55400 batches | lr 5e-05 | ms/batch 240.82313 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    55600 |  55600 batches | lr 5e-05 | ms/batch 243.90603 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    55800 |  55800 batches | lr 5e-05 | ms/batch 239.99749 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    56000 |  56000 batches | lr 5e-05 | ms/batch 234.78354 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  7  6  9  6  4  3  2  8  5  3  8  7  7  4  8  5 11  2  3  8  4  9 11\n",
      "  1 11  9  4  8  3  2 11  5  8  4  7  7  8  3  5  8  2  3  4  6  9  6  7]\n",
      "Target: [ 7  6  9  6  4  3  2  8  5  3  8  7  7  4  8  5 11  2  3  8  4  9 11  1\n",
      " 11  9  4  8  3  2 11  5  8  4  7  7  8  3  5  8  2  3  4  6  9  6  7  7]\n",
      "Teacher forcing: acc:0.09837372448979592\n",
      "Preds:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9]\n",
      "\n",
      "No teacher forcing: acc:0.09837372448979592\n",
      "Preds:  [ 7  6  9  6  4  3  2  8  5  3  8  7  7  4  8  5 11  2  3  8  4  9 11  1\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  14 at step    56000 | time: 989.50s | valid loss 2.30260 | valid ppl   10.0001 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    56200 |  56200 batches | lr 5e-05 | ms/batch 334.86953 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    56400 |  56400 batches | lr 5e-05 | ms/batch 241.99993 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    56600 |  56600 batches | lr 5e-05 | ms/batch 244.38475 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    56800 |  56800 batches | lr 5e-05 | ms/batch 239.53068 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    57000 |  57000 batches | lr 5e-05 | ms/batch 243.53894 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    57200 |  57200 batches | lr 5e-05 | ms/batch 237.92726 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    57400 |  57400 batches | lr 5e-05 | ms/batch 244.53926 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    57600 |  57600 batches | lr 5e-05 | ms/batch 242.63013 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    57800 |  57800 batches | lr 5e-05 | ms/batch 241.18449 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    58000 |  58000 batches | lr 5e-05 | ms/batch 240.09410 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    58200 |  58200 batches | lr 5e-05 | ms/batch 245.77629 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    58400 |  58400 batches | lr 5e-05 | ms/batch 240.05803 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    58600 |  58600 batches | lr 5e-05 | ms/batch 240.32869 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    58800 |  58800 batches | lr 5e-05 | ms/batch 240.06832 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    59000 |  59000 batches | lr 5e-05 | ms/batch 236.85120 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    59200 |  59200 batches | lr 5e-05 | ms/batch 237.40989 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    59400 |  59400 batches | lr 5e-05 | ms/batch 238.71586 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    59600 |  59600 batches | lr 5e-05 | ms/batch 246.54076 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    59800 |  59800 batches | lr 5e-05 | ms/batch 241.37559 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    60000 |  60000 batches | lr 5e-05 | ms/batch 241.97441 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [11  8  3  2  3  9 10  5  9 10  6 10 11  9  6 11  2  9  7  5  7  5  3 11\n",
      "  1 11  3  5  7  5  7  9  2 11  6  9 11 10  6 10  9  5 10  9  3  2  3  8]\n",
      "Target: [ 8  3  2  3  9 10  5  9 10  6 10 11  9  6 11  2  9  7  5  7  5  3 11  1\n",
      " 11  3  5  7  5  7  9  2 11  6  9 11 10  6 10  9  5 10  9  3  2  3  8 11]\n",
      "Teacher forcing: acc:0.098203125\n",
      "Preds:  [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 6 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7]\n",
      "\n",
      "No teacher forcing: acc:0.098203125\n",
      "Preds:  [ 8  3  2  3  9 10  5  9 10  6 10 11  9  6 11  2  9  7  5  7  5  3 11  1\n",
      "  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  15 at step    60000 | time: 984.35s | valid loss 2.30267 | valid ppl   10.0009 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    60200 |  60200 batches | lr 5e-05 | ms/batch 334.52673 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    60400 |  60400 batches | lr 5e-05 | ms/batch 238.86949 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    60600 |  60600 batches | lr 5e-05 | ms/batch 238.77321 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    60800 |  60800 batches | lr 5e-05 | ms/batch 244.14763 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    61000 |  61000 batches | lr 5e-05 | ms/batch 241.71936 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    61200 |  61200 batches | lr 5e-05 | ms/batch 242.56464 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    61400 |  61400 batches | lr 5e-05 | ms/batch 242.33296 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    61600 |  61600 batches | lr 5e-05 | ms/batch 248.12731 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    61800 |  61800 batches | lr 5e-05 | ms/batch 244.43772 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    62000 |  62000 batches | lr 5e-05 | ms/batch 244.27251 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    62200 |  62200 batches | lr 5e-05 | ms/batch 241.04404 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    62400 |  62400 batches | lr 5e-05 | ms/batch 242.01655 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    62600 |  62600 batches | lr 5e-05 | ms/batch 242.98193 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    62800 |  62800 batches | lr 5e-05 | ms/batch 241.05400 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    63000 |  63000 batches | lr 5e-05 | ms/batch 241.37082 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    63200 |  63200 batches | lr 5e-05 | ms/batch 239.50922 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    63400 |  63400 batches | lr 5e-05 | ms/batch 243.40587 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    63600 |  63600 batches | lr 5e-05 | ms/batch 237.84155 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    63800 |  63800 batches | lr 5e-05 | ms/batch 242.06563 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    64000 |  64000 batches | lr 5e-05 | ms/batch 240.20811 | loss 2.30261 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 8  6  9  4 11  4  3  8  4 11 10  6  2  6  6  5  4  4  2 11  4  8  5  3\n",
      "  1  3  5  8  4 11  2  4  4  5  6  6  2  6 10 11  4  8  3  4 11  4  9  6]\n",
      "Target: [ 6  9  4 11  4  3  8  4 11 10  6  2  6  6  5  4  4  2 11  4  8  5  3  1\n",
      "  3  5  8  4 11  2  4  4  5  6  6  2  6 10 11  4  8  3  4 11  4  9  6  8]\n",
      "Teacher forcing: acc:0.10257227891156463\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10257227891156463\n",
      "Preds:  [ 6  9  4 11  4  3  8  4 11 10  6  2  6  6  5  4  4  2 11  4  8  5  3  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  16 at step    64000 | time: 985.99s | valid loss 2.30254 | valid ppl    9.9995 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    64200 |  64200 batches | lr 5e-05 | ms/batch 338.24730 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    64400 |  64400 batches | lr 5e-05 | ms/batch 240.63100 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    64600 |  64600 batches | lr 5e-05 | ms/batch 242.47484 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    64800 |  64800 batches | lr 5e-05 | ms/batch 241.88008 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    65000 |  65000 batches | lr 5e-05 | ms/batch 239.69605 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    65200 |  65200 batches | lr 5e-05 | ms/batch 240.83495 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    65400 |  65400 batches | lr 5e-05 | ms/batch 240.63670 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    65600 |  65600 batches | lr 5e-05 | ms/batch 245.29535 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    65800 |  65800 batches | lr 5e-05 | ms/batch 246.13615 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    66000 |  66000 batches | lr 5e-05 | ms/batch 241.19509 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    66200 |  66200 batches | lr 5e-05 | ms/batch 239.70334 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    66400 |  66400 batches | lr 5e-05 | ms/batch 239.86774 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    66600 |  66600 batches | lr 5e-05 | ms/batch 240.99480 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    66800 |  66800 batches | lr 5e-05 | ms/batch 242.35208 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    67000 |  67000 batches | lr 5e-05 | ms/batch 243.03947 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    67200 |  67200 batches | lr 5e-05 | ms/batch 244.28399 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    67400 |  67400 batches | lr 5e-05 | ms/batch 238.23897 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    67600 |  67600 batches | lr 5e-05 | ms/batch 241.55675 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    67800 |  67800 batches | lr 5e-05 | ms/batch 237.57819 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    68000 |  68000 batches | lr 5e-05 | ms/batch 237.87094 | loss 2.30261 | ppl    10.000\n",
      "|\n",
      "Source: [10 10  2 11  7  2  7 10  5 10  3  7  2  9 10 11  5  4  5 10  5  8 10  6\n",
      "  1  6 10  8  5 10  5  4  5 11 10  9  2  7  3 10  5 10  7  2  7 11  2 10]\n",
      "Target: [10  2 11  7  2  7 10  5 10  3  7  2  9 10 11  5  4  5 10  5  8 10  6  1\n",
      "  6 10  8  5 10  5  4  5 11 10  9  2  7  3 10  5 10  7  2  7 11  2 10 10]\n",
      "Teacher forcing: acc:0.0996875\n",
      "Preds:  [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 4 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8]\n",
      "\n",
      "No teacher forcing: acc:0.0996875\n",
      "Preds:  [10  2 11  7  2  7 10  5 10  3  7  2  9 10 11  5  4  5 10  5  8 10  6  1\n",
      "  4  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  17 at step    68000 | time: 984.91s | valid loss 2.30260 | valid ppl   10.0001 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    68200 |  68200 batches | lr 5e-05 | ms/batch 336.56196 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    68400 |  68400 batches | lr 5e-05 | ms/batch 245.87128 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    68600 |  68600 batches | lr 5e-05 | ms/batch 238.57408 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    68800 |  68800 batches | lr 5e-05 | ms/batch 241.37419 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    69000 |  69000 batches | lr 5e-05 | ms/batch 240.88304 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    69200 |  69200 batches | lr 5e-05 | ms/batch 238.91260 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    69400 |  69400 batches | lr 5e-05 | ms/batch 241.78435 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    69600 |  69600 batches | lr 5e-05 | ms/batch 242.36863 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    69800 |  69800 batches | lr 5e-05 | ms/batch 243.71722 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    70000 |  70000 batches | lr 5e-05 | ms/batch 238.59189 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    70200 |  70200 batches | lr 5e-05 | ms/batch 238.18293 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    70400 |  70400 batches | lr 5e-05 | ms/batch 241.82503 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    70600 |  70600 batches | lr 5e-05 | ms/batch 241.47124 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    70800 |  70800 batches | lr 5e-05 | ms/batch 243.26603 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    71000 |  71000 batches | lr 5e-05 | ms/batch 248.04560 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    71200 |  71200 batches | lr 5e-05 | ms/batch 241.18054 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    71400 |  71400 batches | lr 5e-05 | ms/batch 239.92854 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    71600 |  71600 batches | lr 5e-05 | ms/batch 237.98544 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    71800 |  71800 batches | lr 5e-05 | ms/batch 239.14206 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    72000 |  72000 batches | lr 5e-05 | ms/batch 239.36117 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 6 11  9 11  9  2  3  9 10  6 10  5  7  3  8  8  2  6 11 11 10 11  4  6\n",
      "  1  6  4 11 10 11 11  6  2  8  8  3  7  5 10  6 10  9  3  2  9 11  9 11]\n",
      "Target: [11  9 11  9  2  3  9 10  6 10  5  7  3  8  8  2  6 11 11 10 11  4  6  1\n",
      "  6  4 11 10 11 11  6  2  8  8  3  7  5 10  6 10  9  3  2  9 11  9 11  6]\n",
      "Teacher forcing: acc:0.09966145833333333\n",
      "Preds:  [7 7 7 7 7 7 7 7 7 7 7 7 3 3 3 3 3 3 3 3 3 3 3 3 6 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09966145833333333\n",
      "Preds:  [11  9 11  9  2  3  9 10  6 10  5  7  3  8  8  2  6 11 11 10 11  4  6  1\n",
      "  6  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  18 at step    72000 | time: 983.67s | valid loss 2.30268 | valid ppl   10.0009 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    72200 |  72200 batches | lr 5e-05 | ms/batch 338.15328 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    72400 |  72400 batches | lr 5e-05 | ms/batch 240.28594 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    72600 |  72600 batches | lr 5e-05 | ms/batch 243.46090 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    72800 |  72800 batches | lr 5e-05 | ms/batch 238.38582 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    73000 |  73000 batches | lr 5e-05 | ms/batch 239.72683 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    73200 |  73200 batches | lr 5e-05 | ms/batch 242.97555 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    73400 |  73400 batches | lr 5e-05 | ms/batch 243.10407 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    73600 |  73600 batches | lr 5e-05 | ms/batch 244.06896 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    73800 |  73800 batches | lr 5e-05 | ms/batch 240.73573 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    74000 |  74000 batches | lr 5e-05 | ms/batch 239.68908 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    74200 |  74200 batches | lr 5e-05 | ms/batch 237.99152 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    74400 |  74400 batches | lr 5e-05 | ms/batch 233.31766 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    74600 |  74600 batches | lr 5e-05 | ms/batch 221.98948 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    74800 |  74800 batches | lr 5e-05 | ms/batch 225.73005 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    75000 |  75000 batches | lr 5e-05 | ms/batch 227.03088 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    75200 |  75200 batches | lr 5e-05 | ms/batch 222.62730 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    75400 |  75400 batches | lr 5e-05 | ms/batch 222.92205 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    75600 |  75600 batches | lr 5e-05 | ms/batch 221.75183 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    75800 |  75800 batches | lr 5e-05 | ms/batch 221.88544 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    76000 |  76000 batches | lr 5e-05 | ms/batch 220.76355 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 8  2 10  3  4  3 10  8  8 11 10  6  3 11  9  8  9  3  4  7 11  5  4  6\n",
      "  1  6  4  5 11  7  4  3  9  8  9 11  3  6 10 11  8  8 10  3  4  3 10  2]\n",
      "Target: [ 2 10  3  4  3 10  8  8 11 10  6  3 11  9  8  9  3  4  7 11  5  4  6  1\n",
      "  6  4  5 11  7  4  3  9  8  9 11  3  6 10 11  8  8 10  3  4  3 10  2  8]\n",
      "Teacher forcing: acc:0.10044642857142858\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.10047300170068027\n",
      "Preds:  [ 2 10  3  4  3 10  8  8 11 10  6  3 11  9  8  9  3  4  7 11  5  4  6  1\n",
      "  4  3  3  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  19 at step    76000 | time: 952.74s | valid loss 2.30258 | valid ppl   10.0000 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    76200 |  76200 batches | lr 2.5e-05 | ms/batch 319.24663 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    76400 |  76400 batches | lr 2.5e-05 | ms/batch 221.37757 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    76600 |  76600 batches | lr 2.5e-05 | ms/batch 222.60802 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    76800 |  76800 batches | lr 2.5e-05 | ms/batch 220.74551 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    77000 |  77000 batches | lr 2.5e-05 | ms/batch 222.56327 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    77200 |  77200 batches | lr 2.5e-05 | ms/batch 222.04867 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    77400 |  77400 batches | lr 2.5e-05 | ms/batch 224.56910 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    77600 |  77600 batches | lr 2.5e-05 | ms/batch 229.56921 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    77800 |  77800 batches | lr 2.5e-05 | ms/batch 222.31507 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    78000 |  78000 batches | lr 2.5e-05 | ms/batch 222.95146 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    78200 |  78200 batches | lr 2.5e-05 | ms/batch 221.61264 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    78400 |  78400 batches | lr 2.5e-05 | ms/batch 220.27010 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    78600 |  78600 batches | lr 2.5e-05 | ms/batch 224.11460 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    78800 |  78800 batches | lr 2.5e-05 | ms/batch 222.18791 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    79000 |  79000 batches | lr 2.5e-05 | ms/batch 228.01078 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    79200 |  79200 batches | lr 2.5e-05 | ms/batch 223.69216 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    79400 |  79400 batches | lr 2.5e-05 | ms/batch 222.81862 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    79600 |  79600 batches | lr 2.5e-05 | ms/batch 221.86589 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    79800 |  79800 batches | lr 2.5e-05 | ms/batch 223.53937 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    80000 |  80000 batches | lr 2.5e-05 | ms/batch 227.12233 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [10  5 11 10  9  7 10  9  3 10 11  4  2 10  8  4 10  2  9  7  3  7 11  4\n",
      "  1  4 11  7  3  7  9  2 10  4  8 10  2  4 11 10  3  9 10  7  9 10 11  5]\n",
      "Target: [ 5 11 10  9  7 10  9  3 10 11  4  2 10  8  4 10  2  9  7  3  7 11  4  1\n",
      "  4 11  7  3  7  9  2 10  4  8 10  2  4 11 10  3  9 10  7  9 10 11  5 10]\n",
      "Teacher forcing: acc:0.1\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.1\n",
      "Preds:  [ 5 11 10  9  7 10  9  3 10 11  4  2 10  8  4 10  2  9  7  3  7 11  4  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  20 at step    80000 | time: 912.77s | valid loss 2.30257 | valid ppl    9.9998 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    80200 |  80200 batches | lr 2.5e-05 | ms/batch 318.82581 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    80400 |  80400 batches | lr 2.5e-05 | ms/batch 228.74309 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    80600 |  80600 batches | lr 2.5e-05 | ms/batch 224.23754 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    80800 |  80800 batches | lr 2.5e-05 | ms/batch 223.66370 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    81000 |  81000 batches | lr 2.5e-05 | ms/batch 222.50618 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step    81200 |  81200 batches | lr 2.5e-05 | ms/batch 220.70090 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    81400 |  81400 batches | lr 2.5e-05 | ms/batch 221.94408 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    81600 |  81600 batches | lr 2.5e-05 | ms/batch 223.83525 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    81800 |  81800 batches | lr 2.5e-05 | ms/batch 227.20506 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    82000 |  82000 batches | lr 2.5e-05 | ms/batch 228.39384 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    82200 |  82200 batches | lr 2.5e-05 | ms/batch 225.77037 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    82400 |  82400 batches | lr 2.5e-05 | ms/batch 223.51231 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    82600 |  82600 batches | lr 2.5e-05 | ms/batch 221.68956 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    82800 |  82800 batches | lr 2.5e-05 | ms/batch 221.61304 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    83000 |  83000 batches | lr 2.5e-05 | ms/batch 222.34202 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    83200 |  83200 batches | lr 2.5e-05 | ms/batch 227.32161 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    83400 |  83400 batches | lr 2.5e-05 | ms/batch 224.42556 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    83600 |  83600 batches | lr 2.5e-05 | ms/batch 227.51682 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    83800 |  83800 batches | lr 2.5e-05 | ms/batch 225.18683 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    84000 |  84000 batches | lr 2.5e-05 | ms/batch 220.56118 | loss 2.30260 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 9  8  6 11  6  5  3  9  8  2  7  8  7  9  9  9  7  4 10 11 11 10 10  6\n",
      "  1  6 10 10 11 11 10  4  7  9  9  9  7  8  7  2  8  9  3  5  6 11  6  8]\n",
      "Target: [ 8  6 11  6  5  3  9  8  2  7  8  7  9  9  9  7  4 10 11 11 10 10  6  1\n",
      "  6 10 10 11 11 10  4  7  9  9  9  7  8  7  2  8  9  3  5  6 11  6  8  9]\n",
      "Teacher forcing: acc:0.09840029761904762\n",
      "Preds:  [5 5 5 5 5 5 5 5 5 5 5 5 9 9 9 9 5 9 5 9 9 5 5 5 4 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 5 5 5 5 5]\n",
      "\n",
      "No teacher forcing: acc:0.09962266156462585\n",
      "Preds:  [ 8  6 11  6  5  3  9  8  2  7  8  7  9  9  9  7  4 10 11 11 10 10  6  1\n",
      "  4  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  5  5]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  21 at step    84000 | time: 915.50s | valid loss 2.30260 | valid ppl   10.0001 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    84200 |  84200 batches | lr 2.5e-05 | ms/batch 313.81915 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    84400 |  84400 batches | lr 2.5e-05 | ms/batch 223.65154 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    84600 |  84600 batches | lr 2.5e-05 | ms/batch 220.50425 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step    84800 |  84800 batches | lr 2.5e-05 | ms/batch 221.27173 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    85000 |  85000 batches | lr 2.5e-05 | ms/batch 222.64714 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    85200 |  85200 batches | lr 2.5e-05 | ms/batch 224.32171 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    85400 |  85400 batches | lr 2.5e-05 | ms/batch 225.94508 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    85600 |  85600 batches | lr 2.5e-05 | ms/batch 226.94369 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    85800 |  85800 batches | lr 2.5e-05 | ms/batch 228.41637 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    86000 |  86000 batches | lr 2.5e-05 | ms/batch 224.04190 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    86200 |  86200 batches | lr 2.5e-05 | ms/batch 225.33236 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    86400 |  86400 batches | lr 2.5e-05 | ms/batch 225.05265 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    86600 |  86600 batches | lr 2.5e-05 | ms/batch 222.58148 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    86800 |  86800 batches | lr 2.5e-05 | ms/batch 225.52367 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    87000 |  87000 batches | lr 2.5e-05 | ms/batch 224.94032 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    87200 |  87200 batches | lr 2.5e-05 | ms/batch 227.31804 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    87400 |  87400 batches | lr 2.5e-05 | ms/batch 221.51569 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    87600 |  87600 batches | lr 2.5e-05 | ms/batch 222.58372 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    87800 |  87800 batches | lr 2.5e-05 | ms/batch 223.63778 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    88000 |  88000 batches | lr 2.5e-05 | ms/batch 226.93335 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  8  8  9  7  3  4 10  4  7  5  4  3  7 11  3  6  8  6  4 10  4  3 11\n",
      "  1 11  3  4 10  4  6  8  6  3 11  7  3  4  5  7  4 10  4  3  7  9  8  8]\n",
      "Target: [ 8  8  9  7  3  4 10  4  7  5  4  3  7 11  3  6  8  6  4 10  4  3 11  1\n",
      " 11  3  4 10  4  6  8  6  3 11  7  3  4  5  7  4 10  4  3  7  9  8  8  5]\n",
      "Teacher forcing: acc:0.09864583333333334\n",
      "Preds:  [ 4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4 11 11 11 11 11  4  4 11 11 11 11  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "No teacher forcing: acc:0.09864583333333334\n",
      "Preds:  [ 8  8  9  7  3  4 10  4  7  5  4  3  7 11  3  6  8  6  4 10  4  3 11  1\n",
      "  4 11 11 11 11 11  4  4 11 11 11 11  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  22 at step    88000 | time: 915.27s | valid loss 2.30259 | valid ppl   10.0000 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    88200 |  88200 batches | lr 2.5e-05 | ms/batch 314.62467 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    88400 |  88400 batches | lr 2.5e-05 | ms/batch 224.97886 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    88600 |  88600 batches | lr 2.5e-05 | ms/batch 223.87549 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    88800 |  88800 batches | lr 2.5e-05 | ms/batch 222.45136 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    89000 |  89000 batches | lr 2.5e-05 | ms/batch 225.37694 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    89200 |  89200 batches | lr 2.5e-05 | ms/batch 233.96763 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    89400 |  89400 batches | lr 2.5e-05 | ms/batch 224.65153 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    89600 |  89600 batches | lr 2.5e-05 | ms/batch 226.58265 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step    89800 |  89800 batches | lr 2.5e-05 | ms/batch 224.09340 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    90000 |  90000 batches | lr 2.5e-05 | ms/batch 227.28474 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    90200 |  90200 batches | lr 2.5e-05 | ms/batch 220.66857 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    90400 |  90400 batches | lr 2.5e-05 | ms/batch 220.80424 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    90600 |  90600 batches | lr 2.5e-05 | ms/batch 220.84646 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    90800 |  90800 batches | lr 2.5e-05 | ms/batch 221.90564 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    91000 |  91000 batches | lr 2.5e-05 | ms/batch 224.08124 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    91200 |  91200 batches | lr 2.5e-05 | ms/batch 234.01731 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    91400 |  91400 batches | lr 2.5e-05 | ms/batch 232.48362 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    91600 |  91600 batches | lr 2.5e-05 | ms/batch 221.50945 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    91800 |  91800 batches | lr 2.5e-05 | ms/batch 222.37630 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    92000 |  92000 batches | lr 2.5e-05 | ms/batch 223.54623 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  5  3  6 10  9  9  6  9  5  6 11  3 11  6  3  3  3  4  3  4  2  9  6\n",
      "  1  6  9  2  4  3  4  3  3  3  6 11  3 11  6  5  9  6  9  9 10  6  3  5]\n",
      "Target: [ 5  3  6 10  9  9  6  9  5  6 11  3 11  6  3  3  3  4  3  4  2  9  6  1\n",
      "  6  9  2  4  3  4  3  3  3  6 11  3 11  6  5  9  6  9  9 10  6  3  5  5]\n",
      "Teacher forcing: acc:0.10234375\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 6 6 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10234375\n",
      "Preds:  [ 5  3  6 10  9  9  6  9  5  6 11  3 11  6  3  3  3  4  3  4  2  9  6  1\n",
      "  6  3  3  3  3  3  6  6  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  23 at step    92000 | time: 918.76s | valid loss 2.30256 | valid ppl    9.9998 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    92200 |  92200 batches | lr 2.5e-05 | ms/batch 317.98425 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    92400 |  92400 batches | lr 2.5e-05 | ms/batch 227.02669 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    92600 |  92600 batches | lr 2.5e-05 | ms/batch 231.56895 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    92800 |  92800 batches | lr 2.5e-05 | ms/batch 225.43214 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    93000 |  93000 batches | lr 2.5e-05 | ms/batch 225.50005 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    93200 |  93200 batches | lr 2.5e-05 | ms/batch 226.75686 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    93400 |  93400 batches | lr 2.5e-05 | ms/batch 223.39944 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    93600 |  93600 batches | lr 2.5e-05 | ms/batch 222.19993 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    93800 |  93800 batches | lr 2.5e-05 | ms/batch 225.99132 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    94000 |  94000 batches | lr 2.5e-05 | ms/batch 231.73450 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    94200 |  94200 batches | lr 2.5e-05 | ms/batch 228.73473 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    94400 |  94400 batches | lr 2.5e-05 | ms/batch 227.30592 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    94600 |  94600 batches | lr 2.5e-05 | ms/batch 223.93926 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    94800 |  94800 batches | lr 2.5e-05 | ms/batch 222.58515 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    95000 |  95000 batches | lr 2.5e-05 | ms/batch 222.69073 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    95200 |  95200 batches | lr 2.5e-05 | ms/batch 224.75222 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    95400 |  95400 batches | lr 2.5e-05 | ms/batch 226.49401 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    95600 |  95600 batches | lr 2.5e-05 | ms/batch 221.44736 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    95800 |  95800 batches | lr 2.5e-05 | ms/batch 221.94595 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    96000 |  96000 batches | lr 2.5e-05 | ms/batch 224.04506 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [10  5  9 10  6  3  4  4  4  8  2  4  3  8 10  5  2 10  6  7  8  5  3  6\n",
      "  1  6  3  5  8  7  6 10  2  5 10  8  3  4  2  8  4  4  4  3  6 10  9  5]\n",
      "Target: [ 5  9 10  6  3  4  4  4  8  2  4  3  8 10  5  2 10  6  7  8  5  3  6  1\n",
      "  6  3  5  8  7  6 10  2  5 10  8  3  4  2  8  4  4  4  3  6 10  9  5 10]\n",
      "Teacher forcing: acc:0.09978210034013606\n",
      "Preds:  [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 4 3 3 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8]\n",
      "\n",
      "No teacher forcing: acc:0.09978210034013606\n",
      "Preds:  [ 5  9 10  6  3  4  4  4  8  2  4  3  8 10  5  2 10  6  7  8  5  3  6  1\n",
      "  4  3  3  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  24 at step    96000 | time: 920.06s | valid loss 2.30258 | valid ppl    9.9999 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    96200 |  96200 batches | lr 2.5e-05 | ms/batch 317.87096 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    96400 |  96400 batches | lr 2.5e-05 | ms/batch 223.20741 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    96600 |  96600 batches | lr 2.5e-05 | ms/batch 224.63881 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    96800 |  96800 batches | lr 2.5e-05 | ms/batch 225.71499 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    97000 |  97000 batches | lr 2.5e-05 | ms/batch 225.00269 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    97200 |  97200 batches | lr 2.5e-05 | ms/batch 228.09839 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    97400 |  97400 batches | lr 2.5e-05 | ms/batch 222.55500 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    97600 |  97600 batches | lr 2.5e-05 | ms/batch 223.15304 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    97800 |  97800 batches | lr 2.5e-05 | ms/batch 224.58208 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    98000 |  98000 batches | lr 2.5e-05 | ms/batch 225.38084 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    98200 |  98200 batches | lr 2.5e-05 | ms/batch 228.07843 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    98400 |  98400 batches | lr 2.5e-05 | ms/batch 224.84714 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    98600 |  98600 batches | lr 2.5e-05 | ms/batch 224.47855 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    98800 |  98800 batches | lr 2.5e-05 | ms/batch 229.13636 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    99000 |  99000 batches | lr 2.5e-05 | ms/batch 223.39129 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    99200 |  99200 batches | lr 2.5e-05 | ms/batch 224.03990 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    99400 |  99400 batches | lr 2.5e-05 | ms/batch 223.31093 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    99600 |  99600 batches | lr 2.5e-05 | ms/batch 227.75285 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    99800 |  99800 batches | lr 2.5e-05 | ms/batch 221.69628 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   100000 | 100000 batches | lr 2.5e-05 | ms/batch 224.22365 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 9  7  8  2  7  6  5  7 11  8  3  2  9  2  4  2  3  7 10 10 10  7  2  3\n",
      "  1  3  2  7 10 10 10  7  3  2  4  2  9  2  3  8 11  7  5  6  7  2  8  7]\n",
      "Target: [ 7  8  2  7  6  5  7 11  8  3  2  9  2  4  2  3  7 10 10 10  7  2  3  1\n",
      "  3  2  7 10 10 10  7  3  2  4  2  9  2  3  8 11  7  5  6  7  2  8  7  9]\n",
      "Teacher forcing: acc:0.10075520833333333\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.10075520833333333\n",
      "Preds:  [ 7  8  2  7  6  5  7 11  8  3  2  9  2  4  2  3  7 10 10 10  7  2  3  1\n",
      "  4  3  3  3  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  25 at step   100000 | time: 918.55s | valid loss 2.30255 | valid ppl    9.9997 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   100200 | 100200 batches | lr 1.25e-05 | ms/batch 316.69689 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   100400 | 100400 batches | lr 1.25e-05 | ms/batch 221.90461 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   100600 | 100600 batches | lr 1.25e-05 | ms/batch 221.51982 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   100800 | 100800 batches | lr 1.25e-05 | ms/batch 222.53323 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   101000 | 101000 batches | lr 1.25e-05 | ms/batch 224.47657 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   101200 | 101200 batches | lr 1.25e-05 | ms/batch 220.61737 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   101400 | 101400 batches | lr 1.25e-05 | ms/batch 221.80037 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   101600 | 101600 batches | lr 1.25e-05 | ms/batch 228.32964 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   101800 | 101800 batches | lr 1.25e-05 | ms/batch 225.39690 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   102000 | 102000 batches | lr 1.25e-05 | ms/batch 222.40723 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   102200 | 102200 batches | lr 1.25e-05 | ms/batch 221.64950 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   102400 | 102400 batches | lr 1.25e-05 | ms/batch 226.48984 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   102600 | 102600 batches | lr 1.25e-05 | ms/batch 224.12546 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   102800 | 102800 batches | lr 1.25e-05 | ms/batch 224.33576 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   103000 | 103000 batches | lr 1.25e-05 | ms/batch 225.59982 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   103200 | 103200 batches | lr 1.25e-05 | ms/batch 223.76012 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   103400 | 103400 batches | lr 1.25e-05 | ms/batch 222.04694 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   103600 | 103600 batches | lr 1.25e-05 | ms/batch 222.52857 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   103800 | 103800 batches | lr 1.25e-05 | ms/batch 227.36094 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   104000 | 104000 batches | lr 1.25e-05 | ms/batch 228.09897 | loss 2.30260 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 3  9  9  9  5  3 10 10  5  8  7  6  3  8  5 10  2  5  7  3  6 11  4  6\n",
      "  1  6  4 11  6  3  7  5  2 10  5  8  3  6  7  8  5 10 10  3  5  9  9  9]\n",
      "Target: [ 9  9  9  5  3 10 10  5  8  7  6  3  8  5 10  2  5  7  3  6 11  4  6  1\n",
      "  6  4 11  6  3  7  5  2 10  5  8  3  6  7  8  5 10 10  3  5  9  9  9  3]\n",
      "Teacher forcing: acc:0.09688562925170068\n",
      "Preds:  [ 4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4 11 11 11 11 11 11 11 11 11 11 11  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "No teacher forcing: acc:0.09688562925170068\n",
      "Preds:  [ 9  9  9  5  3 10 10  5  8  7  6  3  8  5 10  2  5  7  3  6 11  4  6  1\n",
      "  4 11 11 11 11 11 11 11 11 11 11 11  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  26 at step   104000 | time: 914.04s | valid loss 2.30260 | valid ppl   10.0002 | valid acc 0.097\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   104200 | 104200 batches | lr 1.25e-05 | ms/batch 319.59052 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   104400 | 104400 batches | lr 1.25e-05 | ms/batch 225.16940 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   104600 | 104600 batches | lr 1.25e-05 | ms/batch 224.42442 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   104800 | 104800 batches | lr 1.25e-05 | ms/batch 224.49929 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   105000 | 105000 batches | lr 1.25e-05 | ms/batch 228.78524 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   105200 | 105200 batches | lr 1.25e-05 | ms/batch 222.24667 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   105400 | 105400 batches | lr 1.25e-05 | ms/batch 223.16389 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   105600 | 105600 batches | lr 1.25e-05 | ms/batch 226.02459 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   105800 | 105800 batches | lr 1.25e-05 | ms/batch 221.51337 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   106000 | 106000 batches | lr 1.25e-05 | ms/batch 223.10946 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   106200 | 106200 batches | lr 1.25e-05 | ms/batch 220.88611 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   106400 | 106400 batches | lr 1.25e-05 | ms/batch 226.63035 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   106600 | 106600 batches | lr 1.25e-05 | ms/batch 223.04479 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   106800 | 106800 batches | lr 1.25e-05 | ms/batch 220.69337 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   107000 | 107000 batches | lr 1.25e-05 | ms/batch 220.22510 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   107200 | 107200 batches | lr 1.25e-05 | ms/batch 222.33538 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   107400 | 107400 batches | lr 1.25e-05 | ms/batch 222.47683 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   107600 | 107600 batches | lr 1.25e-05 | ms/batch 219.17088 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   107800 | 107800 batches | lr 1.25e-05 | ms/batch 223.74903 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   108000 | 108000 batches | lr 1.25e-05 | ms/batch 223.68451 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 9 11  8  4 10  8  5  6  7 10  3  3  8  4 10  3  8  3 11 10 10  6  7 11\n",
      "  1 11  7  6 10 10 11  3  8  3 10  4  8  3  3 10  7  6  5  8 10  4  8 11]\n",
      "Target: [11  8  4 10  8  5  6  7 10  3  3  8  4 10  3  8  3 11 10 10  6  7 11  1\n",
      " 11  7  6 10 10 11  3  8  3 10  4  8  3  3 10  7  6  5  8 10  4  8 11  9]\n",
      "Teacher forcing: acc:0.1034375\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.1034375\n",
      "Preds:  [11  8  4 10  8  5  6  7 10  3  3  8  4 10  3  8  3 11 10 10  6  7 11  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  27 at step   108000 | time: 912.50s | valid loss 2.30256 | valid ppl    9.9997 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   108200 | 108200 batches | lr 1.25e-05 | ms/batch 320.83586 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   108400 | 108400 batches | lr 1.25e-05 | ms/batch 225.75511 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   108600 | 108600 batches | lr 1.25e-05 | ms/batch 223.76102 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   108800 | 108800 batches | lr 1.25e-05 | ms/batch 221.00612 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   109000 | 109000 batches | lr 1.25e-05 | ms/batch 220.92873 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   109200 | 109200 batches | lr 1.25e-05 | ms/batch 227.02821 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   109400 | 109400 batches | lr 1.25e-05 | ms/batch 225.91261 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   109600 | 109600 batches | lr 1.25e-05 | ms/batch 223.04620 | loss 2.30256 | ppl    10.000\n",
      "| epoch   1 step   109800 | 109800 batches | lr 1.25e-05 | ms/batch 222.38947 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   110000 | 110000 batches | lr 1.25e-05 | ms/batch 221.96916 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   110200 | 110200 batches | lr 1.25e-05 | ms/batch 221.55167 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   110400 | 110400 batches | lr 1.25e-05 | ms/batch 223.26718 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   110600 | 110600 batches | lr 1.25e-05 | ms/batch 229.49269 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   110800 | 110800 batches | lr 1.25e-05 | ms/batch 225.49957 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   111000 | 111000 batches | lr 1.25e-05 | ms/batch 222.72623 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   111200 | 111200 batches | lr 1.25e-05 | ms/batch 222.58250 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   111400 | 111400 batches | lr 1.25e-05 | ms/batch 224.78587 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   111600 | 111600 batches | lr 1.25e-05 | ms/batch 223.45467 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   111800 | 111800 batches | lr 1.25e-05 | ms/batch 221.80314 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   112000 | 112000 batches | lr 1.25e-05 | ms/batch 226.28412 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  3  8  8  3  7  3  6  9  6 10  5 11  8  6  9  4 11  8  6  8 11  3  7\n",
      "  1  7  3 11  8  6  8 11  4  9  6  8 11  5 10  6  9  6  3  7  3  8  8  3]\n",
      "Target: [ 3  8  8  3  7  3  6  9  6 10  5 11  8  6  9  4 11  8  6  8 11  3  7  1\n",
      "  7  3 11  8  6  8 11  4  9  6  8 11  5 10  6  9  6  3  7  3  8  8  3  5]\n",
      "Teacher forcing: acc:0.10388020833333333\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10388020833333333\n",
      "Preds:  [ 3  8  8  3  7  3  6  9  6 10  5 11  8  6  9  4 11  8  6  8 11  3  7  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  28 at step   112000 | time: 914.87s | valid loss 2.30256 | valid ppl    9.9997 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   112200 | 112200 batches | lr 1.25e-05 | ms/batch 318.81814 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   112400 | 112400 batches | lr 1.25e-05 | ms/batch 222.71781 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   112600 | 112600 batches | lr 1.25e-05 | ms/batch 222.48142 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   112800 | 112800 batches | lr 1.25e-05 | ms/batch 221.25856 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   113000 | 113000 batches | lr 1.25e-05 | ms/batch 223.07895 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   113200 | 113200 batches | lr 1.25e-05 | ms/batch 225.81264 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   113400 | 113400 batches | lr 1.25e-05 | ms/batch 225.16124 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   113600 | 113600 batches | lr 1.25e-05 | ms/batch 223.34644 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   113800 | 113800 batches | lr 1.25e-05 | ms/batch 225.76454 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   114000 | 114000 batches | lr 1.25e-05 | ms/batch 221.88678 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   114200 | 114200 batches | lr 1.25e-05 | ms/batch 226.95274 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   114400 | 114400 batches | lr 1.25e-05 | ms/batch 223.80338 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   114600 | 114600 batches | lr 1.25e-05 | ms/batch 223.47196 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   114800 | 114800 batches | lr 1.25e-05 | ms/batch 227.57559 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   115000 | 115000 batches | lr 1.25e-05 | ms/batch 222.64915 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   115200 | 115200 batches | lr 1.25e-05 | ms/batch 222.56928 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   115400 | 115400 batches | lr 1.25e-05 | ms/batch 222.76998 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   115600 | 115600 batches | lr 1.25e-05 | ms/batch 226.45075 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   115800 | 115800 batches | lr 1.25e-05 | ms/batch 222.02719 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   116000 | 116000 batches | lr 1.25e-05 | ms/batch 221.23815 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  4  2  9  3 10  5  3  8  5 11  9 10 10  4  8  6  9  9  4  4  6  6 11\n",
      "  1 11  6  6  4  4  9  9  6  8  4 10 10  9 11  5  8  3  5 10  3  9  2  4]\n",
      "Target: [ 4  2  9  3 10  5  3  8  5 11  9 10 10  4  8  6  9  9  4  4  6  6 11  1\n",
      " 11  6  6  4  4  9  9  6  8  4 10 10  9 11  5  8  3  5 10  3  9  2  4  7]\n",
      "Teacher forcing: acc:0.10238626700680271\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10238626700680271\n",
      "Preds:  [ 4  2  9  3 10  5  3  8  5 11  9 10 10  4  8  6  9  9  4  4  6  6 11  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  29 at step   116000 | time: 914.09s | valid loss 2.30261 | valid ppl   10.0002 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   116200 | 116200 batches | lr 1.25e-05 | ms/batch 324.50334 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   116400 | 116400 batches | lr 1.25e-05 | ms/batch 222.73916 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   116600 | 116600 batches | lr 1.25e-05 | ms/batch 224.74968 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   116800 | 116800 batches | lr 1.25e-05 | ms/batch 225.36652 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   117000 | 117000 batches | lr 1.25e-05 | ms/batch 221.25571 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   117200 | 117200 batches | lr 1.25e-05 | ms/batch 225.18357 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   117400 | 117400 batches | lr 1.25e-05 | ms/batch 223.61699 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   117600 | 117600 batches | lr 1.25e-05 | ms/batch 229.06293 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   117800 | 117800 batches | lr 1.25e-05 | ms/batch 228.26270 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   118000 | 118000 batches | lr 1.25e-05 | ms/batch 223.45644 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   118200 | 118200 batches | lr 1.25e-05 | ms/batch 221.08058 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   118400 | 118400 batches | lr 1.25e-05 | ms/batch 224.97878 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   118600 | 118600 batches | lr 1.25e-05 | ms/batch 222.94032 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   118800 | 118800 batches | lr 1.25e-05 | ms/batch 223.87650 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   119000 | 119000 batches | lr 1.25e-05 | ms/batch 223.94524 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   119200 | 119200 batches | lr 1.25e-05 | ms/batch 222.49677 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   119400 | 119400 batches | lr 1.25e-05 | ms/batch 222.03853 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   119600 | 119600 batches | lr 1.25e-05 | ms/batch 221.24819 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   119800 | 119800 batches | lr 1.25e-05 | ms/batch 223.80381 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   120000 | 120000 batches | lr 1.25e-05 | ms/batch 222.37208 | loss 2.30261 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  9  7  3  7  3 10  3  7  4 11  5  4  9  2  2  8 10  9  2  9  8  5  9\n",
      "  1  9  5  8  9  2  9 10  8  2  2  9  4  5 11  4  7  3 10  3  7  3  7  9]\n",
      "Target: [ 9  7  3  7  3 10  3  7  4 11  5  4  9  2  2  8 10  9  2  9  8  5  9  1\n",
      "  9  5  8  9  2  9 10  8  2  2  9  4  5 11  4  7  3 10  3  7  3  7  9  2]\n",
      "Teacher forcing: acc:0.10291666666666667\n",
      "Preds:  [ 6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  4  3 11  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "No teacher forcing: acc:0.10296875\n",
      "Preds:  [ 9  7  3  7  3 10  3  7  4 11  5  4  9  2  2  8 10  9  2  9  8  5  9  1\n",
      "  4  3 11  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  30 at step   120000 | time: 915.29s | valid loss 2.30256 | valid ppl    9.9998 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   120200 | 120200 batches | lr 1.25e-05 | ms/batch 321.12513 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   120400 | 120400 batches | lr 1.25e-05 | ms/batch 222.87050 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   120600 | 120600 batches | lr 1.25e-05 | ms/batch 221.85034 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   120800 | 120800 batches | lr 1.25e-05 | ms/batch 222.12533 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   121000 | 121000 batches | lr 1.25e-05 | ms/batch 219.87124 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   121200 | 121200 batches | lr 1.25e-05 | ms/batch 219.59322 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   121400 | 121400 batches | lr 1.25e-05 | ms/batch 225.05533 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   121600 | 121600 batches | lr 1.25e-05 | ms/batch 225.53410 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   121800 | 121800 batches | lr 1.25e-05 | ms/batch 223.97683 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   122000 | 122000 batches | lr 1.25e-05 | ms/batch 221.91725 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   122200 | 122200 batches | lr 1.25e-05 | ms/batch 224.49707 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   122400 | 122400 batches | lr 1.25e-05 | ms/batch 224.60991 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   122600 | 122600 batches | lr 1.25e-05 | ms/batch 224.05589 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   122800 | 122800 batches | lr 1.25e-05 | ms/batch 224.72817 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   123000 | 123000 batches | lr 1.25e-05 | ms/batch 228.34156 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   123200 | 123200 batches | lr 1.25e-05 | ms/batch 224.82771 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   123400 | 123400 batches | lr 1.25e-05 | ms/batch 224.91248 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   123600 | 123600 batches | lr 1.25e-05 | ms/batch 223.52571 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   123800 | 123800 batches | lr 1.25e-05 | ms/batch 224.62038 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   124000 | 124000 batches | lr 1.25e-05 | ms/batch 223.10817 | loss 2.30258 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [11  3  3  9 10  2  3  6  6  6  6  6  3  5  4  4  7  5  4  4 11  9 11  3\n",
      "  1  3 11  9 11  4  4  5  7  4  4  5  3  6  6  6  6  6  3  2 10  9  3  3]\n",
      "Target: [ 3  3  9 10  2  3  6  6  6  6  6  3  5  4  4  7  5  4  4 11  9 11  3  1\n",
      "  3 11  9 11  4  4  5  7  4  4  5  3  6  6  6  6  6  3  2 10  9  3  3 11]\n",
      "Teacher forcing: acc:0.10238626700680271\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10238626700680271\n",
      "Preds:  [ 3  3  9 10  2  3  6  6  6  6  6  3  5  4  4  7  5  4  4 11  9 11  3  1\n",
      "  4  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  31 at step   124000 | time: 913.67s | valid loss 2.30254 | valid ppl    9.9995 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   124200 | 124200 batches | lr 6.25e-06 | ms/batch 314.27159 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   124400 | 124400 batches | lr 6.25e-06 | ms/batch 226.44490 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   124600 | 124600 batches | lr 6.25e-06 | ms/batch 222.96144 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   124800 | 124800 batches | lr 6.25e-06 | ms/batch 224.84034 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   125000 | 125000 batches | lr 6.25e-06 | ms/batch 235.35432 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   125200 | 125200 batches | lr 6.25e-06 | ms/batch 224.35988 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   125400 | 125400 batches | lr 6.25e-06 | ms/batch 223.29649 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   125600 | 125600 batches | lr 6.25e-06 | ms/batch 221.88839 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   125800 | 125800 batches | lr 6.25e-06 | ms/batch 227.88270 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   126000 | 126000 batches | lr 6.25e-06 | ms/batch 226.52833 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   126200 | 126200 batches | lr 6.25e-06 | ms/batch 226.67756 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   126400 | 126400 batches | lr 6.25e-06 | ms/batch 224.54498 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   126600 | 126600 batches | lr 6.25e-06 | ms/batch 225.68756 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   126800 | 126800 batches | lr 6.25e-06 | ms/batch 226.95132 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   127000 | 127000 batches | lr 6.25e-06 | ms/batch 225.92603 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   127200 | 127200 batches | lr 6.25e-06 | ms/batch 228.25758 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   127400 | 127400 batches | lr 6.25e-06 | ms/batch 225.49871 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   127600 | 127600 batches | lr 6.25e-06 | ms/batch 225.88997 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   127800 | 127800 batches | lr 6.25e-06 | ms/batch 232.41751 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   128000 | 128000 batches | lr 6.25e-06 | ms/batch 234.13500 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  8 11 10 11  9  8  4 11  3  9  8 11  5 10 10 11  2 11 10  8 10  2  2\n",
      "  1  2  2 10  8 10 11  2 11 10 10  5 11  8  9  3 11  4  8  9 11 10 11  8]\n",
      "Target: [ 8 11 10 11  9  8  4 11  3  9  8 11  5 10 10 11  2 11 10  8 10  2  2  1\n",
      "  2  2 10  8 10 11  2 11 10 10  5 11  8  9  3 11  4  8  9 11 10 11  8  5]\n",
      "Teacher forcing: acc:0.10026041666666667\n",
      "Preds:  [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 3 3 3 3 3 3 3 3 3 3 3 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7]\n",
      "\n",
      "No teacher forcing: acc:0.10026041666666667\n",
      "Preds:  [ 8 11 10 11  9  8  4 11  3  9  8 11  5 10 10 11  2 11 10  8 10  2  2  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  32 at step   128000 | time: 925.61s | valid loss 2.30256 | valid ppl    9.9998 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   128200 | 128200 batches | lr 6.25e-06 | ms/batch 326.42810 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   128400 | 128400 batches | lr 6.25e-06 | ms/batch 229.88139 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   128600 | 128600 batches | lr 6.25e-06 | ms/batch 226.36944 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   128800 | 128800 batches | lr 6.25e-06 | ms/batch 225.69226 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   129000 | 129000 batches | lr 6.25e-06 | ms/batch 226.21108 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   129200 | 129200 batches | lr 6.25e-06 | ms/batch 222.61460 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   129400 | 129400 batches | lr 6.25e-06 | ms/batch 223.16092 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   129600 | 129600 batches | lr 6.25e-06 | ms/batch 222.09359 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   129800 | 129800 batches | lr 6.25e-06 | ms/batch 230.06389 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   130000 | 130000 batches | lr 6.25e-06 | ms/batch 226.53450 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   130200 | 130200 batches | lr 6.25e-06 | ms/batch 228.75624 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   130400 | 130400 batches | lr 6.25e-06 | ms/batch 225.89221 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   130600 | 130600 batches | lr 6.25e-06 | ms/batch 223.65991 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   130800 | 130800 batches | lr 6.25e-06 | ms/batch 224.53687 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   131000 | 131000 batches | lr 6.25e-06 | ms/batch 225.46433 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   131200 | 131200 batches | lr 6.25e-06 | ms/batch 229.30169 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   131400 | 131400 batches | lr 6.25e-06 | ms/batch 227.73544 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   131600 | 131600 batches | lr 6.25e-06 | ms/batch 226.23014 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   131800 | 131800 batches | lr 6.25e-06 | ms/batch 226.57462 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   132000 | 132000 batches | lr 6.25e-06 | ms/batch 231.44577 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [10  4  6  9 11 11  4  7  8  2 10  6 10  6  4  2  2  6  7  7  2  2  4  2\n",
      "  1  2  4  2  2  7  7  6  2  2  4  6 10  6 10  2  8  7  4 11 11  9  6  4]\n",
      "Target: [ 4  6  9 11 11  4  7  8  2 10  6 10  6  4  2  2  6  7  7  2  2  4  2  1\n",
      "  2  4  2  2  7  7  6  2  2  4  6 10  6 10  2  8  7  4 11 11  9  6  4 10]\n",
      "Teacher forcing: acc:0.100625\n",
      "Preds:  [ 7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  4  3  3  3  3 11 11 11 11  3  3  3  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "No teacher forcing: acc:0.100625\n",
      "Preds:  [ 4  6  9 11 11  4  7  8  2 10  6 10  6  4  2  2  6  7  7  2  2  4  2  1\n",
      "  4  3  3  3  3 11 11 11 11  3  3  3  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  33 at step   132000 | time: 925.94s | valid loss 2.30258 | valid ppl   10.0000 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   132200 | 132200 batches | lr 6.25e-06 | ms/batch 328.93824 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   132400 | 132400 batches | lr 6.25e-06 | ms/batch 227.21302 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   132600 | 132600 batches | lr 6.25e-06 | ms/batch 228.93244 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   132800 | 132800 batches | lr 6.25e-06 | ms/batch 228.46284 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   133000 | 133000 batches | lr 6.25e-06 | ms/batch 227.18202 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   133200 | 133200 batches | lr 6.25e-06 | ms/batch 225.33414 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   133400 | 133400 batches | lr 6.25e-06 | ms/batch 228.73044 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   133600 | 133600 batches | lr 6.25e-06 | ms/batch 226.25728 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   133800 | 133800 batches | lr 6.25e-06 | ms/batch 234.75800 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   134000 | 134000 batches | lr 6.25e-06 | ms/batch 226.62602 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   134200 | 134200 batches | lr 6.25e-06 | ms/batch 225.98635 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   134400 | 134400 batches | lr 6.25e-06 | ms/batch 226.07603 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   134600 | 134600 batches | lr 6.25e-06 | ms/batch 224.79665 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   134800 | 134800 batches | lr 6.25e-06 | ms/batch 227.87936 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   135000 | 135000 batches | lr 6.25e-06 | ms/batch 227.46571 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   135200 | 135200 batches | lr 6.25e-06 | ms/batch 230.30311 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   135400 | 135400 batches | lr 6.25e-06 | ms/batch 228.72211 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   135600 | 135600 batches | lr 6.25e-06 | ms/batch 227.82669 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   135800 | 135800 batches | lr 6.25e-06 | ms/batch 225.75074 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   136000 | 136000 batches | lr 6.25e-06 | ms/batch 228.24909 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 4  8  8  2  4 11  7  8  8  6  6  9  6  6  2 11  3 11  9 10  8  4  5  2\n",
      "  1  2  5  4  8 10  9 11  3 11  2  6  6  9  6  6  8  8  7 11  4  2  8  8]\n",
      "Target: [ 8  8  2  4 11  7  8  8  6  6  9  6  6  2 11  3 11  9 10  8  4  5  2  1\n",
      "  2  5  4  8 10  9 11  3 11  2  6  6  9  6  6  8  8  7 11  4  2  8  8  4]\n",
      "Teacher forcing: acc:0.10257227891156463\n",
      "Preds:  [7 7 7 7 7 7 7 7 7 7 7 7 6 6 6 6 6 6 6 7 7 6 7 7 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 7 6 6 7 7 7]\n",
      "\n",
      "No teacher forcing: acc:0.10355548469387756\n",
      "Preds:  [ 8  8  2  4 11  7  8  8  6  6  9  6  6  2 11  3 11  9 10  8  4  5  2  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  34 at step   136000 | time: 930.54s | valid loss 2.30258 | valid ppl   10.0000 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   136200 | 136200 batches | lr 6.25e-06 | ms/batch 322.26233 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   136400 | 136400 batches | lr 6.25e-06 | ms/batch 226.01979 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   136600 | 136600 batches | lr 6.25e-06 | ms/batch 233.50772 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   136800 | 136800 batches | lr 6.25e-06 | ms/batch 230.81018 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   137000 | 137000 batches | lr 6.25e-06 | ms/batch 228.91258 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   137200 | 137200 batches | lr 6.25e-06 | ms/batch 227.24663 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   137400 | 137400 batches | lr 6.25e-06 | ms/batch 225.41926 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   137600 | 137600 batches | lr 6.25e-06 | ms/batch 234.73429 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   137800 | 137800 batches | lr 6.25e-06 | ms/batch 224.41363 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   138000 | 138000 batches | lr 6.25e-06 | ms/batch 227.34257 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   138200 | 138200 batches | lr 6.25e-06 | ms/batch 226.07345 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   138400 | 138400 batches | lr 6.25e-06 | ms/batch 225.11274 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   138600 | 138600 batches | lr 6.25e-06 | ms/batch 224.11755 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   138800 | 138800 batches | lr 6.25e-06 | ms/batch 223.14636 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   139000 | 139000 batches | lr 6.25e-06 | ms/batch 222.45234 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   139200 | 139200 batches | lr 6.25e-06 | ms/batch 223.41113 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   139400 | 139400 batches | lr 6.25e-06 | ms/batch 227.27388 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   139600 | 139600 batches | lr 6.25e-06 | ms/batch 222.92727 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   139800 | 139800 batches | lr 6.25e-06 | ms/batch 219.91785 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   140000 | 140000 batches | lr 6.25e-06 | ms/batch 222.83890 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 7  9  2  7 11  2 10  9  6  3  8 11 11  5  3  2  3  3  5  4  6  4  6  9\n",
      "  1  9  6  4  6  4  5  3  3  2  3  5 11 11  8  3  6  9 10  2 11  7  2  9]\n",
      "Target: [ 9  2  7 11  2 10  9  6  3  8 11 11  5  3  2  3  3  5  4  6  4  6  9  1\n",
      "  9  6  4  6  4  5  3  3  2  3  5 11 11  8  3  6  9 10  2 11  7  2  9  7]\n",
      "Teacher forcing: acc:0.10046875\n",
      "Preds:  [5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10046875\n",
      "Preds:  [ 9  2  7 11  2 10  9  6  3  8 11 11  5  3  2  3  3  5  4  6  4  6  9  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  35 at step   140000 | time: 923.60s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   140200 | 140200 batches | lr 6.25e-06 | ms/batch 316.49995 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   140400 | 140400 batches | lr 6.25e-06 | ms/batch 225.06270 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   140600 | 140600 batches | lr 6.25e-06 | ms/batch 228.17897 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   140800 | 140800 batches | lr 6.25e-06 | ms/batch 222.62386 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   141000 | 141000 batches | lr 6.25e-06 | ms/batch 221.22695 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   141200 | 141200 batches | lr 6.25e-06 | ms/batch 223.23318 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   141400 | 141400 batches | lr 6.25e-06 | ms/batch 219.23687 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   141600 | 141600 batches | lr 6.25e-06 | ms/batch 222.02784 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   141800 | 141800 batches | lr 6.25e-06 | ms/batch 224.44692 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   142000 | 142000 batches | lr 6.25e-06 | ms/batch 223.34404 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   142200 | 142200 batches | lr 6.25e-06 | ms/batch 224.74350 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   142400 | 142400 batches | lr 6.25e-06 | ms/batch 224.33569 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   142600 | 142600 batches | lr 6.25e-06 | ms/batch 225.28576 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   142800 | 142800 batches | lr 6.25e-06 | ms/batch 224.01970 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   143000 | 143000 batches | lr 6.25e-06 | ms/batch 221.56692 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   143200 | 143200 batches | lr 6.25e-06 | ms/batch 224.62634 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   143400 | 143400 batches | lr 6.25e-06 | ms/batch 233.01503 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   143600 | 143600 batches | lr 6.25e-06 | ms/batch 228.66772 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   143800 | 143800 batches | lr 6.25e-06 | ms/batch 223.52645 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   144000 | 144000 batches | lr 6.25e-06 | ms/batch 222.14654 | loss 2.30257 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 5  3 11  6  8  6  2  5  5  2  5  8  3  4  8  4 11  9  3  3  4  6  2  9\n",
      "  1  9  2  6  4  3  3  9 11  4  8  4  3  8  5  2  5  5  2  6  8  6 11  3]\n",
      "Target: [ 3 11  6  8  6  2  5  5  2  5  8  3  4  8  4 11  9  3  3  4  6  2  9  1\n",
      "  9  2  6  4  3  3  9 11  4  8  4  3  8  5  2  5  5  2  6  8  6 11  3  5]\n",
      "Teacher forcing: acc:0.10113732993197279\n",
      "Preds:  [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 3 3 3 3 3 3 3 3 3 3 3 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7]\n",
      "\n",
      "No teacher forcing: acc:0.10113732993197279\n",
      "Preds:  [ 3 11  6  8  6  2  5  5  2  5  8  3  4  8  4 11  9  3  3  4  6  2  9  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  36 at step   144000 | time: 915.16s | valid loss 2.30258 | valid ppl    9.9999 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   144200 | 144200 batches | lr 6.25e-06 | ms/batch 314.32158 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   144400 | 144400 batches | lr 6.25e-06 | ms/batch 226.70973 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   144600 | 144600 batches | lr 6.25e-06 | ms/batch 224.44759 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   144800 | 144800 batches | lr 6.25e-06 | ms/batch 227.87988 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   145000 | 145000 batches | lr 6.25e-06 | ms/batch 223.84997 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   145200 | 145200 batches | lr 6.25e-06 | ms/batch 225.16182 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   145400 | 145400 batches | lr 6.25e-06 | ms/batch 224.31942 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   145600 | 145600 batches | lr 6.25e-06 | ms/batch 223.82060 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   145800 | 145800 batches | lr 6.25e-06 | ms/batch 224.86205 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   146000 | 146000 batches | lr 6.25e-06 | ms/batch 225.75435 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   146200 | 146200 batches | lr 6.25e-06 | ms/batch 224.34982 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   146400 | 146400 batches | lr 6.25e-06 | ms/batch 223.29315 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   146600 | 146600 batches | lr 6.25e-06 | ms/batch 223.19673 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   146800 | 146800 batches | lr 6.25e-06 | ms/batch 222.11228 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   147000 | 147000 batches | lr 6.25e-06 | ms/batch 222.81213 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   147200 | 147200 batches | lr 6.25e-06 | ms/batch 221.79807 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   147400 | 147400 batches | lr 6.25e-06 | ms/batch 223.49775 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   147600 | 147600 batches | lr 6.25e-06 | ms/batch 227.87508 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   147800 | 147800 batches | lr 6.25e-06 | ms/batch 224.72586 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   148000 | 148000 batches | lr 6.25e-06 | ms/batch 222.95413 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 4  2  5  9  6 11  8  3  4  2  9 11  4  8 11  9  2  3  4  2 10 10  4  4\n",
      "  1  4  4 10 10  2  4  3  2  9 11  8  4 11  9  2  4  3  8 11  6  9  5  2]\n",
      "Target: [ 2  5  9  6 11  8  3  4  2  9 11  4  8 11  9  2  3  4  2 10 10  4  4  1\n",
      "  4  4 10 10  2  4  3  2  9 11  8  4 11  9  2  4  3  8 11  6  9  5  2  4]\n",
      "Teacher forcing: acc:0.09950520833333333\n",
      "Preds:  [ 6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  4  3  3  3 11 11 11 11 11 11 11 11  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "No teacher forcing: acc:0.09950520833333333\n",
      "Preds:  [ 2  5  9  6 11  8  3  4  2  9 11  4  8 11  9  2  3  4  2 10 10  4  4  1\n",
      "  4  3  3  3 11 11 11 11 11 11 11 11  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  37 at step   148000 | time: 916.13s | valid loss 2.30260 | valid ppl   10.0001 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   148200 | 148200 batches | lr 3.13e-06 | ms/batch 320.49695 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   148400 | 148400 batches | lr 3.13e-06 | ms/batch 224.36317 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   148600 | 148600 batches | lr 3.13e-06 | ms/batch 221.27802 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   148800 | 148800 batches | lr 3.13e-06 | ms/batch 222.46125 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   149000 | 149000 batches | lr 3.13e-06 | ms/batch 226.84050 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   149200 | 149200 batches | lr 3.13e-06 | ms/batch 226.27108 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   149400 | 149400 batches | lr 3.13e-06 | ms/batch 225.68031 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   149600 | 149600 batches | lr 3.13e-06 | ms/batch 224.10844 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   149800 | 149800 batches | lr 3.13e-06 | ms/batch 224.03455 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   150000 | 150000 batches | lr 3.13e-06 | ms/batch 222.56762 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   150200 | 150200 batches | lr 3.13e-06 | ms/batch 227.24234 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   150400 | 150400 batches | lr 3.13e-06 | ms/batch 231.20359 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   150600 | 150600 batches | lr 3.13e-06 | ms/batch 224.18809 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   150800 | 150800 batches | lr 3.13e-06 | ms/batch 220.57745 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   151000 | 151000 batches | lr 3.13e-06 | ms/batch 225.02899 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   151200 | 151200 batches | lr 3.13e-06 | ms/batch 224.49126 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   151400 | 151400 batches | lr 3.13e-06 | ms/batch 224.13601 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   151600 | 151600 batches | lr 3.13e-06 | ms/batch 221.96738 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   151800 | 151800 batches | lr 3.13e-06 | ms/batch 226.73983 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   152000 | 152000 batches | lr 3.13e-06 | ms/batch 222.52555 | loss 2.30258 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 6  3  8  2  9  6  9 11 10  5 10  3  4 10  3  6  8  6 11  5  8 10  6  4\n",
      "  1  4  6 10  8  5 11  6  8  6  3 10  4  3 10  5 10 11  9  6  9  2  8  3]\n",
      "Target: [ 3  8  2  9  6  9 11 10  5 10  3  4 10  3  6  8  6 11  5  8 10  6  4  1\n",
      "  4  6 10  8  5 11  6  8  6  3 10  4  3 10  5 10 11  9  6  9  2  8  3  6]\n",
      "Teacher forcing: acc:0.10364583333333334\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10364583333333334\n",
      "Preds:  [ 3  8  2  9  6  9 11 10  5 10  3  4 10  3  6  8  6 11  5  8 10  6  4  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  38 at step   152000 | time: 917.54s | valid loss 2.30256 | valid ppl    9.9997 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   152200 | 152200 batches | lr 3.13e-06 | ms/batch 318.54266 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   152400 | 152400 batches | lr 3.13e-06 | ms/batch 209.66660 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   152600 | 152600 batches | lr 3.13e-06 | ms/batch 202.18829 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   152800 | 152800 batches | lr 3.13e-06 | ms/batch 203.48658 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   153000 | 153000 batches | lr 3.13e-06 | ms/batch 204.70742 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   153200 | 153200 batches | lr 3.13e-06 | ms/batch 207.37365 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   153400 | 153400 batches | lr 3.13e-06 | ms/batch 203.35193 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   153600 | 153600 batches | lr 3.13e-06 | ms/batch 202.71230 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   153800 | 153800 batches | lr 3.13e-06 | ms/batch 203.70469 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   154000 | 154000 batches | lr 3.13e-06 | ms/batch 202.59565 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   154200 | 154200 batches | lr 3.13e-06 | ms/batch 201.66442 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   154400 | 154400 batches | lr 3.13e-06 | ms/batch 200.22195 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   154600 | 154600 batches | lr 3.13e-06 | ms/batch 204.51169 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   154800 | 154800 batches | lr 3.13e-06 | ms/batch 209.17340 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   155000 | 155000 batches | lr 3.13e-06 | ms/batch 204.41723 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   155200 | 155200 batches | lr 3.13e-06 | ms/batch 193.88456 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   155400 | 155400 batches | lr 3.13e-06 | ms/batch 188.12951 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   155600 | 155600 batches | lr 3.13e-06 | ms/batch 190.38843 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   155800 | 155800 batches | lr 3.13e-06 | ms/batch 187.47007 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   156000 | 156000 batches | lr 3.13e-06 | ms/batch 186.05810 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 4 11  4 10  4 10  5 11  3  5  9  2  8 10  9  4  3  5  3  6 10  2  9  6\n",
      "  1  6  9  2 10  6  3  5  3  4  9 10  8  2  9  5  3 11  5 10  4 10  4 11]\n",
      "Target: [11  4 10  4 10  5 11  3  5  9  2  8 10  9  4  3  5  3  6 10  2  9  6  1\n",
      "  6  9  2 10  6  3  5  3  4  9 10  8  2  9  5  3 11  5 10  4 10  4 11  4]\n",
      "Teacher forcing: acc:0.10403645833333333\n",
      "Preds:  [7 7 7 7 7 7 7 7 7 7 7 7 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10403645833333333\n",
      "Preds:  [11  4 10  4 10  5 11  3  5  9  2  8 10  9  4  3  5  3  6 10  2  9  6  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  39 at step   156000 | time: 822.97s | valid loss 2.30256 | valid ppl    9.9998 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   156200 | 156200 batches | lr 3.13e-06 | ms/batch 277.08132 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   156400 | 156400 batches | lr 3.13e-06 | ms/batch 188.56456 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   156600 | 156600 batches | lr 3.13e-06 | ms/batch 176.53201 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   156800 | 156800 batches | lr 3.13e-06 | ms/batch 177.87291 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   157000 | 157000 batches | lr 3.13e-06 | ms/batch 178.60420 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   157200 | 157200 batches | lr 3.13e-06 | ms/batch 179.51507 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   157400 | 157400 batches | lr 3.13e-06 | ms/batch 178.91281 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   157600 | 157600 batches | lr 3.13e-06 | ms/batch 178.84010 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   157800 | 157800 batches | lr 3.13e-06 | ms/batch 177.28305 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   158000 | 158000 batches | lr 3.13e-06 | ms/batch 181.90932 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   158200 | 158200 batches | lr 3.13e-06 | ms/batch 180.36356 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   158400 | 158400 batches | lr 3.13e-06 | ms/batch 178.07268 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   158600 | 158600 batches | lr 3.13e-06 | ms/batch 178.77413 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   158800 | 158800 batches | lr 3.13e-06 | ms/batch 176.63069 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   159000 | 159000 batches | lr 3.13e-06 | ms/batch 176.91965 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   159200 | 159200 batches | lr 3.13e-06 | ms/batch 179.41674 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   159400 | 159400 batches | lr 3.13e-06 | ms/batch 181.32372 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   159600 | 159600 batches | lr 3.13e-06 | ms/batch 178.20350 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   159800 | 159800 batches | lr 3.13e-06 | ms/batch 185.29835 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   160000 | 160000 batches | lr 3.13e-06 | ms/batch 178.13698 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 9  5  8  6  7  2  7  7  6  7  4  4  2  9  9  4  7 11  9 11  7  2  2  9\n",
      "  1  9  2  2  7 11  9 11  7  4  9  9  2  4  4  7  6  7  7  2  7  6  8  5]\n",
      "Target: [ 5  8  6  7  2  7  7  6  7  4  4  2  9  9  4  7 11  9 11  7  2  2  9  1\n",
      "  9  2  2  7 11  9 11  7  4  9  9  2  4  4  7  6  7  7  2  7  6  8  5  9]\n",
      "Teacher forcing: acc:0.10260416666666666\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10260416666666666\n",
      "Preds:  [ 5  8  6  7  2  7  7  6  7  4  4  2  9  9  4  7 11  9 11  7  2  2  9  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  40 at step   160000 | time: 736.71s | valid loss 2.30259 | valid ppl   10.0001 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   160200 | 160200 batches | lr 3.13e-06 | ms/batch 262.31617 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   160400 | 160400 batches | lr 3.13e-06 | ms/batch 179.79346 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   160600 | 160600 batches | lr 3.13e-06 | ms/batch 184.99397 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   160800 | 160800 batches | lr 3.13e-06 | ms/batch 187.08204 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   161000 | 161000 batches | lr 3.13e-06 | ms/batch 185.61502 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   161200 | 161200 batches | lr 3.13e-06 | ms/batch 183.83292 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   161400 | 161400 batches | lr 3.13e-06 | ms/batch 188.57581 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   161600 | 161600 batches | lr 3.13e-06 | ms/batch 178.81518 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   161800 | 161800 batches | lr 3.13e-06 | ms/batch 180.47989 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   162000 | 162000 batches | lr 3.13e-06 | ms/batch 177.98574 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   162200 | 162200 batches | lr 3.13e-06 | ms/batch 179.35602 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   162400 | 162400 batches | lr 3.13e-06 | ms/batch 180.29700 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   162600 | 162600 batches | lr 3.13e-06 | ms/batch 179.48860 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   162800 | 162800 batches | lr 3.13e-06 | ms/batch 182.08485 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   163000 | 163000 batches | lr 3.13e-06 | ms/batch 187.58431 | loss 2.30256 | ppl    10.000\n",
      "| epoch   1 step   163200 | 163200 batches | lr 3.13e-06 | ms/batch 188.85089 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   163400 | 163400 batches | lr 3.13e-06 | ms/batch 186.71264 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   163600 | 163600 batches | lr 3.13e-06 | ms/batch 183.85848 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   163800 | 163800 batches | lr 3.13e-06 | ms/batch 185.26706 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   164000 | 164000 batches | lr 3.13e-06 | ms/batch 183.59964 | loss 2.30258 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 9 11  2 10  5 10  6 10  6  6  6  4  5  2  8  9  5  5 10  2  5  9  5 10\n",
      "  1 10  5  9  5  2 10  5  5  9  8  2  5  4  6  6  6 10  6 10  5 10  2 11]\n",
      "Target: [11  2 10  5 10  6 10  6  6  6  4  5  2  8  9  5  5 10  2  5  9  5 10  1\n",
      " 10  5  9  5  2 10  5  5  9  8  2  5  4  6  6  6 10  6 10  5 10  2 11  9]\n",
      "Teacher forcing: acc:0.10366177721088435\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10366177721088435\n",
      "Preds:  [11  2 10  5 10  6 10  6  6  6  4  5  2  8  9  5  5 10  2  5  9  5 10  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  41 at step   164000 | time: 749.26s | valid loss 2.30256 | valid ppl    9.9997 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   164200 | 164200 batches | lr 3.13e-06 | ms/batch 262.02522 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   164400 | 164400 batches | lr 3.13e-06 | ms/batch 181.02094 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   164600 | 164600 batches | lr 3.13e-06 | ms/batch 178.32100 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   164800 | 164800 batches | lr 3.13e-06 | ms/batch 179.82281 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   165000 | 165000 batches | lr 3.13e-06 | ms/batch 178.59563 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   165200 | 165200 batches | lr 3.13e-06 | ms/batch 178.08104 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   165400 | 165400 batches | lr 3.13e-06 | ms/batch 180.35595 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   165600 | 165600 batches | lr 3.13e-06 | ms/batch 183.10534 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   165800 | 165800 batches | lr 3.13e-06 | ms/batch 183.30162 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   166000 | 166000 batches | lr 3.13e-06 | ms/batch 180.32548 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   166200 | 166200 batches | lr 3.13e-06 | ms/batch 184.03114 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   166400 | 166400 batches | lr 3.13e-06 | ms/batch 184.01285 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   166600 | 166600 batches | lr 3.13e-06 | ms/batch 187.25628 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   166800 | 166800 batches | lr 3.13e-06 | ms/batch 183.98024 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   167000 | 167000 batches | lr 3.13e-06 | ms/batch 178.76072 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   167200 | 167200 batches | lr 3.13e-06 | ms/batch 180.46445 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   167400 | 167400 batches | lr 3.13e-06 | ms/batch 177.60176 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   167600 | 167600 batches | lr 3.13e-06 | ms/batch 176.35493 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   167800 | 167800 batches | lr 3.13e-06 | ms/batch 178.16581 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   168000 | 168000 batches | lr 3.13e-06 | ms/batch 176.89431 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  3  4 10  6 11 11  3 11  6  6  2  4  6 10 11  2  9  2 10  7  2  9  6\n",
      "  1  6  9  2  7 10  2  9  2 11 10  6  4  2  6  6 11  3 11 11  6 10  4  3]\n",
      "Target: [ 3  4 10  6 11 11  3 11  6  6  2  4  6 10 11  2  9  2 10  7  2  9  6  1\n",
      "  6  9  2  7 10  2  9  2 11 10  6  4  2  6  6 11  3 11 11  6 10  4  3  2]\n",
      "Teacher forcing: acc:0.10231770833333333\n",
      "Preds:  [5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 5 6 5]\n",
      "\n",
      "No teacher forcing: acc:0.10182291666666667\n",
      "Preds:  [ 3  4 10  6 11 11  3 11  6  6  2  4  6 10 11  2  9  2 10  7  2  9  6  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  42 at step   168000 | time: 738.63s | valid loss 2.30257 | valid ppl    9.9998 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   168200 | 168200 batches | lr 3.13e-06 | ms/batch 265.27213 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   168400 | 168400 batches | lr 3.13e-06 | ms/batch 182.79384 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   168600 | 168600 batches | lr 3.13e-06 | ms/batch 181.42534 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   168800 | 168800 batches | lr 3.13e-06 | ms/batch 178.33223 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   169000 | 169000 batches | lr 3.13e-06 | ms/batch 184.87884 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   169200 | 169200 batches | lr 3.13e-06 | ms/batch 183.72243 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   169400 | 169400 batches | lr 3.13e-06 | ms/batch 186.21749 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   169600 | 169600 batches | lr 3.13e-06 | ms/batch 185.33310 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   169800 | 169800 batches | lr 3.13e-06 | ms/batch 187.59556 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   170000 | 170000 batches | lr 3.13e-06 | ms/batch 182.46811 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   170200 | 170200 batches | lr 3.13e-06 | ms/batch 185.96411 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   170400 | 170400 batches | lr 3.13e-06 | ms/batch 185.85485 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   170600 | 170600 batches | lr 3.13e-06 | ms/batch 186.91368 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   170800 | 170800 batches | lr 3.13e-06 | ms/batch 184.93837 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   171000 | 171000 batches | lr 3.13e-06 | ms/batch 186.62615 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   171200 | 171200 batches | lr 3.13e-06 | ms/batch 185.52346 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   171400 | 171400 batches | lr 3.13e-06 | ms/batch 182.22197 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   171600 | 171600 batches | lr 3.13e-06 | ms/batch 185.38748 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   171800 | 171800 batches | lr 3.13e-06 | ms/batch 182.64373 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   172000 | 172000 batches | lr 3.13e-06 | ms/batch 189.21350 | loss 2.30258 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [10  7  5  5  7  9 10  9  2  2 10  6 10  9  6  8  5 11  7  8  6  3  3 10\n",
      "  1 10  3  3  6  8  7 11  5  8  6  9 10  6 10  2  2  9 10  9  7  5  5  7]\n",
      "Target: [ 7  5  5  7  9 10  9  2  2 10  6 10  9  6  8  5 11  7  8  6  3  3 10  1\n",
      " 10  3  3  6  8  7 11  5  8  6  9 10  6 10  2  2  9 10  9  7  5  5  7 10]\n",
      "Teacher forcing: acc:0.10366177721088435\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10366177721088435\n",
      "Preds:  [ 7  5  5  7  9 10  9  2  2 10  6 10  9  6  8  5 11  7  8  6  3  3 10  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  43 at step   172000 | time: 755.09s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   172200 | 172200 batches | lr 1.56e-06 | ms/batch 274.86137 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   172400 | 172400 batches | lr 1.56e-06 | ms/batch 191.55544 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   172600 | 172600 batches | lr 1.56e-06 | ms/batch 190.48159 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   172800 | 172800 batches | lr 1.56e-06 | ms/batch 181.40669 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   173000 | 173000 batches | lr 1.56e-06 | ms/batch 179.04782 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   173200 | 173200 batches | lr 1.56e-06 | ms/batch 182.77951 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   173400 | 173400 batches | lr 1.56e-06 | ms/batch 179.27299 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   173600 | 173600 batches | lr 1.56e-06 | ms/batch 180.50562 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   173800 | 173800 batches | lr 1.56e-06 | ms/batch 180.27239 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   174000 | 174000 batches | lr 1.56e-06 | ms/batch 176.70255 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   174200 | 174200 batches | lr 1.56e-06 | ms/batch 184.86191 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   174400 | 174400 batches | lr 1.56e-06 | ms/batch 177.90516 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   174600 | 174600 batches | lr 1.56e-06 | ms/batch 176.36958 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   174800 | 174800 batches | lr 1.56e-06 | ms/batch 180.41094 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   175000 | 175000 batches | lr 1.56e-06 | ms/batch 180.74672 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   175200 | 175200 batches | lr 1.56e-06 | ms/batch 179.22513 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   175400 | 175400 batches | lr 1.56e-06 | ms/batch 177.98496 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   175600 | 175600 batches | lr 1.56e-06 | ms/batch 178.14358 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   175800 | 175800 batches | lr 1.56e-06 | ms/batch 178.87362 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   176000 | 176000 batches | lr 1.56e-06 | ms/batch 179.12755 | loss 2.30257 | ppl    10.000\n",
      "|\n",
      "Source: [ 8 10  9 11 11  9  6 10  8  6  7  7  6  2  5  8  3  7  6 11  2 11 10  9\n",
      "  1  9 10 11  2 11  6  7  3  8  5  2  6  7  7  6  8 10  6  9 11 11  9 10]\n",
      "Target: [10  9 11 11  9  6 10  8  6  7  7  6  2  5  8  3  7  6 11  2 11 10  9  1\n",
      "  9 10 11  2 11  6  7  3  8  5  2  6  7  7  6  8 10  6  9 11 11  9 10  8]\n",
      "Teacher forcing: acc:0.102734375\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.102734375\n",
      "Preds:  [10  9 11 11  9  6 10  8  6  7  7  6  2  5  8  3  7  6 11  2 11 10  9  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  44 at step   176000 | time: 741.41s | valid loss 2.30257 | valid ppl    9.9998 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   176200 | 176200 batches | lr 1.56e-06 | ms/batch 261.94141 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   176400 | 176400 batches | lr 1.56e-06 | ms/batch 181.94587 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   176600 | 176600 batches | lr 1.56e-06 | ms/batch 182.10581 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   176800 | 176800 batches | lr 1.56e-06 | ms/batch 180.87685 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   177000 | 177000 batches | lr 1.56e-06 | ms/batch 181.56174 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   177200 | 177200 batches | lr 1.56e-06 | ms/batch 184.27307 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   177400 | 177400 batches | lr 1.56e-06 | ms/batch 182.36230 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   177600 | 177600 batches | lr 1.56e-06 | ms/batch 184.07677 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   177800 | 177800 batches | lr 1.56e-06 | ms/batch 184.92185 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   178000 | 178000 batches | lr 1.56e-06 | ms/batch 186.11139 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   178200 | 178200 batches | lr 1.56e-06 | ms/batch 186.16748 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   178400 | 178400 batches | lr 1.56e-06 | ms/batch 187.71060 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   178600 | 178600 batches | lr 1.56e-06 | ms/batch 187.29951 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   178800 | 178800 batches | lr 1.56e-06 | ms/batch 181.33070 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   179000 | 179000 batches | lr 1.56e-06 | ms/batch 178.75981 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   179200 | 179200 batches | lr 1.56e-06 | ms/batch 177.77964 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   179400 | 179400 batches | lr 1.56e-06 | ms/batch 177.67025 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   179600 | 179600 batches | lr 1.56e-06 | ms/batch 181.16900 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   179800 | 179800 batches | lr 1.56e-06 | ms/batch 183.22506 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   180000 | 180000 batches | lr 1.56e-06 | ms/batch 184.03924 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 8  3  3  7 10  3  3  8  7  9  8 10  9  9  4  2  3  9  9  7  5  6  2 11\n",
      "  1 11  2  6  5  7  9  9  3  2  4  9  9 10  8  9  7  8  3  3 10  7  3  3]\n",
      "Target: [ 3  3  7 10  3  3  8  7  9  8 10  9  9  4  2  3  9  9  7  5  6  2 11  1\n",
      " 11  2  6  5  7  9  9  3  2  4  9  9 10  8  9  7  8  3  3 10  7  3  3  8]\n",
      "Teacher forcing: acc:0.10354166666666667\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10354166666666667\n",
      "Preds:  [ 3  3  7 10  3  3  8  7  9  8 10  9  9  4  2  3  9  9  7  5  6  2 11  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  45 at step   180000 | time: 747.42s | valid loss 2.30258 | valid ppl   10.0000 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   180200 | 180200 batches | lr 1.56e-06 | ms/batch 259.95330 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   180400 | 180400 batches | lr 1.56e-06 | ms/batch 194.18137 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   180600 | 180600 batches | lr 1.56e-06 | ms/batch 194.08458 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   180800 | 180800 batches | lr 1.56e-06 | ms/batch 177.65003 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   181000 | 181000 batches | lr 1.56e-06 | ms/batch 180.47399 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   181200 | 181200 batches | lr 1.56e-06 | ms/batch 178.80885 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   181400 | 181400 batches | lr 1.56e-06 | ms/batch 177.05305 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   181600 | 181600 batches | lr 1.56e-06 | ms/batch 181.19296 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   181800 | 181800 batches | lr 1.56e-06 | ms/batch 179.25530 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   182000 | 182000 batches | lr 1.56e-06 | ms/batch 184.01638 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   182200 | 182200 batches | lr 1.56e-06 | ms/batch 180.11031 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   182400 | 182400 batches | lr 1.56e-06 | ms/batch 179.00354 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   182600 | 182600 batches | lr 1.56e-06 | ms/batch 179.53328 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   182800 | 182800 batches | lr 1.56e-06 | ms/batch 179.22823 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   183000 | 183000 batches | lr 1.56e-06 | ms/batch 177.46024 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   183200 | 183200 batches | lr 1.56e-06 | ms/batch 181.80347 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   183400 | 183400 batches | lr 1.56e-06 | ms/batch 183.01524 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   183600 | 183600 batches | lr 1.56e-06 | ms/batch 178.18324 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   183800 | 183800 batches | lr 1.56e-06 | ms/batch 178.09853 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   184000 | 184000 batches | lr 1.56e-06 | ms/batch 177.62468 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 5  6  8  7 10 11  5  4  2 11  3  2  5  3  7  6  7  2  9  6  4  2  8  4\n",
      "  1  4  8  2  4  6  9  2  7  6  7  3  5  2  3 11  2  4  5 11 10  7  8  6]\n",
      "Target: [ 6  8  7 10 11  5  4  2 11  3  2  5  3  7  6  7  2  9  6  4  2  8  4  1\n",
      "  4  8  2  4  6  9  2  7  6  7  3  5  2  3 11  2  4  5 11 10  7  8  6  5]\n",
      "Teacher forcing: acc:0.10318346088435375\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10318346088435375\n",
      "Preds:  [ 6  8  7 10 11  5  4  2 11  3  2  5  3  7  6  7  2  9  6  4  2  8  4  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  46 at step   184000 | time: 739.66s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   184200 | 184200 batches | lr 1.56e-06 | ms/batch 258.44123 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   184400 | 184400 batches | lr 1.56e-06 | ms/batch 178.15469 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   184600 | 184600 batches | lr 1.56e-06 | ms/batch 177.35019 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   184800 | 184800 batches | lr 1.56e-06 | ms/batch 179.51667 | loss 2.30256 | ppl    10.000\n",
      "| epoch   1 step   185000 | 185000 batches | lr 1.56e-06 | ms/batch 185.00332 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   185200 | 185200 batches | lr 1.56e-06 | ms/batch 182.01856 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   185400 | 185400 batches | lr 1.56e-06 | ms/batch 177.96436 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   185600 | 185600 batches | lr 1.56e-06 | ms/batch 187.92952 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   185800 | 185800 batches | lr 1.56e-06 | ms/batch 195.82730 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   186000 | 186000 batches | lr 1.56e-06 | ms/batch 181.84954 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   186200 | 186200 batches | lr 1.56e-06 | ms/batch 177.19946 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   186400 | 186400 batches | lr 1.56e-06 | ms/batch 178.85922 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   186600 | 186600 batches | lr 1.56e-06 | ms/batch 180.42056 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   186800 | 186800 batches | lr 1.56e-06 | ms/batch 183.75769 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   187000 | 187000 batches | lr 1.56e-06 | ms/batch 177.04369 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   187200 | 187200 batches | lr 1.56e-06 | ms/batch 177.96507 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   187400 | 187400 batches | lr 1.56e-06 | ms/batch 178.60240 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   187600 | 187600 batches | lr 1.56e-06 | ms/batch 183.12864 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   187800 | 187800 batches | lr 1.56e-06 | ms/batch 179.45907 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   188000 | 188000 batches | lr 1.56e-06 | ms/batch 177.59290 | loss 2.30256 | ppl    10.000\n",
      "|\n",
      "Source: [ 2 11  5  6  5  7  2 11  5  6  6  5  3  6  5  5  4  9  4  2 10  6  9  7\n",
      "  1  7  9  6 10  2  4  9  4  5  5  6  3  5  6  6  5 11  2  7  5  6  5 11]\n",
      "Target: [11  5  6  5  7  2 11  5  6  6  5  3  6  5  5  4  9  4  2 10  6  9  7  1\n",
      "  7  9  6 10  2  4  9  4  5  5  6  3  5  6  6  5 11  2  7  5  6  5 11  2]\n",
      "Teacher forcing: acc:0.10263020833333333\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10263020833333333\n",
      "Preds:  [11  5  6  5  7  2 11  5  6  6  5  3  6  5  5  4  9  4  2 10  6  9  7  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  47 at step   188000 | time: 739.86s | valid loss 2.30256 | valid ppl    9.9997 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   188200 | 188200 batches | lr 1.56e-06 | ms/batch 259.07768 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   188400 | 188400 batches | lr 1.56e-06 | ms/batch 186.13498 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   188600 | 188600 batches | lr 1.56e-06 | ms/batch 188.83863 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   188800 | 188800 batches | lr 1.56e-06 | ms/batch 186.93508 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   189000 | 189000 batches | lr 1.56e-06 | ms/batch 188.11135 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   189200 | 189200 batches | lr 1.56e-06 | ms/batch 179.07683 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   189400 | 189400 batches | lr 1.56e-06 | ms/batch 179.46479 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   189600 | 189600 batches | lr 1.56e-06 | ms/batch 177.45204 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   189800 | 189800 batches | lr 1.56e-06 | ms/batch 178.92543 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   190000 | 190000 batches | lr 1.56e-06 | ms/batch 180.81479 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   190200 | 190200 batches | lr 1.56e-06 | ms/batch 181.33156 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   190400 | 190400 batches | lr 1.56e-06 | ms/batch 176.10453 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   190600 | 190600 batches | lr 1.56e-06 | ms/batch 177.01267 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   190800 | 190800 batches | lr 1.56e-06 | ms/batch 179.07526 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   191000 | 191000 batches | lr 1.56e-06 | ms/batch 179.01898 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   191200 | 191200 batches | lr 1.56e-06 | ms/batch 180.76313 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   191400 | 191400 batches | lr 1.56e-06 | ms/batch 178.84332 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   191600 | 191600 batches | lr 1.56e-06 | ms/batch 178.64776 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   191800 | 191800 batches | lr 1.56e-06 | ms/batch 177.87550 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   192000 | 192000 batches | lr 1.56e-06 | ms/batch 181.90676 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 4  2  2  7 10  8  3  7 10  4  7  8  3 10 11 11  6  7  3  8  5  6  8  7\n",
      "  1  7  8  6  5  8  3  7  6 11 11 10  3  8  7  4 10  7  3  8 10  7  2  2]\n",
      "Target: [ 2  2  7 10  8  3  7 10  4  7  8  3 10 11 11  6  7  3  8  5  6  8  7  1\n",
      "  7  8  6  5  8  3  7  6 11 11 10  3  8  7  4 10  7  3  8 10  7  2  2  4]\n",
      "Teacher forcing: acc:0.10227997448979592\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10227997448979592\n",
      "Preds:  [ 2  2  7 10  8  3  7 10  4  7  8  3 10 11 11  6  7  3  8  5  6  8  7  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  48 at step   192000 | time: 738.89s | valid loss 2.30258 | valid ppl   10.0000 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   192200 | 192200 batches | lr 1.56e-06 | ms/batch 259.18754 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   192400 | 192400 batches | lr 1.56e-06 | ms/batch 176.02167 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   192600 | 192600 batches | lr 1.56e-06 | ms/batch 179.07155 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   192800 | 192800 batches | lr 1.56e-06 | ms/batch 177.31832 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   193000 | 193000 batches | lr 1.56e-06 | ms/batch 178.95629 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   193200 | 193200 batches | lr 1.56e-06 | ms/batch 179.87621 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   193400 | 193400 batches | lr 1.56e-06 | ms/batch 183.78199 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   193600 | 193600 batches | lr 1.56e-06 | ms/batch 184.89082 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   193800 | 193800 batches | lr 1.56e-06 | ms/batch 177.00426 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   194000 | 194000 batches | lr 1.56e-06 | ms/batch 182.91013 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   194200 | 194200 batches | lr 1.56e-06 | ms/batch 179.84124 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   194400 | 194400 batches | lr 1.56e-06 | ms/batch 177.42537 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   194600 | 194600 batches | lr 1.56e-06 | ms/batch 178.00154 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   194800 | 194800 batches | lr 1.56e-06 | ms/batch 179.29643 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   195000 | 195000 batches | lr 1.56e-06 | ms/batch 178.46675 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   195200 | 195200 batches | lr 1.56e-06 | ms/batch 182.59745 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   195400 | 195400 batches | lr 1.56e-06 | ms/batch 182.08787 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   195600 | 195600 batches | lr 1.56e-06 | ms/batch 185.52393 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   195800 | 195800 batches | lr 1.56e-06 | ms/batch 190.88786 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   196000 | 196000 batches | lr 1.56e-06 | ms/batch 191.18785 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  7  2  9  6  8 11  3  6  8  6  2  3  4  5  8  5  4 10  6 11 10  7  7\n",
      "  1  7  7 10 11  6 10  4  5  8  5  4  3  2  6  8  6  3 11  8  6  9  2  7]\n",
      "Target: [ 7  2  9  6  8 11  3  6  8  6  2  3  4  5  8  5  4 10  6 11 10  7  7  1\n",
      "  7  7 10 11  6 10  4  5  8  5  4  3  2  6  8  6  3 11  8  6  9  2  7  5]\n",
      "Teacher forcing: acc:0.10354166666666667\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10354166666666667\n",
      "Preds:  [ 7  2  9  6  8 11  3  6  8  6  2  3  4  5  8  5  4 10  6 11 10  7  7  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  49 at step   196000 | time: 742.09s | valid loss 2.30256 | valid ppl    9.9998 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   196200 | 196200 batches | lr 7.81e-07 | ms/batch 279.40131 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   196400 | 196400 batches | lr 7.81e-07 | ms/batch 191.78863 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   196600 | 196600 batches | lr 7.81e-07 | ms/batch 184.92761 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   196800 | 196800 batches | lr 7.81e-07 | ms/batch 184.28124 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   197000 | 197000 batches | lr 7.81e-07 | ms/batch 186.25551 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   197200 | 197200 batches | lr 7.81e-07 | ms/batch 179.20381 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   197400 | 197400 batches | lr 7.81e-07 | ms/batch 177.80734 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   197600 | 197600 batches | lr 7.81e-07 | ms/batch 177.47414 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   197800 | 197800 batches | lr 7.81e-07 | ms/batch 179.62041 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   198000 | 198000 batches | lr 7.81e-07 | ms/batch 179.84801 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   198200 | 198200 batches | lr 7.81e-07 | ms/batch 176.90958 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   198400 | 198400 batches | lr 7.81e-07 | ms/batch 177.85212 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   198600 | 198600 batches | lr 7.81e-07 | ms/batch 179.38595 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   198800 | 198800 batches | lr 7.81e-07 | ms/batch 181.66822 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   199000 | 199000 batches | lr 7.81e-07 | ms/batch 177.38093 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   199200 | 199200 batches | lr 7.81e-07 | ms/batch 176.89483 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   199400 | 199400 batches | lr 7.81e-07 | ms/batch 176.52880 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   199600 | 199600 batches | lr 7.81e-07 | ms/batch 179.90548 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   199800 | 199800 batches | lr 7.81e-07 | ms/batch 183.92657 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   200000 | 200000 batches | lr 7.81e-07 | ms/batch 182.70498 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 4  9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10\n",
      "  1 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9]\n",
      "Target: [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9  4]\n",
      "Teacher forcing: acc:0.10380208333333334\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10380208333333334\n",
      "Preds:  [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  50 at step   200000 | time: 741.92s | valid loss 2.30256 | valid ppl    9.9998 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   200200 | 200200 batches | lr 7.81e-07 | ms/batch 264.57143 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   200400 | 200400 batches | lr 7.81e-07 | ms/batch 181.42443 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   200600 | 200600 batches | lr 7.81e-07 | ms/batch 179.41161 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   200800 | 200800 batches | lr 7.81e-07 | ms/batch 178.64476 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   201000 | 201000 batches | lr 7.81e-07 | ms/batch 181.52981 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   201200 | 201200 batches | lr 7.81e-07 | ms/batch 180.97787 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   201400 | 201400 batches | lr 7.81e-07 | ms/batch 177.04708 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   201600 | 201600 batches | lr 7.81e-07 | ms/batch 179.78598 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   201800 | 201800 batches | lr 7.81e-07 | ms/batch 181.52596 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   202000 | 202000 batches | lr 7.81e-07 | ms/batch 178.29268 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   202200 | 202200 batches | lr 7.81e-07 | ms/batch 184.42859 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   202400 | 202400 batches | lr 7.81e-07 | ms/batch 177.40456 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   202600 | 202600 batches | lr 7.81e-07 | ms/batch 178.13329 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   202800 | 202800 batches | lr 7.81e-07 | ms/batch 178.02982 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   203000 | 203000 batches | lr 7.81e-07 | ms/batch 179.49414 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   203200 | 203200 batches | lr 7.81e-07 | ms/batch 178.61904 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   203400 | 203400 batches | lr 7.81e-07 | ms/batch 183.46028 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   203600 | 203600 batches | lr 7.81e-07 | ms/batch 186.42355 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   203800 | 203800 batches | lr 7.81e-07 | ms/batch 179.59471 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   204000 | 204000 batches | lr 7.81e-07 | ms/batch 175.36348 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 4 10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8\n",
      "  1  8  5  8  2  9 10 10  8  2  6  6  5  8 10  2  7  9  3  6  6  5  5 10]\n",
      "Target: [10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8  1\n",
      "  8  5  8  2  9 10 10  8  2  6  6  5  8 10  2  7  9  3  6  6  5  5 10  4]\n",
      "Teacher forcing: acc:0.10238626700680271\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10238626700680271\n",
      "Preds:  [10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  51 at step   204000 | time: 736.78s | valid loss 2.30259 | valid ppl   10.0001 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   204200 | 204200 batches | lr 7.81e-07 | ms/batch 260.45996 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   204400 | 204400 batches | lr 7.81e-07 | ms/batch 181.13175 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   204600 | 204600 batches | lr 7.81e-07 | ms/batch 180.35856 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   204800 | 204800 batches | lr 7.81e-07 | ms/batch 182.00845 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   205000 | 205000 batches | lr 7.81e-07 | ms/batch 177.62434 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   205200 | 205200 batches | lr 7.81e-07 | ms/batch 181.26453 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   205400 | 205400 batches | lr 7.81e-07 | ms/batch 178.12568 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   205600 | 205600 batches | lr 7.81e-07 | ms/batch 180.11311 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   205800 | 205800 batches | lr 7.81e-07 | ms/batch 184.84510 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   206000 | 206000 batches | lr 7.81e-07 | ms/batch 177.67611 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   206200 | 206200 batches | lr 7.81e-07 | ms/batch 181.15259 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   206400 | 206400 batches | lr 7.81e-07 | ms/batch 177.10290 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   206600 | 206600 batches | lr 7.81e-07 | ms/batch 177.44984 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   206800 | 206800 batches | lr 7.81e-07 | ms/batch 176.93114 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   207000 | 207000 batches | lr 7.81e-07 | ms/batch 178.01009 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   207200 | 207200 batches | lr 7.81e-07 | ms/batch 178.45828 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   207400 | 207400 batches | lr 7.81e-07 | ms/batch 176.49957 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   207600 | 207600 batches | lr 7.81e-07 | ms/batch 175.62983 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   207800 | 207800 batches | lr 7.81e-07 | ms/batch 178.84768 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   208000 | 208000 batches | lr 7.81e-07 | ms/batch 179.11761 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 9  7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3\n",
      "  1  3  3  4 11  6  5  9  9  8  3  7  6  2  9 10 10 11  4 11  3  3 10  7]\n",
      "Target: [ 7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3  1\n",
      "  3  3  4 11  6  5  9  9  8  3  7  6  2  9 10 10 11  4 11  3  3 10  7  9]\n",
      "Teacher forcing: acc:0.10346354166666667\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10346354166666667\n",
      "Preds:  [ 7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  52 at step   208000 | time: 732.49s | valid loss 2.30256 | valid ppl    9.9997 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   208200 | 208200 batches | lr 7.81e-07 | ms/batch 259.03550 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   208400 | 208400 batches | lr 7.81e-07 | ms/batch 176.39639 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   208600 | 208600 batches | lr 7.81e-07 | ms/batch 177.78346 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   208800 | 208800 batches | lr 7.81e-07 | ms/batch 179.80444 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   209000 | 209000 batches | lr 7.81e-07 | ms/batch 178.10974 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   209200 | 209200 batches | lr 7.81e-07 | ms/batch 178.85788 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   209400 | 209400 batches | lr 7.81e-07 | ms/batch 180.82115 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   209600 | 209600 batches | lr 7.81e-07 | ms/batch 177.20734 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   209800 | 209800 batches | lr 7.81e-07 | ms/batch 177.36420 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   210000 | 210000 batches | lr 7.81e-07 | ms/batch 178.58129 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   210200 | 210200 batches | lr 7.81e-07 | ms/batch 180.07368 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   210400 | 210400 batches | lr 7.81e-07 | ms/batch 179.76718 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   210600 | 210600 batches | lr 7.81e-07 | ms/batch 180.14734 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   210800 | 210800 batches | lr 7.81e-07 | ms/batch 177.60144 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   211000 | 211000 batches | lr 7.81e-07 | ms/batch 180.13906 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   211200 | 211200 batches | lr 7.81e-07 | ms/batch 178.06393 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   211400 | 211400 batches | lr 7.81e-07 | ms/batch 178.91171 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   211600 | 211600 batches | lr 7.81e-07 | ms/batch 178.35126 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   211800 | 211800 batches | lr 7.81e-07 | ms/batch 179.11347 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   212000 | 212000 batches | lr 7.81e-07 | ms/batch 178.29127 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9\n",
      "  1  9  6  4  6  2  6 10 10 10  8  8  3  9  7 11  2 11  3  3  9  4  6  3]\n",
      "Target: [ 3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9  1\n",
      "  9  6  4  6  2  6 10 10 10  8  8  3  9  7 11  2 11  3  3  9  4  6  3  7]\n",
      "Teacher forcing: acc:0.1018813775510204\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.1018813775510204\n",
      "Preds:  [ 3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  53 at step   212000 | time: 730.40s | valid loss 2.30258 | valid ppl    9.9999 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   212200 | 212200 batches | lr 7.81e-07 | ms/batch 258.87553 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   212400 | 212400 batches | lr 7.81e-07 | ms/batch 178.82300 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   212600 | 212600 batches | lr 7.81e-07 | ms/batch 179.35360 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   212800 | 212800 batches | lr 7.81e-07 | ms/batch 178.72541 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   213000 | 213000 batches | lr 7.81e-07 | ms/batch 178.87709 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   213200 | 213200 batches | lr 7.81e-07 | ms/batch 178.70653 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   213400 | 213400 batches | lr 7.81e-07 | ms/batch 178.59763 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   213600 | 213600 batches | lr 7.81e-07 | ms/batch 181.70945 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   213800 | 213800 batches | lr 7.81e-07 | ms/batch 179.34806 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   214000 | 214000 batches | lr 7.81e-07 | ms/batch 177.76586 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   214200 | 214200 batches | lr 7.81e-07 | ms/batch 179.08634 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   214400 | 214400 batches | lr 7.81e-07 | ms/batch 178.37786 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   214600 | 214600 batches | lr 7.81e-07 | ms/batch 181.07070 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   214800 | 214800 batches | lr 7.81e-07 | ms/batch 179.01927 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   215000 | 215000 batches | lr 7.81e-07 | ms/batch 176.96728 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   215200 | 215200 batches | lr 7.81e-07 | ms/batch 178.21044 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   215400 | 215400 batches | lr 7.81e-07 | ms/batch 178.01701 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   215600 | 215600 batches | lr 7.81e-07 | ms/batch 178.56999 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   215800 | 215800 batches | lr 7.81e-07 | ms/batch 182.60451 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   216000 | 216000 batches | lr 7.81e-07 | ms/batch 179.63862 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10\n",
      "  1 10  8  3  2  6  6  3  3  6  5 10  8  4  2  3  4  6  2 11  7  9  9  3]\n",
      "Target: [ 3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10  1\n",
      " 10  8  3  2  6  6  3  3  6  5 10  8  4  2  3  4  6  2 11  7  9  9  3  2]\n",
      "Teacher forcing: acc:0.10372395833333334\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10372395833333334\n",
      "Preds:  [ 3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  54 at step   216000 | time: 733.33s | valid loss 2.30257 | valid ppl    9.9998 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   216200 | 216200 batches | lr 7.81e-07 | ms/batch 263.47266 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   216400 | 216400 batches | lr 7.81e-07 | ms/batch 185.23082 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   216600 | 216600 batches | lr 7.81e-07 | ms/batch 181.22107 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   216800 | 216800 batches | lr 7.81e-07 | ms/batch 180.28830 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   217000 | 217000 batches | lr 7.81e-07 | ms/batch 182.47916 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   217200 | 217200 batches | lr 7.81e-07 | ms/batch 178.93979 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   217400 | 217400 batches | lr 7.81e-07 | ms/batch 181.74827 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   217600 | 217600 batches | lr 7.81e-07 | ms/batch 177.78874 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   217800 | 217800 batches | lr 7.81e-07 | ms/batch 177.78694 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   218000 | 218000 batches | lr 7.81e-07 | ms/batch 179.66932 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   218200 | 218200 batches | lr 7.81e-07 | ms/batch 179.60937 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   218400 | 218400 batches | lr 7.81e-07 | ms/batch 179.79688 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   218600 | 218600 batches | lr 7.81e-07 | ms/batch 178.33419 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   218800 | 218800 batches | lr 7.81e-07 | ms/batch 177.18500 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   219000 | 219000 batches | lr 7.81e-07 | ms/batch 179.85912 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   219200 | 219200 batches | lr 7.81e-07 | ms/batch 182.63775 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   219400 | 219400 batches | lr 7.81e-07 | ms/batch 179.63195 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   219600 | 219600 batches | lr 7.81e-07 | ms/batch 177.28216 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   219800 | 219800 batches | lr 7.81e-07 | ms/batch 178.69784 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   220000 | 220000 batches | lr 7.81e-07 | ms/batch 178.24740 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3\n",
      "  1  3  3 10  2  5  6  4  2 11 10  6  6  6  7  6  4  7  8  7  2  2 11  3]\n",
      "Target: [ 3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3  1\n",
      "  3  3 10  2  5  6  4  2 11 10  6  6  6  7  6  4  7  8  7  2  2 11  3  2]\n",
      "Teacher forcing: acc:0.10354166666666667\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10354166666666667\n",
      "Preds:  [ 3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  55 at step   220000 | time: 735.46s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   220200 | 220200 batches | lr 3.91e-07 | ms/batch 261.07592 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   220400 | 220400 batches | lr 3.91e-07 | ms/batch 183.38402 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   220600 | 220600 batches | lr 3.91e-07 | ms/batch 189.55248 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   220800 | 220800 batches | lr 3.91e-07 | ms/batch 185.87708 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   221000 | 221000 batches | lr 3.91e-07 | ms/batch 179.73749 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   221200 | 221200 batches | lr 3.91e-07 | ms/batch 179.06043 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   221400 | 221400 batches | lr 3.91e-07 | ms/batch 180.68464 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   221600 | 221600 batches | lr 3.91e-07 | ms/batch 179.32924 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   221800 | 221800 batches | lr 3.91e-07 | ms/batch 179.63362 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   222000 | 222000 batches | lr 3.91e-07 | ms/batch 179.24655 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   222200 | 222200 batches | lr 3.91e-07 | ms/batch 177.69835 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   222400 | 222400 batches | lr 3.91e-07 | ms/batch 178.04083 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   222600 | 222600 batches | lr 3.91e-07 | ms/batch 181.38420 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   222800 | 222800 batches | lr 3.91e-07 | ms/batch 178.81566 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   223000 | 223000 batches | lr 3.91e-07 | ms/batch 176.93303 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   223200 | 223200 batches | lr 3.91e-07 | ms/batch 178.76555 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   223400 | 223400 batches | lr 3.91e-07 | ms/batch 190.87307 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   223600 | 223600 batches | lr 3.91e-07 | ms/batch 188.91370 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   223800 | 223800 batches | lr 3.91e-07 | ms/batch 189.98695 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   224000 | 224000 batches | lr 3.91e-07 | ms/batch 188.71471 | loss 2.30257 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 9 10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7\n",
      "  1  7 11  3  6 10 11  5  3  9  2 11  9  7  4 10  6  8  4  4  3  4  9 10]\n",
      "Target: [10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7  1\n",
      "  7 11  3  6 10 11  5  3  9  2 11  9  7  4 10  6  8  4  4  3  4  9 10  9]\n",
      "Teacher forcing: acc:0.10318346088435375\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10318346088435375\n",
      "Preds:  [10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  56 at step   224000 | time: 746.31s | valid loss 2.30259 | valid ppl   10.0000 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   224200 | 224200 batches | lr 3.91e-07 | ms/batch 277.52084 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   224400 | 224400 batches | lr 3.91e-07 | ms/batch 189.90237 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   224600 | 224600 batches | lr 3.91e-07 | ms/batch 187.52056 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   224800 | 224800 batches | lr 3.91e-07 | ms/batch 176.51402 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   225000 | 225000 batches | lr 3.91e-07 | ms/batch 177.84478 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   225200 | 225200 batches | lr 3.91e-07 | ms/batch 175.94952 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   225400 | 225400 batches | lr 3.91e-07 | ms/batch 171.40162 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   225600 | 225600 batches | lr 3.91e-07 | ms/batch 173.51493 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   225800 | 225800 batches | lr 3.91e-07 | ms/batch 174.36467 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   226000 | 226000 batches | lr 3.91e-07 | ms/batch 174.31422 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   226200 | 226200 batches | lr 3.91e-07 | ms/batch 171.70375 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   226400 | 226400 batches | lr 3.91e-07 | ms/batch 171.92477 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   226600 | 226600 batches | lr 3.91e-07 | ms/batch 171.24646 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   226800 | 226800 batches | lr 3.91e-07 | ms/batch 175.29299 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   227000 | 227000 batches | lr 3.91e-07 | ms/batch 171.59568 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   227200 | 227200 batches | lr 3.91e-07 | ms/batch 171.41538 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   227400 | 227400 batches | lr 3.91e-07 | ms/batch 168.36398 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   227600 | 227600 batches | lr 3.91e-07 | ms/batch 171.24778 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   227800 | 227800 batches | lr 3.91e-07 | ms/batch 175.16233 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   228000 | 228000 batches | lr 3.91e-07 | ms/batch 170.17901 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8\n",
      "  1  8  2  8  3  3 11  4  4 10  5  2  3  7  3  3  5  7  6  6  8  6  5  5]\n",
      "Target: [ 5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8  1\n",
      "  8  2  8  3  3 11  4  4 10  5  2  3  7  3  3  5  7  6  6  8  6  5  5  2]\n",
      "Teacher forcing: acc:0.103359375\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.103359375\n",
      "Preds:  [ 5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  57 at step   228000 | time: 718.43s | valid loss 2.30256 | valid ppl    9.9998 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   228200 | 228200 batches | lr 3.91e-07 | ms/batch 248.57962 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   228400 | 228400 batches | lr 3.91e-07 | ms/batch 172.99829 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   228600 | 228600 batches | lr 3.91e-07 | ms/batch 172.54064 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   228800 | 228800 batches | lr 3.91e-07 | ms/batch 179.15625 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   229000 | 229000 batches | lr 3.91e-07 | ms/batch 172.15187 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   229200 | 229200 batches | lr 3.91e-07 | ms/batch 175.46239 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   229400 | 229400 batches | lr 3.91e-07 | ms/batch 184.05417 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   229600 | 229600 batches | lr 3.91e-07 | ms/batch 175.45311 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   229800 | 229800 batches | lr 3.91e-07 | ms/batch 174.00736 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   230000 | 230000 batches | lr 3.91e-07 | ms/batch 169.16496 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   230200 | 230200 batches | lr 3.91e-07 | ms/batch 168.62653 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   230400 | 230400 batches | lr 3.91e-07 | ms/batch 196.77888 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   230600 | 230600 batches | lr 3.91e-07 | ms/batch 254.26159 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   230800 | 230800 batches | lr 3.91e-07 | ms/batch 191.52242 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   231000 | 231000 batches | lr 3.91e-07 | ms/batch 185.46762 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   231200 | 231200 batches | lr 3.91e-07 | ms/batch 170.27273 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   231400 | 231400 batches | lr 3.91e-07 | ms/batch 173.08600 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   231600 | 231600 batches | lr 3.91e-07 | ms/batch 169.80301 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   231800 | 231800 batches | lr 3.91e-07 | ms/batch 172.48883 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   232000 | 232000 batches | lr 3.91e-07 | ms/batch 168.43609 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 5  3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3\n",
      "  1  3  6  5  9  4  5 11  4  7  7  2  4  3  5  9 11  7  8 11 11 10 10  3]\n",
      "Target: [ 3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3  1\n",
      "  3  6  5  9  4  5 11  4  7  7  2  4  3  5  9 11  7  8 11 11 10 10  3  5]\n",
      "Teacher forcing: acc:0.10193452380952381\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10193452380952381\n",
      "Preds:  [ 3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  58 at step   232000 | time: 734.25s | valid loss 2.30257 | valid ppl    9.9998 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   232200 | 232200 batches | lr 3.91e-07 | ms/batch 252.72314 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   232400 | 232400 batches | lr 3.91e-07 | ms/batch 174.70953 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   232600 | 232600 batches | lr 3.91e-07 | ms/batch 172.13434 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   232800 | 232800 batches | lr 3.91e-07 | ms/batch 170.89132 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   233000 | 233000 batches | lr 3.91e-07 | ms/batch 170.21303 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   233200 | 233200 batches | lr 3.91e-07 | ms/batch 169.13827 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   233400 | 233400 batches | lr 3.91e-07 | ms/batch 172.55904 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   233600 | 233600 batches | lr 3.91e-07 | ms/batch 168.41244 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   233800 | 233800 batches | lr 3.91e-07 | ms/batch 175.39010 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   234000 | 234000 batches | lr 3.91e-07 | ms/batch 167.97086 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   234200 | 234200 batches | lr 3.91e-07 | ms/batch 170.37256 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   234400 | 234400 batches | lr 3.91e-07 | ms/batch 187.04017 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   234600 | 234600 batches | lr 3.91e-07 | ms/batch 190.66199 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   234800 | 234800 batches | lr 3.91e-07 | ms/batch 169.79797 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   235000 | 235000 batches | lr 3.91e-07 | ms/batch 172.47752 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   235200 | 235200 batches | lr 3.91e-07 | ms/batch 169.80591 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   235400 | 235400 batches | lr 3.91e-07 | ms/batch 170.24418 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   235600 | 235600 batches | lr 3.91e-07 | ms/batch 173.75992 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   235800 | 235800 batches | lr 3.91e-07 | ms/batch 174.31255 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   236000 | 236000 batches | lr 3.91e-07 | ms/batch 175.70674 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 4 11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2\n",
      "  1  2  3  8  7  8 10  7 11  7  2  5  6 10  8  2 11  9  3  8 11  7  9 11]\n",
      "Target: [11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2  1\n",
      "  2  3  8  7  8 10  7 11  7  2  5  6 10  8  2 11  9  3  8 11  7  9 11  4]\n",
      "Teacher forcing: acc:0.10270833333333333\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10270833333333333\n",
      "Preds:  [11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  59 at step   236000 | time: 710.22s | valid loss 2.30258 | valid ppl    9.9999 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   236200 | 236200 batches | lr 3.91e-07 | ms/batch 253.37205 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   236400 | 236400 batches | lr 3.91e-07 | ms/batch 171.62997 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   236600 | 236600 batches | lr 3.91e-07 | ms/batch 170.43492 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   236800 | 236800 batches | lr 3.91e-07 | ms/batch 172.90308 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   237000 | 237000 batches | lr 3.91e-07 | ms/batch 171.59775 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   237200 | 237200 batches | lr 3.91e-07 | ms/batch 167.42389 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   237400 | 237400 batches | lr 3.91e-07 | ms/batch 167.25741 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   237600 | 237600 batches | lr 3.91e-07 | ms/batch 169.20116 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   237800 | 237800 batches | lr 3.91e-07 | ms/batch 169.05172 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   238000 | 238000 batches | lr 3.91e-07 | ms/batch 170.08895 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   238200 | 238200 batches | lr 3.91e-07 | ms/batch 172.59427 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   238400 | 238400 batches | lr 3.91e-07 | ms/batch 169.34619 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   238600 | 238600 batches | lr 3.91e-07 | ms/batch 167.03818 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   238800 | 238800 batches | lr 3.91e-07 | ms/batch 169.08267 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   239000 | 239000 batches | lr 3.91e-07 | ms/batch 167.71106 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   239200 | 239200 batches | lr 3.91e-07 | ms/batch 169.47671 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   239400 | 239400 batches | lr 3.91e-07 | ms/batch 168.87958 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   239600 | 239600 batches | lr 3.91e-07 | ms/batch 165.79483 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   239800 | 239800 batches | lr 3.91e-07 | ms/batch 167.97294 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   240000 | 240000 batches | lr 3.91e-07 | ms/batch 168.08414 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 8  3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9\n",
      "  1  9  7  9 10  3  7  9  6  4  4  4  8  8  4  5  9  7  8  6  5  8  6  3]\n",
      "Target: [ 3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9  1\n",
      "  9  7  9 10  3  7  9  6  4  4  4  8  8  4  5  9  7  8  6  5  8  6  3  8]\n",
      "Teacher forcing: acc:0.10380208333333334\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10380208333333334\n",
      "Preds:  [ 3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  60 at step   240000 | time: 693.36s | valid loss 2.30256 | valid ppl    9.9998 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   240200 | 240200 batches | lr 3.91e-07 | ms/batch 247.94508 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   240400 | 240400 batches | lr 3.91e-07 | ms/batch 166.28404 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   240600 | 240600 batches | lr 3.91e-07 | ms/batch 172.16153 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   240800 | 240800 batches | lr 3.91e-07 | ms/batch 170.60063 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   241000 | 241000 batches | lr 3.91e-07 | ms/batch 167.97582 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   241200 | 241200 batches | lr 3.91e-07 | ms/batch 167.76489 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   241400 | 241400 batches | lr 3.91e-07 | ms/batch 166.50783 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   241600 | 241600 batches | lr 3.91e-07 | ms/batch 172.44384 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   241800 | 241800 batches | lr 3.91e-07 | ms/batch 168.73619 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   242000 | 242000 batches | lr 3.91e-07 | ms/batch 170.48205 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   242200 | 242200 batches | lr 3.91e-07 | ms/batch 166.28881 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   242400 | 242400 batches | lr 3.91e-07 | ms/batch 168.72346 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   242600 | 242600 batches | lr 3.91e-07 | ms/batch 177.61103 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   242800 | 242800 batches | lr 3.91e-07 | ms/batch 169.49783 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   243000 | 243000 batches | lr 3.91e-07 | ms/batch 167.36047 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   243200 | 243200 batches | lr 3.91e-07 | ms/batch 172.47164 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   243400 | 243400 batches | lr 3.91e-07 | ms/batch 168.68269 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   243600 | 243600 batches | lr 3.91e-07 | ms/batch 167.55964 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   243800 | 243800 batches | lr 3.91e-07 | ms/batch 167.70292 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   244000 | 244000 batches | lr 3.91e-07 | ms/batch 165.02589 | loss 2.30257 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [11 10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4\n",
      "  1  4 10  4  4  3 11  9  7  8  9  7 10  7  8  3  3  6 10 11  7  7  5 10]\n",
      "Target: [10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4  1\n",
      "  4 10  4  4  3 11  9  7  8  9  7 10  7  8  3  3  6 10 11  7  7  5 10 11]\n",
      "Teacher forcing: acc:0.10408694727891156\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10408694727891156\n",
      "Preds:  [10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  61 at step   244000 | time: 691.64s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.104\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   244200 | 244200 batches | lr 1.95e-07 | ms/batch 241.13800 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   244400 | 244400 batches | lr 1.95e-07 | ms/batch 165.30356 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   244600 | 244600 batches | lr 1.95e-07 | ms/batch 168.65115 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   244800 | 244800 batches | lr 1.95e-07 | ms/batch 164.68342 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   245000 | 245000 batches | lr 1.95e-07 | ms/batch 169.71870 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   245200 | 245200 batches | lr 1.95e-07 | ms/batch 166.79448 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   245400 | 245400 batches | lr 1.95e-07 | ms/batch 169.76273 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   245600 | 245600 batches | lr 1.95e-07 | ms/batch 167.05717 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   245800 | 245800 batches | lr 1.95e-07 | ms/batch 172.23594 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   246000 | 246000 batches | lr 1.95e-07 | ms/batch 172.64007 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   246200 | 246200 batches | lr 1.95e-07 | ms/batch 174.02077 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   246400 | 246400 batches | lr 1.95e-07 | ms/batch 177.73029 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   246600 | 246600 batches | lr 1.95e-07 | ms/batch 176.01077 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   246800 | 246800 batches | lr 1.95e-07 | ms/batch 172.25175 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   247000 | 247000 batches | lr 1.95e-07 | ms/batch 177.38973 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   247200 | 247200 batches | lr 1.95e-07 | ms/batch 171.53563 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   247400 | 247400 batches | lr 1.95e-07 | ms/batch 164.21446 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   247600 | 247600 batches | lr 1.95e-07 | ms/batch 167.66299 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   247800 | 247800 batches | lr 1.95e-07 | ms/batch 172.58205 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   248000 | 248000 batches | lr 1.95e-07 | ms/batch 172.83415 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 4  7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10\n",
      "  1 10 10  7  6  8 10  9  4  6 11 11 11  9  2  4  2  6 10 11  8  6  3  7]\n",
      "Target: [ 7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10  1\n",
      " 10 10  7  6  8 10  9  4  6 11 11 11  9  2  4  2  6 10 11  8  6  3  7  4]\n",
      "Teacher forcing: acc:0.10234375\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 3 3 3 3 3 3 3 3 3 3 3 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10234375\n",
      "Preds:  [ 7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10  1\n",
      "  4  3  3  3  3  3  3  3  3  3  3  3  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  62 at step   248000 | time: 697.62s | valid loss 2.30259 | valid ppl   10.0000 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   248200 | 248200 batches | lr 1.95e-07 | ms/batch 250.67614 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   248400 | 248400 batches | lr 1.95e-07 | ms/batch 173.16354 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   248600 | 248600 batches | lr 1.95e-07 | ms/batch 176.42816 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   248800 | 248800 batches | lr 1.95e-07 | ms/batch 173.89759 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   249000 | 249000 batches | lr 1.95e-07 | ms/batch 171.65538 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   249200 | 249200 batches | lr 1.95e-07 | ms/batch 173.18696 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   249400 | 249400 batches | lr 1.95e-07 | ms/batch 169.37679 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   249600 | 249600 batches | lr 1.95e-07 | ms/batch 170.94720 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   249800 | 249800 batches | lr 1.95e-07 | ms/batch 165.74135 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   250000 | 250000 batches | lr 1.95e-07 | ms/batch 167.29116 | loss 2.30258 | ppl    10.000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "|\n",
      "Source: [11  9 10  6  2  7  9  5 10  3  7  4  7  8  3  6  9  6 10  3  9 11  9 11\n",
      "  1 11  9 11  9  3 10  6  9  6  3  8  7  4  7  3 10  5  9  7  2  6 10  9]\n",
      "Target: [ 9 10  6  2  7  9  5 10  3  7  4  7  8  3  6  9  6 10  3  9 11  9 11  1\n",
      " 11  9 11  9  3 10  6  9  6  3  8  7  4  7  3 10  5  9  7  2  6 10  9 11]\n",
      "Teacher forcing: acc:0.10192708333333333\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10192708333333333\n",
      "Preds:  [ 9 10  6  2  7  9  5 10  3  7  4  7  8  3  6  9  6 10  3  9 11  9 11  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "| End of training | test loss 2.30258 | test ppl   9.99997\n",
      " | test acc 0.102\n",
      "====================================================================================================\n",
      "run_reverse-debug.sh: line 53: cho: command not found\n",
      "run_reverse-debug.sh: line 54: syntax error near unexpected token `fi'\n",
      "run_reverse-debug.sh: line 54: `fi'\n"
     ]
    }
   ],
   "source": [
    "!bash run_reverse-debug.sh train --work_dir ../evaluation/tl12xl0mt12_deeper_lr1e-4_rlrp --lr 0.0001 --tgt_len 12 --eval_tgt_len 12 --mem_len 0 --num_mem_tokens 12 --device_ids 0 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ac73d",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06c2807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  ../evaluation/tl42xl0mt0_lr1e-4_rlrp-retrieval/20220112-172604/log.txt\n",
      "logs_to_tb.py:19: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  logs_to_tb(folder_path, log_dir)\n",
      "Processing  ../evaluation/tl42xl0mt0_lr1e-4_rlrp-retrieval/20220112-172222/log.txt\n",
      "Processing  ../evaluation/tl42xl0mt0_lr1e-4_rlrp-retrieval/20220112-172709/log.txt\n",
      "Processing  ../evaluation/tl42xl0mt0_lr1e-4_rlrp-retrieval/20220112-172652/log.txt\n",
      "Processing  ../evaluation/tl42xl0mt0_lr1e-4_rlrp-retrieval/20220112-172404/log.txt\n",
      "Processing  ../evaluation/tl42xl0mt0_lr1e-4_rlrp-retrieval/20220112-172314/log.txt\n",
      "Processing  ../evaluation/tl8xl0mt8_debug_lr2e-4_c-reverse/20220112-150822/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt12_debug_lr25e-5_c_bpd1-reverse/20220110-192507/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt12_debug_lr25e-5_c_bpd1-reverse/20220110-174336/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt12_debug_lr25e-5_c_bpd1-reverse/20220110-174312/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt12_debug_lr25e-5_c_bpd1-reverse/20220110-192429/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt6-reverse/20220107-135100/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt6-reverse/20220105-133841/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt6-reverse/20220107-165847/log.txt\n",
      "Processing  ../evaluation/tl8xl0mt6_debug_lr25e-5_rlrp-reverse/20220112-130757/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt12_debug_lr1e-4_c-reverse/20220110-130119/log.txt\n",
      "Processing  ../evaluation/tl10xl0mt0_l2h2_lr3e-4_rlrp_as1-retrieval/20220113-101945/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt12_debug_lr25e-5_c-reverse/20220110-130140/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24-reverse/20220108-174137/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24-reverse/20220108-160157/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24-reverse/20220108-152407/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24-reverse/20220105-121837/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24-reverse/20220108-173854/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24-reverse/20220108-160306/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24-reverse/20220108-155300/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24-reverse/20220107-135033/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24-reverse/20220108-173917/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24-reverse/20220108-160030/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24-reverse/20220108-160205/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24-reverse/20220108-164449/log.txt\n",
      "Processing  ../evaluation/tl8xl0mt6_l6_debug_lr25e-5_rlrp-reverse/20220113-024120/log.txt\n",
      "Processing  ../evaluation/tl48xl0mt24-reverse/20220108-155928/log.txt\n",
      "Processing  ../evaluation/tl48xl0mt24-reverse/20220105-112634/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt6_debug_lr15e-5_rlrp_bpd1-reverse/20220112-100257/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24_debug-reverse/20220108-174026/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24_debug-reverse/20220108-165557/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24_debug-reverse/20220108-165621/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24_debug-reverse/20220108-164945/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt24_debug-reverse/20220108-174053/log.txt\n",
      "Processing  ../evaluation/tl10xl0mt0_lr3e-4_rlrp_as2-retrieval/20220112-235057/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt12_debug-reverse/20220110-125811/log.txt\n",
      "Processing  ../evaluation/tl8xl0mt8_debug_lr2e-4-reverse/20220112-150805/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt6_debug_lr25e-5_c_bpd1-reverse/20220111-131252/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt0-reverse/20220105-102019/log.txt\n",
      "Processing  ../evaluation/tl8xl12mt0_cos-reverse/20220111-105613/log.txt\n",
      "Processing  ../evaluation/tl8xl12mt0_cos-reverse/20220111-104440/log.txt\n",
      "Processing  ../evaluation/tl8xl12mt0-reverse/20220111-104415/log.txt\n",
      "Processing  ../evaluation/tl8xl12mt0-reverse/20220111-105558/log.txt\n",
      "Processing  ../evaluation/tl8xl12mt0-reverse/20220105-123810/log.txt\n",
      "Processing  ../evaluation/tl8xl12mt0-reverse/20220111-102715/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt0-reverse/20220105-102015/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt0-reverse/20220105-102012/log.txt\n",
      "Processing  ../evaluation/tl8xl0mt0-reverse/20220105-102032/log.txt\n",
      "Processing  ../evaluation/tl48xl0mt0-reverse/20220105-102004/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt12-reverse/20220109-105001/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt12-reverse/20220109-104952/log.txt\n",
      "Processing  ../evaluation/tl8xl0mt12_debug_lr2e-4-reverse/20220110-130829/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt6_debug_lr15e-5_c_bpd1-reverse/20220111-131342/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt12_deeper_lr1e-4_rlrp-reverse/20220111-165124/log.txt\n",
      "Processing  ../evaluation/tl24xl24mt0-reverse/20220107-135022/log.txt\n",
      "Processing  ../evaluation/tl24xl24mt0-reverse/20220105-111749/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt12-reverse/20220108-181416/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt12-reverse/20220105-121609/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt12-reverse/20220107-135050/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt12-reverse/20220108-152440/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt12-reverse/20220108-160342/log.txt\n",
      "Processing  ../evaluation/tl24xl0mt12-reverse/20220107-154342/log.txt\n",
      "Processing  ../evaluation/tl8xl0mt12-reverse/20220105-145709/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt6_debug_lr25e-5_c-reverse/20220111-105922/log.txt\n",
      "Processing  ../evaluation/tl48xl24mt0-reverse/20220105-105256/log.txt\n",
      "Processing  ../evaluation/tl24xl12mt0-reverse/20220105-111704/log.txt\n",
      "Processing  ../evaluation/tl10xl0mt0_lr1e-4_rlrp-retrieval/20220112-172759/log.txt\n",
      "Processing  ../evaluation/tl10xl0mt0_lr1e-4_rlrp-retrieval/20220113-123542/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt12_debug_lr5e-5_c-reverse/20220110-130640/log.txt\n",
      "Processing  ../evaluation/tl12xl6mt0-reverse/20220109-104910/log.txt\n",
      "Processing  ../evaluation/tl12xl6mt0-reverse/20220105-120041/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt6_debug_lr15e-5_rlrp-reverse/20220113-011250/log.txt\n",
      "Processing  ../evaluation/tl12xl0mt12_debug_lr1e-5_c_bpd1-reverse/20220111-093633/log.txt\n"
     ]
    }
   ],
   "source": [
    "!python3 logs_to_tb.py --path ../evaluation --log_dir ../evaluation_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21737358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from experiment_utils.generate_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b680083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## reverse\n",
    "# # !mkdir ../../data64\n",
    "\n",
    "# BATCH_SIZE = 128\n",
    "# ENC_SEQ_LEN = 64\n",
    "# DEC_SEQ_LEN = 64\n",
    "# NUM_TOKENS = 10\n",
    "# TASK_NAME = 'reverse'\n",
    "# path = '../../data64'\n",
    "# np.random.seed(42)\n",
    "\n",
    "# generator = reverse_generator(batch_size=BATCH_SIZE, enc_seq_len=ENC_SEQ_LEN, dec_seq_len=DEC_SEQ_LEN, num_tokens=NUM_TOKENS)\n",
    "# generate_data(generator, path=path, task_name=TASK_NAME, train_size=100_000, test_size=10_000, val_size=2_000, batch_size=BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79d086c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## copy\n",
    "# # !mkdir ../../data64\n",
    "\n",
    "# BATCH_SIZE = 128\n",
    "# ENC_SEQ_LEN = 64\n",
    "# DEC_SEQ_LEN = 128\n",
    "# NUM_TOKENS = 10\n",
    "# TASK_NAME = 'copy'\n",
    "# path = '../../data64'\n",
    "# np.random.seed(42)\n",
    "\n",
    "# generator = copy_generator(batch_size=BATCH_SIZE, enc_seq_len=ENC_SEQ_LEN, dec_seq_len=DEC_SEQ_LEN, num_tokens=NUM_TOKENS)\n",
    "# generate_data(generator, path=path, task_name=TASK_NAME, train_size=100_000, test_size=10_000, val_size=2_000, batch_size=BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f68f099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[1, 2]\n",
      "Experiment dir : ../evaluation/tl12xl0mt12_debug_lr1e-4_c-reverse/20220110-130119\n",
      "====================================================================================================\n",
      "    - data : /home/admin/x-transformers/data24\n",
      "    - dataset : reverse\n",
      "    - n_layer : 4\n",
      "    - n_head : 4\n",
      "    - d_head : 64\n",
      "    - d_embed : 128\n",
      "    - d_model : 128\n",
      "    - d_inner : 256\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.0001\n",
      "    - mom : 0.0\n",
      "    - scheduler : constant\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 0.0\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 250000\n",
      "    - batch_size : 32\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 12\n",
      "    - eval_tgt_len : 12\n",
      "    - ext_len : 0\n",
      "    - mem_len : 0\n",
      "    - num_mem_tokens : 12\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : False\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : ../evaluation/tl12xl0mt12_debug_lr1e-4_c-reverse/20220110-130119\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : 50\n",
      "    - sample_softmax : -1\n",
      "    - patience : 0\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [1, 2]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : True\n",
      "    - read_mem_from_cache : True\n",
      "    - answer_size : 24\n",
      "    - tied : True\n",
      "    - ntokens : 12\n",
      "    - n_all_param : 924684\n",
      "    - n_nonemb_param : 921088\n",
      "====================================================================================================\n",
      "#params = 924684\n",
      "#non emb params = 921088\n",
      "| epoch   1 step      200 |    200 batches | lr 0.0001 | ms/batch 132.41922 | loss 2.35569 | ppl    10.545\n",
      "| epoch   1 step      400 |    400 batches | lr 0.0001 | ms/batch 120.33616 | loss 2.30867 | ppl    10.061\n",
      "| epoch   1 step      600 |    600 batches | lr 0.0001 | ms/batch 118.61737 | loss 2.30699 | ppl    10.044\n",
      "| epoch   1 step      800 |    800 batches | lr 0.0001 | ms/batch 119.11365 | loss 2.30609 | ppl    10.035\n",
      "| epoch   1 step     1000 |   1000 batches | lr 0.0001 | ms/batch 118.80512 | loss 2.30560 | ppl    10.030\n",
      "| epoch   1 step     1200 |   1200 batches | lr 0.0001 | ms/batch 118.71883 | loss 2.30501 | ppl    10.024\n",
      "| epoch   1 step     1400 |   1400 batches | lr 0.0001 | ms/batch 118.59543 | loss 2.30479 | ppl    10.022\n",
      "| epoch   1 step     1600 |   1600 batches | lr 0.0001 | ms/batch 114.66873 | loss 2.30461 | ppl    10.020\n",
      "| epoch   1 step     1800 |   1800 batches | lr 0.0001 | ms/batch 114.39292 | loss 2.30430 | ppl    10.017\n",
      "| epoch   1 step     2000 |   2000 batches | lr 0.0001 | ms/batch 115.34510 | loss 2.30424 | ppl    10.017\n",
      "| epoch   1 step     2200 |   2200 batches | lr 0.0001 | ms/batch 114.83487 | loss 2.30383 | ppl    10.012\n",
      "| epoch   1 step     2400 |   2400 batches | lr 0.0001 | ms/batch 115.89825 | loss 2.30374 | ppl    10.012\n",
      "| epoch   1 step     2600 |   2600 batches | lr 0.0001 | ms/batch 117.97280 | loss 2.30364 | ppl    10.011\n",
      "| epoch   1 step     2800 |   2800 batches | lr 0.0001 | ms/batch 120.65459 | loss 2.30354 | ppl    10.010\n",
      "| epoch   1 step     3000 |   3000 batches | lr 0.0001 | ms/batch 120.26006 | loss 2.30348 | ppl    10.009\n",
      "| epoch   1 step     3200 |   3200 batches | lr 0.0001 | ms/batch 121.70457 | loss 2.30318 | ppl    10.006\n",
      "| epoch   1 step     3400 |   3400 batches | lr 0.0001 | ms/batch 120.82548 | loss 2.30335 | ppl    10.008\n",
      "| epoch   1 step     3600 |   3600 batches | lr 0.0001 | ms/batch 121.91078 | loss 2.30325 | ppl    10.007\n",
      "| epoch   1 step     3800 |   3800 batches | lr 0.0001 | ms/batch 126.90384 | loss 2.30314 | ppl    10.006\n",
      "| epoch   1 step     4000 |   4000 batches | lr 0.0001 | ms/batch 128.35653 | loss 2.30306 | ppl    10.005\n",
      "|\n",
      "Source: [ 4  9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10\n",
      "  1 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9]\n",
      "Target: [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9  4]\n",
      "Teacher forcing: acc:0.10208333333333333\n",
      "Preds:  [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "No teacher forcing: acc:0.10208333333333333\n",
      "Preds:  [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step     4000 | time: 491.33s | valid loss 2.30282 | valid ppl   10.0023 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     4200 |   4200 batches | lr 0.0001 | ms/batch 185.54019 | loss 2.30316 | ppl    10.006\n",
      "| epoch   1 step     4400 |   4400 batches | lr 0.0001 | ms/batch 132.48739 | loss 2.30310 | ppl    10.005\n",
      "| epoch   1 step     4600 |   4600 batches | lr 0.0001 | ms/batch 129.59396 | loss 2.30301 | ppl    10.004\n",
      "| epoch   1 step     4800 |   4800 batches | lr 0.0001 | ms/batch 130.91823 | loss 2.30310 | ppl    10.005\n",
      "| epoch   1 step     5000 |   5000 batches | lr 0.0001 | ms/batch 128.98069 | loss 2.30308 | ppl    10.005\n",
      "| epoch   1 step     5200 |   5200 batches | lr 0.0001 | ms/batch 130.45281 | loss 2.30294 | ppl    10.004\n",
      "| epoch   1 step     5400 |   5400 batches | lr 0.0001 | ms/batch 128.99629 | loss 2.30287 | ppl    10.003\n",
      "| epoch   1 step     5600 |   5600 batches | lr 0.0001 | ms/batch 126.34663 | loss 2.30295 | ppl    10.004\n",
      "| epoch   1 step     5800 |   5800 batches | lr 0.0001 | ms/batch 126.75183 | loss 2.30286 | ppl    10.003\n",
      "| epoch   1 step     6000 |   6000 batches | lr 0.0001 | ms/batch 129.50631 | loss 2.30293 | ppl    10.003\n",
      "| epoch   1 step     6200 |   6200 batches | lr 0.0001 | ms/batch 124.87662 | loss 2.30293 | ppl    10.003\n",
      "| epoch   1 step     6400 |   6400 batches | lr 0.0001 | ms/batch 123.99574 | loss 2.30283 | ppl    10.002\n",
      "| epoch   1 step     6600 |   6600 batches | lr 0.0001 | ms/batch 123.17655 | loss 2.30287 | ppl    10.003\n",
      "| epoch   1 step     6800 |   6800 batches | lr 0.0001 | ms/batch 126.48953 | loss 2.30283 | ppl    10.002\n",
      "| epoch   1 step     7000 |   7000 batches | lr 0.0001 | ms/batch 126.89448 | loss 2.30285 | ppl    10.003\n",
      "| epoch   1 step     7200 |   7200 batches | lr 0.0001 | ms/batch 125.34897 | loss 2.30289 | ppl    10.003\n",
      "| epoch   1 step     7400 |   7400 batches | lr 0.0001 | ms/batch 124.88052 | loss 2.30282 | ppl    10.002\n",
      "| epoch   1 step     7600 |   7600 batches | lr 0.0001 | ms/batch 127.21427 | loss 2.30282 | ppl    10.002\n",
      "| epoch   1 step     7800 |   7800 batches | lr 0.0001 | ms/batch 125.53540 | loss 2.30283 | ppl    10.002\n",
      "| epoch   1 step     8000 |   8000 batches | lr 0.0001 | ms/batch 127.84791 | loss 2.30278 | ppl    10.002\n",
      "maslina\n",
      "|\n",
      "Source: [ 4 10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8\n",
      "  1  8  5  8  2  9 10 10  8  2  6  6  5  8 10  2  7  9  3  6  6  5  5 10]\n",
      "Target: [10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8  1\n",
      "  8  5  8  2  9 10 10  8  2  6  6  5  8 10  2  7  9  3  6  6  5  5 10  4]\n",
      "Teacher forcing: acc:0.10212053571428571\n",
      "Preds:  [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5]\n",
      "\n",
      "No teacher forcing: acc:0.10212053571428571\n",
      "Preds:  [10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8  1\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step     8000 | time: 520.56s | valid loss 2.30261 | valid ppl   10.0003 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     8200 |   8200 batches | lr 0.0001 | ms/batch 179.61611 | loss 2.30285 | ppl    10.003\n",
      "| epoch   1 step     8400 |   8400 batches | lr 0.0001 | ms/batch 125.57879 | loss 2.30273 | ppl    10.001\n",
      "| epoch   1 step     8600 |   8600 batches | lr 0.0001 | ms/batch 125.30400 | loss 2.30280 | ppl    10.002\n",
      "| epoch   1 step     8800 |   8800 batches | lr 0.0001 | ms/batch 127.74492 | loss 2.30276 | ppl    10.002\n",
      "| epoch   1 step     9000 |   9000 batches | lr 0.0001 | ms/batch 127.12986 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step     9200 |   9200 batches | lr 0.0001 | ms/batch 130.60954 | loss 2.30277 | ppl    10.002\n",
      "| epoch   1 step     9400 |   9400 batches | lr 0.0001 | ms/batch 128.81919 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step     9600 |   9600 batches | lr 0.0001 | ms/batch 128.59463 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step     9800 |   9800 batches | lr 0.0001 | ms/batch 128.42089 | loss 2.30272 | ppl    10.001\n",
      "| epoch   1 step    10000 |  10000 batches | lr 0.0001 | ms/batch 126.11711 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    10200 |  10200 batches | lr 0.0001 | ms/batch 126.56035 | loss 2.30275 | ppl    10.002\n",
      "| epoch   1 step    10400 |  10400 batches | lr 0.0001 | ms/batch 126.23675 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    10600 |  10600 batches | lr 0.0001 | ms/batch 125.93514 | loss 2.30275 | ppl    10.002\n",
      "| epoch   1 step    10800 |  10800 batches | lr 0.0001 | ms/batch 127.76937 | loss 2.30273 | ppl    10.001\n",
      "| epoch   1 step    11000 |  11000 batches | lr 0.0001 | ms/batch 126.19534 | loss 2.30278 | ppl    10.002\n",
      "| epoch   1 step    11200 |  11200 batches | lr 0.0001 | ms/batch 127.24652 | loss 2.30273 | ppl    10.001\n",
      "| epoch   1 step    11400 |  11400 batches | lr 0.0001 | ms/batch 131.45352 | loss 2.30273 | ppl    10.001\n",
      "| epoch   1 step    11600 |  11600 batches | lr 0.0001 | ms/batch 134.09149 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    11800 |  11800 batches | lr 0.0001 | ms/batch 133.76364 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    12000 |  12000 batches | lr 0.0001 | ms/batch 131.61525 | loss 2.30273 | ppl    10.001\n",
      "|\n",
      "Source: [ 9  7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3\n",
      "  1  3  3  4 11  6  5  9  9  8  3  7  6  2  9 10 10 11  4 11  3  3 10  7]\n",
      "Target: [ 7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3  1\n",
      "  3  3  4 11  6  5  9  9  8  3  7  6  2  9 10 10 11  4 11  3  3 10  7  9]\n",
      "Teacher forcing: acc:0.09768229166666667\n",
      "Preds:  [7 7 7 3 3 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 7 3 3 7 3 3 3 7 7 7 7 7 3 3 7 7\n",
      " 7 7 7 7 7 7 7 3 3 7 7]\n",
      "\n",
      "No teacher forcing: acc:0.09755208333333333\n",
      "Preds:  [ 7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3  1\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step    12000 | time: 524.32s | valid loss 2.30259 | valid ppl   10.0001 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    12200 |  12200 batches | lr 0.0001 | ms/batch 187.96978 | loss 2.30272 | ppl    10.001\n",
      "| epoch   1 step    12400 |  12400 batches | lr 0.0001 | ms/batch 133.11044 | loss 2.30272 | ppl    10.001\n",
      "| epoch   1 step    12600 |  12600 batches | lr 0.0001 | ms/batch 132.04441 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    12800 |  12800 batches | lr 0.0001 | ms/batch 131.17081 | loss 2.30272 | ppl    10.001\n",
      "| epoch   1 step    13000 |  13000 batches | lr 0.0001 | ms/batch 132.85988 | loss 2.30270 | ppl    10.001\n",
      "| epoch   1 step    13200 |  13200 batches | lr 0.0001 | ms/batch 131.34341 | loss 2.30270 | ppl    10.001\n",
      "| epoch   1 step    13400 |  13400 batches | lr 0.0001 | ms/batch 130.26159 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    13600 |  13600 batches | lr 0.0001 | ms/batch 131.35868 | loss 2.30273 | ppl    10.001\n",
      "| epoch   1 step    13800 |  13800 batches | lr 0.0001 | ms/batch 131.86641 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    14000 |  14000 batches | lr 0.0001 | ms/batch 130.82924 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    14200 |  14200 batches | lr 0.0001 | ms/batch 128.29205 | loss 2.30275 | ppl    10.002\n",
      "| epoch   1 step    14400 |  14400 batches | lr 0.0001 | ms/batch 128.75239 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    14600 |  14600 batches | lr 0.0001 | ms/batch 126.47329 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    14800 |  14800 batches | lr 0.0001 | ms/batch 130.06788 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    15000 |  15000 batches | lr 0.0001 | ms/batch 124.78567 | loss 2.30270 | ppl    10.001\n",
      "| epoch   1 step    15200 |  15200 batches | lr 0.0001 | ms/batch 125.87473 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    15400 |  15400 batches | lr 0.0001 | ms/batch 130.15981 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    15600 |  15600 batches | lr 0.0001 | ms/batch 129.48048 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    15800 |  15800 batches | lr 0.0001 | ms/batch 131.53962 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    16000 |  16000 batches | lr 0.0001 | ms/batch 130.89061 | loss 2.30273 | ppl    10.001\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9\n",
      "  1  9  6  4  6  2  6 10 10 10  8  8  3  9  7 11  2 11  3  3  9  4  6  3]\n",
      "Target: [ 3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9  1\n",
      "  9  6  4  6  2  6 10 10 10  8  8  3  9  7 11  2 11  3  3  9  4  6  3  7]\n",
      "Teacher forcing: acc:0.09909119897959184\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.09909119897959184\n",
      "Preds:  [ 3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   4 at step    16000 | time: 531.62s | valid loss 2.30268 | valid ppl   10.0009 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    16200 |  16200 batches | lr 0.0001 | ms/batch 187.88301 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    16400 |  16400 batches | lr 0.0001 | ms/batch 130.94799 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    16600 |  16600 batches | lr 0.0001 | ms/batch 123.89818 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    16800 |  16800 batches | lr 0.0001 | ms/batch 125.61976 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    17000 |  17000 batches | lr 0.0001 | ms/batch 125.68785 | loss 2.30270 | ppl    10.001\n",
      "| epoch   1 step    17200 |  17200 batches | lr 0.0001 | ms/batch 127.09979 | loss 2.30273 | ppl    10.001\n",
      "| epoch   1 step    17400 |  17400 batches | lr 0.0001 | ms/batch 128.04544 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    17600 |  17600 batches | lr 0.0001 | ms/batch 128.65855 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    17800 |  17800 batches | lr 0.0001 | ms/batch 128.04106 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    18000 |  18000 batches | lr 0.0001 | ms/batch 129.81164 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    18200 |  18200 batches | lr 0.0001 | ms/batch 128.58640 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    18400 |  18400 batches | lr 0.0001 | ms/batch 130.12157 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    18600 |  18600 batches | lr 0.0001 | ms/batch 127.53345 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    18800 |  18800 batches | lr 0.0001 | ms/batch 127.23582 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    19000 |  19000 batches | lr 0.0001 | ms/batch 127.88163 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    19200 |  19200 batches | lr 0.0001 | ms/batch 130.39953 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    19400 |  19400 batches | lr 0.0001 | ms/batch 128.27046 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    19600 |  19600 batches | lr 0.0001 | ms/batch 128.28256 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    19800 |  19800 batches | lr 0.0001 | ms/batch 127.83628 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    20000 |  20000 batches | lr 0.0001 | ms/batch 129.05178 | loss 2.30268 | ppl    10.001\n",
      "|\n",
      "Source: [ 2  3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10\n",
      "  1 10  8  3  2  6  6  3  3  6  5 10  8  4  2  3  4  6  2 11  7  9  9  3]\n",
      "Target: [ 3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10  1\n",
      " 10  8  3  2  6  6  3  3  6  5 10  8  4  2  3  4  6  2 11  7  9  9  3  2]\n",
      "Teacher forcing: acc:0.101328125\n",
      "Preds:  [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "No teacher forcing: acc:0.101328125\n",
      "Preds:  [ 3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10  1\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   5 at step    20000 | time: 524.09s | valid loss 2.30264 | valid ppl   10.0005 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    20200 |  20200 batches | lr 0.0001 | ms/batch 181.63580 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    20400 |  20400 batches | lr 0.0001 | ms/batch 129.43729 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    20600 |  20600 batches | lr 0.0001 | ms/batch 128.93941 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    20800 |  20800 batches | lr 0.0001 | ms/batch 127.83057 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    21000 |  21000 batches | lr 0.0001 | ms/batch 125.37827 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    21200 |  21200 batches | lr 0.0001 | ms/batch 126.94692 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    21400 |  21400 batches | lr 0.0001 | ms/batch 125.30298 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    21600 |  21600 batches | lr 0.0001 | ms/batch 126.69588 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    21800 |  21800 batches | lr 0.0001 | ms/batch 129.10473 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    22000 |  22000 batches | lr 0.0001 | ms/batch 130.25279 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    22200 |  22200 batches | lr 0.0001 | ms/batch 130.48110 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    22400 |  22400 batches | lr 0.0001 | ms/batch 130.05194 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    22600 |  22600 batches | lr 0.0001 | ms/batch 129.63158 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    22800 |  22800 batches | lr 0.0001 | ms/batch 126.68899 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    23000 |  23000 batches | lr 0.0001 | ms/batch 129.52621 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    23200 |  23200 batches | lr 0.0001 | ms/batch 129.08311 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    23400 |  23400 batches | lr 0.0001 | ms/batch 127.93813 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    23600 |  23600 batches | lr 0.0001 | ms/batch 129.17594 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    23800 |  23800 batches | lr 0.0001 | ms/batch 131.08491 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    24000 |  24000 batches | lr 0.0001 | ms/batch 128.09464 | loss 2.30267 | ppl    10.001\n",
      "|\n",
      "Source: [ 2  3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3\n",
      "  1  3  3 10  2  5  6  4  2 11 10  6  6  6  7  6  4  7  8  7  2  2 11  3]\n",
      "Target: [ 3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3  1\n",
      "  3  3 10  2  5  6  4  2 11 10  6  6  6  7  6  4  7  8  7  2  2 11  3  2]\n",
      "Teacher forcing: acc:0.10182291666666667\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10182291666666667\n",
      "Preds:  [ 3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   6 at step    24000 | time: 524.37s | valid loss 2.30259 | valid ppl   10.0000 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    24200 |  24200 batches | lr 0.0001 | ms/batch 183.73117 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    24400 |  24400 batches | lr 0.0001 | ms/batch 132.19809 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    24600 |  24600 batches | lr 0.0001 | ms/batch 131.83581 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    24800 |  24800 batches | lr 0.0001 | ms/batch 131.88117 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    25000 |  25000 batches | lr 0.0001 | ms/batch 134.35396 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    25200 |  25200 batches | lr 0.0001 | ms/batch 141.29957 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    25400 |  25400 batches | lr 0.0001 | ms/batch 132.37333 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    25600 |  25600 batches | lr 0.0001 | ms/batch 130.38521 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    25800 |  25800 batches | lr 0.0001 | ms/batch 130.80851 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    26000 |  26000 batches | lr 0.0001 | ms/batch 127.77976 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    26200 |  26200 batches | lr 0.0001 | ms/batch 128.30059 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    26400 |  26400 batches | lr 0.0001 | ms/batch 128.71742 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    26600 |  26600 batches | lr 0.0001 | ms/batch 126.43668 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    26800 |  26800 batches | lr 0.0001 | ms/batch 128.55314 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    27000 |  27000 batches | lr 0.0001 | ms/batch 130.44214 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    27200 |  27200 batches | lr 0.0001 | ms/batch 129.95399 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    27400 |  27400 batches | lr 0.0001 | ms/batch 130.96336 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    27600 |  27600 batches | lr 0.0001 | ms/batch 128.84665 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    27800 |  27800 batches | lr 0.0001 | ms/batch 128.59845 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    28000 |  28000 batches | lr 0.0001 | ms/batch 131.92541 | loss 2.30265 | ppl    10.001\n",
      "maslina\n",
      "|\n",
      "Source: [ 9 10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7\n",
      "  1  7 11  3  6 10 11  5  3  9  2 11  9  7  4 10  6  8  4  4  3  4  9 10]\n",
      "Target: [10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7  1\n",
      "  7 11  3  6 10 11  5  3  9  2 11  9  7  4 10  6  8  4  4  3  4  9 10  9]\n",
      "Teacher forcing: acc:0.0975233843537415\n",
      "Preds:  [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "No teacher forcing: acc:0.0975233843537415\n",
      "Preds:  [10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7  1\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   7 at step    28000 | time: 533.74s | valid loss 2.30261 | valid ppl   10.0003 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    28200 |  28200 batches | lr 0.0001 | ms/batch 184.72668 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    28400 |  28400 batches | lr 0.0001 | ms/batch 130.26416 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    28600 |  28600 batches | lr 0.0001 | ms/batch 129.94893 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    28800 |  28800 batches | lr 0.0001 | ms/batch 130.12892 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    29000 |  29000 batches | lr 0.0001 | ms/batch 128.70300 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    29200 |  29200 batches | lr 0.0001 | ms/batch 128.94249 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    29400 |  29400 batches | lr 0.0001 | ms/batch 128.38332 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    29600 |  29600 batches | lr 0.0001 | ms/batch 127.98361 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    29800 |  29800 batches | lr 0.0001 | ms/batch 133.04803 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    30000 |  30000 batches | lr 0.0001 | ms/batch 133.48778 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    30200 |  30200 batches | lr 0.0001 | ms/batch 127.39157 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    30400 |  30400 batches | lr 0.0001 | ms/batch 128.02711 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    30600 |  30600 batches | lr 0.0001 | ms/batch 131.32436 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    30800 |  30800 batches | lr 0.0001 | ms/batch 136.40407 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    31000 |  31000 batches | lr 0.0001 | ms/batch 139.86164 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    31200 |  31200 batches | lr 0.0001 | ms/batch 132.61384 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    31400 |  31400 batches | lr 0.0001 | ms/batch 131.56904 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    31600 |  31600 batches | lr 0.0001 | ms/batch 130.72171 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    31800 |  31800 batches | lr 0.0001 | ms/batch 130.04958 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    32000 |  32000 batches | lr 0.0001 | ms/batch 129.36322 | loss 2.30265 | ppl    10.001\n",
      "|\n",
      "Source: [ 2  5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8\n",
      "  1  8  2  8  3  3 11  4  4 10  5  2  3  7  3  3  5  7  6  6  8  6  5  5]\n",
      "Target: [ 5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8  1\n",
      "  8  2  8  3  3 11  4  4 10  5  2  3  7  3  3  5  7  6  6  8  6  5  5  2]\n",
      "Teacher forcing: acc:0.101015625\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.101015625\n",
      "Preds:  [ 5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   8 at step    32000 | time: 534.65s | valid loss 2.30271 | valid ppl   10.0013 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    32200 |  32200 batches | lr 0.0001 | ms/batch 181.11889 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    32400 |  32400 batches | lr 0.0001 | ms/batch 128.81476 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    32600 |  32600 batches | lr 0.0001 | ms/batch 126.70300 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    32800 |  32800 batches | lr 0.0001 | ms/batch 127.14545 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    33000 |  33000 batches | lr 0.0001 | ms/batch 128.64188 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    33200 |  33200 batches | lr 0.0001 | ms/batch 128.63803 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    33400 |  33400 batches | lr 0.0001 | ms/batch 128.52296 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    33600 |  33600 batches | lr 0.0001 | ms/batch 128.44991 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    33800 |  33800 batches | lr 0.0001 | ms/batch 127.03017 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    34000 |  34000 batches | lr 0.0001 | ms/batch 128.36911 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    34200 |  34200 batches | lr 0.0001 | ms/batch 127.15319 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    34400 |  34400 batches | lr 0.0001 | ms/batch 126.98208 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    34600 |  34600 batches | lr 0.0001 | ms/batch 131.84396 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    34800 |  34800 batches | lr 0.0001 | ms/batch 135.11412 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    35000 |  35000 batches | lr 0.0001 | ms/batch 135.49038 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    35200 |  35200 batches | lr 0.0001 | ms/batch 135.59290 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    35400 |  35400 batches | lr 0.0001 | ms/batch 131.83553 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    35600 |  35600 batches | lr 0.0001 | ms/batch 127.85073 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    35800 |  35800 batches | lr 0.0001 | ms/batch 126.81034 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    36000 |  36000 batches | lr 0.0001 | ms/batch 126.20680 | loss 2.30265 | ppl    10.001\n",
      "maslina\n",
      "|\n",
      "Source: [ 5  3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3\n",
      "  1  3  6  5  9  4  5 11  4  7  7  2  4  3  5  9 11  7  8 11 11 10 10  3]\n",
      "Target: [ 3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3  1\n",
      "  3  6  5  9  4  5 11  4  7  7  2  4  3  5  9 11  7  8 11 11 10 10  3  5]\n",
      "Teacher forcing: acc:0.10018069727891156\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.10018069727891156\n",
      "Preds:  [ 3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   9 at step    36000 | time: 527.60s | valid loss 2.30269 | valid ppl   10.0011 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    36200 |  36200 batches | lr 0.0001 | ms/batch 181.56816 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    36400 |  36400 batches | lr 0.0001 | ms/batch 127.59920 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    36600 |  36600 batches | lr 0.0001 | ms/batch 126.30257 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    36800 |  36800 batches | lr 0.0001 | ms/batch 127.85511 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    37000 |  37000 batches | lr 0.0001 | ms/batch 129.69430 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    37200 |  37200 batches | lr 0.0001 | ms/batch 135.24222 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    37400 |  37400 batches | lr 0.0001 | ms/batch 134.26455 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    37600 |  37600 batches | lr 0.0001 | ms/batch 130.88674 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    37800 |  37800 batches | lr 0.0001 | ms/batch 130.54608 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    38000 |  38000 batches | lr 0.0001 | ms/batch 131.46619 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    38200 |  38200 batches | lr 0.0001 | ms/batch 130.77182 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    38400 |  38400 batches | lr 0.0001 | ms/batch 130.08369 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    38600 |  38600 batches | lr 0.0001 | ms/batch 129.59961 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    38800 |  38800 batches | lr 0.0001 | ms/batch 126.26385 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    39000 |  39000 batches | lr 0.0001 | ms/batch 128.28389 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    39200 |  39200 batches | lr 0.0001 | ms/batch 126.74286 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    39400 |  39400 batches | lr 0.0001 | ms/batch 126.90259 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    39600 |  39600 batches | lr 0.0001 | ms/batch 128.68022 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    39800 |  39800 batches | lr 0.0001 | ms/batch 127.36619 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    40000 |  40000 batches | lr 0.0001 | ms/batch 126.73341 | loss 2.30264 | ppl    10.001\n",
      "|\n",
      "Source: [ 4 11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2\n",
      "  1  2  3  8  7  8 10  7 11  7  2  5  6 10  8  2 11  9  3  8 11  7  9 11]\n",
      "Target: [11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2  1\n",
      "  2  3  8  7  8 10  7 11  7  2  5  6 10  8  2 11  9  3  8 11  7  9 11  4]\n",
      "Teacher forcing: acc:0.09830729166666667\n",
      "Preds:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9]\n",
      "\n",
      "No teacher forcing: acc:0.09830729166666667\n",
      "Preds:  [11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2  1\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  10 at step    40000 | time: 527.60s | valid loss 2.30263 | valid ppl   10.0004 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    40200 |  40200 batches | lr 0.0001 | ms/batch 183.21630 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    40400 |  40400 batches | lr 0.0001 | ms/batch 126.59114 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    40600 |  40600 batches | lr 0.0001 | ms/batch 133.42525 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    40800 |  40800 batches | lr 0.0001 | ms/batch 135.23657 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    41000 |  41000 batches | lr 0.0001 | ms/batch 130.01466 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    41200 |  41200 batches | lr 0.0001 | ms/batch 127.02637 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    41400 |  41400 batches | lr 0.0001 | ms/batch 127.12914 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    41600 |  41600 batches | lr 0.0001 | ms/batch 126.35742 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    41800 |  41800 batches | lr 0.0001 | ms/batch 128.27537 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    42000 |  42000 batches | lr 0.0001 | ms/batch 127.52951 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    42200 |  42200 batches | lr 0.0001 | ms/batch 129.56507 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    42400 |  42400 batches | lr 0.0001 | ms/batch 128.54395 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    42600 |  42600 batches | lr 0.0001 | ms/batch 127.34011 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    42800 |  42800 batches | lr 0.0001 | ms/batch 130.79775 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    43000 |  43000 batches | lr 0.0001 | ms/batch 130.26208 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    43200 |  43200 batches | lr 0.0001 | ms/batch 133.39796 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    43400 |  43400 batches | lr 0.0001 | ms/batch 132.74591 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    43600 |  43600 batches | lr 0.0001 | ms/batch 132.00146 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    43800 |  43800 batches | lr 0.0001 | ms/batch 132.60294 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    44000 |  44000 batches | lr 0.0001 | ms/batch 132.77777 | loss 2.30263 | ppl    10.000\n",
      "|\n",
      "Source: [ 8  3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9\n",
      "  1  9  7  9 10  3  7  9  6  4  4  4  8  8  4  5  9  7  8  6  5  8  6  3]\n",
      "Target: [ 3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9  1\n",
      "  9  7  9 10  3  7  9  6  4  4  4  8  8  4  5  9  7  8  6  5  8  6  3  8]\n",
      "Teacher forcing: acc:0.098671875\n",
      "Preds:  [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "No teacher forcing: acc:0.098671875\n",
      "Preds:  [ 3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9  1\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  11 at step    44000 | time: 531.33s | valid loss 2.30265 | valid ppl   10.0006 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    44200 |  44200 batches | lr 0.0001 | ms/batch 187.64107 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    44400 |  44400 batches | lr 0.0001 | ms/batch 132.09430 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    44600 |  44600 batches | lr 0.0001 | ms/batch 131.75910 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    44800 |  44800 batches | lr 0.0001 | ms/batch 127.77351 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    45000 |  45000 batches | lr 0.0001 | ms/batch 131.90550 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    45200 |  45200 batches | lr 0.0001 | ms/batch 131.53770 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    45400 |  45400 batches | lr 0.0001 | ms/batch 130.23267 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    45600 |  45600 batches | lr 0.0001 | ms/batch 129.82379 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    45800 |  45800 batches | lr 0.0001 | ms/batch 128.78454 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    46000 |  46000 batches | lr 0.0001 | ms/batch 127.71000 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    46200 |  46200 batches | lr 0.0001 | ms/batch 127.69619 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    46400 |  46400 batches | lr 0.0001 | ms/batch 127.70821 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    46600 |  46600 batches | lr 0.0001 | ms/batch 129.04065 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    46800 |  46800 batches | lr 0.0001 | ms/batch 127.47504 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    47000 |  47000 batches | lr 0.0001 | ms/batch 129.11994 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    47200 |  47200 batches | lr 0.0001 | ms/batch 128.66585 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    47400 |  47400 batches | lr 0.0001 | ms/batch 129.55376 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    47600 |  47600 batches | lr 0.0001 | ms/batch 129.34219 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    47800 |  47800 batches | lr 0.0001 | ms/batch 129.47803 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    48000 |  48000 batches | lr 0.0001 | ms/batch 128.16003 | loss 2.30262 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [11 10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4\n",
      "  1  4 10  4  4  3 11  9  7  8  9  7 10  7  8  3  3  6 10 11  7  7  5 10]\n",
      "Target: [10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4  1\n",
      "  4 10  4  4  3 11  9  7  8  9  7 10  7  8  3  3  6 10 11  7  7  5 10 11]\n",
      "Teacher forcing: acc:0.10196109693877552\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10196109693877552\n",
      "Preds:  [10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  12 at step    48000 | time: 528.38s | valid loss 2.30261 | valid ppl   10.0002 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    48200 |  48200 batches | lr 0.0001 | ms/batch 180.94765 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    48400 |  48400 batches | lr 0.0001 | ms/batch 127.09933 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    48600 |  48600 batches | lr 0.0001 | ms/batch 126.97101 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    48800 |  48800 batches | lr 0.0001 | ms/batch 128.23403 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    49000 |  49000 batches | lr 0.0001 | ms/batch 127.12483 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    49200 |  49200 batches | lr 0.0001 | ms/batch 128.61218 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    49400 |  49400 batches | lr 0.0001 | ms/batch 128.55993 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    49600 |  49600 batches | lr 0.0001 | ms/batch 125.38700 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    49800 |  49800 batches | lr 0.0001 | ms/batch 126.47130 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    50000 |  50000 batches | lr 0.0001 | ms/batch 126.46817 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    50200 |  50200 batches | lr 0.0001 | ms/batch 125.90022 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    50400 |  50400 batches | lr 0.0001 | ms/batch 128.51585 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    50600 |  50600 batches | lr 0.0001 | ms/batch 126.37338 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    50800 |  50800 batches | lr 0.0001 | ms/batch 126.09781 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    51000 |  51000 batches | lr 0.0001 | ms/batch 127.44074 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    51200 |  51200 batches | lr 0.0001 | ms/batch 127.56427 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    51400 |  51400 batches | lr 0.0001 | ms/batch 127.70987 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    51600 |  51600 batches | lr 0.0001 | ms/batch 127.30484 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    51800 |  51800 batches | lr 0.0001 | ms/batch 129.37340 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    52000 |  52000 batches | lr 0.0001 | ms/batch 126.80470 | loss 2.30262 | ppl    10.000\n",
      "|\n",
      "Source: [ 4  7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10\n",
      "  1 10 10  7  6  8 10  9  4  6 11 11 11  9  2  4  2  6 10 11  8  6  3  7]\n",
      "Target: [ 7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10  1\n",
      " 10 10  7  6  8 10  9  4  6 11 11 11  9  2  4  2  6 10 11  8  6  3  7  4]\n",
      "Teacher forcing: acc:0.103203125\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.103203125\n",
      "Preds:  [ 7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  13 at step    52000 | time: 520.03s | valid loss 2.30254 | valid ppl    9.9995 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    52200 |  52200 batches | lr 0.0001 | ms/batch 181.23620 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    52400 |  52400 batches | lr 0.0001 | ms/batch 127.51441 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    52600 |  52600 batches | lr 0.0001 | ms/batch 129.41672 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    52800 |  52800 batches | lr 0.0001 | ms/batch 128.39865 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    53000 |  53000 batches | lr 0.0001 | ms/batch 127.90976 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    53200 |  53200 batches | lr 0.0001 | ms/batch 129.39470 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    53400 |  53400 batches | lr 0.0001 | ms/batch 128.58427 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    53600 |  53600 batches | lr 0.0001 | ms/batch 127.90971 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    53800 |  53800 batches | lr 0.0001 | ms/batch 126.92246 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    54000 |  54000 batches | lr 0.0001 | ms/batch 128.30945 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    54200 |  54200 batches | lr 0.0001 | ms/batch 126.61651 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    54400 |  54400 batches | lr 0.0001 | ms/batch 128.53039 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    54600 |  54600 batches | lr 0.0001 | ms/batch 128.35516 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    54800 |  54800 batches | lr 0.0001 | ms/batch 125.84976 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    55000 |  55000 batches | lr 0.0001 | ms/batch 128.53127 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    55200 |  55200 batches | lr 0.0001 | ms/batch 126.89210 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    55400 |  55400 batches | lr 0.0001 | ms/batch 127.59291 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    55600 |  55600 batches | lr 0.0001 | ms/batch 128.17880 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    55800 |  55800 batches | lr 0.0001 | ms/batch 127.03927 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    56000 |  56000 batches | lr 0.0001 | ms/batch 128.07535 | loss 2.30258 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  7  6  9  6  4  3  2  8  5  3  8  7  7  4  8  5 11  2  3  8  4  9 11\n",
      "  1 11  9  4  8  3  2 11  5  8  4  7  7  8  3  5  8  2  3  4  6  9  6  7]\n",
      "Target: [ 7  6  9  6  4  3  2  8  5  3  8  7  7  4  8  5 11  2  3  8  4  9 11  1\n",
      " 11  9  4  8  3  2 11  5  8  4  7  7  8  3  5  8  2  3  4  6  9  6  7  7]\n",
      "Teacher forcing: acc:0.09837372448979592\n",
      "Preds:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9]\n",
      "\n",
      "No teacher forcing: acc:0.09837372448979592\n",
      "Preds:  [ 7  6  9  6  4  3  2  8  5  3  8  7  7  4  8  5 11  2  3  8  4  9 11  1\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  14 at step    56000 | time: 522.05s | valid loss 2.30259 | valid ppl   10.0001 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    56200 |  56200 batches | lr 0.0001 | ms/batch 181.38677 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    56400 |  56400 batches | lr 0.0001 | ms/batch 126.77642 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    56600 |  56600 batches | lr 0.0001 | ms/batch 126.88202 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    56800 |  56800 batches | lr 0.0001 | ms/batch 127.26515 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    57000 |  57000 batches | lr 0.0001 | ms/batch 127.45953 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    57200 |  57200 batches | lr 0.0001 | ms/batch 127.60158 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    57400 |  57400 batches | lr 0.0001 | ms/batch 127.57749 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    57600 |  57600 batches | lr 0.0001 | ms/batch 128.55865 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    57800 |  57800 batches | lr 0.0001 | ms/batch 128.69638 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    58000 |  58000 batches | lr 0.0001 | ms/batch 127.45728 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    58200 |  58200 batches | lr 0.0001 | ms/batch 126.28380 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    58400 |  58400 batches | lr 0.0001 | ms/batch 126.71158 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    58600 |  58600 batches | lr 0.0001 | ms/batch 129.34765 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    58800 |  58800 batches | lr 0.0001 | ms/batch 128.62678 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    59000 |  59000 batches | lr 0.0001 | ms/batch 126.99778 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    59200 |  59200 batches | lr 0.0001 | ms/batch 127.24848 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    59400 |  59400 batches | lr 0.0001 | ms/batch 127.93014 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    59600 |  59600 batches | lr 0.0001 | ms/batch 128.23055 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    59800 |  59800 batches | lr 0.0001 | ms/batch 126.81514 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    60000 |  60000 batches | lr 0.0001 | ms/batch 126.61474 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [11  8  3  2  3  9 10  5  9 10  6 10 11  9  6 11  2  9  7  5  7  5  3 11\n",
      "  1 11  3  5  7  5  7  9  2 11  6  9 11 10  6 10  9  5 10  9  3  2  3  8]\n",
      "Target: [ 8  3  2  3  9 10  5  9 10  6 10 11  9  6 11  2  9  7  5  7  5  3 11  1\n",
      " 11  3  5  7  5  7  9  2 11  6  9 11 10  6 10  9  5 10  9  3  2  3  8 11]\n",
      "Teacher forcing: acc:0.09791666666666667\n",
      "Preds:  [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7]\n",
      "\n",
      "No teacher forcing: acc:0.09791666666666667\n",
      "Preds:  [ 8  3  2  3  9 10  5  9 10  6 10 11  9  6 11  2  9  7  5  7  5  3 11  1\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  15 at step    60000 | time: 521.20s | valid loss 2.30270 | valid ppl   10.0011 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    60200 |  60200 batches | lr 0.0001 | ms/batch 182.58654 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    60400 |  60400 batches | lr 0.0001 | ms/batch 128.44470 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    60600 |  60600 batches | lr 0.0001 | ms/batch 128.68390 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    60800 |  60800 batches | lr 0.0001 | ms/batch 129.49815 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    61000 |  61000 batches | lr 0.0001 | ms/batch 128.76617 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    61200 |  61200 batches | lr 0.0001 | ms/batch 132.15981 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    61400 |  61400 batches | lr 0.0001 | ms/batch 131.69466 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    61600 |  61600 batches | lr 0.0001 | ms/batch 129.93171 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    61800 |  61800 batches | lr 0.0001 | ms/batch 132.76967 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    62000 |  62000 batches | lr 0.0001 | ms/batch 131.67353 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    62200 |  62200 batches | lr 0.0001 | ms/batch 131.15547 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    62400 |  62400 batches | lr 0.0001 | ms/batch 127.57874 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    62600 |  62600 batches | lr 0.0001 | ms/batch 127.98003 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    62800 |  62800 batches | lr 0.0001 | ms/batch 127.97218 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    63000 |  63000 batches | lr 0.0001 | ms/batch 126.89361 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    63200 |  63200 batches | lr 0.0001 | ms/batch 129.24651 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    63400 |  63400 batches | lr 0.0001 | ms/batch 127.96838 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    63600 |  63600 batches | lr 0.0001 | ms/batch 128.17909 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    63800 |  63800 batches | lr 0.0001 | ms/batch 127.25956 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    64000 |  64000 batches | lr 0.0001 | ms/batch 127.03825 | loss 2.30261 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 8  6  9  4 11  4  3  8  4 11 10  6  2  6  6  5  4  4  2 11  4  8  5  3\n",
      "  1  3  5  8  4 11  2  4  4  5  6  6  2  6 10 11  4  8  3  4 11  4  9  6]\n",
      "Target: [ 6  9  4 11  4  3  8  4 11 10  6  2  6  6  5  4  4  2 11  4  8  5  3  1\n",
      "  3  5  8  4 11  2  4  4  5  6  6  2  6 10 11  4  8  3  4 11  4  9  6  8]\n",
      "Teacher forcing: acc:0.10257227891156463\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10257227891156463\n",
      "Preds:  [ 6  9  4 11  4  3  8  4 11 10  6  2  6  6  5  4  4  2 11  4  8  5  3  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  16 at step    64000 | time: 527.24s | valid loss 2.30255 | valid ppl    9.9996 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    64200 |  64200 batches | lr 0.0001 | ms/batch 179.59126 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    64400 |  64400 batches | lr 0.0001 | ms/batch 129.15681 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    64600 |  64600 batches | lr 0.0001 | ms/batch 128.74899 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    64800 |  64800 batches | lr 0.0001 | ms/batch 129.48727 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    65000 |  65000 batches | lr 0.0001 | ms/batch 129.27136 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    65200 |  65200 batches | lr 0.0001 | ms/batch 130.19543 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    65400 |  65400 batches | lr 0.0001 | ms/batch 131.19868 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    65600 |  65600 batches | lr 0.0001 | ms/batch 129.25839 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    65800 |  65800 batches | lr 0.0001 | ms/batch 128.33360 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    66000 |  66000 batches | lr 0.0001 | ms/batch 129.75068 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    66200 |  66200 batches | lr 0.0001 | ms/batch 130.78442 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    66400 |  66400 batches | lr 0.0001 | ms/batch 131.20740 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    66600 |  66600 batches | lr 0.0001 | ms/batch 128.63949 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    66800 |  66800 batches | lr 0.0001 | ms/batch 128.35749 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    67000 |  67000 batches | lr 0.0001 | ms/batch 126.47362 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    67200 |  67200 batches | lr 0.0001 | ms/batch 127.87562 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    67400 |  67400 batches | lr 0.0001 | ms/batch 127.92661 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    67600 |  67600 batches | lr 0.0001 | ms/batch 127.83231 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    67800 |  67800 batches | lr 0.0001 | ms/batch 128.29594 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    68000 |  68000 batches | lr 0.0001 | ms/batch 127.84535 | loss 2.30261 | ppl    10.000\n",
      "|\n",
      "Source: [10 10  2 11  7  2  7 10  5 10  3  7  2  9 10 11  5  4  5 10  5  8 10  6\n",
      "  1  6 10  8  5 10  5  4  5 11 10  9  2  7  3 10  5 10  7  2  7 11  2 10]\n",
      "Target: [10  2 11  7  2  7 10  5 10  3  7  2  9 10 11  5  4  5 10  5  8 10  6  1\n",
      "  6 10  8  5 10  5  4  5 11 10  9  2  7  3 10  5 10  7  2  7 11  2 10 10]\n",
      "Teacher forcing: acc:0.09947916666666666\n",
      "Preds:  [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8]\n",
      "\n",
      "No teacher forcing: acc:0.09947916666666666\n",
      "Preds:  [10  2 11  7  2  7 10  5 10  3  7  2  9 10 11  5  4  5 10  5  8 10  6  1\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  17 at step    68000 | time: 526.08s | valid loss 2.30261 | valid ppl   10.0002 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    68200 |  68200 batches | lr 0.0001 | ms/batch 182.40095 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    68400 |  68400 batches | lr 0.0001 | ms/batch 127.76619 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    68600 |  68600 batches | lr 0.0001 | ms/batch 128.17393 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    68800 |  68800 batches | lr 0.0001 | ms/batch 127.84880 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    69000 |  69000 batches | lr 0.0001 | ms/batch 129.62010 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    69200 |  69200 batches | lr 0.0001 | ms/batch 125.64719 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    69400 |  69400 batches | lr 0.0001 | ms/batch 125.61628 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    69600 |  69600 batches | lr 0.0001 | ms/batch 128.46981 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    69800 |  69800 batches | lr 0.0001 | ms/batch 130.18239 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    70000 |  70000 batches | lr 0.0001 | ms/batch 126.60051 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    70200 |  70200 batches | lr 0.0001 | ms/batch 128.12643 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    70400 |  70400 batches | lr 0.0001 | ms/batch 126.49786 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    70600 |  70600 batches | lr 0.0001 | ms/batch 128.29968 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    70800 |  70800 batches | lr 0.0001 | ms/batch 126.15878 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    71000 |  71000 batches | lr 0.0001 | ms/batch 128.24747 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    71200 |  71200 batches | lr 0.0001 | ms/batch 130.64289 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    71400 |  71400 batches | lr 0.0001 | ms/batch 128.75337 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    71600 |  71600 batches | lr 0.0001 | ms/batch 128.30873 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    71800 |  71800 batches | lr 0.0001 | ms/batch 128.45042 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    72000 |  72000 batches | lr 0.0001 | ms/batch 129.34101 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 6 11  9 11  9  2  3  9 10  6 10  5  7  3  8  8  2  6 11 11 10 11  4  6\n",
      "  1  6  4 11 10 11 11  6  2  8  8  3  7  5 10  6 10  9  3  2  9 11  9 11]\n",
      "Target: [11  9 11  9  2  3  9 10  6 10  5  7  3  8  8  2  6 11 11 10 11  4  6  1\n",
      "  6  4 11 10 11 11  6  2  8  8  3  7  5 10  6 10  9  3  2  9 11  9 11  6]\n",
      "Teacher forcing: acc:0.09942708333333333\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09942708333333333\n",
      "Preds:  [11  9 11  9  2  3  9 10  6 10  5  7  3  8  8  2  6 11 11 10 11  4  6  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  18 at step    72000 | time: 523.16s | valid loss 2.30268 | valid ppl   10.0010 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    72200 |  72200 batches | lr 0.0001 | ms/batch 180.23814 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    72400 |  72400 batches | lr 0.0001 | ms/batch 126.54566 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    72600 |  72600 batches | lr 0.0001 | ms/batch 129.12691 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    72800 |  72800 batches | lr 0.0001 | ms/batch 128.94953 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    73000 |  73000 batches | lr 0.0001 | ms/batch 129.38493 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    73200 |  73200 batches | lr 0.0001 | ms/batch 126.11356 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    73400 |  73400 batches | lr 0.0001 | ms/batch 129.91999 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    73600 |  73600 batches | lr 0.0001 | ms/batch 130.48536 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    73800 |  73800 batches | lr 0.0001 | ms/batch 131.47954 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    74000 |  74000 batches | lr 0.0001 | ms/batch 131.27911 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    74200 |  74200 batches | lr 0.0001 | ms/batch 130.97088 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    74400 |  74400 batches | lr 0.0001 | ms/batch 133.71479 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    74600 |  74600 batches | lr 0.0001 | ms/batch 131.83379 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    74800 |  74800 batches | lr 0.0001 | ms/batch 128.23020 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    75000 |  75000 batches | lr 0.0001 | ms/batch 127.61216 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    75200 |  75200 batches | lr 0.0001 | ms/batch 128.18391 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    75400 |  75400 batches | lr 0.0001 | ms/batch 129.23793 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    75600 |  75600 batches | lr 0.0001 | ms/batch 128.17332 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    75800 |  75800 batches | lr 0.0001 | ms/batch 128.67291 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    76000 |  76000 batches | lr 0.0001 | ms/batch 129.74405 | loss 2.30260 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 8  2 10  3  4  3 10  8  8 11 10  6  3 11  9  8  9  3  4  7 11  5  4  6\n",
      "  1  6  4  5 11  7  4  3  9  8  9 11  3  6 10 11  8  8 10  3  4  3 10  2]\n",
      "Target: [ 2 10  3  4  3 10  8  8 11 10  6  3 11  9  8  9  3  4  7 11  5  4  6  1\n",
      "  6  4  5 11  7  4  3  9  8  9 11  3  6 10 11  8  8 10  3  4  3 10  2  8]\n",
      "Teacher forcing: acc:0.1015359268707483\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.1015359268707483\n",
      "Preds:  [ 2 10  3  4  3 10  8  8 11 10  6  3 11  9  8  9  3  4  7 11  5  4  6  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  19 at step    76000 | time: 527.68s | valid loss 2.30258 | valid ppl   10.0000 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    76200 |  76200 batches | lr 0.0001 | ms/batch 180.54577 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    76400 |  76400 batches | lr 0.0001 | ms/batch 126.12885 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    76600 |  76600 batches | lr 0.0001 | ms/batch 128.86642 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    76800 |  76800 batches | lr 0.0001 | ms/batch 126.98990 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    77000 |  77000 batches | lr 0.0001 | ms/batch 127.72225 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    77200 |  77200 batches | lr 0.0001 | ms/batch 127.29880 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    77400 |  77400 batches | lr 0.0001 | ms/batch 128.07654 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    77600 |  77600 batches | lr 0.0001 | ms/batch 126.75875 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    77800 |  77800 batches | lr 0.0001 | ms/batch 127.86909 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    78000 |  78000 batches | lr 0.0001 | ms/batch 127.56438 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    78200 |  78200 batches | lr 0.0001 | ms/batch 128.56963 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    78400 |  78400 batches | lr 0.0001 | ms/batch 129.51255 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    78600 |  78600 batches | lr 0.0001 | ms/batch 129.07055 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    78800 |  78800 batches | lr 0.0001 | ms/batch 129.35100 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    79000 |  79000 batches | lr 0.0001 | ms/batch 126.88820 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    79200 |  79200 batches | lr 0.0001 | ms/batch 125.96519 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    79400 |  79400 batches | lr 0.0001 | ms/batch 126.89399 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    79600 |  79600 batches | lr 0.0001 | ms/batch 128.12921 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    79800 |  79800 batches | lr 0.0001 | ms/batch 125.75734 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    80000 |  80000 batches | lr 0.0001 | ms/batch 129.12624 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [10  5 11 10  9  7 10  9  3 10 11  4  2 10  8  4 10  2  9  7  3  7 11  4\n",
      "  1  4 11  7  3  7  9  2 10  4  8 10  2  4 11 10  3  9 10  7  9 10 11  5]\n",
      "Target: [ 5 11 10  9  7 10  9  3 10 11  4  2 10  8  4 10  2  9  7  3  7 11  4  1\n",
      "  4 11  7  3  7  9  2 10  4  8 10  2  4 11 10  3  9 10  7  9 10 11  5 10]\n",
      "Teacher forcing: acc:0.1\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.1\n",
      "Preds:  [ 5 11 10  9  7 10  9  3 10 11  4  2 10  8  4 10  2  9  7  3  7 11  4  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  20 at step    80000 | time: 521.66s | valid loss 2.30258 | valid ppl    9.9999 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    80200 |  80200 batches | lr 0.0001 | ms/batch 182.04136 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    80400 |  80400 batches | lr 0.0001 | ms/batch 127.58409 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    80600 |  80600 batches | lr 0.0001 | ms/batch 129.84223 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    80800 |  80800 batches | lr 0.0001 | ms/batch 129.57284 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    81000 |  81000 batches | lr 0.0001 | ms/batch 127.34942 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    81200 |  81200 batches | lr 0.0001 | ms/batch 127.12438 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    81400 |  81400 batches | lr 0.0001 | ms/batch 127.74576 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    81600 |  81600 batches | lr 0.0001 | ms/batch 128.24227 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    81800 |  81800 batches | lr 0.0001 | ms/batch 127.37331 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    82000 |  82000 batches | lr 0.0001 | ms/batch 127.01307 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    82200 |  82200 batches | lr 0.0001 | ms/batch 126.84399 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    82400 |  82400 batches | lr 0.0001 | ms/batch 128.75571 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    82600 |  82600 batches | lr 0.0001 | ms/batch 129.99309 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    82800 |  82800 batches | lr 0.0001 | ms/batch 130.68658 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    83000 |  83000 batches | lr 0.0001 | ms/batch 126.45582 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    83200 |  83200 batches | lr 0.0001 | ms/batch 127.25103 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    83400 |  83400 batches | lr 0.0001 | ms/batch 126.18549 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    83600 |  83600 batches | lr 0.0001 | ms/batch 129.12523 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    83800 |  83800 batches | lr 0.0001 | ms/batch 128.01469 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    84000 |  84000 batches | lr 0.0001 | ms/batch 128.77494 | loss 2.30260 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 9  8  6 11  6  5  3  9  8  2  7  8  7  9  9  9  7  4 10 11 11 10 10  6\n",
      "  1  6 10 10 11 11 10  4  7  9  9  9  7  8  7  2  8  9  3  5  6 11  6  8]\n",
      "Target: [ 8  6 11  6  5  3  9  8  2  7  8  7  9  9  9  7  4 10 11 11 10 10  6  1\n",
      "  6 10 10 11 11 10  4  7  9  9  9  7  8  7  2  8  9  3  5  6 11  6  8  9]\n",
      "Teacher forcing: acc:0.09802827380952381\n",
      "Preds:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9]\n",
      "\n",
      "No teacher forcing: acc:0.09802827380952381\n",
      "Preds:  [ 8  6 11  6  5  3  9  8  2  7  8  7  9  9  9  7  4 10 11 11 10 10  6  1\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  21 at step    84000 | time: 522.95s | valid loss 2.30262 | valid ppl   10.0003 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    84200 |  84200 batches | lr 0.0001 | ms/batch 178.55396 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    84400 |  84400 batches | lr 0.0001 | ms/batch 126.58123 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    84600 |  84600 batches | lr 0.0001 | ms/batch 127.66158 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    84800 |  84800 batches | lr 0.0001 | ms/batch 127.63739 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    85000 |  85000 batches | lr 0.0001 | ms/batch 127.39354 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    85200 |  85200 batches | lr 0.0001 | ms/batch 127.30397 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    85400 |  85400 batches | lr 0.0001 | ms/batch 129.34868 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    85600 |  85600 batches | lr 0.0001 | ms/batch 131.17073 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    85800 |  85800 batches | lr 0.0001 | ms/batch 137.43762 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    86000 |  86000 batches | lr 0.0001 | ms/batch 134.69739 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    86200 |  86200 batches | lr 0.0001 | ms/batch 133.11420 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    86400 |  86400 batches | lr 0.0001 | ms/batch 134.28038 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    86600 |  86600 batches | lr 0.0001 | ms/batch 131.96831 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    86800 |  86800 batches | lr 0.0001 | ms/batch 134.03629 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    87000 |  87000 batches | lr 0.0001 | ms/batch 132.31214 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    87200 |  87200 batches | lr 0.0001 | ms/batch 132.87005 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    87400 |  87400 batches | lr 0.0001 | ms/batch 132.28387 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    87600 |  87600 batches | lr 0.0001 | ms/batch 129.04134 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    87800 |  87800 batches | lr 0.0001 | ms/batch 128.82622 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    88000 |  88000 batches | lr 0.0001 | ms/batch 130.40524 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  8  8  9  7  3  4 10  4  7  5  4  3  7 11  3  6  8  6  4 10  4  3 11\n",
      "  1 11  3  4 10  4  6  8  6  3 11  7  3  4  5  7  4 10  4  3  7  9  8  8]\n",
      "Target: [ 8  8  9  7  3  4 10  4  7  5  4  3  7 11  3  6  8  6  4 10  4  3 11  1\n",
      " 11  3  4 10  4  6  8  6  3 11  7  3  4  5  7  4 10  4  3  7  9  8  8  5]\n",
      "Teacher forcing: acc:0.09997395833333333\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.09997395833333333\n",
      "Preds:  [ 8  8  9  7  3  4 10  4  7  5  4  3  7 11  3  6  8  6  4 10  4  3 11  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  22 at step    88000 | time: 533.91s | valid loss 2.30259 | valid ppl   10.0000 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    88200 |  88200 batches | lr 0.0001 | ms/batch 185.26558 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    88400 |  88400 batches | lr 0.0001 | ms/batch 126.48257 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    88600 |  88600 batches | lr 0.0001 | ms/batch 127.40396 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    88800 |  88800 batches | lr 0.0001 | ms/batch 128.67661 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    89000 |  89000 batches | lr 0.0001 | ms/batch 127.62262 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    89200 |  89200 batches | lr 0.0001 | ms/batch 127.61807 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    89400 |  89400 batches | lr 0.0001 | ms/batch 128.04678 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    89600 |  89600 batches | lr 0.0001 | ms/batch 126.25373 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    89800 |  89800 batches | lr 0.0001 | ms/batch 128.99568 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    90000 |  90000 batches | lr 0.0001 | ms/batch 129.13707 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    90200 |  90200 batches | lr 0.0001 | ms/batch 134.66098 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    90400 |  90400 batches | lr 0.0001 | ms/batch 134.83855 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    90600 |  90600 batches | lr 0.0001 | ms/batch 130.91782 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    90800 |  90800 batches | lr 0.0001 | ms/batch 130.93787 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    91000 |  91000 batches | lr 0.0001 | ms/batch 132.61678 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    91200 |  91200 batches | lr 0.0001 | ms/batch 131.49432 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    91400 |  91400 batches | lr 0.0001 | ms/batch 129.21869 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    91600 |  91600 batches | lr 0.0001 | ms/batch 132.74923 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    91800 |  91800 batches | lr 0.0001 | ms/batch 131.59415 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    92000 |  92000 batches | lr 0.0001 | ms/batch 132.16680 | loss 2.30261 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  5  3  6 10  9  9  6  9  5  6 11  3 11  6  3  3  3  4  3  4  2  9  6\n",
      "  1  6  9  2  4  3  4  3  3  3  6 11  3 11  6  5  9  6  9  9 10  6  3  5]\n",
      "Target: [ 5  3  6 10  9  9  6  9  5  6 11  3 11  6  3  3  3  4  3  4  2  9  6  1\n",
      "  6  9  2  4  3  4  3  3  3  6 11  3 11  6  5  9  6  9  9 10  6  3  5  5]\n",
      "Teacher forcing: acc:0.10127604166666666\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 6 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10127604166666666\n",
      "Preds:  [ 5  3  6 10  9  9  6  9  5  6 11  3 11  6  3  3  3  4  3  4  2  9  6  1\n",
      "  6  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  23 at step    92000 | time: 531.39s | valid loss 2.30256 | valid ppl    9.9998 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    92200 |  92200 batches | lr 0.0001 | ms/batch 188.48820 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    92400 |  92400 batches | lr 0.0001 | ms/batch 129.78657 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    92600 |  92600 batches | lr 0.0001 | ms/batch 129.84236 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    92800 |  92800 batches | lr 0.0001 | ms/batch 128.18473 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    93000 |  93000 batches | lr 0.0001 | ms/batch 132.28195 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    93200 |  93200 batches | lr 0.0001 | ms/batch 127.49001 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    93400 |  93400 batches | lr 0.0001 | ms/batch 128.59503 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    93600 |  93600 batches | lr 0.0001 | ms/batch 128.84723 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    93800 |  93800 batches | lr 0.0001 | ms/batch 125.44687 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    94000 |  94000 batches | lr 0.0001 | ms/batch 126.86308 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    94200 |  94200 batches | lr 0.0001 | ms/batch 127.15648 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    94400 |  94400 batches | lr 0.0001 | ms/batch 127.10114 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    94600 |  94600 batches | lr 0.0001 | ms/batch 126.50140 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    94800 |  94800 batches | lr 0.0001 | ms/batch 128.31946 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    95000 |  95000 batches | lr 0.0001 | ms/batch 127.36026 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    95200 |  95200 batches | lr 0.0001 | ms/batch 126.34923 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    95400 |  95400 batches | lr 0.0001 | ms/batch 127.38433 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    95600 |  95600 batches | lr 0.0001 | ms/batch 127.46900 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    95800 |  95800 batches | lr 0.0001 | ms/batch 125.85461 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    96000 |  96000 batches | lr 0.0001 | ms/batch 127.26117 | loss 2.30260 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [10  5  9 10  6  3  4  4  4  8  2  4  3  8 10  5  2 10  6  7  8  5  3  6\n",
      "  1  6  3  5  8  7  6 10  2  5 10  8  3  4  2  8  4  4  4  3  6 10  9  5]\n",
      "Target: [ 5  9 10  6  3  4  4  4  8  2  4  3  8 10  5  2 10  6  7  8  5  3  6  1\n",
      "  6  3  5  8  7  6 10  2  5 10  8  3  4  2  8  4  4  4  3  6 10  9  5 10]\n",
      "Teacher forcing: acc:0.10076530612244898\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 8 8 8 8 8 8 8 8 8 8 8 8 8 3 3 3 3 3 3 3 3 3 3 3 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8]\n",
      "\n",
      "No teacher forcing: acc:0.10076530612244898\n",
      "Preds:  [ 5  9 10  6  3  4  4  4  8  2  4  3  8 10  5  2 10  6  7  8  5  3  6  1\n",
      "  8  3  3  3  3  3  3  3  3  3  3  3  8  8  8  8  8  8  8  8  8  8  8  8]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  24 at step    96000 | time: 522.74s | valid loss 2.30259 | valid ppl   10.0000 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    96200 |  96200 batches | lr 0.0001 | ms/batch 180.67662 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    96400 |  96400 batches | lr 0.0001 | ms/batch 132.17631 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    96600 |  96600 batches | lr 0.0001 | ms/batch 132.06881 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    96800 |  96800 batches | lr 0.0001 | ms/batch 131.27426 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    97000 |  97000 batches | lr 0.0001 | ms/batch 132.21340 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    97200 |  97200 batches | lr 0.0001 | ms/batch 132.13578 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    97400 |  97400 batches | lr 0.0001 | ms/batch 132.07105 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    97600 |  97600 batches | lr 0.0001 | ms/batch 130.89926 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    97800 |  97800 batches | lr 0.0001 | ms/batch 132.52525 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    98000 |  98000 batches | lr 0.0001 | ms/batch 132.52515 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    98200 |  98200 batches | lr 0.0001 | ms/batch 131.42478 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    98400 |  98400 batches | lr 0.0001 | ms/batch 131.84044 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    98600 |  98600 batches | lr 0.0001 | ms/batch 132.67734 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    98800 |  98800 batches | lr 0.0001 | ms/batch 132.62560 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    99000 |  99000 batches | lr 0.0001 | ms/batch 128.34504 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    99200 |  99200 batches | lr 0.0001 | ms/batch 130.19622 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    99400 |  99400 batches | lr 0.0001 | ms/batch 129.35520 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    99600 |  99600 batches | lr 0.0001 | ms/batch 127.51574 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    99800 |  99800 batches | lr 0.0001 | ms/batch 127.20644 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   100000 | 100000 batches | lr 0.0001 | ms/batch 127.78256 | loss 2.30261 | ppl    10.000\n",
      "|\n",
      "Source: [ 9  7  8  2  7  6  5  7 11  8  3  2  9  2  4  2  3  7 10 10 10  7  2  3\n",
      "  1  3  2  7 10 10 10  7  3  2  4  2  9  2  3  8 11  7  5  6  7  2  8  7]\n",
      "Target: [ 7  8  2  7  6  5  7 11  8  3  2  9  2  4  2  3  7 10 10 10  7  2  3  1\n",
      "  3  2  7 10 10 10  7  3  2  4  2  9  2  3  8 11  7  5  6  7  2  8  7  9]\n",
      "Teacher forcing: acc:0.10026041666666667\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 6 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.10026041666666667\n",
      "Preds:  [ 7  8  2  7  6  5  7 11  8  3  2  9  2  4  2  3  7 10 10 10  7  2  3  1\n",
      "  6  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  25 at step   100000 | time: 533.62s | valid loss 2.30255 | valid ppl    9.9997 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   100200 | 100200 batches | lr 0.0001 | ms/batch 180.17362 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   100400 | 100400 batches | lr 0.0001 | ms/batch 127.75304 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   100600 | 100600 batches | lr 0.0001 | ms/batch 128.29948 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   100800 | 100800 batches | lr 0.0001 | ms/batch 127.75551 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   101000 | 101000 batches | lr 0.0001 | ms/batch 129.62358 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   101200 | 101200 batches | lr 0.0001 | ms/batch 128.15714 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   101400 | 101400 batches | lr 0.0001 | ms/batch 128.88209 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   101600 | 101600 batches | lr 0.0001 | ms/batch 127.34430 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   101800 | 101800 batches | lr 0.0001 | ms/batch 126.24264 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   102000 | 102000 batches | lr 0.0001 | ms/batch 128.04760 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   102200 | 102200 batches | lr 0.0001 | ms/batch 125.93120 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   102400 | 102400 batches | lr 0.0001 | ms/batch 126.30570 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   102600 | 102600 batches | lr 0.0001 | ms/batch 125.88535 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   102800 | 102800 batches | lr 0.0001 | ms/batch 126.20477 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   103000 | 103000 batches | lr 0.0001 | ms/batch 125.58810 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   103200 | 103200 batches | lr 0.0001 | ms/batch 126.62909 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   103400 | 103400 batches | lr 0.0001 | ms/batch 125.76898 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   103600 | 103600 batches | lr 0.0001 | ms/batch 127.94171 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   103800 | 103800 batches | lr 0.0001 | ms/batch 128.40630 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   104000 | 104000 batches | lr 0.0001 | ms/batch 126.66572 | loss 2.30261 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 3  9  9  9  5  3 10 10  5  8  7  6  3  8  5 10  2  5  7  3  6 11  4  6\n",
      "  1  6  4 11  6  3  7  5  2 10  5  8  3  6  7  8  5 10 10  3  5  9  9  9]\n",
      "Target: [ 9  9  9  5  3 10 10  5  8  7  6  3  8  5 10  2  5  7  3  6 11  4  6  1\n",
      "  6  4 11  6  3  7  5  2 10  5  8  3  6  7  8  5 10 10  3  5  9  9  9  3]\n",
      "Teacher forcing: acc:0.09688562925170068\n",
      "Preds:  [ 4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4 11 11 11 11 11 11 11 11 11 11 11  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "No teacher forcing: acc:0.09688562925170068\n",
      "Preds:  [ 9  9  9  5  3 10 10  5  8  7  6  3  8  5 10  2  5  7  3  6 11  4  6  1\n",
      "  4 11 11 11 11 11 11 11 11 11 11 11  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  26 at step   104000 | time: 519.42s | valid loss 2.30261 | valid ppl   10.0003 | valid acc 0.097\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   104200 | 104200 batches | lr 0.0001 | ms/batch 179.54475 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   104400 | 104400 batches | lr 0.0001 | ms/batch 128.86745 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   104600 | 104600 batches | lr 0.0001 | ms/batch 130.54390 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   104800 | 104800 batches | lr 0.0001 | ms/batch 133.68448 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   105000 | 105000 batches | lr 0.0001 | ms/batch 133.17131 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   105200 | 105200 batches | lr 0.0001 | ms/batch 134.18371 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   105400 | 105400 batches | lr 0.0001 | ms/batch 133.43827 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   105600 | 105600 batches | lr 0.0001 | ms/batch 135.08104 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   105800 | 105800 batches | lr 0.0001 | ms/batch 132.73245 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   106000 | 106000 batches | lr 0.0001 | ms/batch 132.13460 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   106200 | 106200 batches | lr 0.0001 | ms/batch 127.08498 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   106400 | 106400 batches | lr 0.0001 | ms/batch 127.89898 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   106600 | 106600 batches | lr 0.0001 | ms/batch 128.27912 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   106800 | 106800 batches | lr 0.0001 | ms/batch 128.02515 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   107000 | 107000 batches | lr 0.0001 | ms/batch 129.57438 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   107200 | 107200 batches | lr 0.0001 | ms/batch 128.31458 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   107400 | 107400 batches | lr 0.0001 | ms/batch 128.32018 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   107600 | 107600 batches | lr 0.0001 | ms/batch 129.17851 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   107800 | 107800 batches | lr 0.0001 | ms/batch 130.49677 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   108000 | 108000 batches | lr 0.0001 | ms/batch 129.81817 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 9 11  8  4 10  8  5  6  7 10  3  3  8  4 10  3  8  3 11 10 10  6  7 11\n",
      "  1 11  7  6 10 10 11  3  8  3 10  4  8  3  3 10  7  6  5  8 10  4  8 11]\n",
      "Target: [11  8  4 10  8  5  6  7 10  3  3  8  4 10  3  8  3 11 10 10  6  7 11  1\n",
      " 11  7  6 10 10 11  3  8  3 10  4  8  3  3 10  7  6  5  8 10  4  8 11  9]\n",
      "Teacher forcing: acc:0.10231770833333333\n",
      "Preds:  [6 6 5 6 5 5 5 6 5 5 5 5 5 5 5 5 5 5 6 5 5 6 6 6 6 3 3 3 3 3 3 3 3 3 3 3 5\n",
      " 5 5 5 5 6 5 5 5 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10260416666666666\n",
      "Preds:  [11  8  4 10  8  5  6  7 10  3  3  8  4 10  3  8  3 11 10 10  6  7 11  1\n",
      "  6  3  3  3  3  3  3  3  3  3  3  3  5  5  5  5  5  5  5  5  5  5  5  5]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  27 at step   108000 | time: 532.50s | valid loss 2.30256 | valid ppl    9.9997 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   108200 | 108200 batches | lr 0.0001 | ms/batch 185.34758 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   108400 | 108400 batches | lr 0.0001 | ms/batch 129.42511 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   108600 | 108600 batches | lr 0.0001 | ms/batch 130.36417 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   108800 | 108800 batches | lr 0.0001 | ms/batch 131.03704 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   109000 | 109000 batches | lr 0.0001 | ms/batch 129.31165 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   109200 | 109200 batches | lr 0.0001 | ms/batch 130.16014 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   109400 | 109400 batches | lr 0.0001 | ms/batch 128.60269 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   109600 | 109600 batches | lr 0.0001 | ms/batch 128.42740 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   109800 | 109800 batches | lr 0.0001 | ms/batch 129.09667 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   110000 | 110000 batches | lr 0.0001 | ms/batch 128.80813 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   110200 | 110200 batches | lr 0.0001 | ms/batch 126.38102 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   110400 | 110400 batches | lr 0.0001 | ms/batch 126.85984 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   110600 | 110600 batches | lr 0.0001 | ms/batch 126.49143 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   110800 | 110800 batches | lr 0.0001 | ms/batch 126.08792 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   111000 | 111000 batches | lr 0.0001 | ms/batch 127.70668 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   111200 | 111200 batches | lr 0.0001 | ms/batch 126.39746 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   111400 | 111400 batches | lr 0.0001 | ms/batch 126.31468 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   111600 | 111600 batches | lr 0.0001 | ms/batch 128.02927 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   111800 | 111800 batches | lr 0.0001 | ms/batch 126.87171 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   112000 | 112000 batches | lr 0.0001 | ms/batch 126.78701 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  3  8  8  3  7  3  6  9  6 10  5 11  8  6  9  4 11  8  6  8 11  3  7\n",
      "  1  7  3 11  8  6  8 11  4  9  6  8 11  5 10  6  9  6  3  7  3  8  8  3]\n",
      "Target: [ 3  8  8  3  7  3  6  9  6 10  5 11  8  6  9  4 11  8  6  8 11  3  7  1\n",
      "  7  3 11  8  6  8 11  4  9  6  8 11  5 10  6  9  6  3  7  3  8  8  3  5]\n",
      "Teacher forcing: acc:0.10223958333333333\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10223958333333333\n",
      "Preds:  [ 3  8  8  3  7  3  6  9  6 10  5 11  8  6  9  4 11  8  6  8 11  3  7  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  28 at step   112000 | time: 523.58s | valid loss 2.30258 | valid ppl   10.0000 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   112200 | 112200 batches | lr 0.0001 | ms/batch 181.14113 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   112400 | 112400 batches | lr 0.0001 | ms/batch 126.30279 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   112600 | 112600 batches | lr 0.0001 | ms/batch 126.97553 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   112800 | 112800 batches | lr 0.0001 | ms/batch 126.43353 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   113000 | 113000 batches | lr 0.0001 | ms/batch 127.27878 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   113200 | 113200 batches | lr 0.0001 | ms/batch 127.60667 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   113400 | 113400 batches | lr 0.0001 | ms/batch 126.76287 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   113600 | 113600 batches | lr 0.0001 | ms/batch 126.98907 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   113800 | 113800 batches | lr 0.0001 | ms/batch 128.74344 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   114000 | 114000 batches | lr 0.0001 | ms/batch 127.40370 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   114200 | 114200 batches | lr 0.0001 | ms/batch 127.43308 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   114400 | 114400 batches | lr 0.0001 | ms/batch 128.95591 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   114600 | 114600 batches | lr 0.0001 | ms/batch 128.61930 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   114800 | 114800 batches | lr 0.0001 | ms/batch 136.36548 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   115000 | 115000 batches | lr 0.0001 | ms/batch 129.20314 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   115200 | 115200 batches | lr 0.0001 | ms/batch 137.30299 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   115400 | 115400 batches | lr 0.0001 | ms/batch 134.98242 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   115600 | 115600 batches | lr 0.0001 | ms/batch 135.21835 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   115800 | 115800 batches | lr 0.0001 | ms/batch 133.57749 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   116000 | 116000 batches | lr 0.0001 | ms/batch 133.28969 | loss 2.30261 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  4  2  9  3 10  5  3  8  5 11  9 10 10  4  8  6  9  9  4  4  6  6 11\n",
      "  1 11  6  6  4  4  9  9  6  8  4 10 10  9 11  5  8  3  5 10  3  9  2  4]\n",
      "Target: [ 4  2  9  3 10  5  3  8  5 11  9 10 10  4  8  6  9  9  4  4  6  6 11  1\n",
      " 11  6  6  4  4  9  9  6  8  4 10 10  9 11  5  8  3  5 10  3  9  2  4  7]\n",
      "Teacher forcing: acc:0.09930378401360544\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09930378401360544\n",
      "Preds:  [ 4  2  9  3 10  5  3  8  5 11  9 10 10  4  8  6  9  9  4  4  6  6 11  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  29 at step   116000 | time: 530.07s | valid loss 2.30262 | valid ppl   10.0003 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   116200 | 116200 batches | lr 0.0001 | ms/batch 184.02871 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   116400 | 116400 batches | lr 0.0001 | ms/batch 131.02308 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   116600 | 116600 batches | lr 0.0001 | ms/batch 131.46279 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   116800 | 116800 batches | lr 0.0001 | ms/batch 131.09960 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   117000 | 117000 batches | lr 0.0001 | ms/batch 133.70653 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   117200 | 117200 batches | lr 0.0001 | ms/batch 129.45691 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   117400 | 117400 batches | lr 0.0001 | ms/batch 133.74897 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   117600 | 117600 batches | lr 0.0001 | ms/batch 128.99936 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   117800 | 117800 batches | lr 0.0001 | ms/batch 128.52814 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   118000 | 118000 batches | lr 0.0001 | ms/batch 126.25261 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   118200 | 118200 batches | lr 0.0001 | ms/batch 127.67248 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   118400 | 118400 batches | lr 0.0001 | ms/batch 126.09549 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   118600 | 118600 batches | lr 0.0001 | ms/batch 127.50873 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   118800 | 118800 batches | lr 0.0001 | ms/batch 126.83428 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   119000 | 119000 batches | lr 0.0001 | ms/batch 128.34234 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   119200 | 119200 batches | lr 0.0001 | ms/batch 127.16472 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   119400 | 119400 batches | lr 0.0001 | ms/batch 127.47517 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   119600 | 119600 batches | lr 0.0001 | ms/batch 127.88947 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   119800 | 119800 batches | lr 0.0001 | ms/batch 128.87220 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   120000 | 120000 batches | lr 0.0001 | ms/batch 126.80538 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  9  7  3  7  3 10  3  7  4 11  5  4  9  2  2  8 10  9  2  9  8  5  9\n",
      "  1  9  5  8  9  2  9 10  8  2  2  9  4  5 11  4  7  3 10  3  7  3  7  9]\n",
      "Target: [ 9  7  3  7  3 10  3  7  4 11  5  4  9  2  2  8 10  9  2  9  8  5  9  1\n",
      "  9  5  8  9  2  9 10  8  2  2  9  4  5 11  4  7  3 10  3  7  3  7  9  2]\n",
      "Teacher forcing: acc:0.10169270833333334\n",
      "Preds:  [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      "  6 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "No teacher forcing: acc:0.10169270833333334\n",
      "Preds:  [ 9  7  3  7  3 10  3  7  4 11  5  4  9  2  2  8 10  9  2  9  8  5  9  1\n",
      "  6 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  30 at step   120000 | time: 526.55s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   120200 | 120200 batches | lr 0.0001 | ms/batch 181.90724 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   120400 | 120400 batches | lr 0.0001 | ms/batch 127.31616 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   120600 | 120600 batches | lr 0.0001 | ms/batch 127.19346 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   120800 | 120800 batches | lr 0.0001 | ms/batch 127.26055 | loss 2.30289 | ppl    10.003\n",
      "| epoch   1 step   121000 | 121000 batches | lr 0.0001 | ms/batch 127.16336 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step   121200 | 121200 batches | lr 0.0001 | ms/batch 127.27651 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step   121400 | 121400 batches | lr 0.0001 | ms/batch 128.39518 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step   121600 | 121600 batches | lr 0.0001 | ms/batch 128.63505 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   121800 | 121800 batches | lr 0.0001 | ms/batch 128.58397 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   122000 | 122000 batches | lr 0.0001 | ms/batch 128.53583 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   122200 | 122200 batches | lr 0.0001 | ms/batch 127.23666 | loss 2.30245 | ppl     9.999\n",
      "| epoch   1 step   122400 | 122400 batches | lr 0.0001 | ms/batch 128.45469 | loss 2.29781 | ppl     9.952\n",
      "| epoch   1 step   122600 | 122600 batches | lr 0.0001 | ms/batch 127.18520 | loss 2.29655 | ppl     9.940\n",
      "| epoch   1 step   122800 | 122800 batches | lr 0.0001 | ms/batch 127.43710 | loss 2.29457 | ppl     9.920\n",
      "| epoch   1 step   123000 | 123000 batches | lr 0.0001 | ms/batch 127.54194 | loss 2.29494 | ppl     9.924\n",
      "| epoch   1 step   123200 | 123200 batches | lr 0.0001 | ms/batch 127.35652 | loss 2.28940 | ppl     9.869\n",
      "| epoch   1 step   123400 | 123400 batches | lr 0.0001 | ms/batch 127.14106 | loss 2.28365 | ppl     9.812\n",
      "| epoch   1 step   123600 | 123600 batches | lr 0.0001 | ms/batch 126.62179 | loss 2.28279 | ppl     9.804\n",
      "| epoch   1 step   123800 | 123800 batches | lr 0.0001 | ms/batch 127.70013 | loss 2.28101 | ppl     9.787\n",
      "| epoch   1 step   124000 | 124000 batches | lr 0.0001 | ms/batch 128.02067 | loss 2.25708 | ppl     9.555\n",
      "maslina\n",
      "|\n",
      "Source: [11  3  3  9 10  2  3  6  6  6  6  6  3  5  4  4  7  5  4  4 11  9 11  3\n",
      "  1  3 11  9 11  4  4  5  7  4  4  5  3  6  6  6  6  6  3  2 10  9  3  3]\n",
      "Target: [ 3  3  9 10  2  3  6  6  6  6  6  3  5  4  4  7  5  4  4 11  9 11  3  1\n",
      "  3 11  9 11  4  4  5  7  4  4  5  3  6  6  6  6  6  3  2 10  9  3  3 11]\n",
      "Teacher forcing: acc:0.13225446428571427\n",
      "Preds:  [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10  4  4  4  4  4  4  4  4  4  4  4 10  8  8  8 10 10 10 10 10 10 10 10]\n",
      "\n",
      "No teacher forcing: acc:0.11769238945578231\n",
      "Preds:  [ 3  3  9 10  2  3  6  6  6  6  6  3  5  4  4  7  5  4  4 11  9 11  3  1\n",
      " 10  4  4  4  4  4  4  4  4  4  4  4 10  8  2  2  2  9  8  8  8  8  8  8]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  31 at step   124000 | time: 521.34s | valid loss 2.23424 | valid ppl    9.3394 | valid acc 0.118\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   124200 | 124200 batches | lr 0.0001 | ms/batch 180.24836 | loss 2.22121 | ppl     9.218\n",
      "| epoch   1 step   124400 | 124400 batches | lr 0.0001 | ms/batch 128.79832 | loss 2.19865 | ppl     9.013\n",
      "| epoch   1 step   124600 | 124600 batches | lr 0.0001 | ms/batch 125.53143 | loss 2.17899 | ppl     8.837\n",
      "| epoch   1 step   124800 | 124800 batches | lr 0.0001 | ms/batch 126.88257 | loss 2.16732 | ppl     8.735\n",
      "| epoch   1 step   125000 | 125000 batches | lr 0.0001 | ms/batch 126.95424 | loss 2.15226 | ppl     8.604\n",
      "| epoch   1 step   125200 | 125200 batches | lr 0.0001 | ms/batch 127.04678 | loss 2.14181 | ppl     8.515\n",
      "| epoch   1 step   125400 | 125400 batches | lr 0.0001 | ms/batch 127.41041 | loss 2.13560 | ppl     8.462\n",
      "| epoch   1 step   125600 | 125600 batches | lr 0.0001 | ms/batch 129.13644 | loss 2.12413 | ppl     8.366\n",
      "| epoch   1 step   125800 | 125800 batches | lr 0.0001 | ms/batch 128.09735 | loss 2.12332 | ppl     8.359\n",
      "| epoch   1 step   126000 | 126000 batches | lr 0.0001 | ms/batch 129.30850 | loss 2.11230 | ppl     8.267\n",
      "| epoch   1 step   126200 | 126200 batches | lr 0.0001 | ms/batch 127.42554 | loss 2.10713 | ppl     8.225\n",
      "| epoch   1 step   126400 | 126400 batches | lr 0.0001 | ms/batch 128.11647 | loss 2.10241 | ppl     8.186\n",
      "| epoch   1 step   126600 | 126600 batches | lr 0.0001 | ms/batch 129.09200 | loss 2.09579 | ppl     8.132\n",
      "| epoch   1 step   126800 | 126800 batches | lr 0.0001 | ms/batch 128.06589 | loss 2.09420 | ppl     8.119\n",
      "| epoch   1 step   127000 | 127000 batches | lr 0.0001 | ms/batch 128.54673 | loss 2.08658 | ppl     8.057\n",
      "| epoch   1 step   127200 | 127200 batches | lr 0.0001 | ms/batch 129.92067 | loss 2.08555 | ppl     8.049\n",
      "| epoch   1 step   127400 | 127400 batches | lr 0.0001 | ms/batch 130.00966 | loss 2.07833 | ppl     7.991\n",
      "| epoch   1 step   127600 | 127600 batches | lr 0.0001 | ms/batch 128.93342 | loss 2.07328 | ppl     7.951\n",
      "| epoch   1 step   127800 | 127800 batches | lr 0.0001 | ms/batch 128.66664 | loss 2.06944 | ppl     7.920\n",
      "| epoch   1 step   128000 | 128000 batches | lr 0.0001 | ms/batch 128.38216 | loss 2.06547 | ppl     7.889\n",
      "|\n",
      "Source: [ 5  8 11 10 11  9  8  4 11  3  9  8 11  5 10 10 11  2 11 10  8 10  2  2\n",
      "  1  2  2 10  8 10 11  2 11 10 10  5 11  8  9  3 11  4  8  9 11 10 11  8]\n",
      "Target: [ 8 11 10 11  9  8  4 11  3  9  8 11  5 10 10 11  2 11 10  8 10  2  2  1\n",
      "  2  2 10  8 10 11  2 11 10 10  5 11  8  9  3 11  4  8  9 11 10 11  8  5]\n",
      "Teacher forcing: acc:0.13817708333333334\n",
      "Preds:  [ 8  8  8  8  8  8  8  8  8  8  8  8  8  8 10 11  7 10 10 10 11 11 11 10\n",
      " 11 11  8  8  8 11 11 11  8  8 11 11  3  3  3  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "No teacher forcing: acc:0.12565104166666666\n",
      "Preds:  [ 8 11 10 11  9  8  4 11  3  9  8 11  5 10 10 11  2 11 10  8 10  2  2  1\n",
      " 11 11  8  8  8 11 11 11  8  8 11 11  3  3  3  4  4  4  4  3  3  3  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  32 at step   128000 | time: 523.56s | valid loss 2.04081 | valid ppl    7.6968 | valid acc 0.126\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   128200 | 128200 batches | lr 0.0001 | ms/batch 182.84094 | loss 2.06309 | ppl     7.870\n",
      "| epoch   1 step   128400 | 128400 batches | lr 0.0001 | ms/batch 127.90483 | loss 2.05637 | ppl     7.818\n",
      "| epoch   1 step   128600 | 128600 batches | lr 0.0001 | ms/batch 128.00197 | loss 2.05635 | ppl     7.817\n",
      "| epoch   1 step   128800 | 128800 batches | lr 0.0001 | ms/batch 131.17532 | loss 2.05300 | ppl     7.791\n",
      "| epoch   1 step   129000 | 129000 batches | lr 0.0001 | ms/batch 130.94479 | loss 2.04894 | ppl     7.760\n",
      "| epoch   1 step   129200 | 129200 batches | lr 0.0001 | ms/batch 128.20525 | loss 2.04702 | ppl     7.745\n",
      "| epoch   1 step   129400 | 129400 batches | lr 0.0001 | ms/batch 127.07157 | loss 2.04602 | ppl     7.737\n",
      "| epoch   1 step   129600 | 129600 batches | lr 0.0001 | ms/batch 127.68897 | loss 2.04430 | ppl     7.724\n",
      "| epoch   1 step   129800 | 129800 batches | lr 0.0001 | ms/batch 128.27562 | loss 2.04051 | ppl     7.695\n",
      "| epoch   1 step   130000 | 130000 batches | lr 0.0001 | ms/batch 128.14627 | loss 2.03812 | ppl     7.676\n",
      "| epoch   1 step   130200 | 130200 batches | lr 0.0001 | ms/batch 128.23138 | loss 2.03249 | ppl     7.633\n",
      "| epoch   1 step   130400 | 130400 batches | lr 0.0001 | ms/batch 125.81175 | loss 2.03133 | ppl     7.624\n",
      "| epoch   1 step   130600 | 130600 batches | lr 0.0001 | ms/batch 127.54100 | loss 2.03177 | ppl     7.628\n",
      "| epoch   1 step   130800 | 130800 batches | lr 0.0001 | ms/batch 128.10359 | loss 2.02555 | ppl     7.580\n",
      "| epoch   1 step   131000 | 131000 batches | lr 0.0001 | ms/batch 127.12003 | loss 2.02818 | ppl     7.600\n",
      "| epoch   1 step   131200 | 131200 batches | lr 0.0001 | ms/batch 126.60050 | loss 2.02157 | ppl     7.550\n",
      "| epoch   1 step   131400 | 131400 batches | lr 0.0001 | ms/batch 128.52101 | loss 2.02277 | ppl     7.559\n",
      "| epoch   1 step   131600 | 131600 batches | lr 0.0001 | ms/batch 126.82268 | loss 2.01852 | ppl     7.527\n",
      "| epoch   1 step   131800 | 131800 batches | lr 0.0001 | ms/batch 126.87447 | loss 2.01752 | ppl     7.520\n",
      "| epoch   1 step   132000 | 132000 batches | lr 0.0001 | ms/batch 126.75448 | loss 2.01688 | ppl     7.515\n",
      "|\n",
      "Source: [10  4  6  9 11 11  4  7  8  2 10  6 10  6  4  2  2  6  7  7  2  2  4  2\n",
      "  1  2  4  2  2  7  7  6  2  2  4  6 10  6 10  2  8  7  4 11 11  9  6  4]\n",
      "Target: [ 4  6  9 11 11  4  7  8  2 10  6 10  6  4  2  2  6  7  7  2  2  4  2  1\n",
      "  2  4  2  2  7  7  6  2  2  4  6 10  6 10  2  8  7  4 11 11  9  6  4 10]\n",
      "Teacher forcing: acc:0.14200520833333333\n",
      "Preds:  [ 9  9  9  9  9  9  9  9  9  9  9  9  2 11  8  8  9  9  9  9  9  9  9  9\n",
      "  2  2  2  2  2  2  2  2  2  2 11  9  8  8  8  8  8  8  8  8  8  8  8  8]\n",
      "\n",
      "No teacher forcing: acc:0.13505208333333332\n",
      "Preds:  [ 4  6  9 11 11  4  7  8  2 10  6 10  6  4  2  2  6  7  7  2  2  4  2  1\n",
      "  2  2  2  2  2  2  2  2  2  2 11  9  8  8  8  8  8  8  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  33 at step   132000 | time: 522.53s | valid loss 2.00161 | valid ppl    7.4010 | valid acc 0.135\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   132200 | 132200 batches | lr 0.0001 | ms/batch 183.89315 | loss 2.01729 | ppl     7.518\n",
      "| epoch   1 step   132400 | 132400 batches | lr 0.0001 | ms/batch 129.93685 | loss 2.01495 | ppl     7.500\n",
      "| epoch   1 step   132600 | 132600 batches | lr 0.0001 | ms/batch 128.10731 | loss 2.01242 | ppl     7.481\n",
      "| epoch   1 step   132800 | 132800 batches | lr 0.0001 | ms/batch 129.11630 | loss 2.01207 | ppl     7.479\n",
      "| epoch   1 step   133000 | 133000 batches | lr 0.0001 | ms/batch 127.56641 | loss 2.01037 | ppl     7.466\n",
      "| epoch   1 step   133200 | 133200 batches | lr 0.0001 | ms/batch 128.81959 | loss 2.00886 | ppl     7.455\n",
      "| epoch   1 step   133400 | 133400 batches | lr 0.0001 | ms/batch 126.88206 | loss 2.01006 | ppl     7.464\n",
      "| epoch   1 step   133600 | 133600 batches | lr 0.0001 | ms/batch 129.30114 | loss 2.00738 | ppl     7.444\n",
      "| epoch   1 step   133800 | 133800 batches | lr 0.0001 | ms/batch 128.41768 | loss 2.00331 | ppl     7.414\n",
      "| epoch   1 step   134000 | 134000 batches | lr 0.0001 | ms/batch 125.66580 | loss 2.00492 | ppl     7.426\n",
      "| epoch   1 step   134200 | 134200 batches | lr 0.0001 | ms/batch 130.69860 | loss 2.00886 | ppl     7.455\n",
      "| epoch   1 step   134400 | 134400 batches | lr 0.0001 | ms/batch 130.81868 | loss 2.00305 | ppl     7.412\n",
      "| epoch   1 step   134600 | 134600 batches | lr 0.0001 | ms/batch 127.71685 | loss 1.99998 | ppl     7.389\n",
      "| epoch   1 step   134800 | 134800 batches | lr 0.0001 | ms/batch 127.45559 | loss 1.99968 | ppl     7.387\n",
      "| epoch   1 step   135000 | 135000 batches | lr 0.0001 | ms/batch 127.72342 | loss 1.99921 | ppl     7.383\n",
      "| epoch   1 step   135200 | 135200 batches | lr 0.0001 | ms/batch 128.59728 | loss 1.99777 | ppl     7.373\n",
      "| epoch   1 step   135400 | 135400 batches | lr 0.0001 | ms/batch 130.21609 | loss 1.99917 | ppl     7.383\n",
      "| epoch   1 step   135600 | 135600 batches | lr 0.0001 | ms/batch 128.81287 | loss 1.99555 | ppl     7.356\n",
      "| epoch   1 step   135800 | 135800 batches | lr 0.0001 | ms/batch 129.00667 | loss 1.99499 | ppl     7.352\n",
      "| epoch   1 step   136000 | 136000 batches | lr 0.0001 | ms/batch 128.19157 | loss 1.99383 | ppl     7.344\n",
      "maslina\n",
      "|\n",
      "Source: [ 4  8  8  2  4 11  7  8  8  6  6  9  6  6  2 11  3 11  9 10  8  4  5  2\n",
      "  1  2  5  4  8 10  9 11  3 11  2  6  6  9  6  6  8  8  7 11  4  2  8  8]\n",
      "Target: [ 8  8  2  4 11  7  8  8  6  6  9  6  6  2 11  3 11  9 10  8  4  5  2  1\n",
      "  2  5  4  8 10  9 11  3 11  2  6  6  9  6  6  8  8  7 11  4  2  8  8  4]\n",
      "Teacher forcing: acc:0.15186543367346939\n",
      "Preds:  [ 3  3  3  3  3  3  3  3  3  3  3  3 10 10 10 10 10 10  7  4  2 11  8  8\n",
      "  2  2  2  3  3  3  2  3  2  2  2  2  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "No teacher forcing: acc:0.15051020408163265\n",
      "Preds:  [ 8  8  2  4 11  7  8  8  6  6  9  6  6  2 11  3 11  9 10  8  4  5  2  1\n",
      "  2  2  2  3  3  3  2  3  2  2  2  2  6  6  6  6  6 10 10 10 10 10 10 10]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  34 at step   136000 | time: 525.35s | valid loss 1.97053 | valid ppl    7.1745 | valid acc 0.151\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   136200 | 136200 batches | lr 0.0001 | ms/batch 192.35025 | loss 1.99494 | ppl     7.352\n",
      "| epoch   1 step   136400 | 136400 batches | lr 0.0001 | ms/batch 132.73243 | loss 1.99423 | ppl     7.347\n",
      "| epoch   1 step   136600 | 136600 batches | lr 0.0001 | ms/batch 126.53255 | loss 1.99323 | ppl     7.339\n",
      "| epoch   1 step   136800 | 136800 batches | lr 0.0001 | ms/batch 129.54337 | loss 1.99382 | ppl     7.344\n",
      "| epoch   1 step   137000 | 137000 batches | lr 0.0001 | ms/batch 128.12774 | loss 1.99359 | ppl     7.342\n",
      "| epoch   1 step   137200 | 137200 batches | lr 0.0001 | ms/batch 127.22983 | loss 1.99330 | ppl     7.340\n",
      "| epoch   1 step   137400 | 137400 batches | lr 0.0001 | ms/batch 128.05945 | loss 1.99188 | ppl     7.329\n",
      "| epoch   1 step   137600 | 137600 batches | lr 0.0001 | ms/batch 128.38124 | loss 1.99207 | ppl     7.331\n",
      "| epoch   1 step   137800 | 137800 batches | lr 0.0001 | ms/batch 127.93230 | loss 1.98773 | ppl     7.299\n",
      "| epoch   1 step   138000 | 138000 batches | lr 0.0001 | ms/batch 127.59197 | loss 1.98774 | ppl     7.299\n",
      "| epoch   1 step   138200 | 138200 batches | lr 0.0001 | ms/batch 128.03498 | loss 1.98978 | ppl     7.314\n",
      "| epoch   1 step   138400 | 138400 batches | lr 0.0001 | ms/batch 127.75085 | loss 1.98850 | ppl     7.305\n",
      "| epoch   1 step   138600 | 138600 batches | lr 0.0001 | ms/batch 128.37645 | loss 1.98785 | ppl     7.300\n",
      "| epoch   1 step   138800 | 138800 batches | lr 0.0001 | ms/batch 127.88252 | loss 1.98571 | ppl     7.284\n",
      "| epoch   1 step   139000 | 139000 batches | lr 0.0001 | ms/batch 127.60877 | loss 1.98602 | ppl     7.286\n",
      "| epoch   1 step   139200 | 139200 batches | lr 0.0001 | ms/batch 128.84175 | loss 1.98691 | ppl     7.293\n",
      "| epoch   1 step   139400 | 139400 batches | lr 0.0001 | ms/batch 128.59166 | loss 1.98684 | ppl     7.292\n",
      "| epoch   1 step   139600 | 139600 batches | lr 0.0001 | ms/batch 127.19320 | loss 1.98475 | ppl     7.277\n",
      "| epoch   1 step   139800 | 139800 batches | lr 0.0001 | ms/batch 132.10503 | loss 1.98565 | ppl     7.284\n",
      "| epoch   1 step   140000 | 140000 batches | lr 0.0001 | ms/batch 133.93873 | loss 1.98431 | ppl     7.274\n",
      "|\n",
      "Source: [ 7  9  2  7 11  2 10  9  6  3  8 11 11  5  3  2  3  3  5  4  6  4  6  9\n",
      "  1  9  6  4  6  4  5  3  3  2  3  5 11 11  8  3  6  9 10  2 11  7  2  9]\n",
      "Target: [ 9  2  7 11  2 10  9  6  3  8 11 11  5  3  2  3  3  5  4  6  4  6  9  1\n",
      "  9  6  4  6  4  5  3  3  2  3  5 11 11  8  3  6  9 10  2 11  7  2  9  7]\n",
      "Teacher forcing: acc:0.14947916666666666\n",
      "Preds:  [10 10 10 10 10 10 10 10 10 10 10 10  3  3  5  3  6  6  4  3  3  3  3  8\n",
      "  3  6  6  6  6  5  3 10  5 10  6  6  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "No teacher forcing: acc:0.15057291666666667\n",
      "Preds:  [ 9  2  7 11  2 10  9  6  3  8 11 11  5  3  2  3  3  5  4  6  4  6  9  1\n",
      "  3  6  6  6  6  5  3 10  5 10  6  6  7  7  7  7  7  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  35 at step   140000 | time: 528.01s | valid loss 1.96773 | valid ppl    7.1544 | valid acc 0.151\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   140200 | 140200 batches | lr 0.0001 | ms/batch 187.77137 | loss 1.98261 | ppl     7.262\n",
      "| epoch   1 step   140400 | 140400 batches | lr 0.0001 | ms/batch 133.12088 | loss 1.98167 | ppl     7.255\n",
      "| epoch   1 step   140600 | 140600 batches | lr 0.0001 | ms/batch 132.74968 | loss 1.98387 | ppl     7.271\n",
      "| epoch   1 step   140800 | 140800 batches | lr 0.0001 | ms/batch 133.37483 | loss 1.98367 | ppl     7.269\n",
      "| epoch   1 step   141000 | 141000 batches | lr 0.0001 | ms/batch 132.41222 | loss 1.98112 | ppl     7.251\n",
      "| epoch   1 step   141200 | 141200 batches | lr 0.0001 | ms/batch 127.10177 | loss 1.98208 | ppl     7.258\n",
      "| epoch   1 step   141400 | 141400 batches | lr 0.0001 | ms/batch 126.85342 | loss 1.98311 | ppl     7.265\n",
      "| epoch   1 step   141600 | 141600 batches | lr 0.0001 | ms/batch 127.60692 | loss 1.98180 | ppl     7.256\n",
      "| epoch   1 step   141800 | 141800 batches | lr 0.0001 | ms/batch 126.73050 | loss 1.98045 | ppl     7.246\n",
      "| epoch   1 step   142000 | 142000 batches | lr 0.0001 | ms/batch 126.53964 | loss 1.97951 | ppl     7.239\n",
      "| epoch   1 step   142200 | 142200 batches | lr 0.0001 | ms/batch 128.59569 | loss 1.97962 | ppl     7.240\n",
      "| epoch   1 step   142400 | 142400 batches | lr 0.0001 | ms/batch 130.81935 | loss 1.98051 | ppl     7.246\n",
      "| epoch   1 step   142600 | 142600 batches | lr 0.0001 | ms/batch 128.77452 | loss 1.97906 | ppl     7.236\n",
      "| epoch   1 step   142800 | 142800 batches | lr 0.0001 | ms/batch 129.66646 | loss 1.97992 | ppl     7.242\n",
      "| epoch   1 step   143000 | 143000 batches | lr 0.0001 | ms/batch 130.22950 | loss 1.97993 | ppl     7.242\n",
      "| epoch   1 step   143200 | 143200 batches | lr 0.0001 | ms/batch 128.67761 | loss 1.97993 | ppl     7.242\n",
      "| epoch   1 step   143400 | 143400 batches | lr 0.0001 | ms/batch 131.79295 | loss 1.97707 | ppl     7.222\n",
      "| epoch   1 step   143600 | 143600 batches | lr 0.0001 | ms/batch 133.30057 | loss 1.97912 | ppl     7.236\n",
      "| epoch   1 step   143800 | 143800 batches | lr 0.0001 | ms/batch 131.29751 | loss 1.97807 | ppl     7.229\n",
      "| epoch   1 step   144000 | 144000 batches | lr 0.0001 | ms/batch 131.29291 | loss 1.97732 | ppl     7.223\n",
      "maslina\n",
      "|\n",
      "Source: [ 5  3 11  6  8  6  2  5  5  2  5  8  3  4  8  4 11  9  3  3  4  6  2  9\n",
      "  1  9  2  6  4  3  3  9 11  4  8  4  3  8  5  2  5  5  2  6  8  6 11  3]\n",
      "Target: [ 3 11  6  8  6  2  5  5  2  5  8  3  4  8  4 11  9  3  3  4  6  2  9  1\n",
      "  9  2  6  4  3  3  9 11  4  8  4  3  8  5  2  5  5  2  6  8  6 11  3  5]\n",
      "Teacher forcing: acc:0.1486766581632653\n",
      "Preds:  [ 8  8  8  8  8  8  8  8  8  8  8  8  5 11  5  8  8  7  5  5 11 11  8  5\n",
      "  8  5  5  5  8  8  8  7  7  8  5  8  7  7  7  7  7  7  7  3  3  7  7  3]\n",
      "\n",
      "No teacher forcing: acc:0.14955357142857142\n",
      "Preds:  [ 3 11  6  8  6  2  5  5  2  5  8  3  4  8  4 11  9  3  3  4  6  2  9  1\n",
      "  8  5  5  5  8  8  8  7  7  8  5  8  7  7  7  7  7  3  3  3  3  3  3  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  36 at step   144000 | time: 531.40s | valid loss 1.96729 | valid ppl    7.1513 | valid acc 0.15\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   144200 | 144200 batches | lr 0.0001 | ms/batch 188.35840 | loss 1.97746 | ppl     7.224\n",
      "| epoch   1 step   144400 | 144400 batches | lr 0.0001 | ms/batch 131.76451 | loss 1.97843 | ppl     7.231\n",
      "| epoch   1 step   144600 | 144600 batches | lr 0.0001 | ms/batch 131.24256 | loss 1.97703 | ppl     7.221\n",
      "| epoch   1 step   144800 | 144800 batches | lr 0.0001 | ms/batch 133.61405 | loss 1.97722 | ppl     7.223\n",
      "| epoch   1 step   145000 | 145000 batches | lr 0.0001 | ms/batch 133.93946 | loss 1.97478 | ppl     7.205\n",
      "| epoch   1 step   145200 | 145200 batches | lr 0.0001 | ms/batch 134.57146 | loss 1.97527 | ppl     7.209\n",
      "| epoch   1 step   145400 | 145400 batches | lr 0.0001 | ms/batch 132.30019 | loss 1.97666 | ppl     7.219\n",
      "| epoch   1 step   145600 | 145600 batches | lr 0.0001 | ms/batch 131.84740 | loss 1.97894 | ppl     7.235\n",
      "| epoch   1 step   145800 | 145800 batches | lr 0.0001 | ms/batch 131.57717 | loss 1.97547 | ppl     7.210\n",
      "| epoch   1 step   146000 | 146000 batches | lr 0.0001 | ms/batch 132.57749 | loss 1.97472 | ppl     7.205\n",
      "| epoch   1 step   146200 | 146200 batches | lr 0.0001 | ms/batch 132.70436 | loss 1.97480 | ppl     7.205\n",
      "| epoch   1 step   146400 | 146400 batches | lr 0.0001 | ms/batch 131.16615 | loss 1.97540 | ppl     7.209\n",
      "| epoch   1 step   146600 | 146600 batches | lr 0.0001 | ms/batch 131.99442 | loss 1.97461 | ppl     7.204\n",
      "| epoch   1 step   146800 | 146800 batches | lr 0.0001 | ms/batch 132.25055 | loss 1.97464 | ppl     7.204\n",
      "| epoch   1 step   147000 | 147000 batches | lr 0.0001 | ms/batch 132.01811 | loss 1.97561 | ppl     7.211\n",
      "| epoch   1 step   147200 | 147200 batches | lr 0.0001 | ms/batch 132.50050 | loss 1.97506 | ppl     7.207\n",
      "| epoch   1 step   147400 | 147400 batches | lr 0.0001 | ms/batch 133.04869 | loss 1.97452 | ppl     7.203\n",
      "| epoch   1 step   147600 | 147600 batches | lr 0.0001 | ms/batch 137.11092 | loss 1.97602 | ppl     7.214\n",
      "| epoch   1 step   147800 | 147800 batches | lr 0.0001 | ms/batch 136.85446 | loss 1.97482 | ppl     7.205\n",
      "| epoch   1 step   148000 | 148000 batches | lr 0.0001 | ms/batch 136.15689 | loss 1.97340 | ppl     7.195\n",
      "|\n",
      "Source: [ 4  2  5  9  6 11  8  3  4  2  9 11  4  8 11  9  2  3  4  2 10 10  4  4\n",
      "  1  4  4 10 10  2  4  3  2  9 11  8  4 11  9  2  4  3  8 11  6  9  5  2]\n",
      "Target: [ 2  5  9  6 11  8  3  4  2  9 11  4  8 11  9  2  3  4  2 10 10  4  4  1\n",
      "  4  4 10 10  2  4  3  2  9 11  8  4 11  9  2  4  3  8 11  6  9  5  2  4]\n",
      "Teacher forcing: acc:0.14921875\n",
      "Preds:  [ 8  8  8  8  8  8  8  8  8  8  8  8  8  9  8  6  9  6  9  9  6  6  9  9\n",
      "  6  6 10 10  6  6 10  6 10  6 10  6  2  2  2  2  2  2  2  2  2  2  2  2]\n",
      "\n",
      "No teacher forcing: acc:0.15036458333333333\n",
      "Preds:  [ 2  5  9  6 11  8  3  4  2  9 11  4  8 11  9  2  3  4  2 10 10  4  4  1\n",
      "  6  6 10 10  6  6 10  6 10  6 10  6  2  2  2  2  2 10 10  2  2  2  2  2]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  37 at step   148000 | time: 543.99s | valid loss 1.96628 | valid ppl    7.1440 | valid acc 0.15\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   148200 | 148200 batches | lr 0.0001 | ms/batch 196.18267 | loss 1.97245 | ppl     7.188\n",
      "| epoch   1 step   148400 | 148400 batches | lr 0.0001 | ms/batch 134.49056 | loss 1.97393 | ppl     7.199\n",
      "| epoch   1 step   148600 | 148600 batches | lr 0.0001 | ms/batch 135.68431 | loss 1.97275 | ppl     7.190\n",
      "| epoch   1 step   148800 | 148800 batches | lr 0.0001 | ms/batch 132.77390 | loss 1.97477 | ppl     7.205\n",
      "| epoch   1 step   149000 | 149000 batches | lr 0.0001 | ms/batch 131.82215 | loss 1.97258 | ppl     7.189\n",
      "| epoch   1 step   149200 | 149200 batches | lr 0.0001 | ms/batch 132.19554 | loss 1.97287 | ppl     7.191\n",
      "| epoch   1 step   149400 | 149400 batches | lr 0.0001 | ms/batch 132.41825 | loss 1.97270 | ppl     7.190\n",
      "| epoch   1 step   149600 | 149600 batches | lr 0.0001 | ms/batch 131.50860 | loss 1.97117 | ppl     7.179\n",
      "| epoch   1 step   149800 | 149800 batches | lr 0.0001 | ms/batch 130.83493 | loss 1.97321 | ppl     7.194\n",
      "| epoch   1 step   150000 | 150000 batches | lr 0.0001 | ms/batch 132.73330 | loss 1.97179 | ppl     7.184\n",
      "| epoch   1 step   150200 | 150200 batches | lr 0.0001 | ms/batch 132.32723 | loss 1.97188 | ppl     7.184\n",
      "| epoch   1 step   150400 | 150400 batches | lr 0.0001 | ms/batch 135.72010 | loss 1.97157 | ppl     7.182\n",
      "| epoch   1 step   150600 | 150600 batches | lr 0.0001 | ms/batch 135.40979 | loss 1.97296 | ppl     7.192\n",
      "| epoch   1 step   150800 | 150800 batches | lr 0.0001 | ms/batch 133.72654 | loss 1.97122 | ppl     7.179\n",
      "| epoch   1 step   151000 | 151000 batches | lr 0.0001 | ms/batch 136.53124 | loss 1.97115 | ppl     7.179\n",
      "| epoch   1 step   151200 | 151200 batches | lr 0.0001 | ms/batch 134.10058 | loss 1.96716 | ppl     7.150\n",
      "| epoch   1 step   151400 | 151400 batches | lr 0.0001 | ms/batch 135.52560 | loss 1.95667 | ppl     7.076\n",
      "| epoch   1 step   151600 | 151600 batches | lr 0.0001 | ms/batch 132.68688 | loss 1.94899 | ppl     7.022\n",
      "| epoch   1 step   151800 | 151800 batches | lr 0.0001 | ms/batch 131.90864 | loss 1.94696 | ppl     7.007\n",
      "| epoch   1 step   152000 | 152000 batches | lr 0.0001 | ms/batch 132.06557 | loss 1.93847 | ppl     6.948\n",
      "maslina\n",
      "|\n",
      "Source: [ 6  3  8  2  9  6  9 11 10  5 10  3  4 10  3  6  8  6 11  5  8 10  6  4\n",
      "  1  4  6 10  8  5 11  6  8  6  3 10  4  3 10  5 10 11  9  6  9  2  8  3]\n",
      "Target: [ 3  8  2  9  6  9 11 10  5 10  3  4 10  3  6  8  6 11  5  8 10  6  4  1\n",
      "  4  6 10  8  5 11  6  8  6  3 10  4  3 10  5 10 11  9  6  9  2  8  3  6]\n",
      "Teacher forcing: acc:0.20104166666666667\n",
      "Preds:  [ 8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  5 10 10 10  3  5  3 10\n",
      "  5  6  8  8  5  5  7  8  6 10  8  6 10 10 10 10 10 10 10 10 10 10 10  5]\n",
      "\n",
      "No teacher forcing: acc:0.1925\n",
      "Preds:  [ 3  8  2  9  6  9 11 10  5 10  3  4 10  3  6  8  6 11  5  8 10  6  4  1\n",
      "  5  6  8  8  5  5  7  8  6 10  3  3 10 10 10 10  5 10 10  5 10  5  5 10]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  38 at step   152000 | time: 545.92s | valid loss 1.90099 | valid ppl    6.6925 | valid acc 0.193\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   152200 | 152200 batches | lr 0.0001 | ms/batch 189.23794 | loss 1.93387 | ppl     6.916\n",
      "| epoch   1 step   152400 | 152400 batches | lr 0.0001 | ms/batch 131.03163 | loss 1.92994 | ppl     6.889\n",
      "| epoch   1 step   152600 | 152600 batches | lr 0.0001 | ms/batch 130.07210 | loss 1.92443 | ppl     6.851\n",
      "| epoch   1 step   152800 | 152800 batches | lr 0.0001 | ms/batch 132.83610 | loss 1.92578 | ppl     6.860\n",
      "| epoch   1 step   153000 | 153000 batches | lr 0.0001 | ms/batch 130.39975 | loss 1.91876 | ppl     6.813\n",
      "| epoch   1 step   153200 | 153200 batches | lr 0.0001 | ms/batch 130.55163 | loss 1.91742 | ppl     6.803\n",
      "| epoch   1 step   153400 | 153400 batches | lr 0.0001 | ms/batch 130.74114 | loss 1.91750 | ppl     6.804\n",
      "| epoch   1 step   153600 | 153600 batches | lr 0.0001 | ms/batch 132.48962 | loss 1.90889 | ppl     6.746\n",
      "| epoch   1 step   153800 | 153800 batches | lr 0.0001 | ms/batch 131.01603 | loss 1.90475 | ppl     6.718\n",
      "| epoch   1 step   154000 | 154000 batches | lr 0.0001 | ms/batch 131.22999 | loss 1.90568 | ppl     6.724\n",
      "| epoch   1 step   154200 | 154200 batches | lr 0.0001 | ms/batch 130.99841 | loss 1.89933 | ppl     6.681\n",
      "| epoch   1 step   154400 | 154400 batches | lr 0.0001 | ms/batch 130.71170 | loss 1.89897 | ppl     6.679\n",
      "| epoch   1 step   154600 | 154600 batches | lr 0.0001 | ms/batch 131.57233 | loss 1.89391 | ppl     6.645\n",
      "| epoch   1 step   154800 | 154800 batches | lr 0.0001 | ms/batch 131.76645 | loss 1.89508 | ppl     6.653\n",
      "| epoch   1 step   155000 | 155000 batches | lr 0.0001 | ms/batch 131.58457 | loss 1.89312 | ppl     6.640\n",
      "| epoch   1 step   155200 | 155200 batches | lr 0.0001 | ms/batch 131.57167 | loss 1.88736 | ppl     6.602\n",
      "| epoch   1 step   155400 | 155400 batches | lr 0.0001 | ms/batch 130.66039 | loss 1.88647 | ppl     6.596\n",
      "| epoch   1 step   155600 | 155600 batches | lr 0.0001 | ms/batch 131.20075 | loss 1.88279 | ppl     6.572\n",
      "| epoch   1 step   155800 | 155800 batches | lr 0.0001 | ms/batch 131.98610 | loss 1.88062 | ppl     6.558\n",
      "| epoch   1 step   156000 | 156000 batches | lr 0.0001 | ms/batch 130.69212 | loss 1.87619 | ppl     6.529\n",
      "|\n",
      "Source: [ 4 11  4 10  4 10  5 11  3  5  9  2  8 10  9  4  3  5  3  6 10  2  9  6\n",
      "  1  6  9  2 10  6  3  5  3  4  9 10  8  2  9  5  3 11  5 10  4 10  4 11]\n",
      "Target: [11  4 10  4 10  5 11  3  5  9  2  8 10  9  4  3  5  3  6 10  2  9  6  1\n",
      "  6  9  2 10  6  3  5  3  4  9 10  8  2  9  5  3 11  5 10  4 10  4 11  4]\n",
      "Teacher forcing: acc:0.21106770833333333\n",
      "Preds:  [ 8  8  8  8  8  8  8  8  8  8  8  8  5  5  6 10 10 10  3  9  7  9  3  9\n",
      " 11  9 11  8 11  9 11  3 11  3  8  8 10 10  3 10  7 10 10 10 10 10  7 10]\n",
      "\n",
      "No teacher forcing: acc:0.20799479166666668\n",
      "Preds:  [11  4 10  4 10  5 11  3  5  9  2  8 10  9  4  3  5  3  6 10  2  9  6  1\n",
      " 11  9 11  8 11  9 11  3 11  3  8  8 10 10 10 10  7  7  7  7 10 10 10  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  39 at step   156000 | time: 536.40s | valid loss 1.83477 | valid ppl    6.2637 | valid acc 0.208\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   156200 | 156200 batches | lr 0.0001 | ms/batch 192.89441 | loss 1.87467 | ppl     6.519\n",
      "| epoch   1 step   156400 | 156400 batches | lr 0.0001 | ms/batch 134.49633 | loss 1.86873 | ppl     6.480\n",
      "| epoch   1 step   156600 | 156600 batches | lr 0.0001 | ms/batch 135.85661 | loss 1.86551 | ppl     6.459\n",
      "| epoch   1 step   156800 | 156800 batches | lr 0.0001 | ms/batch 135.94070 | loss 1.86091 | ppl     6.430\n",
      "| epoch   1 step   157000 | 157000 batches | lr 0.0001 | ms/batch 135.98539 | loss 1.85251 | ppl     6.376\n",
      "| epoch   1 step   157200 | 157200 batches | lr 0.0001 | ms/batch 134.64814 | loss 1.84692 | ppl     6.340\n",
      "| epoch   1 step   157400 | 157400 batches | lr 0.0001 | ms/batch 135.00033 | loss 1.83834 | ppl     6.286\n",
      "| epoch   1 step   157600 | 157600 batches | lr 0.0001 | ms/batch 135.17927 | loss 1.83144 | ppl     6.243\n",
      "| epoch   1 step   157800 | 157800 batches | lr 0.0001 | ms/batch 136.98853 | loss 1.82906 | ppl     6.228\n",
      "| epoch   1 step   158000 | 158000 batches | lr 0.0001 | ms/batch 136.93623 | loss 1.82159 | ppl     6.182\n",
      "| epoch   1 step   158200 | 158200 batches | lr 0.0001 | ms/batch 137.80145 | loss 1.81484 | ppl     6.140\n",
      "| epoch   1 step   158400 | 158400 batches | lr 0.0001 | ms/batch 136.84960 | loss 1.81239 | ppl     6.125\n",
      "| epoch   1 step   158600 | 158600 batches | lr 0.0001 | ms/batch 136.14517 | loss 1.80529 | ppl     6.082\n",
      "| epoch   1 step   158800 | 158800 batches | lr 0.0001 | ms/batch 138.47196 | loss 1.80508 | ppl     6.080\n",
      "| epoch   1 step   159000 | 159000 batches | lr 0.0001 | ms/batch 137.19581 | loss 1.79696 | ppl     6.031\n",
      "| epoch   1 step   159200 | 159200 batches | lr 0.0001 | ms/batch 135.16390 | loss 1.79398 | ppl     6.013\n",
      "| epoch   1 step   159400 | 159400 batches | lr 0.0001 | ms/batch 135.64929 | loss 1.79375 | ppl     6.012\n",
      "| epoch   1 step   159600 | 159600 batches | lr 0.0001 | ms/batch 137.21371 | loss 1.79251 | ppl     6.005\n",
      "| epoch   1 step   159800 | 159800 batches | lr 0.0001 | ms/batch 136.50122 | loss 1.78971 | ppl     5.988\n",
      "| epoch   1 step   160000 | 160000 batches | lr 0.0001 | ms/batch 137.13970 | loss 1.78996 | ppl     5.989\n",
      "|\n",
      "Source: [ 9  5  8  6  7  2  7  7  6  7  4  4  2  9  9  4  7 11  9 11  7  2  2  9\n",
      "  1  9  2  2  7 11  9 11  7  4  9  9  2  4  4  7  6  7  7  2  7  6  8  5]\n",
      "Target: [ 5  8  6  7  2  7  7  6  7  4  4  2  9  9  4  7 11  9 11  7  2  2  9  1\n",
      "  9  2  2  7 11  9 11  7  4  9  9  2  4  4  7  6  7  7  2  7  6  8  5  9]\n",
      "Teacher forcing: acc:0.25002604166666664\n",
      "Preds:  [10 10  9  9  9 10 10 10 10 10 10 10  9  9  3  9  9  9  9  9  9 10 10  9\n",
      "  9 11 11 11 11  9 11 11 11  9  9 11  6  6  6  6  6  7  7  7  7  7  7  7]\n",
      "\n",
      "No teacher forcing: acc:0.24838541666666666\n",
      "Preds:  [ 5  8  6  7  2  7  7  6  7  4  4  2  9  9  4  7 11  9 11  7  2  2  9  1\n",
      "  9 11 11 11 11  9 11 11 11  9  9 11  6  6  6  6  6  6  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  40 at step   160000 | time: 556.28s | valid loss 1.76587 | valid ppl    5.8467 | valid acc 0.248\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   160200 | 160200 batches | lr 0.0001 | ms/batch 186.49925 | loss 1.78648 | ppl     5.968\n",
      "| epoch   1 step   160400 | 160400 batches | lr 0.0001 | ms/batch 132.29386 | loss 1.78537 | ppl     5.962\n",
      "| epoch   1 step   160600 | 160600 batches | lr 0.0001 | ms/batch 131.99247 | loss 1.78360 | ppl     5.951\n",
      "| epoch   1 step   160800 | 160800 batches | lr 0.0001 | ms/batch 132.88615 | loss 1.78121 | ppl     5.937\n",
      "| epoch   1 step   161000 | 161000 batches | lr 0.0001 | ms/batch 133.02222 | loss 1.78270 | ppl     5.946\n",
      "| epoch   1 step   161200 | 161200 batches | lr 0.0001 | ms/batch 132.04320 | loss 1.78094 | ppl     5.935\n",
      "| epoch   1 step   161400 | 161400 batches | lr 0.0001 | ms/batch 132.94141 | loss 1.78136 | ppl     5.938\n",
      "| epoch   1 step   161600 | 161600 batches | lr 0.0001 | ms/batch 132.13130 | loss 1.76840 | ppl     5.861\n",
      "| epoch   1 step   161800 | 161800 batches | lr 0.0001 | ms/batch 131.71300 | loss 1.74920 | ppl     5.750\n",
      "| epoch   1 step   162000 | 162000 batches | lr 0.0001 | ms/batch 131.13479 | loss 1.74168 | ppl     5.707\n",
      "| epoch   1 step   162200 | 162200 batches | lr 0.0001 | ms/batch 130.64530 | loss 1.73151 | ppl     5.649\n",
      "| epoch   1 step   162400 | 162400 batches | lr 0.0001 | ms/batch 132.41443 | loss 1.73461 | ppl     5.667\n",
      "| epoch   1 step   162600 | 162600 batches | lr 0.0001 | ms/batch 131.83437 | loss 1.72868 | ppl     5.633\n",
      "| epoch   1 step   162800 | 162800 batches | lr 0.0001 | ms/batch 131.67884 | loss 1.72086 | ppl     5.589\n",
      "| epoch   1 step   163000 | 163000 batches | lr 0.0001 | ms/batch 132.78076 | loss 1.68479 | ppl     5.391\n",
      "| epoch   1 step   163200 | 163200 batches | lr 0.0001 | ms/batch 132.03918 | loss 1.63430 | ppl     5.126\n",
      "| epoch   1 step   163400 | 163400 batches | lr 0.0001 | ms/batch 131.89956 | loss 1.59490 | ppl     4.928\n",
      "| epoch   1 step   163600 | 163600 batches | lr 0.0001 | ms/batch 131.83330 | loss 1.55589 | ppl     4.739\n",
      "| epoch   1 step   163800 | 163800 batches | lr 0.0001 | ms/batch 130.82553 | loss 1.52604 | ppl     4.600\n",
      "| epoch   1 step   164000 | 164000 batches | lr 0.0001 | ms/batch 130.14240 | loss 1.49353 | ppl     4.453\n",
      "maslina\n",
      "|\n",
      "Source: [ 9 11  2 10  5 10  6 10  6  6  6  4  5  2  8  9  5  5 10  2  5  9  5 10\n",
      "  1 10  5  9  5  2 10  5  5  9  8  2  5  4  6  6  6 10  6 10  5 10  2 11]\n",
      "Target: [11  2 10  5 10  6 10  6  6  6  4  5  2  8  9  5  5 10  2  5  9  5 10  1\n",
      " 10  5  9  5  2 10  5  5  9  8  2  5  4  6  6  6 10  6 10  5 10  2 11  9]\n",
      "Teacher forcing: acc:0.5018335459183674\n",
      "Preds:  [ 8  8 10 10 10  8 10  8 10 10 10  9  9  9  7 11  7  9  9  3  3  3  9  7\n",
      " 10  6  9  5  2 10  5  5  9  8  2  5  6  6  7  6  6  9  6  9  6  9  6  7]\n",
      "\n",
      "No teacher forcing: acc:0.4925063775510204\n",
      "Preds:  [11  2 10  5 10  6 10  6  6  6  4  5  2  8  9  5  5 10  2  5  9  5 10  1\n",
      " 10  6  9  5  2 10  5  5  9  8  2  5  6  6  6  6  6  6  6  6  6 10  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  41 at step   164000 | time: 538.25s | valid loss 1.36169 | valid ppl    3.9028 | valid acc 0.493\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   164200 | 164200 batches | lr 0.0001 | ms/batch 185.81489 | loss 1.47188 | ppl     4.357\n",
      "| epoch   1 step   164400 | 164400 batches | lr 0.0001 | ms/batch 130.75402 | loss 1.44030 | ppl     4.222\n",
      "| epoch   1 step   164600 | 164600 batches | lr 0.0001 | ms/batch 131.64901 | loss 1.43055 | ppl     4.181\n",
      "| epoch   1 step   164800 | 164800 batches | lr 0.0001 | ms/batch 130.17076 | loss 1.39644 | ppl     4.041\n",
      "| epoch   1 step   165000 | 165000 batches | lr 0.0001 | ms/batch 130.43694 | loss 1.38300 | ppl     3.987\n",
      "| epoch   1 step   165200 | 165200 batches | lr 0.0001 | ms/batch 131.60240 | loss 1.35933 | ppl     3.894\n",
      "| epoch   1 step   165400 | 165400 batches | lr 0.0001 | ms/batch 131.47504 | loss 1.35399 | ppl     3.873\n",
      "| epoch   1 step   165600 | 165600 batches | lr 0.0001 | ms/batch 130.76181 | loss 1.33753 | ppl     3.810\n",
      "| epoch   1 step   165800 | 165800 batches | lr 0.0001 | ms/batch 130.27784 | loss 1.33980 | ppl     3.818\n",
      "| epoch   1 step   166000 | 166000 batches | lr 0.0001 | ms/batch 130.64988 | loss 1.33343 | ppl     3.794\n",
      "| epoch   1 step   166200 | 166200 batches | lr 0.0001 | ms/batch 130.71855 | loss 1.32486 | ppl     3.762\n",
      "| epoch   1 step   166400 | 166400 batches | lr 0.0001 | ms/batch 130.11823 | loss 1.29304 | ppl     3.644\n",
      "| epoch   1 step   166600 | 166600 batches | lr 0.0001 | ms/batch 131.17628 | loss 1.31049 | ppl     3.708\n",
      "| epoch   1 step   166800 | 166800 batches | lr 0.0001 | ms/batch 131.27068 | loss 1.30214 | ppl     3.677\n",
      "| epoch   1 step   167000 | 167000 batches | lr 0.0001 | ms/batch 132.44615 | loss 1.29274 | ppl     3.643\n",
      "| epoch   1 step   167200 | 167200 batches | lr 0.0001 | ms/batch 134.00741 | loss 1.26377 | ppl     3.539\n",
      "| epoch   1 step   167400 | 167400 batches | lr 0.0001 | ms/batch 133.43440 | loss 1.28486 | ppl     3.614\n",
      "| epoch   1 step   167600 | 167600 batches | lr 0.0001 | ms/batch 129.82374 | loss 1.25882 | ppl     3.521\n",
      "| epoch   1 step   167800 | 167800 batches | lr 0.0001 | ms/batch 131.70611 | loss 1.27530 | ppl     3.580\n",
      "| epoch   1 step   168000 | 168000 batches | lr 0.0001 | ms/batch 130.93031 | loss 1.28246 | ppl     3.605\n",
      "|\n",
      "Source: [ 2  3  4 10  6 11 11  3 11  6  6  2  4  6 10 11  2  9  2 10  7  2  9  6\n",
      "  1  6  9  2  7 10  2  9  2 11 10  6  4  2  6  6 11  3 11 11  6 10  4  3]\n",
      "Target: [ 3  4 10  6 11 11  3 11  6  6  2  4  6 10 11  2  9  2 10  7  2  9  6  1\n",
      "  6  9  2  7 10  2  9  2 11 10  6  4  2  6  6 11  3 11 11  6 10  4  3  2]\n",
      "Teacher forcing: acc:0.5490104166666666\n",
      "Preds:  [ 3  8  3 10  7  8  8  8  8 10  9  9 11  6  6  6  6  6 11 11  6  2  5 11\n",
      "  6  9  2  7 10  2  9  2 11 10  6  4  2  2  7  7  7  3  7  7  2  3  4  3]\n",
      "\n",
      "No teacher forcing: acc:0.54953125\n",
      "Preds:  [ 3  4 10  6 11 11  3 11  6  6  2  4  6 10 11  2  9  2 10  7  2  9  6  1\n",
      "  6  9  2  7 10  2  9  2 11 10  6  4  2  2  2  2  2  2  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  42 at step   168000 | time: 536.13s | valid loss 1.15713 | valid ppl    3.1808 | valid acc 0.55\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   168200 | 168200 batches | lr 0.0001 | ms/batch 187.51755 | loss 1.24249 | ppl     3.464\n",
      "| epoch   1 step   168400 | 168400 batches | lr 0.0001 | ms/batch 130.90024 | loss 1.21822 | ppl     3.381\n",
      "| epoch   1 step   168600 | 168600 batches | lr 0.0001 | ms/batch 130.35352 | loss 1.24629 | ppl     3.477\n",
      "| epoch   1 step   168800 | 168800 batches | lr 0.0001 | ms/batch 130.82179 | loss 1.25697 | ppl     3.515\n",
      "| epoch   1 step   169000 | 169000 batches | lr 0.0001 | ms/batch 130.83040 | loss 1.21006 | ppl     3.354\n",
      "| epoch   1 step   169200 | 169200 batches | lr 0.0001 | ms/batch 129.80516 | loss 1.25595 | ppl     3.511\n",
      "| epoch   1 step   169400 | 169400 batches | lr 0.0001 | ms/batch 130.73364 | loss 1.23425 | ppl     3.436\n",
      "| epoch   1 step   169600 | 169600 batches | lr 0.0001 | ms/batch 131.65320 | loss 1.25311 | ppl     3.501\n",
      "| epoch   1 step   169800 | 169800 batches | lr 0.0001 | ms/batch 131.40878 | loss 1.24022 | ppl     3.456\n",
      "| epoch   1 step   170000 | 170000 batches | lr 0.0001 | ms/batch 132.49332 | loss 1.23971 | ppl     3.455\n",
      "| epoch   1 step   170200 | 170200 batches | lr 0.0001 | ms/batch 130.50082 | loss 1.24912 | ppl     3.487\n",
      "| epoch   1 step   170400 | 170400 batches | lr 0.0001 | ms/batch 131.98037 | loss 1.22198 | ppl     3.394\n",
      "| epoch   1 step   170600 | 170600 batches | lr 0.0001 | ms/batch 131.20346 | loss 1.22223 | ppl     3.395\n",
      "| epoch   1 step   170800 | 170800 batches | lr 0.0001 | ms/batch 132.00758 | loss 1.24153 | ppl     3.461\n",
      "| epoch   1 step   171000 | 171000 batches | lr 0.0001 | ms/batch 132.59189 | loss 1.21794 | ppl     3.380\n",
      "| epoch   1 step   171200 | 171200 batches | lr 0.0001 | ms/batch 134.71602 | loss 1.20872 | ppl     3.349\n",
      "| epoch   1 step   171400 | 171400 batches | lr 0.0001 | ms/batch 133.96828 | loss 1.24418 | ppl     3.470\n",
      "| epoch   1 step   171600 | 171600 batches | lr 0.0001 | ms/batch 130.96812 | loss 1.24316 | ppl     3.467\n",
      "| epoch   1 step   171800 | 171800 batches | lr 0.0001 | ms/batch 130.06429 | loss 1.20118 | ppl     3.324\n",
      "| epoch   1 step   172000 | 172000 batches | lr 0.0001 | ms/batch 131.11285 | loss 1.23064 | ppl     3.423\n",
      "maslina\n",
      "|\n",
      "Source: [10  7  5  5  7  9 10  9  2  2 10  6 10  9  6  8  5 11  7  8  6  3  3 10\n",
      "  1 10  3  3  6  8  7 11  5  8  6  9 10  6 10  2  2  9 10  9  7  5  5  7]\n",
      "Target: [ 7  5  5  7  9 10  9  2  2 10  6 10  9  6  8  5 11  7  8  6  3  3 10  1\n",
      " 10  3  3  6  8  7 11  5  8  6  9 10  6 10  2  2  9 10  9  7  5  5  7 10]\n",
      "Teacher forcing: acc:0.5485756802721088\n",
      "Preds:  [ 8  8  2  9  8  8  8  8 11  7 11 11  6  6  5  6  6  6  2  6  2  2  2  5\n",
      " 10  3  3  6  8  7 11  5  8  6  9 10  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "No teacher forcing: acc:0.5492665816326531\n",
      "Preds:  [ 7  5  5  7  9 10  9  2  2 10  6 10  9  6  8  5 11  7  8  6  3  3 10  1\n",
      " 10  3  3  6  8  7 11  5  8  6  9 10  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  43 at step   172000 | time: 536.95s | valid loss 1.15210 | valid ppl    3.1648 | valid acc 0.549\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   172200 | 172200 batches | lr 0.0001 | ms/batch 187.84982 | loss 1.24126 | ppl     3.460\n",
      "| epoch   1 step   172400 | 172400 batches | lr 0.0001 | ms/batch 130.96477 | loss 1.22565 | ppl     3.406\n",
      "| epoch   1 step   172600 | 172600 batches | lr 0.0001 | ms/batch 130.31998 | loss 1.20236 | ppl     3.328\n",
      "| epoch   1 step   172800 | 172800 batches | lr 0.0001 | ms/batch 132.97659 | loss 1.22379 | ppl     3.400\n",
      "| epoch   1 step   173000 | 173000 batches | lr 0.0001 | ms/batch 132.30814 | loss 1.21009 | ppl     3.354\n",
      "| epoch   1 step   173200 | 173200 batches | lr 0.0001 | ms/batch 134.45710 | loss 1.22089 | ppl     3.390\n",
      "| epoch   1 step   173400 | 173400 batches | lr 0.0001 | ms/batch 132.16244 | loss 1.20340 | ppl     3.331\n",
      "| epoch   1 step   173600 | 173600 batches | lr 0.0001 | ms/batch 136.08481 | loss 1.21538 | ppl     3.372\n",
      "| epoch   1 step   173800 | 173800 batches | lr 0.0001 | ms/batch 138.92606 | loss 1.24545 | ppl     3.475\n",
      "| epoch   1 step   174000 | 174000 batches | lr 0.0001 | ms/batch 141.87061 | loss 1.20712 | ppl     3.344\n",
      "| epoch   1 step   174200 | 174200 batches | lr 0.0001 | ms/batch 142.54494 | loss 1.20091 | ppl     3.323\n",
      "| epoch   1 step   174400 | 174400 batches | lr 0.0001 | ms/batch 144.57052 | loss 1.23269 | ppl     3.430\n",
      "| epoch   1 step   174600 | 174600 batches | lr 0.0001 | ms/batch 150.47160 | loss 1.22908 | ppl     3.418\n",
      "| epoch   1 step   174800 | 174800 batches | lr 0.0001 | ms/batch 145.43164 | loss 1.24263 | ppl     3.465\n",
      "| epoch   1 step   175000 | 175000 batches | lr 0.0001 | ms/batch 143.15166 | loss 1.21320 | ppl     3.364\n",
      "| epoch   1 step   175200 | 175200 batches | lr 0.0001 | ms/batch 143.03653 | loss 1.22143 | ppl     3.392\n",
      "| epoch   1 step   175400 | 175400 batches | lr 0.0001 | ms/batch 142.78517 | loss 1.21703 | ppl     3.377\n",
      "| epoch   1 step   175600 | 175600 batches | lr 0.0001 | ms/batch 143.14831 | loss 1.22413 | ppl     3.401\n",
      "| epoch   1 step   175800 | 175800 batches | lr 0.0001 | ms/batch 141.88770 | loss 1.22555 | ppl     3.406\n",
      "| epoch   1 step   176000 | 176000 batches | lr 0.0001 | ms/batch 144.04715 | loss 1.21953 | ppl     3.386\n",
      "|\n",
      "Source: [ 8 10  9 11 11  9  6 10  8  6  7  7  6  2  5  8  3  7  6 11  2 11 10  9\n",
      "  1  9 10 11  2 11  6  7  3  8  5  2  6  7  7  6  8 10  6  9 11 11  9 10]\n",
      "Target: [10  9 11 11  9  6 10  8  6  7  7  6  2  5  8  3  7  6 11  2 11 10  9  1\n",
      "  9 10 11  2 11  6  7  3  8  5  2  6  7  7  6  8 10  6  9 11 11  9 10  8]\n",
      "Teacher forcing: acc:0.5495833333333333\n",
      "Preds:  [ 8  8  8  8  4  8  4  8  6 11  7  4  5  5  5 11  2  2 10  8  9 10  5  2\n",
      "  9 10 11  2 11  6  7  3  8  5  2  6  2  2  4  2  4  2  2  2  4  4  2  4]\n",
      "\n",
      "No teacher forcing: acc:0.5491927083333333\n",
      "Preds:  [10  9 11 11  9  6 10  8  6  7  7  6  2  5  8  3  7  6 11  2 11 10  9  1\n",
      "  9 10 11  2 11  6  7  3  8  5  2  6  2  2  2  2  2  2  2  2  2  2  2  2]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  44 at step   176000 | time: 568.47s | valid loss 1.15178 | valid ppl    3.1638 | valid acc 0.549\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   176200 | 176200 batches | lr 0.0001 | ms/batch 203.37138 | loss 1.22713 | ppl     3.411\n",
      "| epoch   1 step   176400 | 176400 batches | lr 0.0001 | ms/batch 141.46219 | loss 1.20428 | ppl     3.334\n",
      "| epoch   1 step   176600 | 176600 batches | lr 0.0001 | ms/batch 143.49390 | loss 1.19787 | ppl     3.313\n",
      "| epoch   1 step   176800 | 176800 batches | lr 0.0001 | ms/batch 146.14355 | loss 1.22786 | ppl     3.414\n",
      "| epoch   1 step   177000 | 177000 batches | lr 0.0001 | ms/batch 145.27456 | loss 1.22660 | ppl     3.410\n",
      "| epoch   1 step   177200 | 177200 batches | lr 0.0001 | ms/batch 144.39861 | loss 1.19616 | ppl     3.307\n",
      "| epoch   1 step   177400 | 177400 batches | lr 0.0001 | ms/batch 142.17292 | loss 1.20558 | ppl     3.339\n",
      "| epoch   1 step   177600 | 177600 batches | lr 0.0001 | ms/batch 143.31042 | loss 1.23144 | ppl     3.426\n",
      "| epoch   1 step   177800 | 177800 batches | lr 0.0001 | ms/batch 143.38006 | loss 1.23513 | ppl     3.439\n",
      "| epoch   1 step   178000 | 178000 batches | lr 0.0001 | ms/batch 142.37950 | loss 1.19659 | ppl     3.309\n",
      "| epoch   1 step   178200 | 178200 batches | lr 0.0001 | ms/batch 143.36578 | loss 1.20528 | ppl     3.338\n",
      "| epoch   1 step   178400 | 178400 batches | lr 0.0001 | ms/batch 141.63510 | loss 1.23622 | ppl     3.443\n",
      "| epoch   1 step   178600 | 178600 batches | lr 0.0001 | ms/batch 143.57158 | loss 1.20424 | ppl     3.334\n",
      "| epoch   1 step   178800 | 178800 batches | lr 0.0001 | ms/batch 142.35846 | loss 1.19464 | ppl     3.302\n",
      "| epoch   1 step   179000 | 179000 batches | lr 0.0001 | ms/batch 142.95816 | loss 1.21710 | ppl     3.377\n",
      "| epoch   1 step   179200 | 179200 batches | lr 0.0001 | ms/batch 142.85379 | loss 1.18698 | ppl     3.277\n",
      "| epoch   1 step   179400 | 179400 batches | lr 0.0001 | ms/batch 143.10029 | loss 1.25017 | ppl     3.491\n",
      "| epoch   1 step   179600 | 179600 batches | lr 0.0001 | ms/batch 143.64969 | loss 1.20313 | ppl     3.331\n",
      "| epoch   1 step   179800 | 179800 batches | lr 0.0001 | ms/batch 144.65598 | loss 1.21462 | ppl     3.369\n",
      "| epoch   1 step   180000 | 180000 batches | lr 0.0001 | ms/batch 146.85784 | loss 1.19079 | ppl     3.290\n",
      "|\n",
      "Source: [ 8  3  3  7 10  3  3  8  7  9  8 10  9  9  4  2  3  9  9  7  5  6  2 11\n",
      "  1 11  2  6  5  7  9  9  3  2  4  9  9 10  8  9  7  8  3  3 10  7  3  3]\n",
      "Target: [ 3  3  7 10  3  3  8  7  9  8 10  9  9  4  2  3  9  9  7  5  6  2 11  1\n",
      " 11  2  6  5  7  9  9  3  2  4  9  9 10  8  9  7  8  3  3 10  7  3  3  8]\n",
      "Teacher forcing: acc:0.5487239583333333\n",
      "Preds:  [10  8  8  8  8  8  8  8  7  8 11  6  6  5  6  6  5  5  5  2  6 11  3 11\n",
      " 11  2  6  5  7  9  9  3  2  4  9  9  9  4  4  9  4  4  4  4  9  4  4  4]\n",
      "\n",
      "No teacher forcing: acc:0.5485677083333333\n",
      "Preds:  [ 3  3  7 10  3  3  8  7  9  8 10  9  9  4  2  3  9  9  7  5  6  2 11  1\n",
      " 11  2  6  5  7  9  9  3  2  4  9  9  9  9  9  9  9  9  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  45 at step   180000 | time: 586.26s | valid loss 1.15151 | valid ppl    3.1630 | valid acc 0.549\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   180200 | 180200 batches | lr 0.0001 | ms/batch 204.85625 | loss 1.19241 | ppl     3.295\n",
      "| epoch   1 step   180400 | 180400 batches | lr 0.0001 | ms/batch 143.62942 | loss 1.20737 | ppl     3.345\n",
      "| epoch   1 step   180600 | 180600 batches | lr 0.0001 | ms/batch 142.35695 | loss 1.21329 | ppl     3.365\n",
      "| epoch   1 step   180800 | 180800 batches | lr 0.0001 | ms/batch 144.17054 | loss 1.17947 | ppl     3.253\n",
      "| epoch   1 step   181000 | 181000 batches | lr 0.0001 | ms/batch 142.03522 | loss 1.20930 | ppl     3.351\n",
      "| epoch   1 step   181200 | 181200 batches | lr 0.0001 | ms/batch 141.55882 | loss 1.20710 | ppl     3.344\n",
      "| epoch   1 step   181400 | 181400 batches | lr 0.0001 | ms/batch 141.96178 | loss 1.24363 | ppl     3.468\n",
      "| epoch   1 step   181600 | 181600 batches | lr 0.0001 | ms/batch 141.27627 | loss 1.19404 | ppl     3.300\n",
      "| epoch   1 step   181800 | 181800 batches | lr 0.0001 | ms/batch 139.13436 | loss 1.19596 | ppl     3.307\n",
      "| epoch   1 step   182000 | 182000 batches | lr 0.0001 | ms/batch 138.20752 | loss 1.21632 | ppl     3.375\n",
      "| epoch   1 step   182200 | 182200 batches | lr 0.0001 | ms/batch 139.36319 | loss 1.22251 | ppl     3.396\n",
      "| epoch   1 step   182400 | 182400 batches | lr 0.0001 | ms/batch 139.90963 | loss 1.19230 | ppl     3.295\n",
      "| epoch   1 step   182600 | 182600 batches | lr 0.0001 | ms/batch 140.71559 | loss 1.19730 | ppl     3.311\n",
      "| epoch   1 step   182800 | 182800 batches | lr 0.0001 | ms/batch 141.50839 | loss 1.24521 | ppl     3.474\n",
      "| epoch   1 step   183000 | 183000 batches | lr 0.0001 | ms/batch 139.25241 | loss 1.20982 | ppl     3.353\n",
      "| epoch   1 step   183200 | 183200 batches | lr 0.0001 | ms/batch 140.32221 | loss 1.18783 | ppl     3.280\n",
      "| epoch   1 step   183400 | 183400 batches | lr 0.0001 | ms/batch 142.31759 | loss 1.19256 | ppl     3.295\n",
      "| epoch   1 step   183600 | 183600 batches | lr 0.0001 | ms/batch 141.60718 | loss 1.19737 | ppl     3.311\n",
      "| epoch   1 step   183800 | 183800 batches | lr 0.0001 | ms/batch 139.38599 | loss 1.19665 | ppl     3.309\n",
      "| epoch   1 step   184000 | 184000 batches | lr 0.0001 | ms/batch 142.17756 | loss 1.21593 | ppl     3.373\n",
      "maslina\n",
      "|\n",
      "Source: [ 5  6  8  7 10 11  5  4  2 11  3  2  5  3  7  6  7  2  9  6  4  2  8  4\n",
      "  1  4  8  2  4  6  9  2  7  6  7  3  5  2  3 11  2  4  5 11 10  7  8  6]\n",
      "Target: [ 6  8  7 10 11  5  4  2 11  3  2  5  3  7  6  7  2  9  6  4  2  8  4  1\n",
      "  4  8  2  4  6  9  2  7  6  7  3  5  2  3 11  2  4  5 11 10  7  8  6  5]\n",
      "Teacher forcing: acc:0.5482568027210885\n",
      "Preds:  [ 3  9  8  8  8  8  9  3  3  7  8  3 11  6  5  5  2 11  5 10  7  9 11  7\n",
      "  4  8  2  4  6  9  2  7  6  7  3  5  2  2  7  7  2  7  2  7  7  7  7  7]\n",
      "\n",
      "No teacher forcing: acc:0.5489742772108843\n",
      "Preds:  [ 6  8  7 10 11  5  4  2 11  3  2  5  3  7  6  7  2  9  6  4  2  8  4  1\n",
      "  4  8  2  4  6  9  2  7  6  7  3  5  2  2  2  2  2  2  2  2  2  2  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  46 at step   184000 | time: 576.58s | valid loss 1.15153 | valid ppl    3.1630 | valid acc 0.549\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   184200 | 184200 batches | lr 0.0001 | ms/batch 198.24682 | loss 1.24296 | ppl     3.466\n",
      "| epoch   1 step   184400 | 184400 batches | lr 0.0001 | ms/batch 140.14309 | loss 1.18932 | ppl     3.285\n",
      "| epoch   1 step   184600 | 184600 batches | lr 0.0001 | ms/batch 139.22639 | loss 1.20693 | ppl     3.343\n",
      "| epoch   1 step   184800 | 184800 batches | lr 0.0001 | ms/batch 145.56781 | loss 1.19460 | ppl     3.302\n",
      "| epoch   1 step   185000 | 185000 batches | lr 0.0001 | ms/batch 147.80726 | loss 1.20824 | ppl     3.348\n",
      "| epoch   1 step   185200 | 185200 batches | lr 0.0001 | ms/batch 139.63869 | loss 1.19034 | ppl     3.288\n",
      "| epoch   1 step   185400 | 185400 batches | lr 0.0001 | ms/batch 138.39004 | loss 1.20314 | ppl     3.331\n",
      "| epoch   1 step   185600 | 185600 batches | lr 0.0001 | ms/batch 138.31362 | loss 1.18481 | ppl     3.270\n",
      "| epoch   1 step   185800 | 185800 batches | lr 0.0001 | ms/batch 136.70251 | loss 1.19931 | ppl     3.318\n",
      "| epoch   1 step   186000 | 186000 batches | lr 0.0001 | ms/batch 139.00730 | loss 1.20088 | ppl     3.323\n",
      "| epoch   1 step   186200 | 186200 batches | lr 0.0001 | ms/batch 139.05142 | loss 1.21154 | ppl     3.359\n",
      "| epoch   1 step   186400 | 186400 batches | lr 0.0001 | ms/batch 140.89975 | loss 1.20274 | ppl     3.329\n",
      "| epoch   1 step   186600 | 186600 batches | lr 0.0001 | ms/batch 140.38895 | loss 1.19207 | ppl     3.294\n",
      "| epoch   1 step   186800 | 186800 batches | lr 0.0001 | ms/batch 140.05423 | loss 1.21439 | ppl     3.368\n",
      "| epoch   1 step   187000 | 187000 batches | lr 0.0001 | ms/batch 138.49696 | loss 1.23479 | ppl     3.438\n",
      "| epoch   1 step   187200 | 187200 batches | lr 0.0001 | ms/batch 140.66054 | loss 1.21532 | ppl     3.371\n",
      "| epoch   1 step   187400 | 187400 batches | lr 0.0001 | ms/batch 142.46398 | loss 1.19873 | ppl     3.316\n",
      "| epoch   1 step   187600 | 187600 batches | lr 0.0001 | ms/batch 140.71046 | loss 1.19599 | ppl     3.307\n",
      "| epoch   1 step   187800 | 187800 batches | lr 0.0001 | ms/batch 140.02448 | loss 1.20521 | ppl     3.337\n",
      "| epoch   1 step   188000 | 188000 batches | lr 0.0001 | ms/batch 140.24519 | loss 1.19572 | ppl     3.306\n",
      "|\n",
      "Source: [ 2 11  5  6  5  7  2 11  5  6  6  5  3  6  5  5  4  9  4  2 10  6  9  7\n",
      "  1  7  9  6 10  2  4  9  4  5  5  6  3  5  6  6  5 11  2  7  5  6  5 11]\n",
      "Target: [11  5  6  5  7  2 11  5  6  6  5  3  6  5  5  4  9  4  2 10  6  9  7  1\n",
      "  7  9  6 10  2  4  9  4  5  5  6  3  5  6  6  5 11  2  7  5  6  5 11  2]\n",
      "Teacher forcing: acc:0.5509635416666666\n",
      "Preds:  [ 3  8  9  9  9  8  3  9  9  9  9  3  6  5  5  5 11  6 11  7  6 10  5  2\n",
      "  7  9  6 10  2  4  9  4  5  5  6  3 11 11 11 11 11 11  8 11 11 11  8 11]\n",
      "\n",
      "No teacher forcing: acc:0.550234375\n",
      "Preds:  [11  5  6  5  7  2 11  5  6  6  5  3  6  5  5  4  9  4  2 10  6  9  7  1\n",
      "  7  9  6 10  2  4  9  4  5  5  6  3 11 11 11 11 11 11 11 11 11 11 11  8]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  47 at step   188000 | time: 573.32s | valid loss 1.15144 | valid ppl    3.1627 | valid acc 0.55\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   188200 | 188200 batches | lr 0.0001 | ms/batch 200.42872 | loss 1.20347 | ppl     3.332\n",
      "| epoch   1 step   188400 | 188400 batches | lr 0.0001 | ms/batch 141.83077 | loss 1.18516 | ppl     3.271\n",
      "| epoch   1 step   188600 | 188600 batches | lr 0.0001 | ms/batch 142.40650 | loss 1.17798 | ppl     3.248\n",
      "| epoch   1 step   188800 | 188800 batches | lr 0.0001 | ms/batch 143.57885 | loss 1.19508 | ppl     3.304\n",
      "| epoch   1 step   189000 | 189000 batches | lr 0.0001 | ms/batch 144.85468 | loss 1.18956 | ppl     3.286\n",
      "| epoch   1 step   189200 | 189200 batches | lr 0.0001 | ms/batch 148.63434 | loss 1.20542 | ppl     3.338\n",
      "| epoch   1 step   189400 | 189400 batches | lr 0.0001 | ms/batch 144.40505 | loss 1.19592 | ppl     3.307\n",
      "| epoch   1 step   189600 | 189600 batches | lr 0.0001 | ms/batch 142.51160 | loss 1.19632 | ppl     3.308\n",
      "| epoch   1 step   189800 | 189800 batches | lr 0.0001 | ms/batch 141.52846 | loss 1.19570 | ppl     3.306\n",
      "| epoch   1 step   190000 | 190000 batches | lr 0.0001 | ms/batch 138.05685 | loss 1.18540 | ppl     3.272\n",
      "| epoch   1 step   190200 | 190200 batches | lr 0.0001 | ms/batch 137.53454 | loss 1.18503 | ppl     3.271\n",
      "| epoch   1 step   190400 | 190400 batches | lr 0.0001 | ms/batch 137.70590 | loss 1.18392 | ppl     3.267\n",
      "| epoch   1 step   190600 | 190600 batches | lr 0.0001 | ms/batch 139.29620 | loss 1.21713 | ppl     3.377\n",
      "| epoch   1 step   190800 | 190800 batches | lr 0.0001 | ms/batch 138.64325 | loss 1.21224 | ppl     3.361\n",
      "| epoch   1 step   191000 | 191000 batches | lr 0.0001 | ms/batch 139.71835 | loss 1.19934 | ppl     3.318\n",
      "| epoch   1 step   191200 | 191200 batches | lr 0.0001 | ms/batch 139.19929 | loss 1.20614 | ppl     3.341\n",
      "| epoch   1 step   191400 | 191400 batches | lr 0.0001 | ms/batch 139.03217 | loss 1.20600 | ppl     3.340\n",
      "| epoch   1 step   191600 | 191600 batches | lr 0.0001 | ms/batch 137.81548 | loss 1.18246 | ppl     3.262\n",
      "| epoch   1 step   191800 | 191800 batches | lr 0.0001 | ms/batch 138.81436 | loss 1.19031 | ppl     3.288\n",
      "| epoch   1 step   192000 | 192000 batches | lr 0.0001 | ms/batch 138.80144 | loss 1.18009 | ppl     3.255\n",
      "maslina\n",
      "|\n",
      "Source: [ 4  2  2  7 10  8  3  7 10  4  7  8  3 10 11 11  6  7  3  8  5  6  8  7\n",
      "  1  7  8  6  5  8  3  7  6 11 11 10  3  8  7  4 10  7  3  8 10  7  2  2]\n",
      "Target: [ 2  2  7 10  8  3  7 10  4  7  8  3 10 11 11  6  7  3  8  5  6  8  7  1\n",
      "  7  8  6  5  8  3  7  6 11 11 10  3  8  7  4 10  7  3  8 10  7  2  2  4]\n",
      "Teacher forcing: acc:0.5514455782312925\n",
      "Preds:  [ 3  3  3  8  8  8  8  8  8  3  8 11  5  5  5  2  5  2  5  6  2  2  8  4\n",
      "  7  8  6  5  8  3  7  6 11 11 10  3  2  8  7  8 11  7  2  8 11  7  8  8]\n",
      "\n",
      "No teacher forcing: acc:0.5512861394557823\n",
      "Preds:  [ 2  2  7 10  8  3  7 10  4  7  8  3 10 11 11  6  7  3  8  5  6  8  7  1\n",
      "  7  8  6  5  8  3  7  6 11 11 10  3  2  8  8  8  8  8  8  8  8  8  8  8]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  48 at step   192000 | time: 574.61s | valid loss 1.15139 | valid ppl    3.1626 | valid acc 0.551\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   192200 | 192200 batches | lr 0.0001 | ms/batch 195.68860 | loss 1.18339 | ppl     3.265\n",
      "| epoch   1 step   192400 | 192400 batches | lr 0.0001 | ms/batch 139.16592 | loss 1.19912 | ppl     3.317\n",
      "| epoch   1 step   192600 | 192600 batches | lr 0.0001 | ms/batch 140.76058 | loss 1.19657 | ppl     3.309\n",
      "| epoch   1 step   192800 | 192800 batches | lr 0.0001 | ms/batch 140.88216 | loss 1.19281 | ppl     3.296\n",
      "| epoch   1 step   193000 | 193000 batches | lr 0.0001 | ms/batch 141.30369 | loss 1.19026 | ppl     3.288\n",
      "| epoch   1 step   193200 | 193200 batches | lr 0.0001 | ms/batch 142.09253 | loss 1.18601 | ppl     3.274\n",
      "| epoch   1 step   193400 | 193400 batches | lr 0.0001 | ms/batch 148.63420 | loss 1.17964 | ppl     3.253\n",
      "| epoch   1 step   193600 | 193600 batches | lr 0.0001 | ms/batch 146.51653 | loss 1.18223 | ppl     3.262\n",
      "| epoch   1 step   193800 | 193800 batches | lr 0.0001 | ms/batch 141.31318 | loss 1.18367 | ppl     3.266\n",
      "| epoch   1 step   194000 | 194000 batches | lr 0.0001 | ms/batch 138.94483 | loss 1.18718 | ppl     3.278\n",
      "| epoch   1 step   194200 | 194200 batches | lr 0.0001 | ms/batch 140.55333 | loss 1.17825 | ppl     3.249\n",
      "| epoch   1 step   194400 | 194400 batches | lr 0.0001 | ms/batch 139.56814 | loss 1.18774 | ppl     3.280\n",
      "| epoch   1 step   194600 | 194600 batches | lr 0.0001 | ms/batch 140.14399 | loss 1.19487 | ppl     3.303\n",
      "| epoch   1 step   194800 | 194800 batches | lr 0.0001 | ms/batch 141.01985 | loss 1.20572 | ppl     3.339\n",
      "| epoch   1 step   195000 | 195000 batches | lr 0.0001 | ms/batch 140.62622 | loss 1.18325 | ppl     3.265\n",
      "| epoch   1 step   195200 | 195200 batches | lr 0.0001 | ms/batch 138.98359 | loss 1.18070 | ppl     3.257\n",
      "| epoch   1 step   195400 | 195400 batches | lr 0.0001 | ms/batch 138.82231 | loss 1.19275 | ppl     3.296\n",
      "| epoch   1 step   195600 | 195600 batches | lr 0.0001 | ms/batch 138.90656 | loss 1.18615 | ppl     3.274\n",
      "| epoch   1 step   195800 | 195800 batches | lr 0.0001 | ms/batch 137.99303 | loss 1.18502 | ppl     3.271\n",
      "| epoch   1 step   196000 | 196000 batches | lr 0.0001 | ms/batch 138.18208 | loss 1.19078 | ppl     3.290\n",
      "|\n",
      "Source: [ 5  7  2  9  6  8 11  3  6  8  6  2  3  4  5  8  5  4 10  6 11 10  7  7\n",
      "  1  7  7 10 11  6 10  4  5  8  5  4  3  2  6  8  6  3 11  8  6  9  2  7]\n",
      "Target: [ 7  2  9  6  8 11  3  6  8  6  2  3  4  5  8  5  4 10  6 11 10  7  7  1\n",
      "  7  7 10 11  6 10  4  5  8  5  4  3  2  6  8  6  3 11  8  6  9  2  7  5]\n",
      "Teacher forcing: acc:0.55078125\n",
      "Preds:  [ 3  8  3  8  9  9  8  8  9  7  3  3  5  5  5  6  5  6  6 11 11  6  2  2\n",
      "  7  7 10 11  6 10  4  5  8  5  4  3  3  3  3 10  3  3  3  3  3  3 10  3]\n",
      "\n",
      "No teacher forcing: acc:0.5500520833333333\n",
      "Preds:  [ 7  2  9  6  8 11  3  6  8  6  2  3  4  5  8  5  4 10  6 11 10  7  7  1\n",
      "  7  7 10 11  6 10  4  5  8  5  4  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  49 at step   196000 | time: 574.43s | valid loss 1.15142 | valid ppl    3.1627 | valid acc 0.55\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   196200 | 196200 batches | lr 0.0001 | ms/batch 199.73226 | loss 1.18151 | ppl     3.259\n",
      "| epoch   1 step   196400 | 196400 batches | lr 0.0001 | ms/batch 140.58295 | loss 1.18128 | ppl     3.259\n",
      "| epoch   1 step   196600 | 196600 batches | lr 0.0001 | ms/batch 138.26299 | loss 1.18822 | ppl     3.281\n",
      "| epoch   1 step   196800 | 196800 batches | lr 0.0001 | ms/batch 137.80807 | loss 1.18417 | ppl     3.268\n",
      "| epoch   1 step   197000 | 197000 batches | lr 0.0001 | ms/batch 138.61896 | loss 1.20373 | ppl     3.333\n",
      "| epoch   1 step   197200 | 197200 batches | lr 0.0001 | ms/batch 138.17762 | loss 1.19545 | ppl     3.305\n",
      "| epoch   1 step   197400 | 197400 batches | lr 0.0001 | ms/batch 138.44949 | loss 1.20714 | ppl     3.344\n",
      "| epoch   1 step   197600 | 197600 batches | lr 0.0001 | ms/batch 140.22539 | loss 1.19016 | ppl     3.288\n",
      "| epoch   1 step   197800 | 197800 batches | lr 0.0001 | ms/batch 138.47224 | loss 1.18941 | ppl     3.285\n",
      "| epoch   1 step   198000 | 198000 batches | lr 0.0001 | ms/batch 140.47117 | loss 1.19014 | ppl     3.288\n",
      "| epoch   1 step   198200 | 198200 batches | lr 0.0001 | ms/batch 138.19779 | loss 1.17171 | ppl     3.228\n",
      "| epoch   1 step   198400 | 198400 batches | lr 0.0001 | ms/batch 140.20509 | loss 1.19029 | ppl     3.288\n",
      "| epoch   1 step   198600 | 198600 batches | lr 0.0001 | ms/batch 139.01365 | loss 1.18320 | ppl     3.265\n",
      "| epoch   1 step   198800 | 198800 batches | lr 0.0001 | ms/batch 139.82863 | loss 1.18229 | ppl     3.262\n",
      "| epoch   1 step   199000 | 199000 batches | lr 0.0001 | ms/batch 140.28800 | loss 1.18397 | ppl     3.267\n",
      "| epoch   1 step   199200 | 199200 batches | lr 0.0001 | ms/batch 136.84164 | loss 1.19671 | ppl     3.309\n",
      "| epoch   1 step   199400 | 199400 batches | lr 0.0001 | ms/batch 139.06484 | loss 1.19425 | ppl     3.301\n",
      "| epoch   1 step   199600 | 199600 batches | lr 0.0001 | ms/batch 136.19792 | loss 1.19748 | ppl     3.312\n",
      "| epoch   1 step   199800 | 199800 batches | lr 0.0001 | ms/batch 140.32395 | loss 1.22363 | ppl     3.400\n",
      "| epoch   1 step   200000 | 200000 batches | lr 0.0001 | ms/batch 140.56954 | loss 1.17972 | ppl     3.253\n",
      "|\n",
      "Source: [ 4  9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10\n",
      "  1 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9]\n",
      "Target: [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9  4]\n",
      "Teacher forcing: acc:0.55015625\n",
      "Preds:  [ 3  8  8  7 10  3  8  9  3  8  7  8  6  6  5  5  5  6  6  5  6  3  7  6\n",
      " 10  4  4  6 11  4  8  7  9  6  5 10 11  7 11 11  7 11  7 11 11 11 11 11]\n",
      "\n",
      "No teacher forcing: acc:0.5508072916666666\n",
      "Preds:  [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10  4  4  6 11  4  8  7  9  6  5 10 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  50 at step   200000 | time: 568.10s | valid loss 1.15145 | valid ppl    3.1628 | valid acc 0.551\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   200200 | 200200 batches | lr 0.0001 | ms/batch 197.21991 | loss 1.17692 | ppl     3.244\n",
      "| epoch   1 step   200400 | 200400 batches | lr 0.0001 | ms/batch 140.19249 | loss 1.18211 | ppl     3.261\n",
      "| epoch   1 step   200600 | 200600 batches | lr 0.0001 | ms/batch 139.84001 | loss 1.18598 | ppl     3.274\n",
      "| epoch   1 step   200800 | 200800 batches | lr 0.0001 | ms/batch 140.48247 | loss 1.17050 | ppl     3.224\n",
      "| epoch   1 step   201000 | 201000 batches | lr 0.0001 | ms/batch 138.57471 | loss 1.19053 | ppl     3.289\n",
      "| epoch   1 step   201200 | 201200 batches | lr 0.0001 | ms/batch 138.88095 | loss 1.19821 | ppl     3.314\n",
      "| epoch   1 step   201400 | 201400 batches | lr 0.0001 | ms/batch 139.55960 | loss 1.18166 | ppl     3.260\n",
      "| epoch   1 step   201600 | 201600 batches | lr 0.0001 | ms/batch 140.01459 | loss 1.17577 | ppl     3.241\n",
      "| epoch   1 step   201800 | 201800 batches | lr 0.0001 | ms/batch 138.39873 | loss 1.19587 | ppl     3.306\n",
      "| epoch   1 step   202000 | 202000 batches | lr 0.0001 | ms/batch 138.46283 | loss 1.19247 | ppl     3.295\n",
      "| epoch   1 step   202200 | 202200 batches | lr 0.0001 | ms/batch 139.08279 | loss 1.17790 | ppl     3.248\n",
      "| epoch   1 step   202400 | 202400 batches | lr 0.0001 | ms/batch 139.55121 | loss 1.17800 | ppl     3.248\n",
      "| epoch   1 step   202600 | 202600 batches | lr 0.0001 | ms/batch 138.29062 | loss 1.18240 | ppl     3.262\n",
      "| epoch   1 step   202800 | 202800 batches | lr 0.0001 | ms/batch 140.62876 | loss 1.19149 | ppl     3.292\n",
      "| epoch   1 step   203000 | 203000 batches | lr 0.0001 | ms/batch 140.85568 | loss 1.18715 | ppl     3.278\n",
      "| epoch   1 step   203200 | 203200 batches | lr 0.0001 | ms/batch 140.07607 | loss 1.18305 | ppl     3.264\n",
      "| epoch   1 step   203400 | 203400 batches | lr 0.0001 | ms/batch 138.53665 | loss 1.19827 | ppl     3.314\n",
      "| epoch   1 step   203600 | 203600 batches | lr 0.0001 | ms/batch 139.10294 | loss 1.16918 | ppl     3.219\n",
      "| epoch   1 step   203800 | 203800 batches | lr 0.0001 | ms/batch 138.88113 | loss 1.18750 | ppl     3.279\n",
      "| epoch   1 step   204000 | 204000 batches | lr 0.0001 | ms/batch 140.07489 | loss 1.19199 | ppl     3.294\n",
      "maslina\n",
      "|\n",
      "Source: [ 4 10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8\n",
      "  1  8  5  8  2  9 10 10  8  2  6  6  5  8 10  2  7  9  3  6  6  5  5 10]\n",
      "Target: [10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8  1\n",
      "  8  5  8  2  9 10 10  8  2  6  6  5  8 10  2  7  9  3  6  6  5  5 10  4]\n",
      "Teacher forcing: acc:0.5496917517006803\n",
      "Preds:  [ 3  8  3  3  9  9  8  8  8  3  9 11  5  5  5  5  6  6  6  5  9  6  9  6\n",
      "  8  5  8  2  9 10 10  8  2  6  6  5  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "No teacher forcing: acc:0.5496917517006803\n",
      "Preds:  [10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8  1\n",
      "  8  5  8  2  9 10 10  8  2  6  6  5  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  51 at step   204000 | time: 569.25s | valid loss 1.15138 | valid ppl    3.1626 | valid acc 0.55\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   204200 | 204200 batches | lr 0.0001 | ms/batch 195.71746 | loss 1.19459 | ppl     3.302\n",
      "| epoch   1 step   204400 | 204400 batches | lr 0.0001 | ms/batch 139.88524 | loss 1.17706 | ppl     3.245\n",
      "| epoch   1 step   204600 | 204600 batches | lr 0.0001 | ms/batch 138.52798 | loss 1.18212 | ppl     3.261\n",
      "| epoch   1 step   204800 | 204800 batches | lr 0.0001 | ms/batch 139.21698 | loss 1.18532 | ppl     3.272\n",
      "| epoch   1 step   205000 | 205000 batches | lr 0.0001 | ms/batch 138.32332 | loss 1.18220 | ppl     3.262\n",
      "| epoch   1 step   205200 | 205200 batches | lr 0.0001 | ms/batch 141.07085 | loss 1.17876 | ppl     3.250\n",
      "| epoch   1 step   205400 | 205400 batches | lr 0.0001 | ms/batch 140.83699 | loss 1.18315 | ppl     3.265\n",
      "| epoch   1 step   205600 | 205600 batches | lr 0.0001 | ms/batch 142.54903 | loss 1.18872 | ppl     3.283\n",
      "| epoch   1 step   205800 | 205800 batches | lr 0.0001 | ms/batch 143.95414 | loss 1.17432 | ppl     3.236\n",
      "| epoch   1 step   206000 | 206000 batches | lr 0.0001 | ms/batch 142.55137 | loss 1.17564 | ppl     3.240\n",
      "| epoch   1 step   206200 | 206200 batches | lr 0.0001 | ms/batch 143.66779 | loss 1.16813 | ppl     3.216\n",
      "| epoch   1 step   206400 | 206400 batches | lr 0.0001 | ms/batch 144.89428 | loss 1.16608 | ppl     3.209\n",
      "| epoch   1 step   206600 | 206600 batches | lr 0.0001 | ms/batch 142.56145 | loss 1.19476 | ppl     3.303\n",
      "| epoch   1 step   206800 | 206800 batches | lr 0.0001 | ms/batch 143.62255 | loss 1.18998 | ppl     3.287\n",
      "| epoch   1 step   207000 | 207000 batches | lr 0.0001 | ms/batch 142.71895 | loss 1.18842 | ppl     3.282\n",
      "| epoch   1 step   207200 | 207200 batches | lr 0.0001 | ms/batch 141.69072 | loss 1.17509 | ppl     3.238\n",
      "| epoch   1 step   207400 | 207400 batches | lr 0.0001 | ms/batch 145.30512 | loss 1.19628 | ppl     3.308\n",
      "| epoch   1 step   207600 | 207600 batches | lr 0.0001 | ms/batch 143.72843 | loss 1.18119 | ppl     3.258\n",
      "| epoch   1 step   207800 | 207800 batches | lr 0.0001 | ms/batch 143.37444 | loss 1.17615 | ppl     3.242\n",
      "| epoch   1 step   208000 | 208000 batches | lr 0.0001 | ms/batch 141.49114 | loss 1.16503 | ppl     3.206\n",
      "|\n",
      "Source: [ 9  7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3\n",
      "  1  3  3  4 11  6  5  9  9  8  3  7  6  2  9 10 10 11  4 11  3  3 10  7]\n",
      "Target: [ 7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3  1\n",
      "  3  3  4 11  6  5  9  9  8  3  7  6  2  9 10 10 11  4 11  3  3 10  7  9]\n",
      "Teacher forcing: acc:0.5498697916666667\n",
      "Preds:  [ 8  8  8  8  8  8  3  9 10 10  8  3  6  5  5  6  5  6  6 10  8  4  2  5\n",
      "  3  3  4 11  6  5  9  9  8  3  7  6  5  5  5  5  5  5  5  5  5  5  5  5]\n",
      "\n",
      "No teacher forcing: acc:0.5498697916666667\n",
      "Preds:  [ 7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3  1\n",
      "  3  3  4 11  6  5  9  9  8  3  7  6  5  5  5  5  5  5  5  5  5  5  5  5]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  52 at step   208000 | time: 579.20s | valid loss 1.15139 | valid ppl    3.1626 | valid acc 0.55\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   208200 | 208200 batches | lr 0.0001 | ms/batch 201.60707 | loss 1.17540 | ppl     3.239\n",
      "| epoch   1 step   208400 | 208400 batches | lr 0.0001 | ms/batch 141.90448 | loss 1.18123 | ppl     3.258\n",
      "| epoch   1 step   208600 | 208600 batches | lr 0.0001 | ms/batch 139.88376 | loss 1.18352 | ppl     3.266\n",
      "| epoch   1 step   208800 | 208800 batches | lr 0.0001 | ms/batch 139.51425 | loss 1.17372 | ppl     3.234\n",
      "| epoch   1 step   209000 | 209000 batches | lr 0.0001 | ms/batch 138.22595 | loss 1.18008 | ppl     3.255\n",
      "| epoch   1 step   209200 | 209200 batches | lr 0.0001 | ms/batch 139.59583 | loss 1.17275 | ppl     3.231\n",
      "| epoch   1 step   209400 | 209400 batches | lr 0.0001 | ms/batch 138.75168 | loss 1.19194 | ppl     3.293\n",
      "| epoch   1 step   209600 | 209600 batches | lr 0.0001 | ms/batch 137.34455 | loss 1.18833 | ppl     3.282\n",
      "| epoch   1 step   209800 | 209800 batches | lr 0.0001 | ms/batch 138.38981 | loss 1.17307 | ppl     3.232\n",
      "| epoch   1 step   210000 | 210000 batches | lr 0.0001 | ms/batch 139.10449 | loss 1.16490 | ppl     3.206\n",
      "| epoch   1 step   210200 | 210200 batches | lr 0.0001 | ms/batch 139.38668 | loss 1.18595 | ppl     3.274\n",
      "| epoch   1 step   210400 | 210400 batches | lr 0.0001 | ms/batch 139.22391 | loss 1.17131 | ppl     3.226\n",
      "| epoch   1 step   210600 | 210600 batches | lr 0.0001 | ms/batch 138.89751 | loss 1.18502 | ppl     3.271\n",
      "| epoch   1 step   210800 | 210800 batches | lr 0.0001 | ms/batch 138.92788 | loss 1.18861 | ppl     3.283\n",
      "| epoch   1 step   211000 | 211000 batches | lr 0.0001 | ms/batch 138.74994 | loss 1.17504 | ppl     3.238\n",
      "| epoch   1 step   211200 | 211200 batches | lr 0.0001 | ms/batch 137.71353 | loss 1.17738 | ppl     3.246\n",
      "| epoch   1 step   211400 | 211400 batches | lr 0.0001 | ms/batch 137.98267 | loss 1.20205 | ppl     3.327\n",
      "| epoch   1 step   211600 | 211600 batches | lr 0.0001 | ms/batch 139.71657 | loss 1.18875 | ppl     3.283\n",
      "| epoch   1 step   211800 | 211800 batches | lr 0.0001 | ms/batch 140.40323 | loss 1.19568 | ppl     3.306\n",
      "| epoch   1 step   212000 | 212000 batches | lr 0.0001 | ms/batch 141.02896 | loss 1.17223 | ppl     3.229\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9\n",
      "  1  9  6  4  6  2  6 10 10 10  8  8  3  9  7 11  2 11  3  3  9  4  6  3]\n",
      "Target: [ 3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9  1\n",
      "  9  6  4  6  2  6 10 10 10  8  8  3  9  7 11  2 11  3  3  9  4  6  3  7]\n",
      "Teacher forcing: acc:0.5497714710884354\n",
      "Preds:  [ 8  8  9  9  8  8  8  8  9  8  8  8  5  6  6  6  6  6  5  2  2  7 11  6\n",
      "  9  6  4  6  2  6 10 10 10  8  8  3  3  3  7  3  3  3  3  3  3  3  7  3]\n",
      "\n",
      "No teacher forcing: acc:0.5499574829931972\n",
      "Preds:  [ 3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9  1\n",
      "  9  6  4  6  2  6 10 10 10  8  8  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  53 at step   212000 | time: 569.06s | valid loss 1.15146 | valid ppl    3.1628 | valid acc 0.55\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   212200 | 212200 batches | lr 0.0001 | ms/batch 196.34016 | loss 1.17032 | ppl     3.223\n",
      "| epoch   1 step   212400 | 212400 batches | lr 0.0001 | ms/batch 139.75390 | loss 1.16478 | ppl     3.205\n",
      "| epoch   1 step   212600 | 212600 batches | lr 0.0001 | ms/batch 140.63464 | loss 1.20473 | ppl     3.336\n",
      "| epoch   1 step   212800 | 212800 batches | lr 0.0001 | ms/batch 139.72610 | loss 1.19690 | ppl     3.310\n",
      "| epoch   1 step   213000 | 213000 batches | lr 0.0001 | ms/batch 138.32726 | loss 1.18474 | ppl     3.270\n",
      "| epoch   1 step   213200 | 213200 batches | lr 0.0001 | ms/batch 140.23160 | loss 1.19369 | ppl     3.299\n",
      "| epoch   1 step   213400 | 213400 batches | lr 0.0001 | ms/batch 139.83284 | loss 1.17515 | ppl     3.239\n",
      "| epoch   1 step   213600 | 213600 batches | lr 0.0001 | ms/batch 139.20284 | loss 1.18831 | ppl     3.282\n",
      "| epoch   1 step   213800 | 213800 batches | lr 0.0001 | ms/batch 140.61165 | loss 1.16939 | ppl     3.220\n",
      "| epoch   1 step   214000 | 214000 batches | lr 0.0001 | ms/batch 139.81291 | loss 1.17163 | ppl     3.227\n",
      "| epoch   1 step   214200 | 214200 batches | lr 0.0001 | ms/batch 139.26372 | loss 1.19583 | ppl     3.306\n",
      "| epoch   1 step   214400 | 214400 batches | lr 0.0001 | ms/batch 140.72067 | loss 1.22403 | ppl     3.401\n",
      "| epoch   1 step   214600 | 214600 batches | lr 0.0001 | ms/batch 141.82294 | loss 1.19942 | ppl     3.318\n",
      "| epoch   1 step   214800 | 214800 batches | lr 0.0001 | ms/batch 145.24095 | loss 1.18150 | ppl     3.259\n",
      "| epoch   1 step   215000 | 215000 batches | lr 0.0001 | ms/batch 143.16578 | loss 1.20705 | ppl     3.344\n",
      "| epoch   1 step   215200 | 215200 batches | lr 0.0001 | ms/batch 142.59405 | loss 1.18005 | ppl     3.255\n",
      "| epoch   1 step   215400 | 215400 batches | lr 0.0001 | ms/batch 144.08882 | loss 1.20096 | ppl     3.323\n",
      "| epoch   1 step   215600 | 215600 batches | lr 0.0001 | ms/batch 144.13980 | loss 1.17072 | ppl     3.224\n",
      "| epoch   1 step   215800 | 215800 batches | lr 0.0001 | ms/batch 141.52700 | loss 1.17066 | ppl     3.224\n",
      "| epoch   1 step   216000 | 216000 batches | lr 0.0001 | ms/batch 141.05813 | loss 1.17160 | ppl     3.227\n",
      "|\n",
      "Source: [ 2  3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10\n",
      "  1 10  8  3  2  6  6  3  3  6  5 10  8  4  2  3  4  6  2 11  7  9  9  3]\n",
      "Target: [ 3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10  1\n",
      " 10  8  3  2  6  6  3  3  6  5 10  8  4  2  3  4  6  2 11  7  9  9  3  2]\n",
      "Teacher forcing: acc:0.5488802083333333\n",
      "Preds:  [ 9  8  8  8  8  8  9  9  9  8  9  7  5  5  5  2  2  5  5  2  3  7  6  5\n",
      " 10  8  3  2  6  6  3  3  6  5 10  8  4  4  2  2  4  4  2  4  4  4  4  4]\n",
      "\n",
      "No teacher forcing: acc:0.5483072916666667\n",
      "Preds:  [ 3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10  1\n",
      " 10  8  3  2  6  6  3  3  6  5 10  8  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  54 at step   216000 | time: 575.84s | valid loss 1.15133 | valid ppl    3.1624 | valid acc 0.548\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   216200 | 216200 batches | lr 0.0001 | ms/batch 197.46457 | loss 1.16193 | ppl     3.196\n",
      "| epoch   1 step   216400 | 216400 batches | lr 0.0001 | ms/batch 139.62917 | loss 1.20923 | ppl     3.351\n",
      "| epoch   1 step   216600 | 216600 batches | lr 0.0001 | ms/batch 137.48142 | loss 1.17268 | ppl     3.231\n",
      "| epoch   1 step   216800 | 216800 batches | lr 0.0001 | ms/batch 138.49745 | loss 1.17754 | ppl     3.246\n",
      "| epoch   1 step   217000 | 217000 batches | lr 0.0001 | ms/batch 138.49915 | loss 1.18431 | ppl     3.268\n",
      "| epoch   1 step   217200 | 217200 batches | lr 0.0001 | ms/batch 138.97130 | loss 1.16223 | ppl     3.197\n",
      "| epoch   1 step   217400 | 217400 batches | lr 0.0001 | ms/batch 140.04138 | loss 1.16987 | ppl     3.222\n",
      "| epoch   1 step   217600 | 217600 batches | lr 0.0001 | ms/batch 139.07226 | loss 1.18191 | ppl     3.261\n",
      "| epoch   1 step   217800 | 217800 batches | lr 0.0001 | ms/batch 138.61664 | loss 1.16836 | ppl     3.217\n",
      "| epoch   1 step   218000 | 218000 batches | lr 0.0001 | ms/batch 141.67827 | loss 1.17065 | ppl     3.224\n",
      "| epoch   1 step   218200 | 218200 batches | lr 0.0001 | ms/batch 142.26292 | loss 1.17030 | ppl     3.223\n",
      "| epoch   1 step   218400 | 218400 batches | lr 0.0001 | ms/batch 139.56925 | loss 1.17753 | ppl     3.246\n",
      "| epoch   1 step   218600 | 218600 batches | lr 0.0001 | ms/batch 137.13135 | loss 1.18052 | ppl     3.256\n",
      "| epoch   1 step   218800 | 218800 batches | lr 0.0001 | ms/batch 138.43190 | loss 1.16694 | ppl     3.212\n",
      "| epoch   1 step   219000 | 219000 batches | lr 0.0001 | ms/batch 140.04986 | loss 1.17528 | ppl     3.239\n",
      "| epoch   1 step   219200 | 219200 batches | lr 0.0001 | ms/batch 138.88732 | loss 1.16232 | ppl     3.197\n",
      "| epoch   1 step   219400 | 219400 batches | lr 0.0001 | ms/batch 137.84355 | loss 1.19315 | ppl     3.297\n",
      "| epoch   1 step   219600 | 219600 batches | lr 0.0001 | ms/batch 140.06615 | loss 1.18794 | ppl     3.280\n",
      "| epoch   1 step   219800 | 219800 batches | lr 0.0001 | ms/batch 140.05891 | loss 1.19782 | ppl     3.313\n",
      "| epoch   1 step   220000 | 220000 batches | lr 0.0001 | ms/batch 141.08234 | loss 1.18974 | ppl     3.286\n",
      "|\n",
      "Source: [ 2  3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3\n",
      "  1  3  3 10  2  5  6  4  2 11 10  6  6  6  7  6  4  7  8  7  2  2 11  3]\n",
      "Target: [ 3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3  1\n",
      "  3  3 10  2  5  6  4  2 11 10  6  6  6  7  6  4  7  8  7  2  2 11  3  2]\n",
      "Teacher forcing: acc:0.5490364583333334\n",
      "Preds:  [ 9  8  8  9  9  8 10  8  4  9  8  9  6  4  4  4  4  7  6  9  3  7  2  2\n",
      "  3  3 10  2  5  6  4  2 11 10  6  6  2  2  2  2  2  2  2  2  2  2  2  2]\n",
      "\n",
      "No teacher forcing: acc:0.5490364583333334\n",
      "Preds:  [ 3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3  1\n",
      "  3  3 10  2  5  6  4  2 11 10  6  6  2  2  2  2  2  2  2  2  2  2  2  2]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  55 at step   220000 | time: 569.02s | valid loss 1.15145 | valid ppl    3.1628 | valid acc 0.549\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   220200 | 220200 batches | lr 0.0001 | ms/batch 198.83483 | loss 1.17730 | ppl     3.246\n",
      "| epoch   1 step   220400 | 220400 batches | lr 0.0001 | ms/batch 140.05682 | loss 1.16817 | ppl     3.216\n",
      "| epoch   1 step   220600 | 220600 batches | lr 0.0001 | ms/batch 138.43509 | loss 1.18037 | ppl     3.256\n",
      "| epoch   1 step   220800 | 220800 batches | lr 0.0001 | ms/batch 139.17622 | loss 1.16925 | ppl     3.220\n",
      "| epoch   1 step   221000 | 221000 batches | lr 0.0001 | ms/batch 138.40845 | loss 1.16312 | ppl     3.200\n",
      "| epoch   1 step   221200 | 221200 batches | lr 0.0001 | ms/batch 142.06277 | loss 1.17142 | ppl     3.227\n",
      "| epoch   1 step   221400 | 221400 batches | lr 0.0001 | ms/batch 141.51378 | loss 1.17921 | ppl     3.252\n",
      "| epoch   1 step   221600 | 221600 batches | lr 0.0001 | ms/batch 140.01613 | loss 1.16897 | ppl     3.219\n",
      "| epoch   1 step   221800 | 221800 batches | lr 0.0001 | ms/batch 139.11853 | loss 1.16667 | ppl     3.211\n",
      "| epoch   1 step   222000 | 222000 batches | lr 0.0001 | ms/batch 138.28746 | loss 1.18636 | ppl     3.275\n",
      "| epoch   1 step   222200 | 222200 batches | lr 0.0001 | ms/batch 139.38107 | loss 1.17540 | ppl     3.239\n",
      "| epoch   1 step   222400 | 222400 batches | lr 0.0001 | ms/batch 138.69780 | loss 1.17332 | ppl     3.233\n",
      "| epoch   1 step   222600 | 222600 batches | lr 0.0001 | ms/batch 140.19944 | loss 1.17422 | ppl     3.236\n",
      "| epoch   1 step   222800 | 222800 batches | lr 0.0001 | ms/batch 137.60842 | loss 1.18243 | ppl     3.262\n",
      "| epoch   1 step   223000 | 223000 batches | lr 0.0001 | ms/batch 139.55675 | loss 1.18481 | ppl     3.270\n",
      "| epoch   1 step   223200 | 223200 batches | lr 0.0001 | ms/batch 138.54859 | loss 1.19324 | ppl     3.298\n",
      "| epoch   1 step   223400 | 223400 batches | lr 0.0001 | ms/batch 144.80714 | loss 1.17054 | ppl     3.224\n",
      "| epoch   1 step   223600 | 223600 batches | lr 0.0001 | ms/batch 147.89031 | loss 1.16283 | ppl     3.199\n",
      "| epoch   1 step   223800 | 223800 batches | lr 0.0001 | ms/batch 142.86527 | loss 1.16043 | ppl     3.191\n",
      "| epoch   1 step   224000 | 224000 batches | lr 0.0001 | ms/batch 138.05651 | loss 1.16588 | ppl     3.209\n",
      "maslina\n",
      "|\n",
      "Source: [ 9 10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7\n",
      "  1  7 11  3  6 10 11  5  3  9  2 11  9  7  4 10  6  8  4  4  3  4  9 10]\n",
      "Target: [10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7  1\n",
      "  7 11  3  6 10 11  5  3  9  2 11  9  7  4 10  6  8  4  4  3  4  9 10  9]\n",
      "Teacher forcing: acc:0.5503295068027211\n",
      "Preds:  [ 8  8  8  7  8  9  3  7  9  8  7  8 11  5  9  7  5 11  7  7  2  4 11  4\n",
      "  7 11  3  6 10 11  5  3  9  2 11  9 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "No teacher forcing: acc:0.5503826530612245\n",
      "Preds:  [10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7  1\n",
      "  7 11  3  6 10 11  5  3  9  2 11  9 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  56 at step   224000 | time: 572.62s | valid loss 1.15132 | valid ppl    3.1624 | valid acc 0.55\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   224200 | 224200 batches | lr 0.0001 | ms/batch 198.16735 | loss 1.17415 | ppl     3.235\n",
      "| epoch   1 step   224400 | 224400 batches | lr 0.0001 | ms/batch 139.02354 | loss 1.18485 | ppl     3.270\n",
      "| epoch   1 step   224600 | 224600 batches | lr 0.0001 | ms/batch 141.22650 | loss 1.19193 | ppl     3.293\n",
      "| epoch   1 step   224800 | 224800 batches | lr 0.0001 | ms/batch 137.74389 | loss 1.18294 | ppl     3.264\n",
      "| epoch   1 step   225000 | 225000 batches | lr 0.0001 | ms/batch 136.71667 | loss 1.20941 | ppl     3.351\n",
      "| epoch   1 step   225200 | 225200 batches | lr 0.0001 | ms/batch 137.96227 | loss 1.17075 | ppl     3.224\n",
      "| epoch   1 step   225400 | 225400 batches | lr 0.0001 | ms/batch 139.51932 | loss 1.17632 | ppl     3.242\n",
      "| epoch   1 step   225600 | 225600 batches | lr 0.0001 | ms/batch 138.25484 | loss 1.18582 | ppl     3.273\n",
      "| epoch   1 step   225800 | 225800 batches | lr 0.0001 | ms/batch 139.26797 | loss 1.17557 | ppl     3.240\n",
      "| epoch   1 step   226000 | 226000 batches | lr 0.0001 | ms/batch 140.01267 | loss 1.16504 | ppl     3.206\n",
      "| epoch   1 step   226200 | 226200 batches | lr 0.0001 | ms/batch 140.98011 | loss 1.18832 | ppl     3.282\n",
      "| epoch   1 step   226400 | 226400 batches | lr 0.0001 | ms/batch 142.57737 | loss 1.19307 | ppl     3.297\n",
      "| epoch   1 step   226600 | 226600 batches | lr 0.0001 | ms/batch 139.61899 | loss 1.19462 | ppl     3.302\n",
      "| epoch   1 step   226800 | 226800 batches | lr 0.0001 | ms/batch 140.53551 | loss 1.18701 | ppl     3.277\n",
      "| epoch   1 step   227000 | 227000 batches | lr 0.0001 | ms/batch 138.66615 | loss 1.18574 | ppl     3.273\n",
      "| epoch   1 step   227200 | 227200 batches | lr 0.0001 | ms/batch 139.22217 | loss 1.17379 | ppl     3.234\n",
      "| epoch   1 step   227400 | 227400 batches | lr 0.0001 | ms/batch 139.39339 | loss 1.17785 | ppl     3.247\n",
      "| epoch   1 step   227600 | 227600 batches | lr 0.0001 | ms/batch 139.03235 | loss 1.18333 | ppl     3.265\n",
      "| epoch   1 step   227800 | 227800 batches | lr 0.0001 | ms/batch 138.41761 | loss 1.17981 | ppl     3.254\n",
      "| epoch   1 step   228000 | 228000 batches | lr 0.0001 | ms/batch 142.81626 | loss 1.18967 | ppl     3.286\n",
      "|\n",
      "Source: [ 2  5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8\n",
      "  1  8  2  8  3  3 11  4  4 10  5  2  3  7  3  3  5  7  6  6  8  6  5  5]\n",
      "Target: [ 5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8  1\n",
      "  8  2  8  3  3 11  4  4 10  5  2  3  7  3  3  5  7  6  6  8  6  5  5  2]\n",
      "Teacher forcing: acc:0.549453125\n",
      "Preds:  [ 2  9  9 10 10 10 10  8  9  8  8  8  3  3  9  2  7  7  3  2  2 11  7  6\n",
      "  8  2  8  3  3 11  4  4 10  5  2  3  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "No teacher forcing: acc:0.5494791666666666\n",
      "Preds:  [ 5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8  1\n",
      "  8  2  8  3  3 11  4  4 10  5  2  3  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  57 at step   228000 | time: 570.04s | valid loss 1.15135 | valid ppl    3.1625 | valid acc 0.549\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   228200 | 228200 batches | lr 0.0001 | ms/batch 199.98418 | loss 1.17289 | ppl     3.231\n",
      "| epoch   1 step   228400 | 228400 batches | lr 0.0001 | ms/batch 138.21877 | loss 1.19300 | ppl     3.297\n",
      "| epoch   1 step   228600 | 228600 batches | lr 0.0001 | ms/batch 139.84436 | loss 1.17400 | ppl     3.235\n",
      "| epoch   1 step   228800 | 228800 batches | lr 0.0001 | ms/batch 138.21536 | loss 1.17039 | ppl     3.223\n",
      "| epoch   1 step   229000 | 229000 batches | lr 0.0001 | ms/batch 138.06434 | loss 1.17624 | ppl     3.242\n",
      "| epoch   1 step   229200 | 229200 batches | lr 0.0001 | ms/batch 141.95355 | loss 1.17563 | ppl     3.240\n",
      "| epoch   1 step   229400 | 229400 batches | lr 0.0001 | ms/batch 142.62236 | loss 1.17443 | ppl     3.236\n",
      "| epoch   1 step   229600 | 229600 batches | lr 0.0001 | ms/batch 142.72048 | loss 1.18478 | ppl     3.270\n",
      "| epoch   1 step   229800 | 229800 batches | lr 0.0001 | ms/batch 149.87240 | loss 1.17350 | ppl     3.233\n",
      "| epoch   1 step   230000 | 230000 batches | lr 0.0001 | ms/batch 148.88953 | loss 1.18109 | ppl     3.258\n",
      "| epoch   1 step   230200 | 230200 batches | lr 0.0001 | ms/batch 141.68109 | loss 1.18127 | ppl     3.259\n",
      "| epoch   1 step   230400 | 230400 batches | lr 0.0001 | ms/batch 139.27874 | loss 1.16907 | ppl     3.219\n",
      "| epoch   1 step   230600 | 230600 batches | lr 0.0001 | ms/batch 139.82651 | loss 1.18973 | ppl     3.286\n",
      "| epoch   1 step   230800 | 230800 batches | lr 0.0001 | ms/batch 139.25784 | loss 1.17747 | ppl     3.246\n",
      "| epoch   1 step   231000 | 231000 batches | lr 0.0001 | ms/batch 139.21835 | loss 1.17736 | ppl     3.246\n",
      "| epoch   1 step   231200 | 231200 batches | lr 0.0001 | ms/batch 138.56174 | loss 1.16497 | ppl     3.206\n",
      "| epoch   1 step   231400 | 231400 batches | lr 0.0001 | ms/batch 139.20382 | loss 1.18874 | ppl     3.283\n",
      "| epoch   1 step   231600 | 231600 batches | lr 0.0001 | ms/batch 139.93693 | loss 1.17724 | ppl     3.245\n",
      "| epoch   1 step   231800 | 231800 batches | lr 0.0001 | ms/batch 139.24508 | loss 1.16328 | ppl     3.200\n",
      "| epoch   1 step   232000 | 232000 batches | lr 0.0001 | ms/batch 139.30708 | loss 1.17715 | ppl     3.245\n",
      "maslina\n",
      "|\n",
      "Source: [ 5  3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3\n",
      "  1  3  6  5  9  4  5 11  4  7  7  2  4  3  5  9 11  7  8 11 11 10 10  3]\n",
      "Target: [ 3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3  1\n",
      "  3  6  5  9  4  5 11  4  7  7  2  4  3  5  9 11  7  8 11 11 10 10  3  5]\n",
      "Teacher forcing: acc:0.5487616921768708\n",
      "Preds:  [10  8  8  8  8  8  8  8  8  8  9  8 11  6  2  2  2  2  2  3  6 11  8  2\n",
      "  3  6  5  9  4  5 11  4  7  7  2  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "No teacher forcing: acc:0.5487616921768708\n",
      "Preds:  [ 3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3  1\n",
      "  3  6  5  9  4  5 11  4  7  7  2  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  58 at step   232000 | time: 574.75s | valid loss 1.15137 | valid ppl    3.1625 | valid acc 0.549\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   232200 | 232200 batches | lr 0.0001 | ms/batch 196.55480 | loss 1.16847 | ppl     3.217\n",
      "| epoch   1 step   232400 | 232400 batches | lr 0.0001 | ms/batch 139.86173 | loss 1.17199 | ppl     3.228\n",
      "| epoch   1 step   232600 | 232600 batches | lr 0.0001 | ms/batch 139.16719 | loss 1.16820 | ppl     3.216\n",
      "| epoch   1 step   232800 | 232800 batches | lr 0.0001 | ms/batch 141.42316 | loss 1.19912 | ppl     3.317\n",
      "| epoch   1 step   233000 | 233000 batches | lr 0.0001 | ms/batch 139.23403 | loss 1.19212 | ppl     3.294\n",
      "| epoch   1 step   233200 | 233200 batches | lr 0.0001 | ms/batch 140.69093 | loss 1.18206 | ppl     3.261\n",
      "| epoch   1 step   233400 | 233400 batches | lr 0.0001 | ms/batch 138.86473 | loss 1.18086 | ppl     3.257\n",
      "| epoch   1 step   233600 | 233600 batches | lr 0.0001 | ms/batch 139.80646 | loss 1.17444 | ppl     3.236\n",
      "| epoch   1 step   233800 | 233800 batches | lr 0.0001 | ms/batch 140.87437 | loss 1.18274 | ppl     3.263\n",
      "| epoch   1 step   234000 | 234000 batches | lr 0.0001 | ms/batch 141.13435 | loss 1.17534 | ppl     3.239\n",
      "| epoch   1 step   234200 | 234200 batches | lr 0.0001 | ms/batch 141.74991 | loss 1.16726 | ppl     3.213\n",
      "| epoch   1 step   234400 | 234400 batches | lr 0.0001 | ms/batch 141.71353 | loss 1.17961 | ppl     3.253\n",
      "| epoch   1 step   234600 | 234600 batches | lr 0.0001 | ms/batch 143.78937 | loss 1.18673 | ppl     3.276\n",
      "| epoch   1 step   234800 | 234800 batches | lr 0.0001 | ms/batch 142.01261 | loss 1.18310 | ppl     3.264\n",
      "| epoch   1 step   235000 | 235000 batches | lr 0.0001 | ms/batch 141.19235 | loss 1.17493 | ppl     3.238\n",
      "| epoch   1 step   235200 | 235200 batches | lr 0.0001 | ms/batch 140.24706 | loss 1.18957 | ppl     3.286\n",
      "| epoch   1 step   235400 | 235400 batches | lr 0.0001 | ms/batch 139.41726 | loss 1.18548 | ppl     3.272\n",
      "| epoch   1 step   235600 | 235600 batches | lr 0.0001 | ms/batch 140.93010 | loss 1.17607 | ppl     3.242\n",
      "| epoch   1 step   235800 | 235800 batches | lr 0.0001 | ms/batch 138.92393 | loss 1.16564 | ppl     3.208\n",
      "| epoch   1 step   236000 | 236000 batches | lr 0.0001 | ms/batch 142.06562 | loss 1.17489 | ppl     3.238\n",
      "|\n",
      "Source: [ 4 11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2\n",
      "  1  2  3  8  7  8 10  7 11  7  2  5  6 10  8  2 11  9  3  8 11  7  9 11]\n",
      "Target: [11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2  1\n",
      "  2  3  8  7  8 10  7 11  7  2  5  6 10  8  2 11  9  3  8 11  7  9 11  4]\n",
      "Teacher forcing: acc:0.5488541666666666\n",
      "Preds:  [ 9  8  8  8  8 10  8  8 10  9  9  8  7  7  7  2  4  4  4  4  8  8  2  3\n",
      "  2  3  8  7  8 10  7 11  7  2  5  6  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "No teacher forcing: acc:0.54890625\n",
      "Preds:  [11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2  1\n",
      "  2  3  8  7  8 10  7 11  7  2  5  6  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  59 at step   236000 | time: 574.25s | valid loss 1.15147 | valid ppl    3.1628 | valid acc 0.549\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   236200 | 236200 batches | lr 0.0001 | ms/batch 198.34092 | loss 1.17300 | ppl     3.232\n",
      "| epoch   1 step   236400 | 236400 batches | lr 0.0001 | ms/batch 137.48560 | loss 1.18030 | ppl     3.255\n",
      "| epoch   1 step   236600 | 236600 batches | lr 0.0001 | ms/batch 139.96700 | loss 1.17514 | ppl     3.239\n",
      "| epoch   1 step   236800 | 236800 batches | lr 0.0001 | ms/batch 139.15918 | loss 1.17849 | ppl     3.249\n",
      "| epoch   1 step   237000 | 237000 batches | lr 0.0001 | ms/batch 139.98879 | loss 1.17652 | ppl     3.243\n",
      "| epoch   1 step   237200 | 237200 batches | lr 0.0001 | ms/batch 140.21903 | loss 1.18349 | ppl     3.266\n",
      "| epoch   1 step   237400 | 237400 batches | lr 0.0001 | ms/batch 139.55539 | loss 1.16817 | ppl     3.216\n",
      "| epoch   1 step   237600 | 237600 batches | lr 0.0001 | ms/batch 140.00978 | loss 1.17012 | ppl     3.222\n",
      "| epoch   1 step   237800 | 237800 batches | lr 0.0001 | ms/batch 139.05226 | loss 1.19376 | ppl     3.299\n",
      "| epoch   1 step   238000 | 238000 batches | lr 0.0001 | ms/batch 140.67424 | loss 1.16727 | ppl     3.213\n",
      "| epoch   1 step   238200 | 238200 batches | lr 0.0001 | ms/batch 140.34695 | loss 1.16767 | ppl     3.214\n",
      "| epoch   1 step   238400 | 238400 batches | lr 0.0001 | ms/batch 141.44830 | loss 1.18482 | ppl     3.270\n",
      "| epoch   1 step   238600 | 238600 batches | lr 0.0001 | ms/batch 140.25744 | loss 1.18280 | ppl     3.263\n",
      "| epoch   1 step   238800 | 238800 batches | lr 0.0001 | ms/batch 139.83193 | loss 1.17558 | ppl     3.240\n",
      "| epoch   1 step   239000 | 239000 batches | lr 0.0001 | ms/batch 141.33171 | loss 1.17268 | ppl     3.231\n",
      "| epoch   1 step   239200 | 239200 batches | lr 0.0001 | ms/batch 136.69987 | loss 1.16923 | ppl     3.220\n",
      "| epoch   1 step   239400 | 239400 batches | lr 0.0001 | ms/batch 140.11117 | loss 1.19267 | ppl     3.296\n",
      "| epoch   1 step   239600 | 239600 batches | lr 0.0001 | ms/batch 140.63200 | loss 1.17103 | ppl     3.225\n",
      "| epoch   1 step   239800 | 239800 batches | lr 0.0001 | ms/batch 139.83438 | loss 1.18317 | ppl     3.265\n",
      "| epoch   1 step   240000 | 240000 batches | lr 0.0001 | ms/batch 140.76939 | loss 1.17614 | ppl     3.242\n",
      "|\n",
      "Source: [ 8  3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9\n",
      "  1  9  7  9 10  3  7  9  6  4  4  4  8  8  4  5  9  7  8  6  5  8  6  3]\n",
      "Target: [ 3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9  1\n",
      "  9  7  9 10  3  7  9  6  4  4  4  8  8  4  5  9  7  8  6  5  8  6  3  8]\n",
      "Teacher forcing: acc:0.5501302083333334\n",
      "Preds:  [10  8 10  8 10 10 10  8  8 10 10 10  6  6  6  6  6  2  2  2  6  6  7  2\n",
      "  9  7  9 10  3  7  9  6  4  4  4  8  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "No teacher forcing: acc:0.5502083333333333\n",
      "Preds:  [ 3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9  1\n",
      "  9  7  9 10  3  7  9  6  4  4  4  8  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  60 at step   240000 | time: 571.37s | valid loss 1.15130 | valid ppl    3.1623 | valid acc 0.55\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   240200 | 240200 batches | lr 0.0001 | ms/batch 199.27234 | loss 1.17715 | ppl     3.245\n",
      "| epoch   1 step   240400 | 240400 batches | lr 0.0001 | ms/batch 142.03268 | loss 1.18763 | ppl     3.279\n",
      "| epoch   1 step   240600 | 240600 batches | lr 0.0001 | ms/batch 140.62901 | loss 1.16970 | ppl     3.221\n",
      "| epoch   1 step   240800 | 240800 batches | lr 0.0001 | ms/batch 138.43639 | loss 1.16903 | ppl     3.219\n",
      "| epoch   1 step   241000 | 241000 batches | lr 0.0001 | ms/batch 140.73830 | loss 1.17275 | ppl     3.231\n",
      "| epoch   1 step   241200 | 241200 batches | lr 0.0001 | ms/batch 140.94612 | loss 1.18415 | ppl     3.268\n",
      "| epoch   1 step   241400 | 241400 batches | lr 0.0001 | ms/batch 138.99950 | loss 1.17796 | ppl     3.248\n",
      "| epoch   1 step   241600 | 241600 batches | lr 0.0001 | ms/batch 139.67710 | loss 1.17382 | ppl     3.234\n",
      "| epoch   1 step   241800 | 241800 batches | lr 0.0001 | ms/batch 141.26185 | loss 1.15897 | ppl     3.187\n",
      "| epoch   1 step   242000 | 242000 batches | lr 0.0001 | ms/batch 141.78256 | loss 1.17567 | ppl     3.240\n",
      "| epoch   1 step   242200 | 242200 batches | lr 0.0001 | ms/batch 143.10687 | loss 1.19093 | ppl     3.290\n",
      "| epoch   1 step   242400 | 242400 batches | lr 0.0001 | ms/batch 141.51047 | loss 1.17442 | ppl     3.236\n",
      "| epoch   1 step   242600 | 242600 batches | lr 0.0001 | ms/batch 141.08780 | loss 1.15622 | ppl     3.178\n",
      "| epoch   1 step   242800 | 242800 batches | lr 0.0001 | ms/batch 140.88642 | loss 1.16354 | ppl     3.201\n",
      "| epoch   1 step   243000 | 243000 batches | lr 0.0001 | ms/batch 140.53568 | loss 1.18566 | ppl     3.273\n",
      "| epoch   1 step   243200 | 243200 batches | lr 0.0001 | ms/batch 139.03380 | loss 1.17365 | ppl     3.234\n",
      "| epoch   1 step   243400 | 243400 batches | lr 0.0001 | ms/batch 141.92323 | loss 1.16764 | ppl     3.214\n",
      "| epoch   1 step   243600 | 243600 batches | lr 0.0001 | ms/batch 140.88966 | loss 1.16878 | ppl     3.218\n",
      "| epoch   1 step   243800 | 243800 batches | lr 0.0001 | ms/batch 138.68261 | loss 1.16648 | ppl     3.211\n",
      "| epoch   1 step   244000 | 244000 batches | lr 0.0001 | ms/batch 138.44623 | loss 1.16708 | ppl     3.213\n",
      "maslina\n",
      "|\n",
      "Source: [11 10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4\n",
      "  1  4 10  4  4  3 11  9  7  8  9  7 10  7  8  3  3  6 10 11  7  7  5 10]\n",
      "Target: [10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4  1\n",
      "  4 10  4  4  3 11  9  7  8  9  7 10  7  8  3  3  6 10 11  7  7  5 10 11]\n",
      "Teacher forcing: acc:0.5482833758503401\n",
      "Preds:  [ 8  8  9  8  8  8 10 10  8  8  8  8  6  5  5  6  5  6  2  2 11  4  6  6\n",
      "  4 10  4  4  3 11  9  7  8  9  7 10 10  7 10  3  3 10 10  9  7  7 10 10]\n",
      "\n",
      "No teacher forcing: acc:0.5489211309523809\n",
      "Preds:  [10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4  1\n",
      "  4 10  4  4  3 11  9  7  8  9  7 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  61 at step   244000 | time: 573.40s | valid loss 1.15131 | valid ppl    3.1623 | valid acc 0.549\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   244200 | 244200 batches | lr 0.0001 | ms/batch 197.23435 | loss 1.17297 | ppl     3.232\n",
      "| epoch   1 step   244400 | 244400 batches | lr 0.0001 | ms/batch 139.83591 | loss 1.17938 | ppl     3.252\n",
      "| epoch   1 step   244600 | 244600 batches | lr 0.0001 | ms/batch 142.93717 | loss 1.16801 | ppl     3.216\n",
      "| epoch   1 step   244800 | 244800 batches | lr 0.0001 | ms/batch 139.67492 | loss 1.18663 | ppl     3.276\n",
      "| epoch   1 step   245000 | 245000 batches | lr 0.0001 | ms/batch 138.91069 | loss 1.16759 | ppl     3.214\n",
      "| epoch   1 step   245200 | 245200 batches | lr 0.0001 | ms/batch 139.58665 | loss 1.17783 | ppl     3.247\n",
      "| epoch   1 step   245400 | 245400 batches | lr 0.0001 | ms/batch 139.69750 | loss 1.19901 | ppl     3.317\n",
      "| epoch   1 step   245600 | 245600 batches | lr 0.0001 | ms/batch 140.30621 | loss 1.17188 | ppl     3.228\n",
      "| epoch   1 step   245800 | 245800 batches | lr 0.0001 | ms/batch 143.38036 | loss 1.16849 | ppl     3.217\n",
      "| epoch   1 step   246000 | 246000 batches | lr 0.0001 | ms/batch 143.90244 | loss 1.17116 | ppl     3.226\n",
      "| epoch   1 step   246200 | 246200 batches | lr 0.0001 | ms/batch 141.48017 | loss 1.18205 | ppl     3.261\n",
      "| epoch   1 step   246400 | 246400 batches | lr 0.0001 | ms/batch 143.98219 | loss 1.17374 | ppl     3.234\n",
      "| epoch   1 step   246600 | 246600 batches | lr 0.0001 | ms/batch 143.81854 | loss 1.16458 | ppl     3.205\n",
      "| epoch   1 step   246800 | 246800 batches | lr 0.0001 | ms/batch 144.12710 | loss 1.16394 | ppl     3.203\n",
      "| epoch   1 step   247000 | 247000 batches | lr 0.0001 | ms/batch 145.79168 | loss 1.19188 | ppl     3.293\n",
      "| epoch   1 step   247200 | 247200 batches | lr 0.0001 | ms/batch 143.64176 | loss 1.17617 | ppl     3.242\n",
      "| epoch   1 step   247400 | 247400 batches | lr 0.0001 | ms/batch 145.42913 | loss 1.17418 | ppl     3.235\n",
      "| epoch   1 step   247600 | 247600 batches | lr 0.0001 | ms/batch 143.49616 | loss 1.17417 | ppl     3.235\n",
      "| epoch   1 step   247800 | 247800 batches | lr 0.0001 | ms/batch 140.48490 | loss 1.17719 | ppl     3.245\n",
      "| epoch   1 step   248000 | 248000 batches | lr 0.0001 | ms/batch 141.30539 | loss 1.18056 | ppl     3.256\n",
      "|\n",
      "Source: [ 4  7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10\n",
      "  1 10 10  7  6  8 10  9  4  6 11 11 11  9  2  4  2  6 10 11  8  6  3  7]\n",
      "Target: [ 7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10  1\n",
      " 10 10  7  6  8 10  9  4  6 11 11 11  9  2  4  2  6 10 11  8  6  3  7  4]\n",
      "Teacher forcing: acc:0.5519791666666667\n",
      "Preds:  [ 3  8  8 10 10  8  8 10  9  3  9  8  2  2  2  2  2  2  2  2  4  4  2  2\n",
      " 10 10  7  6  8 10  9  4  6 11 11 11  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "No teacher forcing: acc:0.5521354166666667\n",
      "Preds:  [ 7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10  1\n",
      " 10 10  7  6  8 10  9  4  6 11 11 11  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  62 at step   248000 | time: 580.29s | valid loss 1.15123 | valid ppl    3.1621 | valid acc 0.552\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   248200 | 248200 batches | lr 0.0001 | ms/batch 199.87858 | loss 1.16996 | ppl     3.222\n",
      "| epoch   1 step   248400 | 248400 batches | lr 0.0001 | ms/batch 138.90036 | loss 1.17004 | ppl     3.222\n",
      "| epoch   1 step   248600 | 248600 batches | lr 0.0001 | ms/batch 139.02979 | loss 1.19234 | ppl     3.295\n",
      "| epoch   1 step   248800 | 248800 batches | lr 0.0001 | ms/batch 139.28341 | loss 1.16365 | ppl     3.202\n",
      "| epoch   1 step   249000 | 249000 batches | lr 0.0001 | ms/batch 142.21489 | loss 1.16929 | ppl     3.220\n",
      "| epoch   1 step   249200 | 249200 batches | lr 0.0001 | ms/batch 133.17009 | loss 1.17641 | ppl     3.243\n",
      "| epoch   1 step   249400 | 249400 batches | lr 0.0001 | ms/batch 130.55271 | loss 1.17485 | ppl     3.238\n",
      "| epoch   1 step   249600 | 249600 batches | lr 0.0001 | ms/batch 127.89379 | loss 1.17085 | ppl     3.225\n",
      "| epoch   1 step   249800 | 249800 batches | lr 0.0001 | ms/batch 127.81371 | loss 1.15768 | ppl     3.183\n",
      "| epoch   1 step   250000 | 250000 batches | lr 0.0001 | ms/batch 129.70085 | loss 1.16047 | ppl     3.191\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "|\n",
      "Source: [11  9 10  6  2  7  9  5 10  3  7  4  7  8  3  6  9  6 10  3  9 11  9 11\n",
      "  1 11  9 11  9  3 10  6  9  6  3  8  7  4  7  3 10  5  9  7  2  6 10  9]\n",
      "Target: [ 9 10  6  2  7  9  5 10  3  7  4  7  8  3  6  9  6 10  3  9 11  9 11  1\n",
      " 11  9 11  9  3 10  6  9  6  3  8  7  4  7  3 10  5  9  7  2  6 10  9 11]\n",
      "Teacher forcing: acc:0.5498177083333333\n",
      "Preds:  [ 8  8  8  6  9  8  8  9  8  8  8  4  2  2  2  2  2  2  2  2  2 11  2  4\n",
      " 11  9 11  9  3 10  6  9  6  3  8  7  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "No teacher forcing: acc:0.55\n",
      "Preds:  [ 9 10  6  2  7  9  5 10  3  7  4  7  8  3  6  9  6 10  3  9 11  9 11  1\n",
      " 11  9 11  9  3 10  6  9  6  3  8  7  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "| End of training | test loss 1.15143 | test ppl   3.16273\n",
      " | test acc 0.55\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "!bash run_reverse-debug.sh train --work_dir ../evaluation/tl12xl0mt12_debug_lr1e-4_c --lr 0.0001 --tgt_len 12 --eval_tgt_len 12 --mem_len 0 --num_mem_tokens 12 --device_ids 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "687aac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[3, 2]\n",
      "Experiment dir : ../evaluation/tl24xl0mt24_debug-reverse/20220108-165621\n",
      "====================================================================================================\n",
      "    - data : /home/admin/x-transformers/data24\n",
      "    - dataset : reverse\n",
      "    - n_layer : 4\n",
      "    - n_head : 4\n",
      "    - d_head : 64\n",
      "    - d_embed : 128\n",
      "    - d_model : 128\n",
      "    - d_inner : 256\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.0005\n",
      "    - mom : 0.0\n",
      "    - scheduler : cosine\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 0.00025\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 200000\n",
      "    - batch_size : 32\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 24\n",
      "    - eval_tgt_len : 24\n",
      "    - ext_len : 0\n",
      "    - mem_len : 0\n",
      "    - num_mem_tokens : 24\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : False\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : ../evaluation/tl24xl0mt24_debug-reverse/20220108-165621\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : 50\n",
      "    - sample_softmax : -1\n",
      "    - patience : 0\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [3, 2]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : True\n",
      "    - read_mem_from_cache : True\n",
      "    - answer_size : 24\n",
      "    - tied : True\n",
      "    - ntokens : 12\n",
      "    - n_all_param : 926220\n",
      "    - n_nonemb_param : 921088\n",
      "====================================================================================================\n",
      "#params = 926220\n",
      "#non emb params = 921088\n",
      "/home/admin/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "| epoch   1 step      200 |    200 batches | lr 0.0005 | ms/batch 81.84384 | loss 2.31922 | ppl    10.168\n",
      "| epoch   1 step      400 |    400 batches | lr 0.0005 | ms/batch 66.85328 | loss 2.30569 | ppl    10.031\n",
      "| epoch   1 step      600 |    600 batches | lr 0.0005 | ms/batch 65.29199 | loss 2.30500 | ppl    10.024\n",
      "| epoch   1 step      800 |    800 batches | lr 0.0005 | ms/batch 67.36676 | loss 2.30439 | ppl    10.018\n",
      "| epoch   1 step     1000 |   1000 batches | lr 0.0005 | ms/batch 65.24987 | loss 2.30406 | ppl    10.015\n",
      "| epoch   1 step     1200 |   1200 batches | lr 0.0005 | ms/batch 67.41135 | loss 2.30390 | ppl    10.013\n",
      "| epoch   1 step     1400 |   1400 batches | lr 0.0005 | ms/batch 65.72903 | loss 2.30361 | ppl    10.010\n",
      "| epoch   1 step     1600 |   1600 batches | lr 0.0005 | ms/batch 66.08220 | loss 2.30348 | ppl    10.009\n",
      "| epoch   1 step     1800 |   1800 batches | lr 0.0005 | ms/batch 66.76828 | loss 2.30353 | ppl    10.009\n",
      "| epoch   1 step     2000 |   2000 batches | lr 0.0005 | ms/batch 66.50247 | loss 2.30346 | ppl    10.009\n",
      "| epoch   1 step     2200 |   2200 batches | lr 0.0005 | ms/batch 65.43936 | loss 2.30310 | ppl    10.005\n",
      "| epoch   1 step     2400 |   2400 batches | lr 0.0005 | ms/batch 66.19509 | loss 2.30317 | ppl    10.006\n",
      "| epoch   1 step     2600 |   2600 batches | lr 0.0005 | ms/batch 66.25396 | loss 2.30317 | ppl    10.006\n",
      "| epoch   1 step     2800 |   2800 batches | lr 0.0005 | ms/batch 67.94252 | loss 2.30313 | ppl    10.005\n",
      "| epoch   1 step     3000 |   3000 batches | lr 0.0005 | ms/batch 66.15712 | loss 2.30305 | ppl    10.005\n",
      "| epoch   1 step     3200 |   3200 batches | lr 0.0005 | ms/batch 68.08991 | loss 2.30300 | ppl    10.004\n",
      "| epoch   1 step     3400 |   3400 batches | lr 0.0005 | ms/batch 64.08748 | loss 2.30309 | ppl    10.005\n",
      "| epoch   1 step     3600 |   3600 batches | lr 0.0005 | ms/batch 67.63022 | loss 2.30304 | ppl    10.005\n",
      "| epoch   1 step     3800 |   3800 batches | lr 0.0005 | ms/batch 68.75995 | loss 2.30299 | ppl    10.004\n",
      "| epoch   1 step     4000 |   4000 batches | lr 0.0005 | ms/batch 69.52397 | loss 2.30293 | ppl    10.003\n",
      "|\n",
      "Source: [ 4  9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10\n",
      "  1 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9]\n",
      "Target: [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9  4]\n",
      "Teacher forcing: acc:0.10208333333333333\n",
      "Preds:  [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "No teacher forcing: acc:0.10208333333333333\n",
      "Preds:  [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step     4000 | time: 278.26s | valid loss 2.30279 | valid ppl   10.0021 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     4200 |   4200 batches | lr 0.000499 | ms/batch 112.40203 | loss 2.30300 | ppl    10.004\n",
      "| epoch   1 step     4400 |   4400 batches | lr 0.000499 | ms/batch 69.14683 | loss 2.30298 | ppl    10.004\n",
      "| epoch   1 step     4600 |   4600 batches | lr 0.000499 | ms/batch 69.27993 | loss 2.30290 | ppl    10.003\n",
      "| epoch   1 step     4800 |   4800 batches | lr 0.000499 | ms/batch 70.59019 | loss 2.30301 | ppl    10.004\n",
      "| epoch   1 step     5000 |   5000 batches | lr 0.000499 | ms/batch 69.59881 | loss 2.30302 | ppl    10.004\n",
      "| epoch   1 step     5200 |   5200 batches | lr 0.000499 | ms/batch 70.29522 | loss 2.30288 | ppl    10.003\n",
      "| epoch   1 step     5400 |   5400 batches | lr 0.000499 | ms/batch 69.36976 | loss 2.30283 | ppl    10.002\n",
      "| epoch   1 step     5600 |   5600 batches | lr 0.000499 | ms/batch 68.08458 | loss 2.30292 | ppl    10.003\n",
      "| epoch   1 step     5800 |   5800 batches | lr 0.000499 | ms/batch 69.61361 | loss 2.30276 | ppl    10.002\n",
      "| epoch   1 step     6000 |   6000 batches | lr 0.000499 | ms/batch 68.08413 | loss 2.30286 | ppl    10.003\n",
      "| epoch   1 step     6200 |   6200 batches | lr 0.000499 | ms/batch 66.87248 | loss 2.30286 | ppl    10.003\n",
      "| epoch   1 step     6400 |   6400 batches | lr 0.000499 | ms/batch 67.98723 | loss 2.30279 | ppl    10.002\n",
      "| epoch   1 step     6600 |   6600 batches | lr 0.000499 | ms/batch 66.78426 | loss 2.30283 | ppl    10.002\n",
      "| epoch   1 step     6800 |   6800 batches | lr 0.000499 | ms/batch 68.67045 | loss 2.30284 | ppl    10.003\n",
      "| epoch   1 step     7000 |   7000 batches | lr 0.000498 | ms/batch 67.77807 | loss 2.30278 | ppl    10.002\n",
      "| epoch   1 step     7200 |   7200 batches | lr 0.000498 | ms/batch 68.56333 | loss 2.30283 | ppl    10.002\n",
      "| epoch   1 step     7400 |   7400 batches | lr 0.000498 | ms/batch 67.52579 | loss 2.30279 | ppl    10.002\n",
      "| epoch   1 step     7600 |   7600 batches | lr 0.000498 | ms/batch 67.04662 | loss 2.30278 | ppl    10.002\n",
      "| epoch   1 step     7800 |   7800 batches | lr 0.000498 | ms/batch 67.45038 | loss 2.30282 | ppl    10.002\n",
      "| epoch   1 step     8000 |   8000 batches | lr 0.000498 | ms/batch 65.16289 | loss 2.30277 | ppl    10.002\n",
      "maslina\n",
      "|\n",
      "Source: [ 4 10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8\n",
      "  1  8  5  8  2  9 10 10  8  2  6  6  5  8 10  2  7  9  3  6  6  5  5 10]\n",
      "Target: [10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8  1\n",
      "  8  5  8  2  9 10 10  8  2  6  6  5  8 10  2  7  9  3  6  6  5  5 10  4]\n",
      "Teacher forcing: acc:0.10212053571428571\n",
      "Preds:  [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5]\n",
      "\n",
      "No teacher forcing: acc:0.10212053571428571\n",
      "Preds:  [10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8  1\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step     8000 | time: 281.70s | valid loss 2.30261 | valid ppl   10.0002 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     8200 |   8200 batches | lr 0.000498 | ms/batch 108.12473 | loss 2.30281 | ppl    10.002\n",
      "| epoch   1 step     8400 |   8400 batches | lr 0.000498 | ms/batch 68.41310 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    10400 |  10400 batches | lr 0.000497 | ms/batch 68.44893 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    10600 |  10600 batches | lr 0.000497 | ms/batch 67.89066 | loss 2.30270 | ppl    10.001\n",
      "| epoch   1 step    10800 |  10800 batches | lr 0.000496 | ms/batch 68.44835 | loss 2.30270 | ppl    10.001\n",
      "| epoch   1 step    11000 |  11000 batches | lr 0.000496 | ms/batch 70.29774 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    11200 |  11200 batches | lr 0.000496 | ms/batch 69.31343 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    11400 |  11400 batches | lr 0.000496 | ms/batch 69.35472 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    11600 |  11600 batches | lr 0.000496 | ms/batch 66.13676 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    11800 |  11800 batches | lr 0.000496 | ms/batch 66.46573 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    12000 |  12000 batches | lr 0.000496 | ms/batch 67.51931 | loss 2.30269 | ppl    10.001\n",
      "|\n",
      "Source: [ 9  7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3\n",
      "  1  3  3  4 11  6  5  9  9  8  3  7  6  2  9 10 10 11  4 11  3  3 10  7]\n",
      "Target: [ 7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3  1\n",
      "  3  3  4 11  6  5  9  9  8  3  7  6  2  9 10 10 11  4 11  3  3 10  7  9]\n",
      "Teacher forcing: acc:0.10182291666666667\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10182291666666667\n",
      "Preds:  [ 7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step    12000 | time: 280.85s | valid loss 2.30261 | valid ppl   10.0002 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    12200 |  12200 batches | lr 0.000495 | ms/batch 108.10088 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    12400 |  12400 batches | lr 0.000495 | ms/batch 65.23807 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    12600 |  12600 batches | lr 0.000495 | ms/batch 68.01240 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    12800 |  12800 batches | lr 0.000495 | ms/batch 67.57133 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    13000 |  13000 batches | lr 0.000495 | ms/batch 67.80013 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    13200 |  13200 batches | lr 0.000495 | ms/batch 66.03245 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    13400 |  13400 batches | lr 0.000494 | ms/batch 65.74588 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    13600 |  13600 batches | lr 0.000494 | ms/batch 66.30392 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    13800 |  13800 batches | lr 0.000494 | ms/batch 66.74387 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    14000 |  14000 batches | lr 0.000494 | ms/batch 68.06052 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    14200 |  14200 batches | lr 0.000494 | ms/batch 68.39481 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    14400 |  14400 batches | lr 0.000494 | ms/batch 67.33239 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    14600 |  14600 batches | lr 0.000493 | ms/batch 65.67157 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    14800 |  14800 batches | lr 0.000493 | ms/batch 67.39793 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    15000 |  15000 batches | lr 0.000493 | ms/batch 66.17648 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    15200 |  15200 batches | lr 0.000493 | ms/batch 65.45987 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    15400 |  15400 batches | lr 0.000493 | ms/batch 66.51132 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    15600 |  15600 batches | lr 0.000493 | ms/batch 66.67241 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    15800 |  15800 batches | lr 0.000492 | ms/batch 67.46259 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    16000 |  16000 batches | lr 0.000492 | ms/batch 66.54353 | loss 2.30268 | ppl    10.001\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9\n",
      "  1  9  6  4  6  2  6 10 10 10  8  8  3  9  7 11  2 11  3  3  9  4  6  3]\n",
      "Target: [ 3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9  1\n",
      "  9  6  4  6  2  6 10 10 10  8  8  3  9  7 11  2 11  3  3  9  4  6  3  7]\n",
      "Teacher forcing: acc:0.09943664965986394\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09943664965986394\n",
      "Preds:  [ 3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   4 at step    16000 | time: 275.37s | valid loss 2.30264 | valid ppl   10.0005 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    16200 |  16200 batches | lr 0.000492 | ms/batch 106.41487 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    16400 |  16400 batches | lr 0.000492 | ms/batch 66.09341 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    16600 |  16600 batches | lr 0.000492 | ms/batch 66.76172 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    16800 |  16800 batches | lr 0.000491 | ms/batch 65.51151 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    17000 |  17000 batches | lr 0.000491 | ms/batch 64.86595 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    17200 |  17200 batches | lr 0.000491 | ms/batch 67.46678 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    17400 |  17400 batches | lr 0.000491 | ms/batch 65.32695 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    17600 |  17600 batches | lr 0.000491 | ms/batch 66.95496 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    17800 |  17800 batches | lr 0.00049 | ms/batch 66.11669 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    18000 |  18000 batches | lr 0.00049 | ms/batch 68.16566 | loss 2.30305 | ppl    10.005\n",
      "| epoch   1 step    18200 |  18200 batches | lr 0.00049 | ms/batch 67.24540 | loss 2.30270 | ppl    10.001\n",
      "| epoch   1 step    18400 |  18400 batches | lr 0.00049 | ms/batch 67.72300 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    18600 |  18600 batches | lr 0.000489 | ms/batch 67.81747 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    18800 |  18800 batches | lr 0.000489 | ms/batch 65.35889 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    19000 |  19000 batches | lr 0.000489 | ms/batch 66.21867 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    19200 |  19200 batches | lr 0.000489 | ms/batch 66.44553 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    19400 |  19400 batches | lr 0.000488 | ms/batch 65.37444 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    19600 |  19600 batches | lr 0.000488 | ms/batch 66.84370 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    19800 |  19800 batches | lr 0.000488 | ms/batch 65.19985 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    20000 |  20000 batches | lr 0.000488 | ms/batch 66.15032 | loss 2.30263 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10\n",
      "  1 10  8  3  2  6  6  3  3  6  5 10  8  4  2  3  4  6  2 11  7  9  9  3]\n",
      "Target: [ 3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10  1\n",
      " 10  8  3  2  6  6  3  3  6  5 10  8  4  2  3  4  6  2 11  7  9  9  3  2]\n",
      "Teacher forcing: acc:0.09794270833333334\n",
      "Preds:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9]\n",
      "\n",
      "No teacher forcing: acc:0.09794270833333334\n",
      "Preds:  [ 3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10  1\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   5 at step    20000 | time: 273.75s | valid loss 2.30260 | valid ppl   10.0002 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    20200 |  20200 batches | lr 0.000488 | ms/batch 108.13882 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    20400 |  20400 batches | lr 0.000487 | ms/batch 68.00602 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    20600 |  20600 batches | lr 0.000487 | ms/batch 67.41774 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    20800 |  20800 batches | lr 0.000487 | ms/batch 67.64313 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    21000 |  21000 batches | lr 0.000487 | ms/batch 67.39516 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    21200 |  21200 batches | lr 0.000486 | ms/batch 68.84187 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    21400 |  21400 batches | lr 0.000486 | ms/batch 68.20717 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    21600 |  21600 batches | lr 0.000486 | ms/batch 66.48755 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    21800 |  21800 batches | lr 0.000485 | ms/batch 65.17568 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    22000 |  22000 batches | lr 0.000485 | ms/batch 66.52663 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    22200 |  22200 batches | lr 0.000485 | ms/batch 67.38525 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    22400 |  22400 batches | lr 0.000485 | ms/batch 66.80902 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    22600 |  22600 batches | lr 0.000484 | ms/batch 69.17787 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    22800 |  22800 batches | lr 0.000484 | ms/batch 69.60406 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    23000 |  23000 batches | lr 0.000484 | ms/batch 67.86357 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    23200 |  23200 batches | lr 0.000484 | ms/batch 65.57446 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    23400 |  23400 batches | lr 0.000483 | ms/batch 66.34288 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    23600 |  23600 batches | lr 0.000483 | ms/batch 67.34134 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    23800 |  23800 batches | lr 0.000483 | ms/batch 66.43874 | loss 2.30264 | ppl    10.000\n",
      "| epoch   1 step    24000 |  24000 batches | lr 0.000482 | ms/batch 66.69745 | loss 2.30262 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3\n",
      "  1  3  3 10  2  5  6  4  2 11 10  6  6  6  7  6  4  7  8  7  2  2 11  3]\n",
      "Target: [ 3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3  1\n",
      "  3  3 10  2  5  6  4  2 11 10  6  6  6  7  6  4  7  8  7  2  2 11  3  2]\n",
      "Teacher forcing: acc:0.09919270833333334\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.09919270833333334\n",
      "Preds:  [ 3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   6 at step    24000 | time: 277.33s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    24200 |  24200 batches | lr 0.000482 | ms/batch 106.67844 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    24400 |  24400 batches | lr 0.000482 | ms/batch 67.59283 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    24600 |  24600 batches | lr 0.000482 | ms/batch 66.97158 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    24800 |  24800 batches | lr 0.000481 | ms/batch 67.68110 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    25000 |  25000 batches | lr 0.000481 | ms/batch 67.09949 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    25200 |  25200 batches | lr 0.000481 | ms/batch 66.76399 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    25400 |  25400 batches | lr 0.00048 | ms/batch 67.24239 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    25600 |  25600 batches | lr 0.00048 | ms/batch 67.21967 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    25800 |  25800 batches | lr 0.00048 | ms/batch 67.03952 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    26000 |  26000 batches | lr 0.000479 | ms/batch 67.53821 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    26200 |  26200 batches | lr 0.000479 | ms/batch 64.92373 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    26400 |  26400 batches | lr 0.000479 | ms/batch 65.49663 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    26600 |  26600 batches | lr 0.000478 | ms/batch 65.13577 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    26800 |  26800 batches | lr 0.000478 | ms/batch 66.79267 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    27000 |  27000 batches | lr 0.000478 | ms/batch 66.35213 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    27200 |  27200 batches | lr 0.000478 | ms/batch 67.24779 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    27400 |  27400 batches | lr 0.000477 | ms/batch 67.64431 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    27600 |  27600 batches | lr 0.000477 | ms/batch 67.20612 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    27800 |  27800 batches | lr 0.000477 | ms/batch 68.10760 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    28000 |  28000 batches | lr 0.000476 | ms/batch 66.31565 | loss 2.30263 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 9 10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7\n",
      "  1  7 11  3  6 10 11  5  3  9  2 11  9  7  4 10  6  8  4  4  3  4  9 10]\n",
      "Target: [10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7  1\n",
      "  7 11  3  6 10 11  5  3  9  2 11  9  7  4 10  6  8  4  4  3  4  9 10  9]\n",
      "Teacher forcing: acc:0.0975233843537415\n",
      "Preds:  [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "No teacher forcing: acc:0.0975233843537415\n",
      "Preds:  [10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7  1\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   7 at step    28000 | time: 275.05s | valid loss 2.30259 | valid ppl   10.0000 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    28200 |  28200 batches | lr 0.000476 | ms/batch 106.15657 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    28400 |  28400 batches | lr 0.000476 | ms/batch 66.54076 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    28600 |  28600 batches | lr 0.000475 | ms/batch 67.84142 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    28800 |  28800 batches | lr 0.000475 | ms/batch 69.16094 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    29000 |  29000 batches | lr 0.000475 | ms/batch 68.92844 | loss 2.30276 | ppl    10.002\n",
      "| epoch   1 step    29200 |  29200 batches | lr 0.000474 | ms/batch 69.27799 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    29400 |  29400 batches | lr 0.000474 | ms/batch 69.74627 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    29600 |  29600 batches | lr 0.000473 | ms/batch 69.39683 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    29800 |  29800 batches | lr 0.000473 | ms/batch 69.52884 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    30000 |  30000 batches | lr 0.000473 | ms/batch 69.23177 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    30200 |  30200 batches | lr 0.000472 | ms/batch 70.37963 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    30400 |  30400 batches | lr 0.000472 | ms/batch 70.97443 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    30600 |  30600 batches | lr 0.000472 | ms/batch 69.76158 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    30800 |  30800 batches | lr 0.000471 | ms/batch 69.09694 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    31000 |  31000 batches | lr 0.000471 | ms/batch 68.40587 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    31200 |  31200 batches | lr 0.000471 | ms/batch 72.35283 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    31400 |  31400 batches | lr 0.00047 | ms/batch 71.07545 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    31600 |  31600 batches | lr 0.00047 | ms/batch 70.87381 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    31800 |  31800 batches | lr 0.000469 | ms/batch 69.24042 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    32000 |  32000 batches | lr 0.000469 | ms/batch 67.52372 | loss 2.30261 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8\n",
      "  1  8  2  8  3  3 11  4  4 10  5  2  3  7  3  3  5  7  6  6  8  6  5  5]\n",
      "Target: [ 5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8  1\n",
      "  8  2  8  3  3 11  4  4 10  5  2  3  7  3  3  5  7  6  6  8  6  5  5  2]\n",
      "Teacher forcing: acc:0.101015625\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.101015625\n",
      "Preds:  [ 5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   8 at step    32000 | time: 285.69s | valid loss 2.30267 | valid ppl   10.0009 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    32200 |  32200 batches | lr 0.000469 | ms/batch 108.95122 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    32400 |  32400 batches | lr 0.000468 | ms/batch 67.03446 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    32600 |  32600 batches | lr 0.000468 | ms/batch 67.31020 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    32800 |  32800 batches | lr 0.000468 | ms/batch 67.19156 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    33000 |  33000 batches | lr 0.000467 | ms/batch 67.36343 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    33200 |  33200 batches | lr 0.000467 | ms/batch 67.32069 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    33400 |  33400 batches | lr 0.000466 | ms/batch 66.07203 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    33600 |  33600 batches | lr 0.000466 | ms/batch 67.45199 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    33800 |  33800 batches | lr 0.000466 | ms/batch 67.19236 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    34000 |  34000 batches | lr 0.000465 | ms/batch 67.63638 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    34200 |  34200 batches | lr 0.000465 | ms/batch 67.23952 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    34400 |  34400 batches | lr 0.000464 | ms/batch 67.15346 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    34600 |  34600 batches | lr 0.000464 | ms/batch 66.65469 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    34800 |  34800 batches | lr 0.000464 | ms/batch 67.44578 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    35000 |  35000 batches | lr 0.000463 | ms/batch 67.06588 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    35200 |  35200 batches | lr 0.000463 | ms/batch 68.81427 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    35400 |  35400 batches | lr 0.000462 | ms/batch 71.58060 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    35600 |  35600 batches | lr 0.000462 | ms/batch 69.51669 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    35800 |  35800 batches | lr 0.000462 | ms/batch 71.61220 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    36000 |  36000 batches | lr 0.000461 | ms/batch 66.64839 | loss 2.30264 | ppl    10.001\n",
      "maslina\n",
      "|\n",
      "Source: [ 5  3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3\n",
      "  1  3  6  5  9  4  5 11  4  7  7  2  4  3  5  9 11  7  8 11 11 10 10  3]\n",
      "Target: [ 3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3  1\n",
      "  3  6  5  9  4  5 11  4  7  7  2  4  3  5  9 11  7  8 11 11 10 10  3  5]\n",
      "Teacher forcing: acc:0.10018069727891156\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.10018069727891156\n",
      "Preds:  [ 3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   9 at step    36000 | time: 279.04s | valid loss 2.30263 | valid ppl   10.0005 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    36200 |  36200 batches | lr 0.000461 | ms/batch 108.77308 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    36400 |  36400 batches | lr 0.00046 | ms/batch 66.96158 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    36600 |  36600 batches | lr 0.00046 | ms/batch 67.63669 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    36800 |  36800 batches | lr 0.000459 | ms/batch 66.42793 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    37000 |  37000 batches | lr 0.000459 | ms/batch 64.43863 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    37200 |  37200 batches | lr 0.000459 | ms/batch 69.16475 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    37400 |  37400 batches | lr 0.000458 | ms/batch 70.89733 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    37600 |  37600 batches | lr 0.000458 | ms/batch 71.43955 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    37800 |  37800 batches | lr 0.000457 | ms/batch 71.31908 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    38000 |  38000 batches | lr 0.000457 | ms/batch 73.92007 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    38200 |  38200 batches | lr 0.000456 | ms/batch 81.50451 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    38400 |  38400 batches | lr 0.000456 | ms/batch 73.67679 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    38600 |  38600 batches | lr 0.000455 | ms/batch 75.84788 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    38800 |  38800 batches | lr 0.000455 | ms/batch 75.99930 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    39000 |  39000 batches | lr 0.000455 | ms/batch 76.66158 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    39200 |  39200 batches | lr 0.000454 | ms/batch 75.54030 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    39400 |  39400 batches | lr 0.000454 | ms/batch 74.89924 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    39600 |  39600 batches | lr 0.000453 | ms/batch 74.86499 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    39800 |  39800 batches | lr 0.000453 | ms/batch 75.96744 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    40000 |  40000 batches | lr 0.000452 | ms/batch 75.34531 | loss 2.30263 | ppl    10.000\n",
      "|\n",
      "Source: [ 4 11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2\n",
      "  1  2  3  8  7  8 10  7 11  7  2  5  6 10  8  2 11  9  3  8 11  7  9 11]\n",
      "Target: [11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2  1\n",
      "  2  3  8  7  8 10  7 11  7  2  5  6 10  8  2 11  9  3  8 11  7  9 11  4]\n",
      "Teacher forcing: acc:0.09830729166666667\n",
      "Preds:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9]\n",
      "\n",
      "No teacher forcing: acc:0.09830729166666667\n",
      "Preds:  [11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2  1\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  10 at step    40000 | time: 299.66s | valid loss 2.30263 | valid ppl   10.0004 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    40200 |  40200 batches | lr 0.000452 | ms/batch 121.88134 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    40400 |  40400 batches | lr 0.000451 | ms/batch 75.91406 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    40600 |  40600 batches | lr 0.000451 | ms/batch 76.13526 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    40800 |  40800 batches | lr 0.00045 | ms/batch 75.59968 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    41000 |  41000 batches | lr 0.00045 | ms/batch 76.03846 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    41200 |  41200 batches | lr 0.000449 | ms/batch 76.40779 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    41400 |  41400 batches | lr 0.000449 | ms/batch 73.88988 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    41600 |  41600 batches | lr 0.000448 | ms/batch 75.49158 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    41800 |  41800 batches | lr 0.000448 | ms/batch 75.06238 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    42000 |  42000 batches | lr 0.000448 | ms/batch 76.17758 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    42200 |  42200 batches | lr 0.000447 | ms/batch 76.36432 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    42400 |  42400 batches | lr 0.000447 | ms/batch 75.43827 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    42600 |  42600 batches | lr 0.000446 | ms/batch 75.91718 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    42800 |  42800 batches | lr 0.000446 | ms/batch 74.87043 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    43000 |  43000 batches | lr 0.000445 | ms/batch 75.47303 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    43200 |  43200 batches | lr 0.000445 | ms/batch 74.99493 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    43400 |  43400 batches | lr 0.000444 | ms/batch 74.07587 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    43600 |  43600 batches | lr 0.000444 | ms/batch 75.80821 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    43800 |  43800 batches | lr 0.000443 | ms/batch 75.06665 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    44000 |  44000 batches | lr 0.000443 | ms/batch 76.59617 | loss 2.30261 | ppl    10.000\n",
      "|\n",
      "Source: [ 8  3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9\n",
      "  1  9  7  9 10  3  7  9  6  4  4  4  8  8  4  5  9  7  8  6  5  8  6  3]\n",
      "Target: [ 3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9  1\n",
      "  9  7  9 10  3  7  9  6  4  4  4  8  8  4  5  9  7  8  6  5  8  6  3  8]\n",
      "Teacher forcing: acc:0.098671875\n",
      "Preds:  [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "No teacher forcing: acc:0.098671875\n",
      "Preds:  [ 3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9  1\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  11 at step    44000 | time: 311.76s | valid loss 2.30261 | valid ppl   10.0003 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    44200 |  44200 batches | lr 0.000442 | ms/batch 125.90284 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    44400 |  44400 batches | lr 0.000442 | ms/batch 77.30526 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    44600 |  44600 batches | lr 0.000441 | ms/batch 75.78150 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    44800 |  44800 batches | lr 0.000441 | ms/batch 75.16999 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    45000 |  45000 batches | lr 0.00044 | ms/batch 76.00840 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    45200 |  45200 batches | lr 0.00044 | ms/batch 75.59186 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    45400 |  45400 batches | lr 0.000439 | ms/batch 73.73236 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    45600 |  45600 batches | lr 0.000439 | ms/batch 76.50293 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    45800 |  45800 batches | lr 0.000438 | ms/batch 76.71284 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    46000 |  46000 batches | lr 0.000438 | ms/batch 76.90723 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    46200 |  46200 batches | lr 0.000437 | ms/batch 77.07419 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    46400 |  46400 batches | lr 0.000436 | ms/batch 76.11171 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    46600 |  46600 batches | lr 0.000436 | ms/batch 79.01449 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    46800 |  46800 batches | lr 0.000435 | ms/batch 78.55692 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    47000 |  47000 batches | lr 0.000435 | ms/batch 77.98806 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    47200 |  47200 batches | lr 0.000434 | ms/batch 78.64916 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    47400 |  47400 batches | lr 0.000434 | ms/batch 77.82417 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    47600 |  47600 batches | lr 0.000433 | ms/batch 78.15607 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    47800 |  47800 batches | lr 0.000433 | ms/batch 77.28733 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    48000 |  48000 batches | lr 0.000432 | ms/batch 78.49485 | loss 2.30261 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [11 10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4\n",
      "  1  4 10  4  4  3 11  9  7  8  9  7 10  7  8  3  3  6 10 11  7  7  5 10]\n",
      "Target: [10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4  1\n",
      "  4 10  4  4  3 11  9  7  8  9  7 10  7  8  3  3  6 10 11  7  7  5 10 11]\n",
      "Teacher forcing: acc:0.10196109693877552\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10196109693877552\n",
      "Preds:  [10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  12 at step    48000 | time: 317.52s | valid loss 2.30260 | valid ppl   10.0002 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    48200 |  48200 batches | lr 0.000432 | ms/batch 123.17132 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    48400 |  48400 batches | lr 0.000431 | ms/batch 75.58780 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    48600 |  48600 batches | lr 0.000431 | ms/batch 77.63390 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    48800 |  48800 batches | lr 0.00043 | ms/batch 76.94800 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    49000 |  49000 batches | lr 0.00043 | ms/batch 75.24121 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    49200 |  49200 batches | lr 0.000429 | ms/batch 75.28255 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    49400 |  49400 batches | lr 0.000428 | ms/batch 78.53846 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    49600 |  49600 batches | lr 0.000428 | ms/batch 75.31976 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    49800 |  49800 batches | lr 0.000427 | ms/batch 74.63291 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    50000 |  50000 batches | lr 0.000427 | ms/batch 75.39847 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    50200 |  50200 batches | lr 0.000426 | ms/batch 76.51587 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    50400 |  50400 batches | lr 0.000426 | ms/batch 76.13008 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    50600 |  50600 batches | lr 0.000425 | ms/batch 75.29075 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    50800 |  50800 batches | lr 0.000425 | ms/batch 75.49001 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    51000 |  51000 batches | lr 0.000424 | ms/batch 75.20129 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    51200 |  51200 batches | lr 0.000423 | ms/batch 76.59857 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    51400 |  51400 batches | lr 0.000423 | ms/batch 75.27034 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    51600 |  51600 batches | lr 0.000422 | ms/batch 74.70957 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    51800 |  51800 batches | lr 0.000422 | ms/batch 74.78221 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    52000 |  52000 batches | lr 0.000421 | ms/batch 75.15032 | loss 2.30262 | ppl    10.000\n",
      "|\n",
      "Source: [ 4  7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10\n",
      "  1 10 10  7  6  8 10  9  4  6 11 11 11  9  2  4  2  6 10 11  8  6  3  7]\n",
      "Target: [ 7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10  1\n",
      " 10 10  7  6  8 10  9  4  6 11 11 11  9  2  4  2  6 10 11  8  6  3  7  4]\n",
      "Teacher forcing: acc:0.103203125\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.103203125\n",
      "Preds:  [ 7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  13 at step    52000 | time: 312.61s | valid loss 2.30255 | valid ppl    9.9996 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    52200 |  52200 batches | lr 0.000421 | ms/batch 122.57982 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    52400 |  52400 batches | lr 0.00042 | ms/batch 75.27534 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    52600 |  52600 batches | lr 0.000419 | ms/batch 74.68922 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    52800 |  52800 batches | lr 0.000419 | ms/batch 75.85698 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    53000 |  53000 batches | lr 0.000418 | ms/batch 74.82566 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    53200 |  53200 batches | lr 0.000418 | ms/batch 74.99172 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    53400 |  53400 batches | lr 0.000417 | ms/batch 75.23712 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    53600 |  53600 batches | lr 0.000417 | ms/batch 74.51897 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    53800 |  53800 batches | lr 0.000416 | ms/batch 76.29456 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    54000 |  54000 batches | lr 0.000415 | ms/batch 75.66475 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    54200 |  54200 batches | lr 0.000415 | ms/batch 75.35488 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    54400 |  54400 batches | lr 0.000414 | ms/batch 76.91440 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    54600 |  54600 batches | lr 0.000414 | ms/batch 74.57960 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    54800 |  54800 batches | lr 0.000413 | ms/batch 76.08599 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    55000 |  55000 batches | lr 0.000412 | ms/batch 75.23244 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    55200 |  55200 batches | lr 0.000412 | ms/batch 77.38475 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    55400 |  55400 batches | lr 0.000411 | ms/batch 74.96683 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    55600 |  55600 batches | lr 0.000411 | ms/batch 74.55298 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    55800 |  55800 batches | lr 0.00041 | ms/batch 76.60234 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    56000 |  56000 batches | lr 0.000409 | ms/batch 75.21550 | loss 2.30258 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  7  6  9  6  4  3  2  8  5  3  8  7  7  4  8  5 11  2  3  8  4  9 11\n",
      "  1 11  9  4  8  3  2 11  5  8  4  7  7  8  3  5  8  2  3  4  6  9  6  7]\n",
      "Target: [ 7  6  9  6  4  3  2  8  5  3  8  7  7  4  8  5 11  2  3  8  4  9 11  1\n",
      " 11  9  4  8  3  2 11  5  8  4  7  7  8  3  5  8  2  3  4  6  9  6  7  7]\n",
      "Teacher forcing: acc:0.09837372448979592\n",
      "Preds:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9]\n",
      "\n",
      "No teacher forcing: acc:0.09837372448979592\n",
      "Preds:  [ 7  6  9  6  4  3  2  8  5  3  8  7  7  4  8  5 11  2  3  8  4  9 11  1\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  14 at step    56000 | time: 311.37s | valid loss 2.30259 | valid ppl   10.0000 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    56200 |  56200 batches | lr 0.000409 | ms/batch 120.24050 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    56400 |  56400 batches | lr 0.000408 | ms/batch 72.47235 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    56600 |  56600 batches | lr 0.000408 | ms/batch 71.97689 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    56800 |  56800 batches | lr 0.000407 | ms/batch 73.02959 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    57000 |  57000 batches | lr 0.000406 | ms/batch 71.91422 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    57200 |  57200 batches | lr 0.000406 | ms/batch 72.44890 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    57400 |  57400 batches | lr 0.000405 | ms/batch 73.59461 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    57600 |  57600 batches | lr 0.000404 | ms/batch 72.01879 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    57800 |  57800 batches | lr 0.000404 | ms/batch 71.96239 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    58000 |  58000 batches | lr 0.000403 | ms/batch 72.29397 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    58200 |  58200 batches | lr 0.000403 | ms/batch 71.97346 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    58400 |  58400 batches | lr 0.000402 | ms/batch 73.07160 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    58600 |  58600 batches | lr 0.000401 | ms/batch 73.76028 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    58800 |  58800 batches | lr 0.000401 | ms/batch 74.47529 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    59000 |  59000 batches | lr 0.0004 | ms/batch 73.49767 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    59200 |  59200 batches | lr 0.000399 | ms/batch 74.41965 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    59400 |  59400 batches | lr 0.000399 | ms/batch 72.92961 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    59600 |  59600 batches | lr 0.000398 | ms/batch 73.22093 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    59800 |  59800 batches | lr 0.000398 | ms/batch 74.28044 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    60000 |  60000 batches | lr 0.000397 | ms/batch 72.40325 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [11  8  3  2  3  9 10  5  9 10  6 10 11  9  6 11  2  9  7  5  7  5  3 11\n",
      "  1 11  3  5  7  5  7  9  2 11  6  9 11 10  6 10  9  5 10  9  3  2  3  8]\n",
      "Target: [ 8  3  2  3  9 10  5  9 10  6 10 11  9  6 11  2  9  7  5  7  5  3 11  1\n",
      " 11  3  5  7  5  7  9  2 11  6  9 11 10  6 10  9  5 10  9  3  2  3  8 11]\n",
      "Teacher forcing: acc:0.09791666666666667\n",
      "Preds:  [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7]\n",
      "\n",
      "No teacher forcing: acc:0.09791666666666667\n",
      "Preds:  [ 8  3  2  3  9 10  5  9 10  6 10 11  9  6 11  2  9  7  5  7  5  3 11  1\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  15 at step    60000 | time: 301.24s | valid loss 2.30270 | valid ppl   10.0012 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    60200 |  60200 batches | lr 0.000396 | ms/batch 122.24230 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    60400 |  60400 batches | lr 0.000396 | ms/batch 72.44784 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    60600 |  60600 batches | lr 0.000395 | ms/batch 71.61611 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    60800 |  60800 batches | lr 0.000394 | ms/batch 71.74740 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    61000 |  61000 batches | lr 0.000394 | ms/batch 71.78182 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    61200 |  61200 batches | lr 0.000393 | ms/batch 72.13846 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    61400 |  61400 batches | lr 0.000392 | ms/batch 72.39120 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    61600 |  61600 batches | lr 0.000392 | ms/batch 73.46108 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    61800 |  61800 batches | lr 0.000391 | ms/batch 73.21547 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    62000 |  62000 batches | lr 0.000391 | ms/batch 73.90976 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    62200 |  62200 batches | lr 0.00039 | ms/batch 72.23967 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    62400 |  62400 batches | lr 0.000389 | ms/batch 74.05818 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    62600 |  62600 batches | lr 0.000389 | ms/batch 72.82133 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    62800 |  62800 batches | lr 0.000388 | ms/batch 72.56918 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    63000 |  63000 batches | lr 0.000387 | ms/batch 71.79180 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    63200 |  63200 batches | lr 0.000387 | ms/batch 71.29805 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    63400 |  63400 batches | lr 0.000386 | ms/batch 71.84489 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    63600 |  63600 batches | lr 0.000385 | ms/batch 71.48811 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    63800 |  63800 batches | lr 0.000385 | ms/batch 72.14499 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    64000 |  64000 batches | lr 0.000384 | ms/batch 71.69044 | loss 2.30261 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 8  6  9  4 11  4  3  8  4 11 10  6  2  6  6  5  4  4  2 11  4  8  5  3\n",
      "  1  3  5  8  4 11  2  4  4  5  6  6  2  6 10 11  4  8  3  4 11  4  9  6]\n",
      "Target: [ 6  9  4 11  4  3  8  4 11 10  6  2  6  6  5  4  4  2 11  4  8  5  3  1\n",
      "  3  5  8  4 11  2  4  4  5  6  6  2  6 10 11  4  8  3  4 11  4  9  6  8]\n",
      "Teacher forcing: acc:0.10257227891156463\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10257227891156463\n",
      "Preds:  [ 6  9  4 11  4  3  8  4 11 10  6  2  6  6  5  4  4  2 11  4  8  5  3  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  16 at step    64000 | time: 299.05s | valid loss 2.30254 | valid ppl    9.9996 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    64200 |  64200 batches | lr 0.000383 | ms/batch 118.27624 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    64400 |  64400 batches | lr 0.000383 | ms/batch 71.97450 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    64600 |  64600 batches | lr 0.000382 | ms/batch 71.47914 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    64800 |  64800 batches | lr 0.000381 | ms/batch 71.61913 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    65000 |  65000 batches | lr 0.000381 | ms/batch 71.47790 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    65200 |  65200 batches | lr 0.00038 | ms/batch 72.59298 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    65400 |  65400 batches | lr 0.000379 | ms/batch 71.89938 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    65600 |  65600 batches | lr 0.000379 | ms/batch 72.81065 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    65800 |  65800 batches | lr 0.000378 | ms/batch 71.68692 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    66000 |  66000 batches | lr 0.000377 | ms/batch 71.30537 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    66200 |  66200 batches | lr 0.000377 | ms/batch 70.45362 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    66400 |  66400 batches | lr 0.000376 | ms/batch 71.88341 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    66600 |  66600 batches | lr 0.000375 | ms/batch 72.48968 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    66800 |  66800 batches | lr 0.000375 | ms/batch 72.49022 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    67000 |  67000 batches | lr 0.000374 | ms/batch 70.91224 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    67200 |  67200 batches | lr 0.000373 | ms/batch 72.55764 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    67400 |  67400 batches | lr 0.000372 | ms/batch 72.77870 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    67600 |  67600 batches | lr 0.000372 | ms/batch 72.08003 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    67800 |  67800 batches | lr 0.000371 | ms/batch 71.68080 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    68000 |  68000 batches | lr 0.00037 | ms/batch 71.95224 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [10 10  2 11  7  2  7 10  5 10  3  7  2  9 10 11  5  4  5 10  5  8 10  6\n",
      "  1  6 10  8  5 10  5  4  5 11 10  9  2  7  3 10  5 10  7  2  7 11  2 10]\n",
      "Target: [10  2 11  7  2  7 10  5 10  3  7  2  9 10 11  5  4  5 10  5  8 10  6  1\n",
      "  6 10  8  5 10  5  4  5 11 10  9  2  7  3 10  5 10  7  2  7 11  2 10 10]\n",
      "Teacher forcing: acc:0.09947916666666666\n",
      "Preds:  [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8]\n",
      "\n",
      "No teacher forcing: acc:0.09947916666666666\n",
      "Preds:  [10  2 11  7  2  7 10  5 10  3  7  2  9 10 11  5  4  5 10  5  8 10  6  1\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  17 at step    68000 | time: 297.00s | valid loss 2.30259 | valid ppl   10.0001 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    68200 |  68200 batches | lr 0.00037 | ms/batch 118.45850 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    68400 |  68400 batches | lr 0.000369 | ms/batch 71.19588 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    68600 |  68600 batches | lr 0.000368 | ms/batch 72.77623 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    68800 |  68800 batches | lr 0.000368 | ms/batch 70.50130 | loss 2.30301 | ppl    10.004\n",
      "| epoch   1 step    69000 |  69000 batches | lr 0.000367 | ms/batch 71.38952 | loss 2.30277 | ppl    10.002\n",
      "| epoch   1 step    69200 |  69200 batches | lr 0.000366 | ms/batch 72.13250 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    69400 |  69400 batches | lr 0.000366 | ms/batch 71.62207 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    69600 |  69600 batches | lr 0.000365 | ms/batch 72.66563 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    69800 |  69800 batches | lr 0.000364 | ms/batch 72.65855 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    70000 |  70000 batches | lr 0.000363 | ms/batch 72.28561 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    70200 |  70200 batches | lr 0.000363 | ms/batch 72.22189 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    70400 |  70400 batches | lr 0.000362 | ms/batch 72.29618 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    70600 |  70600 batches | lr 0.000361 | ms/batch 71.51975 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    70800 |  70800 batches | lr 0.000361 | ms/batch 71.94068 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    71000 |  71000 batches | lr 0.00036 | ms/batch 72.03639 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    71200 |  71200 batches | lr 0.000359 | ms/batch 71.82783 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    71400 |  71400 batches | lr 0.000359 | ms/batch 72.55807 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    71600 |  71600 batches | lr 0.000358 | ms/batch 73.17406 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    71800 |  71800 batches | lr 0.000357 | ms/batch 72.89821 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    72000 |  72000 batches | lr 0.000356 | ms/batch 72.39021 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 6 11  9 11  9  2  3  9 10  6 10  5  7  3  8  8  2  6 11 11 10 11  4  6\n",
      "  1  6  4 11 10 11 11  6  2  8  8  3  7  5 10  6 10  9  3  2  9 11  9 11]\n",
      "Target: [11  9 11  9  2  3  9 10  6 10  5  7  3  8  8  2  6 11 11 10 11  4  6  1\n",
      "  6  4 11 10 11 11  6  2  8  8  3  7  5 10  6 10  9  3  2  9 11  9 11  6]\n",
      "Teacher forcing: acc:0.09942708333333333\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09942708333333333\n",
      "Preds:  [11  9 11  9  2  3  9 10  6 10  5  7  3  8  8  2  6 11 11 10 11  4  6  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  18 at step    72000 | time: 297.76s | valid loss 2.30266 | valid ppl   10.0008 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    72200 |  72200 batches | lr 0.000356 | ms/batch 120.41020 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    72400 |  72400 batches | lr 0.000355 | ms/batch 72.85514 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    72600 |  72600 batches | lr 0.000354 | ms/batch 72.93955 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    72800 |  72800 batches | lr 0.000354 | ms/batch 71.95245 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    73000 |  73000 batches | lr 0.000353 | ms/batch 72.66973 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    73200 |  73200 batches | lr 0.000352 | ms/batch 71.98755 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    73400 |  73400 batches | lr 0.000351 | ms/batch 72.92064 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    73600 |  73600 batches | lr 0.000351 | ms/batch 72.15735 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    73800 |  73800 batches | lr 0.00035 | ms/batch 72.79417 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    74000 |  74000 batches | lr 0.000349 | ms/batch 73.62934 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    74200 |  74200 batches | lr 0.000349 | ms/batch 74.06872 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    74400 |  74400 batches | lr 0.000348 | ms/batch 72.46639 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    74600 |  74600 batches | lr 0.000347 | ms/batch 71.70407 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    74800 |  74800 batches | lr 0.000346 | ms/batch 72.33872 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    75000 |  75000 batches | lr 0.000346 | ms/batch 71.69794 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    75200 |  75200 batches | lr 0.000345 | ms/batch 72.02251 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    75400 |  75400 batches | lr 0.000344 | ms/batch 72.26155 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    75600 |  75600 batches | lr 0.000343 | ms/batch 72.42382 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    75800 |  75800 batches | lr 0.000343 | ms/batch 71.95552 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    76000 |  76000 batches | lr 0.000342 | ms/batch 72.83126 | loss 2.30260 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 8  2 10  3  4  3 10  8  8 11 10  6  3 11  9  8  9  3  4  7 11  5  4  6\n",
      "  1  6  4  5 11  7  4  3  9  8  9 11  3  6 10 11  8  8 10  3  4  3 10  2]\n",
      "Target: [ 2 10  3  4  3 10  8  8 11 10  6  3 11  9  8  9  3  4  7 11  5  4  6  1\n",
      "  6  4  5 11  7  4  3  9  8  9 11  3  6 10 11  8  8 10  3  4  3 10  2  8]\n",
      "Teacher forcing: acc:0.1015359268707483\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.1015359268707483\n",
      "Preds:  [ 2 10  3  4  3 10  8  8 11 10  6  3 11  9  8  9  3  4  7 11  5  4  6  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  19 at step    76000 | time: 299.31s | valid loss 2.30258 | valid ppl   10.0000 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    76200 |  76200 batches | lr 0.000341 | ms/batch 118.04082 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    76400 |  76400 batches | lr 0.000341 | ms/batch 73.20710 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    76600 |  76600 batches | lr 0.00034 | ms/batch 72.40421 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    76800 |  76800 batches | lr 0.000339 | ms/batch 70.92752 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    77000 |  77000 batches | lr 0.000338 | ms/batch 71.32117 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    77200 |  77200 batches | lr 0.000338 | ms/batch 72.15851 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    77400 |  77400 batches | lr 0.000337 | ms/batch 71.23405 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    77600 |  77600 batches | lr 0.000336 | ms/batch 73.15916 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    77800 |  77800 batches | lr 0.000335 | ms/batch 71.30711 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    78000 |  78000 batches | lr 0.000335 | ms/batch 71.15757 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    78200 |  78200 batches | lr 0.000334 | ms/batch 71.46772 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    78400 |  78400 batches | lr 0.000333 | ms/batch 72.17200 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    78600 |  78600 batches | lr 0.000332 | ms/batch 71.73972 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    78800 |  78800 batches | lr 0.000332 | ms/batch 72.35999 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    79000 |  79000 batches | lr 0.000331 | ms/batch 71.96207 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    79200 |  79200 batches | lr 0.00033 | ms/batch 72.10419 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    79400 |  79400 batches | lr 0.000329 | ms/batch 72.13390 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    79600 |  79600 batches | lr 0.000329 | ms/batch 71.16677 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    79800 |  79800 batches | lr 0.000328 | ms/batch 70.87236 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    80000 |  80000 batches | lr 0.000327 | ms/batch 71.75466 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [10  5 11 10  9  7 10  9  3 10 11  4  2 10  8  4 10  2  9  7  3  7 11  4\n",
      "  1  4 11  7  3  7  9  2 10  4  8 10  2  4 11 10  3  9 10  7  9 10 11  5]\n",
      "Target: [ 5 11 10  9  7 10  9  3 10 11  4  2 10  8  4 10  2  9  7  3  7 11  4  1\n",
      "  4 11  7  3  7  9  2 10  4  8 10  2  4 11 10  3  9 10  7  9 10 11  5 10]\n",
      "Teacher forcing: acc:0.1\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.1\n",
      "Preds:  [ 5 11 10  9  7 10  9  3 10 11  4  2 10  8  4 10  2  9  7  3  7 11  4  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  20 at step    80000 | time: 296.96s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    80200 |  80200 batches | lr 0.000327 | ms/batch 120.36843 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    80400 |  80400 batches | lr 0.000326 | ms/batch 70.68003 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    80600 |  80600 batches | lr 0.000325 | ms/batch 72.10789 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    80800 |  80800 batches | lr 0.000324 | ms/batch 71.92443 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    81000 |  81000 batches | lr 0.000324 | ms/batch 72.72595 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    81200 |  81200 batches | lr 0.000323 | ms/batch 72.95059 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    81400 |  81400 batches | lr 0.000322 | ms/batch 72.48088 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    81600 |  81600 batches | lr 0.000321 | ms/batch 72.32718 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    81800 |  81800 batches | lr 0.000321 | ms/batch 72.38221 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    82000 |  82000 batches | lr 0.00032 | ms/batch 72.07778 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    82200 |  82200 batches | lr 0.000319 | ms/batch 72.13822 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    82400 |  82400 batches | lr 0.000318 | ms/batch 72.40926 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    82600 |  82600 batches | lr 0.000317 | ms/batch 72.19322 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    82800 |  82800 batches | lr 0.000317 | ms/batch 70.68165 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    83000 |  83000 batches | lr 0.000316 | ms/batch 71.46252 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    83200 |  83200 batches | lr 0.000315 | ms/batch 71.51987 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    83400 |  83400 batches | lr 0.000314 | ms/batch 71.50953 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    83600 |  83600 batches | lr 0.000314 | ms/batch 71.94535 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    83800 |  83800 batches | lr 0.000313 | ms/batch 72.19599 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    84000 |  84000 batches | lr 0.000312 | ms/batch 73.82377 | loss 2.30260 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 9  8  6 11  6  5  3  9  8  2  7  8  7  9  9  9  7  4 10 11 11 10 10  6\n",
      "  1  6 10 10 11 11 10  4  7  9  9  9  7  8  7  2  8  9  3  5  6 11  6  8]\n",
      "Target: [ 8  6 11  6  5  3  9  8  2  7  8  7  9  9  9  7  4 10 11 11 10 10  6  1\n",
      "  6 10 10 11 11 10  4  7  9  9  9  7  8  7  2  8  9  3  5  6 11  6  8  9]\n",
      "Teacher forcing: acc:0.09802827380952381\n",
      "Preds:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9]\n",
      "\n",
      "No teacher forcing: acc:0.09802827380952381\n",
      "Preds:  [ 8  6 11  6  5  3  9  8  2  7  8  7  9  9  9  7  4 10 11 11 10 10  6  1\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  21 at step    84000 | time: 297.68s | valid loss 2.30262 | valid ppl   10.0004 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    84200 |  84200 batches | lr 0.000311 | ms/batch 119.90934 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    84400 |  84400 batches | lr 0.000311 | ms/batch 73.06868 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    84600 |  84600 batches | lr 0.00031 | ms/batch 72.96306 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    84800 |  84800 batches | lr 0.000309 | ms/batch 72.86355 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    85000 |  85000 batches | lr 0.000308 | ms/batch 74.08515 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    85200 |  85200 batches | lr 0.000308 | ms/batch 74.01996 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    85400 |  85400 batches | lr 0.000307 | ms/batch 72.35438 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    85600 |  85600 batches | lr 0.000306 | ms/batch 72.77417 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    85800 |  85800 batches | lr 0.000305 | ms/batch 73.80361 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    86000 |  86000 batches | lr 0.000305 | ms/batch 73.77131 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    86200 |  86200 batches | lr 0.000304 | ms/batch 73.45905 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    86400 |  86400 batches | lr 0.000303 | ms/batch 71.64955 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    86600 |  86600 batches | lr 0.000302 | ms/batch 71.69186 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    86800 |  86800 batches | lr 0.000301 | ms/batch 72.24922 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    87000 |  87000 batches | lr 0.000301 | ms/batch 71.94049 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    87200 |  87200 batches | lr 0.0003 | ms/batch 72.36126 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    87400 |  87400 batches | lr 0.000299 | ms/batch 71.61931 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    87600 |  87600 batches | lr 0.000298 | ms/batch 71.92993 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    87800 |  87800 batches | lr 0.000298 | ms/batch 72.76166 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    88000 |  88000 batches | lr 0.000297 | ms/batch 72.34901 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  8  8  9  7  3  4 10  4  7  5  4  3  7 11  3  6  8  6  4 10  4  3 11\n",
      "  1 11  3  4 10  4  6  8  6  3 11  7  3  4  5  7  4 10  4  3  7  9  8  8]\n",
      "Target: [ 8  8  9  7  3  4 10  4  7  5  4  3  7 11  3  6  8  6  4 10  4  3 11  1\n",
      " 11  3  4 10  4  6  8  6  3 11  7  3  4  5  7  4 10  4  3  7  9  8  8  5]\n",
      "Teacher forcing: acc:0.09997395833333333\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.09997395833333333\n",
      "Preds:  [ 8  8  9  7  3  4 10  4  7  5  4  3  7 11  3  6  8  6  4 10  4  3 11  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  22 at step    88000 | time: 300.53s | valid loss 2.30258 | valid ppl   10.0000 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    88200 |  88200 batches | lr 0.000296 | ms/batch 119.25945 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    88400 |  88400 batches | lr 0.000295 | ms/batch 73.64174 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    88600 |  88600 batches | lr 0.000295 | ms/batch 73.09067 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    88800 |  88800 batches | lr 0.000294 | ms/batch 72.44161 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    89000 |  89000 batches | lr 0.000293 | ms/batch 71.82514 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    89200 |  89200 batches | lr 0.000292 | ms/batch 71.33997 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    89400 |  89400 batches | lr 0.000291 | ms/batch 71.93370 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    89600 |  89600 batches | lr 0.000291 | ms/batch 72.41221 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    89800 |  89800 batches | lr 0.00029 | ms/batch 71.79917 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    90000 |  90000 batches | lr 0.000289 | ms/batch 72.02432 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    90200 |  90200 batches | lr 0.000288 | ms/batch 71.44283 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    90400 |  90400 batches | lr 0.000288 | ms/batch 71.76240 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    90600 |  90600 batches | lr 0.000287 | ms/batch 71.70867 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    90800 |  90800 batches | lr 0.000286 | ms/batch 71.77642 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    91000 |  91000 batches | lr 0.000285 | ms/batch 72.11017 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    91200 |  91200 batches | lr 0.000284 | ms/batch 71.18953 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    91400 |  91400 batches | lr 0.000284 | ms/batch 70.93773 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    91600 |  91600 batches | lr 0.000283 | ms/batch 71.92029 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    91800 |  91800 batches | lr 0.000282 | ms/batch 70.10813 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    92000 |  92000 batches | lr 0.000281 | ms/batch 71.37317 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  5  3  6 10  9  9  6  9  5  6 11  3 11  6  3  3  3  4  3  4  2  9  6\n",
      "  1  6  9  2  4  3  4  3  3  3  6 11  3 11  6  5  9  6  9  9 10  6  3  5]\n",
      "Target: [ 5  3  6 10  9  9  6  9  5  6 11  3 11  6  3  3  3  4  3  4  2  9  6  1\n",
      "  6  9  2  4  3  4  3  3  3  6 11  3 11  6  5  9  6  9  9 10  6  3  5  5]\n",
      "Teacher forcing: acc:0.10111979166666667\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10111979166666667\n",
      "Preds:  [ 5  3  6 10  9  9  6  9  5  6 11  3 11  6  3  3  3  4  3  4  2  9  6  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  23 at step    92000 | time: 296.65s | valid loss 2.30257 | valid ppl    9.9998 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    92200 |  92200 batches | lr 0.000281 | ms/batch 117.45630 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    92400 |  92400 batches | lr 0.00028 | ms/batch 70.71593 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    92600 |  92600 batches | lr 0.000279 | ms/batch 71.12536 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    92800 |  92800 batches | lr 0.000278 | ms/batch 71.36493 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    93000 |  93000 batches | lr 0.000277 | ms/batch 71.71700 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    93200 |  93200 batches | lr 0.000277 | ms/batch 70.44856 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    93400 |  93400 batches | lr 0.000276 | ms/batch 70.80506 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    93600 |  93600 batches | lr 0.000275 | ms/batch 71.71465 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    93800 |  93800 batches | lr 0.000274 | ms/batch 72.19987 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    94000 |  94000 batches | lr 0.000274 | ms/batch 71.49692 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    94200 |  94200 batches | lr 0.000273 | ms/batch 71.57533 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    94400 |  94400 batches | lr 0.000272 | ms/batch 72.30028 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    94600 |  94600 batches | lr 0.000271 | ms/batch 71.45966 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    94800 |  94800 batches | lr 0.00027 | ms/batch 70.97350 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    95000 |  95000 batches | lr 0.00027 | ms/batch 71.27053 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    95200 |  95200 batches | lr 0.000269 | ms/batch 72.44570 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    95400 |  95400 batches | lr 0.000268 | ms/batch 71.67097 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    95600 |  95600 batches | lr 0.000267 | ms/batch 70.41309 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    95800 |  95800 batches | lr 0.000266 | ms/batch 71.64344 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    96000 |  96000 batches | lr 0.000266 | ms/batch 71.31028 | loss 2.30260 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [10  5  9 10  6  3  4  4  4  8  2  4  3  8 10  5  2 10  6  7  8  5  3  6\n",
      "  1  6  3  5  8  7  6 10  2  5 10  8  3  4  2  8  4  4  4  3  6 10  9  5]\n",
      "Target: [ 5  9 10  6  3  4  4  4  8  2  4  3  8 10  5  2 10  6  7  8  5  3  6  1\n",
      "  6  3  5  8  7  6 10  2  5 10  8  3  4  2  8  4  4  4  3  6 10  9  5 10]\n",
      "Teacher forcing: acc:0.09978210034013606\n",
      "Preds:  [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8]\n",
      "\n",
      "No teacher forcing: acc:0.09978210034013606\n",
      "Preds:  [ 5  9 10  6  3  4  4  4  8  2  4  3  8 10  5  2 10  6  7  8  5  3  6  1\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  24 at step    96000 | time: 294.85s | valid loss 2.30258 | valid ppl   10.0000 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    96200 |  96200 batches | lr 0.000265 | ms/batch 117.11900 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    96400 |  96400 batches | lr 0.000264 | ms/batch 71.37573 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    96600 |  96600 batches | lr 0.000263 | ms/batch 71.06816 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    96800 |  96800 batches | lr 0.000263 | ms/batch 71.12274 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    97000 |  97000 batches | lr 0.000262 | ms/batch 71.15676 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    97200 |  97200 batches | lr 0.000261 | ms/batch 70.13669 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    97400 |  97400 batches | lr 0.00026 | ms/batch 72.20516 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    97600 |  97600 batches | lr 0.000259 | ms/batch 71.64891 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    97800 |  97800 batches | lr 0.000259 | ms/batch 72.66679 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    98000 |  98000 batches | lr 0.000258 | ms/batch 71.34057 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    98200 |  98200 batches | lr 0.000257 | ms/batch 72.43913 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    98400 |  98400 batches | lr 0.000256 | ms/batch 72.37179 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    98600 |  98600 batches | lr 0.000255 | ms/batch 70.84145 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    98800 |  98800 batches | lr 0.000255 | ms/batch 70.96510 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    99000 |  99000 batches | lr 0.000254 | ms/batch 71.10706 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    99200 |  99200 batches | lr 0.000253 | ms/batch 70.42715 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    99400 |  99400 batches | lr 0.000252 | ms/batch 70.18386 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    99600 |  99600 batches | lr 0.000252 | ms/batch 71.08725 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    99800 |  99800 batches | lr 0.000251 | ms/batch 70.33998 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   100000 | 100000 batches | lr 0.00025 | ms/batch 70.50663 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 9  7  8  2  7  6  5  7 11  8  3  2  9  2  4  2  3  7 10 10 10  7  2  3\n",
      "  1  3  2  7 10 10 10  7  3  2  4  2  9  2  3  8 11  7  5  6  7  2  8  7]\n",
      "Target: [ 7  8  2  7  6  5  7 11  8  3  2  9  2  4  2  3  7 10 10 10  7  2  3  1\n",
      "  3  2  7 10 10 10  7  3  2  4  2  9  2  3  8 11  7  5  6  7  2  8  7  9]\n",
      "Teacher forcing: acc:0.10075520833333333\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.10075520833333333\n",
      "Preds:  [ 7  8  2  7  6  5  7 11  8  3  2  9  2  4  2  3  7 10 10 10  7  2  3  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  25 at step   100000 | time: 294.12s | valid loss 2.30255 | valid ppl    9.9997 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   100200 | 100200 batches | lr 0.000249 | ms/batch 118.41908 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   100400 | 100400 batches | lr 0.000248 | ms/batch 71.65587 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   100600 | 100600 batches | lr 0.000248 | ms/batch 71.12755 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   100800 | 100800 batches | lr 0.000247 | ms/batch 71.27071 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   101000 | 101000 batches | lr 0.000246 | ms/batch 71.41878 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   101200 | 101200 batches | lr 0.000245 | ms/batch 71.37160 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   101400 | 101400 batches | lr 0.000245 | ms/batch 70.42745 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   101600 | 101600 batches | lr 0.000244 | ms/batch 71.74385 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   101800 | 101800 batches | lr 0.000243 | ms/batch 71.98213 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   102000 | 102000 batches | lr 0.000242 | ms/batch 71.95378 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   102200 | 102200 batches | lr 0.000241 | ms/batch 73.38635 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   102400 | 102400 batches | lr 0.000241 | ms/batch 72.35168 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   102600 | 102600 batches | lr 0.00024 | ms/batch 72.14141 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   102800 | 102800 batches | lr 0.000239 | ms/batch 71.86810 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   103000 | 103000 batches | lr 0.000238 | ms/batch 70.70832 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   103200 | 103200 batches | lr 0.000237 | ms/batch 72.15987 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   103400 | 103400 batches | lr 0.000237 | ms/batch 72.08971 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   103600 | 103600 batches | lr 0.000236 | ms/batch 71.75497 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   103800 | 103800 batches | lr 0.000235 | ms/batch 71.68543 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   104000 | 104000 batches | lr 0.000234 | ms/batch 71.13216 | loss 2.30261 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 3  9  9  9  5  3 10 10  5  8  7  6  3  8  5 10  2  5  7  3  6 11  4  6\n",
      "  1  6  4 11  6  3  7  5  2 10  5  8  3  6  7  8  5 10 10  3  5  9  9  9]\n",
      "Target: [ 9  9  9  5  3 10 10  5  8  7  6  3  8  5 10  2  5  7  3  6 11  4  6  1\n",
      "  6  4 11  6  3  7  5  2 10  5  8  3  6  7  8  5 10 10  3  5  9  9  9  3]\n",
      "Teacher forcing: acc:0.09980867346938775\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.09980867346938775\n",
      "Preds:  [ 9  9  9  5  3 10 10  5  8  7  6  3  8  5 10  2  5  7  3  6 11  4  6  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  26 at step   104000 | time: 295.91s | valid loss 2.30260 | valid ppl   10.0002 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   104200 | 104200 batches | lr 0.000234 | ms/batch 116.99699 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   104400 | 104400 batches | lr 0.000233 | ms/batch 72.41827 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   104600 | 104600 batches | lr 0.000232 | ms/batch 70.60381 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   104800 | 104800 batches | lr 0.000231 | ms/batch 71.24234 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   105000 | 105000 batches | lr 0.00023 | ms/batch 70.39882 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   105200 | 105200 batches | lr 0.00023 | ms/batch 72.07152 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   105400 | 105400 batches | lr 0.000229 | ms/batch 71.65339 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   105600 | 105600 batches | lr 0.000228 | ms/batch 70.87186 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   105800 | 105800 batches | lr 0.000227 | ms/batch 70.91813 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   106000 | 106000 batches | lr 0.000226 | ms/batch 70.31744 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   106200 | 106200 batches | lr 0.000226 | ms/batch 70.75186 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   106400 | 106400 batches | lr 0.000225 | ms/batch 72.99364 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   106600 | 106600 batches | lr 0.000224 | ms/batch 70.79450 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   106800 | 106800 batches | lr 0.000223 | ms/batch 70.79138 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   107000 | 107000 batches | lr 0.000223 | ms/batch 71.50296 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   107200 | 107200 batches | lr 0.000222 | ms/batch 71.31228 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   107400 | 107400 batches | lr 0.000221 | ms/batch 72.17774 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   107600 | 107600 batches | lr 0.00022 | ms/batch 72.76806 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   107800 | 107800 batches | lr 0.000219 | ms/batch 72.27615 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   108000 | 108000 batches | lr 0.000219 | ms/batch 72.09359 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 9 11  8  4 10  8  5  6  7 10  3  3  8  4 10  3  8  3 11 10 10  6  7 11\n",
      "  1 11  7  6 10 10 11  3  8  3 10  4  8  3  3 10  7  6  5  8 10  4  8 11]\n",
      "Target: [11  8  4 10  8  5  6  7 10  3  3  8  4 10  3  8  3 11 10 10  6  7 11  1\n",
      " 11  7  6 10 10 11  3  8  3 10  4  8  3  3 10  7  6  5  8 10  4  8 11  9]\n",
      "Teacher forcing: acc:0.09950520833333333\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09950520833333333\n",
      "Preds:  [11  8  4 10  8  5  6  7 10  3  3  8  4 10  3  8  3 11 10 10  6  7 11  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  27 at step   108000 | time: 295.23s | valid loss 2.30256 | valid ppl    9.9997 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   108200 | 108200 batches | lr 0.000218 | ms/batch 118.85206 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   108400 | 108400 batches | lr 0.000217 | ms/batch 73.37365 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   108600 | 108600 batches | lr 0.000216 | ms/batch 71.68142 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   108800 | 108800 batches | lr 0.000216 | ms/batch 70.95468 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   109000 | 109000 batches | lr 0.000215 | ms/batch 72.29208 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   109200 | 109200 batches | lr 0.000214 | ms/batch 71.08986 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   109400 | 109400 batches | lr 0.000213 | ms/batch 73.42836 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   109600 | 109600 batches | lr 0.000212 | ms/batch 71.58754 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   111400 | 111400 batches | lr 0.000205 | ms/batch 72.32904 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   111600 | 111600 batches | lr 0.000205 | ms/batch 72.03816 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   111800 | 111800 batches | lr 0.000204 | ms/batch 71.68223 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   112000 | 112000 batches | lr 0.000203 | ms/batch 72.75153 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  3  8  8  3  7  3  6  9  6 10  5 11  8  6  9  4 11  8  6  8 11  3  7\n",
      "  1  7  3 11  8  6  8 11  4  9  6  8 11  5 10  6  9  6  3  7  3  8  8  3]\n",
      "Target: [ 3  8  8  3  7  3  6  9  6 10  5 11  8  6  9  4 11  8  6  8 11  3  7  1\n",
      "  7  3 11  8  6  8 11  4  9  6  8 11  5 10  6  9  6  3  7  3  8  8  3  5]\n",
      "Teacher forcing: acc:0.10223958333333333\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10223958333333333\n",
      "Preds:  [ 3  8  8  3  7  3  6  9  6 10  5 11  8  6  9  4 11  8  6  8 11  3  7  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  28 at step   112000 | time: 297.40s | valid loss 2.30258 | valid ppl   10.0000 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   112200 | 112200 batches | lr 0.000202 | ms/batch 117.59900 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   112400 | 112400 batches | lr 0.000202 | ms/batch 73.18216 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   112600 | 112600 batches | lr 0.000201 | ms/batch 71.77796 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   112800 | 112800 batches | lr 0.0002 | ms/batch 71.85650 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   113000 | 113000 batches | lr 0.000199 | ms/batch 71.42497 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   113200 | 113200 batches | lr 0.000199 | ms/batch 72.22407 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   113400 | 113400 batches | lr 0.000198 | ms/batch 72.30936 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   113600 | 113600 batches | lr 0.000197 | ms/batch 71.44643 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   113800 | 113800 batches | lr 0.000196 | ms/batch 70.84929 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   114000 | 114000 batches | lr 0.000195 | ms/batch 71.35107 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   114200 | 114200 batches | lr 0.000195 | ms/batch 71.97322 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   114400 | 114400 batches | lr 0.000194 | ms/batch 71.55828 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   114600 | 114600 batches | lr 0.000193 | ms/batch 71.81422 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   114800 | 114800 batches | lr 0.000192 | ms/batch 71.28585 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   115000 | 115000 batches | lr 0.000192 | ms/batch 72.18018 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   115200 | 115200 batches | lr 0.000191 | ms/batch 71.59774 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   115400 | 115400 batches | lr 0.00019 | ms/batch 72.26584 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   115600 | 115600 batches | lr 0.000189 | ms/batch 72.39403 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   115800 | 115800 batches | lr 0.000189 | ms/batch 71.62031 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   116000 | 116000 batches | lr 0.000188 | ms/batch 73.01558 | loss 2.30261 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  4  2  9  3 10  5  3  8  5 11  9 10 10  4  8  6  9  9  4  4  6  6 11\n",
      "  1 11  6  6  4  4  9  9  6  8  4 10 10  9 11  5  8  3  5 10  3  9  2  4]\n",
      "Target: [ 4  2  9  3 10  5  3  8  5 11  9 10 10  4  8  6  9  9  4  4  6  6 11  1\n",
      " 11  6  6  4  4  9  9  6  8  4 10 10  9 11  5  8  3  5 10  3  9  2  4  7]\n",
      "Teacher forcing: acc:0.09930378401360544\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09930378401360544\n",
      "Preds:  [ 4  2  9  3 10  5  3  8  5 11  9 10 10  4  8  6  9  9  4  4  6  6 11  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  29 at step   116000 | time: 296.59s | valid loss 2.30261 | valid ppl   10.0003 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   116200 | 116200 batches | lr 0.000187 | ms/batch 119.47176 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   116400 | 116400 batches | lr 0.000186 | ms/batch 73.13852 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   116600 | 116600 batches | lr 0.000186 | ms/batch 72.78494 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   116800 | 116800 batches | lr 0.000185 | ms/batch 73.11551 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   117000 | 117000 batches | lr 0.000184 | ms/batch 72.56338 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   117200 | 117200 batches | lr 0.000183 | ms/batch 71.79915 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   117400 | 117400 batches | lr 0.000183 | ms/batch 72.63466 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   117600 | 117600 batches | lr 0.000182 | ms/batch 73.05880 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   117800 | 117800 batches | lr 0.000181 | ms/batch 71.29376 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   118000 | 118000 batches | lr 0.00018 | ms/batch 72.63109 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   118200 | 118200 batches | lr 0.000179 | ms/batch 71.54468 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   118400 | 118400 batches | lr 0.000179 | ms/batch 72.59329 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   118600 | 118600 batches | lr 0.000178 | ms/batch 72.44512 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   118800 | 118800 batches | lr 0.000177 | ms/batch 71.16458 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   119000 | 119000 batches | lr 0.000176 | ms/batch 71.04031 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   119200 | 119200 batches | lr 0.000176 | ms/batch 71.80623 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   119400 | 119400 batches | lr 0.000175 | ms/batch 70.41745 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   119600 | 119600 batches | lr 0.000174 | ms/batch 71.76987 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   119800 | 119800 batches | lr 0.000173 | ms/batch 71.81016 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   120000 | 120000 batches | lr 0.000173 | ms/batch 71.78418 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  9  7  3  7  3 10  3  7  4 11  5  4  9  2  2  8 10  9  2  9  8  5  9\n",
      "  1  9  5  8  9  2  9 10  8  2  2  9  4  5 11  4  7  3 10  3  7  3  7  9]\n",
      "Target: [ 9  7  3  7  3 10  3  7  4 11  5  4  9  2  2  8 10  9  2  9  8  5  9  1\n",
      "  9  5  8  9  2  9 10  8  2  2  9  4  5 11  4  7  3 10  3  7  3  7  9  2]\n",
      "Teacher forcing: acc:0.10309895833333334\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10309895833333334\n",
      "Preds:  [ 9  7  3  7  3 10  3  7  4 11  5  4  9  2  2  8 10  9  2  9  8  5  9  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  30 at step   120000 | time: 297.92s | valid loss 2.30257 | valid ppl    9.9998 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   120200 | 120200 batches | lr 0.000172 | ms/batch 117.69165 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   120400 | 120400 batches | lr 0.000171 | ms/batch 70.94112 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   120600 | 120600 batches | lr 0.000171 | ms/batch 71.18359 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   120800 | 120800 batches | lr 0.00017 | ms/batch 71.35317 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   121000 | 121000 batches | lr 0.000169 | ms/batch 72.17534 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   121200 | 121200 batches | lr 0.000168 | ms/batch 71.46942 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   121400 | 121400 batches | lr 0.000168 | ms/batch 71.12444 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   121600 | 121600 batches | lr 0.000167 | ms/batch 71.91393 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   121800 | 121800 batches | lr 0.000166 | ms/batch 72.22226 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   122000 | 122000 batches | lr 0.000165 | ms/batch 70.53673 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   122200 | 122200 batches | lr 0.000165 | ms/batch 72.42162 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   122400 | 122400 batches | lr 0.000164 | ms/batch 72.19636 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   122600 | 122600 batches | lr 0.000163 | ms/batch 71.97163 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   122800 | 122800 batches | lr 0.000162 | ms/batch 71.79228 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   123000 | 123000 batches | lr 0.000162 | ms/batch 70.44907 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   123200 | 123200 batches | lr 0.000161 | ms/batch 71.03637 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   123400 | 123400 batches | lr 0.00016 | ms/batch 71.99548 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   123600 | 123600 batches | lr 0.000159 | ms/batch 71.74336 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   123800 | 123800 batches | lr 0.000159 | ms/batch 71.75317 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   124000 | 124000 batches | lr 0.000158 | ms/batch 71.85310 | loss 2.30258 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [11  3  3  9 10  2  3  6  6  6  6  6  3  5  4  4  7  5  4  4 11  9 11  3\n",
      "  1  3 11  9 11  4  4  5  7  4  4  5  3  6  6  6  6  6  3  2 10  9  3  3]\n",
      "Target: [ 3  3  9 10  2  3  6  6  6  6  6  3  5  4  4  7  5  4  4 11  9 11  3  1\n",
      "  3 11  9 11  4  4  5  7  4  4  5  3  6  6  6  6  6  3  2 10  9  3  3 11]\n",
      "Teacher forcing: acc:0.10281143707482993\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10281143707482993\n",
      "Preds:  [ 3  3  9 10  2  3  6  6  6  6  6  3  5  4  4  7  5  4  4 11  9 11  3  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  31 at step   124000 | time: 295.49s | valid loss 2.30254 | valid ppl    9.9995 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   124200 | 124200 batches | lr 0.000157 | ms/batch 117.39390 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   124400 | 124400 batches | lr 0.000157 | ms/batch 73.33817 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   124600 | 124600 batches | lr 0.000156 | ms/batch 75.98090 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   124800 | 124800 batches | lr 0.000155 | ms/batch 74.98710 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   125000 | 125000 batches | lr 0.000154 | ms/batch 73.75417 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   125200 | 125200 batches | lr 0.000154 | ms/batch 71.23734 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   125400 | 125400 batches | lr 0.000153 | ms/batch 71.02194 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   125600 | 125600 batches | lr 0.000152 | ms/batch 71.99471 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   125800 | 125800 batches | lr 0.000151 | ms/batch 70.39376 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   126000 | 126000 batches | lr 0.000151 | ms/batch 71.81921 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   126200 | 126200 batches | lr 0.00015 | ms/batch 71.42146 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   126400 | 126400 batches | lr 0.000149 | ms/batch 71.68853 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   126600 | 126600 batches | lr 0.000149 | ms/batch 72.66038 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   126800 | 126800 batches | lr 0.000148 | ms/batch 71.36145 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   127000 | 127000 batches | lr 0.000147 | ms/batch 71.12729 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   127200 | 127200 batches | lr 0.000146 | ms/batch 71.46326 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   127400 | 127400 batches | lr 0.000146 | ms/batch 70.79660 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   127600 | 127600 batches | lr 0.000145 | ms/batch 71.85662 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   127800 | 127800 batches | lr 0.000144 | ms/batch 71.12291 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   128000 | 128000 batches | lr 0.000144 | ms/batch 71.06032 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  8 11 10 11  9  8  4 11  3  9  8 11  5 10 10 11  2 11 10  8 10  2  2\n",
      "  1  2  2 10  8 10 11  2 11 10 10  5 11  8  9  3 11  4  8  9 11 10 11  8]\n",
      "Target: [ 8 11 10 11  9  8  4 11  3  9  8 11  5 10 10 11  2 11 10  8 10  2  2  1\n",
      "  2  2 10  8 10 11  2 11 10 10  5 11  8  9  3 11  4  8  9 11 10 11  8  5]\n",
      "Teacher forcing: acc:0.09911458333333334\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09911458333333334\n",
      "Preds:  [ 8 11 10 11  9  8  4 11  3  9  8 11  5 10 10 11  2 11 10  8 10  2  2  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  32 at step   128000 | time: 297.47s | valid loss 2.30258 | valid ppl    9.9999 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   128200 | 128200 batches | lr 0.000143 | ms/batch 119.00178 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   128400 | 128400 batches | lr 0.000142 | ms/batch 71.33158 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   128600 | 128600 batches | lr 0.000141 | ms/batch 72.47856 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   128800 | 128800 batches | lr 0.000141 | ms/batch 72.59333 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   129000 | 129000 batches | lr 0.00014 | ms/batch 72.32746 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   129200 | 129200 batches | lr 0.000139 | ms/batch 71.81832 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   129400 | 129400 batches | lr 0.000139 | ms/batch 72.32190 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   129600 | 129600 batches | lr 0.000138 | ms/batch 72.79802 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   129800 | 129800 batches | lr 0.000137 | ms/batch 72.43069 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   130000 | 130000 batches | lr 0.000137 | ms/batch 71.75053 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   130200 | 130200 batches | lr 0.000136 | ms/batch 71.87653 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   130400 | 130400 batches | lr 0.000135 | ms/batch 71.78647 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   130600 | 130600 batches | lr 0.000134 | ms/batch 74.05306 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   130800 | 130800 batches | lr 0.000134 | ms/batch 72.37940 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   131000 | 131000 batches | lr 0.000133 | ms/batch 73.13366 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   131200 | 131200 batches | lr 0.000132 | ms/batch 70.91054 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   131400 | 131400 batches | lr 0.000132 | ms/batch 71.01708 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   131600 | 131600 batches | lr 0.000131 | ms/batch 72.10219 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   131800 | 131800 batches | lr 0.00013 | ms/batch 70.86581 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   132000 | 132000 batches | lr 0.00013 | ms/batch 71.25651 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [10  4  6  9 11 11  4  7  8  2 10  6 10  6  4  2  2  6  7  7  2  2  4  2\n",
      "  1  2  4  2  2  7  7  6  2  2  4  6 10  6 10  2  8  7  4 11 11  9  6  4]\n",
      "Target: [ 4  6  9 11 11  4  7  8  2 10  6 10  6  4  2  2  6  7  7  2  2  4  2  1\n",
      "  2  4  2  2  7  7  6  2  2  4  6 10  6 10  2  8  7  4 11 11  9  6  4 10]\n",
      "Teacher forcing: acc:0.09955729166666667\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.09955729166666667\n",
      "Preds:  [ 4  6  9 11 11  4  7  8  2 10  6 10  6  4  2  2  6  7  7  2  2  4  2  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  33 at step   132000 | time: 297.74s | valid loss 2.30261 | valid ppl   10.0002 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   132200 | 132200 batches | lr 0.000129 | ms/batch 119.28760 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   132400 | 132400 batches | lr 0.000128 | ms/batch 70.98000 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   132600 | 132600 batches | lr 0.000128 | ms/batch 72.27432 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   132800 | 132800 batches | lr 0.000127 | ms/batch 71.72955 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   133000 | 133000 batches | lr 0.000126 | ms/batch 72.64936 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   133200 | 133200 batches | lr 0.000125 | ms/batch 71.95428 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   133400 | 133400 batches | lr 0.000125 | ms/batch 70.80837 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   133600 | 133600 batches | lr 0.000124 | ms/batch 71.33291 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   133800 | 133800 batches | lr 0.000123 | ms/batch 72.22130 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   134000 | 134000 batches | lr 0.000123 | ms/batch 72.06055 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   134200 | 134200 batches | lr 0.000122 | ms/batch 72.23322 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   134400 | 134400 batches | lr 0.000121 | ms/batch 73.25335 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   134600 | 134600 batches | lr 0.000121 | ms/batch 71.75085 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   134800 | 134800 batches | lr 0.00012 | ms/batch 71.50040 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   135000 | 135000 batches | lr 0.000119 | ms/batch 71.00407 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   135200 | 135200 batches | lr 0.000119 | ms/batch 71.81960 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step   135400 | 135400 batches | lr 0.000118 | ms/batch 71.41370 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   135600 | 135600 batches | lr 0.000117 | ms/batch 72.55359 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   135800 | 135800 batches | lr 0.000117 | ms/batch 71.44247 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   136000 | 136000 batches | lr 0.000116 | ms/batch 71.94260 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 4  8  8  2  4 11  7  8  8  6  6  9  6  6  2 11  3 11  9 10  8  4  5  2\n",
      "  1  2  5  4  8 10  9 11  3 11  2  6  6  9  6  6  8  8  7 11  4  2  8  8]\n",
      "Target: [ 8  8  2  4 11  7  8  8  6  6  9  6  6  2 11  3 11  9 10  8  4  5  2  1\n",
      "  2  5  4  8 10  9 11  3 11  2  6  6  9  6  6  8  8  7 11  4  2  8  8  4]\n",
      "Teacher forcing: acc:0.10068558673469388\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10068558673469388\n",
      "Preds:  [ 8  8  2  4 11  7  8  8  6  6  9  6  6  2 11  3 11  9 10  8  4  5  2  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  34 at step   136000 | time: 296.61s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   136200 | 136200 batches | lr 0.000115 | ms/batch 117.51518 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   136400 | 136400 batches | lr 0.000115 | ms/batch 71.71595 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   136600 | 136600 batches | lr 0.000114 | ms/batch 72.23144 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   136800 | 136800 batches | lr 0.000113 | ms/batch 71.69794 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   137000 | 137000 batches | lr 0.000113 | ms/batch 71.27822 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   137200 | 137200 batches | lr 0.000112 | ms/batch 71.73415 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   137400 | 137400 batches | lr 0.000111 | ms/batch 71.22958 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   137600 | 137600 batches | lr 0.000111 | ms/batch 70.66376 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   137800 | 137800 batches | lr 0.00011 | ms/batch 71.97271 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   138000 | 138000 batches | lr 0.000109 | ms/batch 71.96042 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   138200 | 138200 batches | lr 0.000109 | ms/batch 71.96019 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   138400 | 138400 batches | lr 0.000108 | ms/batch 72.04169 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   138600 | 138600 batches | lr 0.000108 | ms/batch 72.54578 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   138800 | 138800 batches | lr 0.000107 | ms/batch 72.47819 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   139000 | 139000 batches | lr 0.000106 | ms/batch 71.14245 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   139200 | 139200 batches | lr 0.000106 | ms/batch 71.22812 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   139400 | 139400 batches | lr 0.000105 | ms/batch 71.35255 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   139600 | 139600 batches | lr 0.000104 | ms/batch 71.23123 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   139800 | 139800 batches | lr 0.000104 | ms/batch 71.64858 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   140000 | 140000 batches | lr 0.000103 | ms/batch 71.89112 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 7  9  2  7 11  2 10  9  6  3  8 11 11  5  3  2  3  3  5  4  6  4  6  9\n",
      "  1  9  6  4  6  4  5  3  3  2  3  5 11 11  8  3  6  9 10  2 11  7  2  9]\n",
      "Target: [ 9  2  7 11  2 10  9  6  3  8 11 11  5  3  2  3  3  5  4  6  4  6  9  1\n",
      "  9  6  4  6  4  5  3  3  2  3  5 11 11  8  3  6  9 10  2 11  7  2  9  7]\n",
      "Teacher forcing: acc:0.10002604166666666\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10002604166666666\n",
      "Preds:  [ 9  2  7 11  2 10  9  6  3  8 11 11  5  3  2  3  3  5  4  6  4  6  9  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  35 at step   140000 | time: 295.88s | valid loss 2.30258 | valid ppl    9.9999 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   140200 | 140200 batches | lr 0.000102 | ms/batch 117.29871 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   140400 | 140400 batches | lr 0.000102 | ms/batch 71.56744 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   140600 | 140600 batches | lr 0.000101 | ms/batch 71.49344 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   140800 | 140800 batches | lr 0.000101 | ms/batch 71.75276 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   141000 | 141000 batches | lr 9.99e-05 | ms/batch 73.17697 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   141200 | 141200 batches | lr 9.93e-05 | ms/batch 72.69747 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   141400 | 141400 batches | lr 9.86e-05 | ms/batch 71.79157 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   141600 | 141600 batches | lr 9.8e-05 | ms/batch 72.59438 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   141800 | 141800 batches | lr 9.74e-05 | ms/batch 71.72588 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   142000 | 142000 batches | lr 9.68e-05 | ms/batch 71.72218 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   142200 | 142200 batches | lr 9.62e-05 | ms/batch 72.36995 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   142400 | 142400 batches | lr 9.55e-05 | ms/batch 72.17557 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   142600 | 142600 batches | lr 9.49e-05 | ms/batch 72.73775 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   142800 | 142800 batches | lr 9.43e-05 | ms/batch 72.15415 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   143000 | 143000 batches | lr 9.37e-05 | ms/batch 72.08344 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   143200 | 143200 batches | lr 9.31e-05 | ms/batch 72.36558 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   143400 | 143400 batches | lr 9.25e-05 | ms/batch 73.01503 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   143600 | 143600 batches | lr 9.19e-05 | ms/batch 71.97660 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   143800 | 143800 batches | lr 9.12e-05 | ms/batch 71.93846 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   144000 | 144000 batches | lr 9.06e-05 | ms/batch 71.29086 | loss 2.30258 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 5  3 11  6  8  6  2  5  5  2  5  8  3  4  8  4 11  9  3  3  4  6  2  9\n",
      "  1  9  2  6  4  3  3  9 11  4  8  4  3  8  5  2  5  5  2  6  8  6 11  3]\n",
      "Target: [ 3 11  6  8  6  2  5  5  2  5  8  3  4  8  4 11  9  3  3  4  6  2  9  1\n",
      "  9  2  6  4  3  3  9 11  4  8  4  3  8  5  2  5  5  2  6  8  6 11  3  5]\n",
      "Teacher forcing: acc:0.10068558673469388\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10068558673469388\n",
      "Preds:  [ 3 11  6  8  6  2  5  5  2  5  8  3  4  8  4 11  9  3  3  4  6  2  9  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  36 at step   144000 | time: 297.57s | valid loss 2.30260 | valid ppl   10.0001 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   144200 | 144200 batches | lr 9e-05 | ms/batch 117.76329 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   144400 | 144400 batches | lr 8.94e-05 | ms/batch 72.34112 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   144600 | 144600 batches | lr 8.88e-05 | ms/batch 71.90659 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   144800 | 144800 batches | lr 8.82e-05 | ms/batch 71.46169 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   145000 | 145000 batches | lr 8.76e-05 | ms/batch 71.01995 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   145200 | 145200 batches | lr 8.7e-05 | ms/batch 71.25532 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   145400 | 145400 batches | lr 8.64e-05 | ms/batch 72.49003 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   145600 | 145600 batches | lr 8.59e-05 | ms/batch 72.41967 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   145800 | 145800 batches | lr 8.53e-05 | ms/batch 71.80052 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   146000 | 146000 batches | lr 8.47e-05 | ms/batch 71.47153 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   146200 | 146200 batches | lr 8.41e-05 | ms/batch 71.51231 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   146400 | 146400 batches | lr 8.35e-05 | ms/batch 71.32180 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   146600 | 146600 batches | lr 8.29e-05 | ms/batch 72.12422 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   146800 | 146800 batches | lr 8.23e-05 | ms/batch 72.07519 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   147000 | 147000 batches | lr 8.17e-05 | ms/batch 71.75478 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   147200 | 147200 batches | lr 8.12e-05 | ms/batch 72.59533 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   147400 | 147400 batches | lr 8.06e-05 | ms/batch 71.51763 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   147600 | 147600 batches | lr 8e-05 | ms/batch 72.63920 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   147800 | 147800 batches | lr 7.94e-05 | ms/batch 72.32154 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   148000 | 148000 batches | lr 7.89e-05 | ms/batch 71.45893 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 4  2  5  9  6 11  8  3  4  2  9 11  4  8 11  9  2  3  4  2 10 10  4  4\n",
      "  1  4  4 10 10  2  4  3  2  9 11  8  4 11  9  2  4  3  8 11  6  9  5  2]\n",
      "Target: [ 2  5  9  6 11  8  3  4  2  9 11  4  8 11  9  2  3  4  2 10 10  4  4  1\n",
      "  4  4 10 10  2  4  3  2  9 11  8  4 11  9  2  4  3  8 11  6  9  5  2  4]\n",
      "Teacher forcing: acc:0.09908854166666667\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.09908854166666667\n",
      "Preds:  [ 2  5  9  6 11  8  3  4  2  9 11  4  8 11  9  2  3  4  2 10 10  4  4  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  37 at step   148000 | time: 296.68s | valid loss 2.30260 | valid ppl   10.0001 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   148200 | 148200 batches | lr 7.83e-05 | ms/batch 117.00203 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   148400 | 148400 batches | lr 7.77e-05 | ms/batch 72.07826 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   148600 | 148600 batches | lr 7.72e-05 | ms/batch 72.04548 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   148800 | 148800 batches | lr 7.66e-05 | ms/batch 71.74223 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   149000 | 149000 batches | lr 7.6e-05 | ms/batch 72.12451 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   149200 | 149200 batches | lr 7.55e-05 | ms/batch 71.18068 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   149400 | 149400 batches | lr 7.49e-05 | ms/batch 71.00223 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   149600 | 149600 batches | lr 7.43e-05 | ms/batch 70.67203 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   149800 | 149800 batches | lr 7.38e-05 | ms/batch 71.25469 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   150000 | 150000 batches | lr 7.32e-05 | ms/batch 70.94819 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   150200 | 150200 batches | lr 7.27e-05 | ms/batch 71.36752 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   150400 | 150400 batches | lr 7.21e-05 | ms/batch 70.98405 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   150600 | 150600 batches | lr 7.16e-05 | ms/batch 70.51952 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   150800 | 150800 batches | lr 7.1e-05 | ms/batch 72.51264 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   151000 | 151000 batches | lr 7.05e-05 | ms/batch 71.43736 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   151200 | 151200 batches | lr 6.99e-05 | ms/batch 71.25115 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   151400 | 151400 batches | lr 6.94e-05 | ms/batch 72.03043 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   151600 | 151600 batches | lr 6.88e-05 | ms/batch 73.89938 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   151800 | 151800 batches | lr 6.83e-05 | ms/batch 72.15120 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   152000 | 152000 batches | lr 6.78e-05 | ms/batch 71.44422 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 6  3  8  2  9  6  9 11 10  5 10  3  4 10  3  6  8  6 11  5  8 10  6  4\n",
      "  1  4  6 10  8  5 11  6  8  6  3 10  4  3 10  5 10 11  9  6  9  2  8  3]\n",
      "Target: [ 3  8  2  9  6  9 11 10  5 10  3  4 10  3  6  8  6 11  5  8 10  6  4  1\n",
      "  4  6 10  8  5 11  6  8  6  3 10  4  3 10  5 10 11  9  6  9  2  8  3  6]\n",
      "Teacher forcing: acc:0.10294270833333333\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10294270833333333\n",
      "Preds:  [ 3  8  2  9  6  9 11 10  5 10  3  4 10  3  6  8  6 11  5  8 10  6  4  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  38 at step   152000 | time: 295.66s | valid loss 2.30255 | valid ppl    9.9996 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   152200 | 152200 batches | lr 6.72e-05 | ms/batch 119.18564 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   152400 | 152400 batches | lr 6.67e-05 | ms/batch 71.99149 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   152600 | 152600 batches | lr 6.62e-05 | ms/batch 73.23275 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   152800 | 152800 batches | lr 6.56e-05 | ms/batch 72.10272 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   153000 | 153000 batches | lr 6.51e-05 | ms/batch 71.18456 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   153200 | 153200 batches | lr 6.46e-05 | ms/batch 72.45117 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   153400 | 153400 batches | lr 6.4e-05 | ms/batch 70.45458 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   153600 | 153600 batches | lr 6.35e-05 | ms/batch 71.87875 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   153800 | 153800 batches | lr 6.3e-05 | ms/batch 70.70375 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   154000 | 154000 batches | lr 6.25e-05 | ms/batch 71.77299 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   154200 | 154200 batches | lr 6.2e-05 | ms/batch 71.87462 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   154400 | 154400 batches | lr 6.14e-05 | ms/batch 71.87647 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   154600 | 154600 batches | lr 6.09e-05 | ms/batch 71.70919 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   154800 | 154800 batches | lr 6.04e-05 | ms/batch 71.05769 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   155000 | 155000 batches | lr 5.99e-05 | ms/batch 72.71203 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   155200 | 155200 batches | lr 5.94e-05 | ms/batch 72.45842 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   155400 | 155400 batches | lr 5.89e-05 | ms/batch 71.51097 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   155600 | 155600 batches | lr 5.84e-05 | ms/batch 72.04466 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   155800 | 155800 batches | lr 5.79e-05 | ms/batch 72.55536 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   156000 | 156000 batches | lr 5.74e-05 | ms/batch 71.98631 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 4 11  4 10  4 10  5 11  3  5  9  2  8 10  9  4  3  5  3  6 10  2  9  6\n",
      "  1  6  9  2 10  6  3  5  3  4  9 10  8  2  9  5  3 11  5 10  4 10  4 11]\n",
      "Target: [11  4 10  4 10  5 11  3  5  9  2  8 10  9  4  3  5  3  6 10  2  9  6  1\n",
      "  6  9  2 10  6  3  5  3  4  9 10  8  2  9  5  3 11  5 10  4 10  4 11  4]\n",
      "Teacher forcing: acc:0.10244791666666667\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10244791666666667\n",
      "Preds:  [11  4 10  4 10  5 11  3  5  9  2  8 10  9  4  3  5  3  6 10  2  9  6  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  39 at step   156000 | time: 296.84s | valid loss 2.30256 | valid ppl    9.9998 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   156200 | 156200 batches | lr 5.69e-05 | ms/batch 118.13770 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   156400 | 156400 batches | lr 5.64e-05 | ms/batch 72.81405 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   156600 | 156600 batches | lr 5.59e-05 | ms/batch 73.28790 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   156800 | 156800 batches | lr 5.54e-05 | ms/batch 72.63268 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   157000 | 157000 batches | lr 5.49e-05 | ms/batch 71.55124 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   157200 | 157200 batches | lr 5.44e-05 | ms/batch 72.08565 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   157400 | 157400 batches | lr 5.39e-05 | ms/batch 71.24241 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   157600 | 157600 batches | lr 5.34e-05 | ms/batch 72.09638 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   157800 | 157800 batches | lr 5.29e-05 | ms/batch 72.07435 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   158000 | 158000 batches | lr 5.25e-05 | ms/batch 71.93800 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   158200 | 158200 batches | lr 5.2e-05 | ms/batch 71.28473 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   158400 | 158400 batches | lr 5.15e-05 | ms/batch 70.58340 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   158600 | 158600 batches | lr 5.1e-05 | ms/batch 71.36574 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   158800 | 158800 batches | lr 5.06e-05 | ms/batch 72.16944 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   159000 | 159000 batches | lr 5.01e-05 | ms/batch 71.22394 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   159200 | 159200 batches | lr 4.96e-05 | ms/batch 71.50544 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   159400 | 159400 batches | lr 4.91e-05 | ms/batch 73.16005 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   159600 | 159600 batches | lr 4.87e-05 | ms/batch 74.73498 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   159800 | 159800 batches | lr 4.82e-05 | ms/batch 73.31856 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   160000 | 160000 batches | lr 4.77e-05 | ms/batch 73.83085 | loss 2.30257 | ppl    10.000\n",
      "|\n",
      "Source: [ 9  5  8  6  7  2  7  7  6  7  4  4  2  9  9  4  7 11  9 11  7  2  2  9\n",
      "  1  9  2  2  7 11  9 11  7  4  9  9  2  4  4  7  6  7  7  2  7  6  8  5]\n",
      "Target: [ 5  8  6  7  2  7  7  6  7  4  4  2  9  9  4  7 11  9 11  7  2  2  9  1\n",
      "  9  2  2  7 11  9 11  7  4  9  9  2  4  4  7  6  7  7  2  7  6  8  5  9]\n",
      "Teacher forcing: acc:0.10270833333333333\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10270833333333333\n",
      "Preds:  [ 5  8  6  7  2  7  7  6  7  4  4  2  9  9  4  7 11  9 11  7  2  2  9  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  40 at step   160000 | time: 298.49s | valid loss 2.30260 | valid ppl   10.0002 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   160200 | 160200 batches | lr 4.73e-05 | ms/batch 120.24556 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   160400 | 160400 batches | lr 4.68e-05 | ms/batch 72.05771 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   160600 | 160600 batches | lr 4.64e-05 | ms/batch 71.20120 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   160800 | 160800 batches | lr 4.59e-05 | ms/batch 72.14844 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   161000 | 161000 batches | lr 4.55e-05 | ms/batch 70.82679 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   161200 | 161200 batches | lr 4.5e-05 | ms/batch 72.48607 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   161400 | 161400 batches | lr 4.46e-05 | ms/batch 71.62459 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   161600 | 161600 batches | lr 4.41e-05 | ms/batch 71.64569 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   161800 | 161800 batches | lr 4.37e-05 | ms/batch 71.98488 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   162000 | 162000 batches | lr 4.32e-05 | ms/batch 71.99412 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   162200 | 162200 batches | lr 4.28e-05 | ms/batch 72.31283 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   162400 | 162400 batches | lr 4.24e-05 | ms/batch 72.23822 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   162600 | 162600 batches | lr 4.19e-05 | ms/batch 71.64811 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   162800 | 162800 batches | lr 4.15e-05 | ms/batch 71.82199 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   163000 | 163000 batches | lr 4.1e-05 | ms/batch 71.53802 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   163200 | 163200 batches | lr 4.06e-05 | ms/batch 72.97137 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   163400 | 163400 batches | lr 4.02e-05 | ms/batch 71.61967 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   163600 | 163600 batches | lr 3.98e-05 | ms/batch 72.97854 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   163800 | 163800 batches | lr 3.93e-05 | ms/batch 71.08719 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   164000 | 164000 batches | lr 3.89e-05 | ms/batch 72.23958 | loss 2.30258 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 9 11  2 10  5 10  6 10  6  6  6  4  5  2  8  9  5  5 10  2  5  9  5 10\n",
      "  1 10  5  9  5  2 10  5  5  9  8  2  5  4  6  6  6 10  6 10  5 10  2 11]\n",
      "Target: [11  2 10  5 10  6 10  6  6  6  4  5  2  8  9  5  5 10  2  5  9  5 10  1\n",
      " 10  5  9  5  2 10  5  5  9  8  2  5  4  6  6  6 10  6 10  5 10  2 11  9]\n",
      "Teacher forcing: acc:0.10305059523809523\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10305059523809523\n",
      "Preds:  [11  2 10  5 10  6 10  6  6  6  4  5  2  8  9  5  5 10  2  5  9  5 10  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  41 at step   164000 | time: 296.90s | valid loss 2.30256 | valid ppl    9.9997 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   164200 | 164200 batches | lr 3.85e-05 | ms/batch 116.63415 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   164400 | 164400 batches | lr 3.81e-05 | ms/batch 71.97808 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   164600 | 164600 batches | lr 3.77e-05 | ms/batch 71.67061 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   164800 | 164800 batches | lr 3.73e-05 | ms/batch 72.09438 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   165000 | 165000 batches | lr 3.68e-05 | ms/batch 71.89101 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   165200 | 165200 batches | lr 3.64e-05 | ms/batch 71.21552 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   165400 | 165400 batches | lr 3.6e-05 | ms/batch 70.91898 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   165600 | 165600 batches | lr 3.56e-05 | ms/batch 72.60665 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   165800 | 165800 batches | lr 3.52e-05 | ms/batch 71.49272 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   166000 | 166000 batches | lr 3.48e-05 | ms/batch 72.62735 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   166200 | 166200 batches | lr 3.44e-05 | ms/batch 71.67389 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   166400 | 166400 batches | lr 3.4e-05 | ms/batch 72.41861 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   166600 | 166600 batches | lr 3.36e-05 | ms/batch 72.46795 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   166800 | 166800 batches | lr 3.32e-05 | ms/batch 72.41659 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   167000 | 167000 batches | lr 3.28e-05 | ms/batch 72.00115 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   167200 | 167200 batches | lr 3.25e-05 | ms/batch 71.53660 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   167400 | 167400 batches | lr 3.21e-05 | ms/batch 72.12102 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   167600 | 167600 batches | lr 3.17e-05 | ms/batch 72.52394 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   167800 | 167800 batches | lr 3.13e-05 | ms/batch 72.86672 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   168000 | 168000 batches | lr 3.09e-05 | ms/batch 71.64508 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  3  4 10  6 11 11  3 11  6  6  2  4  6 10 11  2  9  2 10  7  2  9  6\n",
      "  1  6  9  2  7 10  2  9  2 11 10  6  4  2  6  6 11  3 11 11  6 10  4  3]\n",
      "Target: [ 3  4 10  6 11 11  3 11  6  6  2  4  6 10 11  2  9  2 10  7  2  9  6  1\n",
      "  6  9  2  7 10  2  9  2 11 10  6  4  2  6  6 11  3 11 11  6 10  4  3  2]\n",
      "Teacher forcing: acc:0.099453125\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.099453125\n",
      "Preds:  [ 3  4 10  6 11 11  3 11  6  6  2  4  6 10 11  2  9  2 10  7  2  9  6  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  42 at step   168000 | time: 296.67s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   168200 | 168200 batches | lr 3.05e-05 | ms/batch 116.57801 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   168400 | 168400 batches | lr 3.02e-05 | ms/batch 72.06610 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   168600 | 168600 batches | lr 2.98e-05 | ms/batch 72.53549 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   168800 | 168800 batches | lr 2.94e-05 | ms/batch 72.32386 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   169000 | 169000 batches | lr 2.91e-05 | ms/batch 72.60049 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   169200 | 169200 batches | lr 2.87e-05 | ms/batch 71.40109 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   169400 | 169400 batches | lr 2.83e-05 | ms/batch 71.21649 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   169600 | 169600 batches | lr 2.8e-05 | ms/batch 72.91314 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   169800 | 169800 batches | lr 2.76e-05 | ms/batch 73.68723 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   170000 | 170000 batches | lr 2.72e-05 | ms/batch 73.85319 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   170200 | 170200 batches | lr 2.69e-05 | ms/batch 73.37616 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   170400 | 170400 batches | lr 2.65e-05 | ms/batch 73.30248 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   170600 | 170600 batches | lr 2.62e-05 | ms/batch 70.42600 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   170800 | 170800 batches | lr 2.58e-05 | ms/batch 70.74843 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   171000 | 171000 batches | lr 2.55e-05 | ms/batch 69.40785 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   171200 | 171200 batches | lr 2.51e-05 | ms/batch 69.41708 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   171400 | 171400 batches | lr 2.48e-05 | ms/batch 71.22587 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   171600 | 171600 batches | lr 2.45e-05 | ms/batch 70.73163 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   171800 | 171800 batches | lr 2.41e-05 | ms/batch 69.06110 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   172000 | 172000 batches | lr 2.38e-05 | ms/batch 69.32580 | loss 2.30258 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [10  7  5  5  7  9 10  9  2  2 10  6 10  9  6  8  5 11  7  8  6  3  3 10\n",
      "  1 10  3  3  6  8  7 11  5  8  6  9 10  6 10  2  2  9 10  9  7  5  5  7]\n",
      "Target: [ 7  5  5  7  9 10  9  2  2 10  6 10  9  6  8  5 11  7  8  6  3  3 10  1\n",
      " 10  3  3  6  8  7 11  5  8  6  9 10  6 10  2  2  9 10  9  7  5  5  7 10]\n",
      "Teacher forcing: acc:0.09954294217687075\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09954294217687075\n",
      "Preds:  [ 7  5  5  7  9 10  9  2  2 10  6 10  9  6  8  5 11  7  8  6  3  3 10  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  43 at step   172000 | time: 294.42s | valid loss 2.30258 | valid ppl    9.9999 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   172200 | 172200 batches | lr 2.35e-05 | ms/batch 109.95289 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   172400 | 172400 batches | lr 2.31e-05 | ms/batch 68.10168 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   172600 | 172600 batches | lr 2.28e-05 | ms/batch 69.65118 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   172800 | 172800 batches | lr 2.25e-05 | ms/batch 69.41996 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   173000 | 173000 batches | lr 2.21e-05 | ms/batch 68.98410 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   173200 | 173200 batches | lr 2.18e-05 | ms/batch 70.84334 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   173400 | 173400 batches | lr 2.15e-05 | ms/batch 69.15371 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   173600 | 173600 batches | lr 2.12e-05 | ms/batch 69.45221 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   173800 | 173800 batches | lr 2.09e-05 | ms/batch 69.89174 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   174000 | 174000 batches | lr 2.06e-05 | ms/batch 71.24870 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   174200 | 174200 batches | lr 2.03e-05 | ms/batch 70.29403 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   174400 | 174400 batches | lr 1.99e-05 | ms/batch 70.02930 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   174600 | 174600 batches | lr 1.96e-05 | ms/batch 70.68005 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   174800 | 174800 batches | lr 1.93e-05 | ms/batch 70.41937 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   175000 | 175000 batches | lr 1.9e-05 | ms/batch 71.52762 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   175200 | 175200 batches | lr 1.87e-05 | ms/batch 70.40527 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   175400 | 175400 batches | lr 1.84e-05 | ms/batch 70.27214 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   175600 | 175600 batches | lr 1.81e-05 | ms/batch 68.89575 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   175800 | 175800 batches | lr 1.78e-05 | ms/batch 69.45727 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   176000 | 176000 batches | lr 1.76e-05 | ms/batch 68.50198 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 8 10  9 11 11  9  6 10  8  6  7  7  6  2  5  8  3  7  6 11  2 11 10  9\n",
      "  1  9 10 11  2 11  6  7  3  8  5  2  6  7  7  6  8 10  6  9 11 11  9 10]\n",
      "Target: [10  9 11 11  9  6 10  8  6  7  7  6  2  5  8  3  7  6 11  2 11 10  9  1\n",
      "  9 10 11  2 11  6  7  3  8  5  2  6  7  7  6  8 10  6  9 11 11  9 10  8]\n",
      "Teacher forcing: acc:0.10192708333333333\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10192708333333333\n",
      "Preds:  [10  9 11 11  9  6 10  8  6  7  7  6  2  5  8  3  7  6 11  2 11 10  9  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  44 at step   176000 | time: 288.61s | valid loss 2.30258 | valid ppl    9.9999 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   176200 | 176200 batches | lr 1.73e-05 | ms/batch 116.65234 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   176400 | 176400 batches | lr 1.7e-05 | ms/batch 69.11764 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   176600 | 176600 batches | lr 1.67e-05 | ms/batch 69.79098 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   176800 | 176800 batches | lr 1.64e-05 | ms/batch 70.62428 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   177000 | 177000 batches | lr 1.61e-05 | ms/batch 69.76264 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   177200 | 177200 batches | lr 1.59e-05 | ms/batch 69.74070 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   177400 | 177400 batches | lr 1.56e-05 | ms/batch 69.91975 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   177600 | 177600 batches | lr 1.53e-05 | ms/batch 68.61269 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   177800 | 177800 batches | lr 1.5e-05 | ms/batch 69.98730 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   178000 | 178000 batches | lr 1.48e-05 | ms/batch 69.71840 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   178200 | 178200 batches | lr 1.45e-05 | ms/batch 70.35716 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   178400 | 178400 batches | lr 1.43e-05 | ms/batch 69.25513 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   178600 | 178600 batches | lr 1.4e-05 | ms/batch 69.80067 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   178800 | 178800 batches | lr 1.37e-05 | ms/batch 69.18102 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   179000 | 179000 batches | lr 1.35e-05 | ms/batch 69.45104 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   179200 | 179200 batches | lr 1.32e-05 | ms/batch 70.15001 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   179400 | 179400 batches | lr 1.3e-05 | ms/batch 70.83779 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   179600 | 179600 batches | lr 1.27e-05 | ms/batch 70.61904 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   179800 | 179800 batches | lr 1.25e-05 | ms/batch 70.27828 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   180000 | 180000 batches | lr 1.22e-05 | ms/batch 70.94660 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 8  3  3  7 10  3  3  8  7  9  8 10  9  9  4  2  3  9  9  7  5  6  2 11\n",
      "  1 11  2  6  5  7  9  9  3  2  4  9  9 10  8  9  7  8  3  3 10  7  3  3]\n",
      "Target: [ 3  3  7 10  3  3  8  7  9  8 10  9  9  4  2  3  9  9  7  5  6  2 11  1\n",
      " 11  2  6  5  7  9  9  3  2  4  9  9 10  8  9  7  8  3  3 10  7  3  3  8]\n",
      "Teacher forcing: acc:0.09984375\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09984375\n",
      "Preds:  [ 3  3  7 10  3  3  8  7  9  8 10  9  9  4  2  3  9  9  7  5  6  2 11  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  45 at step   180000 | time: 289.22s | valid loss 2.30258 | valid ppl    9.9999 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   180200 | 180200 batches | lr 1.2e-05 | ms/batch 122.14146 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   180400 | 180400 batches | lr 1.18e-05 | ms/batch 75.15856 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   180600 | 180600 batches | lr 1.15e-05 | ms/batch 72.84711 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   180800 | 180800 batches | lr 1.13e-05 | ms/batch 71.23074 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   181000 | 181000 batches | lr 1.11e-05 | ms/batch 71.77698 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   181200 | 181200 batches | lr 1.08e-05 | ms/batch 70.86053 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   181400 | 181400 batches | lr 1.06e-05 | ms/batch 71.49557 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   181600 | 181600 batches | lr 1.04e-05 | ms/batch 73.72653 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   181800 | 181800 batches | lr 1.01e-05 | ms/batch 78.32256 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   182000 | 182000 batches | lr 9.93e-06 | ms/batch 74.13908 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   182200 | 182200 batches | lr 9.71e-06 | ms/batch 71.99954 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   182400 | 182400 batches | lr 9.49e-06 | ms/batch 70.14654 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   182600 | 182600 batches | lr 9.28e-06 | ms/batch 69.16965 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   182800 | 182800 batches | lr 9.07e-06 | ms/batch 70.38491 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   183000 | 183000 batches | lr 8.86e-06 | ms/batch 70.10814 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   183200 | 183200 batches | lr 8.65e-06 | ms/batch 69.11599 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   183400 | 183400 batches | lr 8.45e-06 | ms/batch 70.56783 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   183600 | 183600 batches | lr 8.25e-06 | ms/batch 70.25088 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   183800 | 183800 batches | lr 8.05e-06 | ms/batch 69.44300 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   184000 | 184000 batches | lr 7.85e-06 | ms/batch 71.42534 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 5  6  8  7 10 11  5  4  2 11  3  2  5  3  7  6  7  2  9  6  4  2  8  4\n",
      "  1  4  8  2  4  6  9  2  7  6  7  3  5  2  3 11  2  4  5 11 10  7  8  6]\n",
      "Target: [ 6  8  7 10 11  5  4  2 11  3  2  5  3  7  6  7  2  9  6  4  2  8  4  1\n",
      "  4  8  2  4  6  9  2  7  6  7  3  5  2  3 11  2  4  5 11 10  7  8  6  5]\n",
      "Teacher forcing: acc:0.10073873299319729\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10073873299319729\n",
      "Preds:  [ 6  8  7 10 11  5  4  2 11  3  2  5  3  7  6  7  2  9  6  4  2  8  4  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  46 at step   184000 | time: 296.80s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   184200 | 184200 batches | lr 7.66e-06 | ms/batch 116.95496 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   184400 | 184400 batches | lr 7.47e-06 | ms/batch 70.94978 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   184600 | 184600 batches | lr 7.28e-06 | ms/batch 69.76522 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   184800 | 184800 batches | lr 7.09e-06 | ms/batch 70.28228 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   185000 | 185000 batches | lr 6.91e-06 | ms/batch 69.97317 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   185200 | 185200 batches | lr 6.73e-06 | ms/batch 69.61271 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step   185400 | 185400 batches | lr 6.55e-06 | ms/batch 70.22722 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   185600 | 185600 batches | lr 6.37e-06 | ms/batch 71.88597 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   185800 | 185800 batches | lr 6.19e-06 | ms/batch 71.26392 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   186000 | 186000 batches | lr 6.02e-06 | ms/batch 70.08053 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   186200 | 186200 batches | lr 5.85e-06 | ms/batch 69.44238 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   186400 | 186400 batches | lr 5.68e-06 | ms/batch 71.21871 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   186600 | 186600 batches | lr 5.52e-06 | ms/batch 69.28789 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   186800 | 186800 batches | lr 5.35e-06 | ms/batch 70.05874 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   187000 | 187000 batches | lr 5.19e-06 | ms/batch 71.14259 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   187200 | 187200 batches | lr 5.04e-06 | ms/batch 70.57128 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   187400 | 187400 batches | lr 4.88e-06 | ms/batch 71.21662 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   187600 | 187600 batches | lr 4.73e-06 | ms/batch 69.72237 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   187800 | 187800 batches | lr 4.58e-06 | ms/batch 70.40965 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   188000 | 188000 batches | lr 4.43e-06 | ms/batch 70.36280 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 2 11  5  6  5  7  2 11  5  6  6  5  3  6  5  5  4  9  4  2 10  6  9  7\n",
      "  1  7  9  6 10  2  4  9  4  5  5  6  3  5  6  6  5 11  2  7  5  6  5 11]\n",
      "Target: [11  5  6  5  7  2 11  5  6  6  5  3  6  5  5  4  9  4  2 10  6  9  7  1\n",
      "  7  9  6 10  2  4  9  4  5  5  6  3  5  6  6  5 11  2  7  5  6  5 11  2]\n",
      "Teacher forcing: acc:0.100390625\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.100390625\n",
      "Preds:  [11  5  6  5  7  2 11  5  6  6  5  3  6  5  5  4  9  4  2 10  6  9  7  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  47 at step   188000 | time: 290.82s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   188200 | 188200 batches | lr 4.28e-06 | ms/batch 116.46379 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   188400 | 188400 batches | lr 4.14e-06 | ms/batch 70.71837 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   188600 | 188600 batches | lr 4e-06 | ms/batch 69.90059 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   188800 | 188800 batches | lr 3.86e-06 | ms/batch 68.82391 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   189000 | 189000 batches | lr 3.72e-06 | ms/batch 70.69627 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   189200 | 189200 batches | lr 3.59e-06 | ms/batch 69.94825 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   189400 | 189400 batches | lr 3.46e-06 | ms/batch 69.50480 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   189600 | 189600 batches | lr 3.33e-06 | ms/batch 71.38334 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   189800 | 189800 batches | lr 3.2e-06 | ms/batch 70.42694 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   190000 | 190000 batches | lr 3.08e-06 | ms/batch 70.24073 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   190200 | 190200 batches | lr 2.96e-06 | ms/batch 70.67953 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   190400 | 190400 batches | lr 2.84e-06 | ms/batch 71.03439 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   190600 | 190600 batches | lr 2.72e-06 | ms/batch 70.46487 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step   190800 | 190800 batches | lr 2.61e-06 | ms/batch 69.59493 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   191000 | 191000 batches | lr 2.49e-06 | ms/batch 70.02851 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   191200 | 191200 batches | lr 2.38e-06 | ms/batch 70.71105 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   191400 | 191400 batches | lr 2.28e-06 | ms/batch 70.78174 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   191600 | 191600 batches | lr 2.17e-06 | ms/batch 70.19583 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   191800 | 191800 batches | lr 2.07e-06 | ms/batch 70.19096 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   192000 | 192000 batches | lr 1.97e-06 | ms/batch 69.68273 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 4  2  2  7 10  8  3  7 10  4  7  8  3 10 11 11  6  7  3  8  5  6  8  7\n",
      "  1  7  8  6  5  8  3  7  6 11 11 10  3  8  7  4 10  7  3  8 10  7  2  2]\n",
      "Target: [ 2  2  7 10  8  3  7 10  4  7  8  3 10 11 11  6  7  3  8  5  6  8  7  1\n",
      "  7  8  6  5  8  3  7  6 11 11 10  3  8  7  4 10  7  3  8 10  7  2  2  4]\n",
      "Teacher forcing: acc:0.09863945578231292\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09863945578231292\n",
      "Preds:  [ 2  2  7 10  8  3  7 10  4  7  8  3 10 11 11  6  7  3  8  5  6  8  7  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  48 at step   192000 | time: 290.11s | valid loss 2.30259 | valid ppl   10.0000 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   192200 | 192200 batches | lr 1.87e-06 | ms/batch 115.72141 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   192400 | 192400 batches | lr 1.78e-06 | ms/batch 68.69477 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   192600 | 192600 batches | lr 1.69e-06 | ms/batch 70.45386 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   192800 | 192800 batches | lr 1.6e-06 | ms/batch 70.37517 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   193000 | 193000 batches | lr 1.51e-06 | ms/batch 69.69508 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   193200 | 193200 batches | lr 1.42e-06 | ms/batch 69.17972 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   193400 | 193400 batches | lr 1.34e-06 | ms/batch 70.99610 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   193600 | 193600 batches | lr 1.26e-06 | ms/batch 69.36566 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   193800 | 193800 batches | lr 1.18e-06 | ms/batch 69.53627 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   194000 | 194000 batches | lr 1.11e-06 | ms/batch 68.26715 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   194200 | 194200 batches | lr 1.04e-06 | ms/batch 69.23679 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   194400 | 194400 batches | lr 9.67e-07 | ms/batch 69.44253 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   194600 | 194600 batches | lr 8.99e-07 | ms/batch 68.56989 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   194800 | 194800 batches | lr 8.34e-07 | ms/batch 70.88528 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   195000 | 195000 batches | lr 7.71e-07 | ms/batch 69.43929 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   195200 | 195200 batches | lr 7.1e-07 | ms/batch 70.37635 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   195400 | 195400 batches | lr 6.52e-07 | ms/batch 70.22162 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   195600 | 195600 batches | lr 5.97e-07 | ms/batch 70.55347 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   195800 | 195800 batches | lr 5.44e-07 | ms/batch 70.18786 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   196000 | 196000 batches | lr 4.93e-07 | ms/batch 71.22649 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 5  7  2  9  6  8 11  3  6  8  6  2  3  4  5  8  5  4 10  6 11 10  7  7\n",
      "  1  7  7 10 11  6 10  4  5  8  5  4  3  2  6  8  6  3 11  8  6  9  2  7]\n",
      "Target: [ 7  2  9  6  8 11  3  6  8  6  2  3  4  5  8  5  4 10  6 11 10  7  7  1\n",
      "  7  7 10 11  6 10  4  5  8  5  4  3  2  6  8  6  3 11  8  6  9  2  7  5]\n",
      "Teacher forcing: acc:0.10106770833333334\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10106770833333334\n",
      "Preds:  [ 7  2  9  6  8 11  3  6  8  6  2  3  4  5  8  5  4 10  6 11 10  7  7  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  49 at step   196000 | time: 288.83s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step   196200 | 196200 batches | lr 4.45e-07 | ms/batch 117.34054 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   196400 | 196400 batches | lr 4e-07 | ms/batch 70.42255 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   196600 | 196600 batches | lr 3.56e-07 | ms/batch 69.96950 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   196800 | 196800 batches | lr 3.16e-07 | ms/batch 70.98394 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   197000 | 197000 batches | lr 2.78e-07 | ms/batch 70.15114 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   197200 | 197200 batches | lr 2.42e-07 | ms/batch 70.62605 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   197400 | 197400 batches | lr 2.08e-07 | ms/batch 71.50345 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   197600 | 197600 batches | lr 1.78e-07 | ms/batch 70.56966 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   197800 | 197800 batches | lr 1.49e-07 | ms/batch 70.92052 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   198000 | 198000 batches | lr 1.23e-07 | ms/batch 70.27564 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   198200 | 198200 batches | lr 9.99e-08 | ms/batch 71.36959 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   198400 | 198400 batches | lr 7.9e-08 | ms/batch 70.44707 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   198600 | 198600 batches | lr 6.04e-08 | ms/batch 71.16103 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   198800 | 198800 batches | lr 4.44e-08 | ms/batch 72.38427 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   199000 | 199000 batches | lr 3.08e-08 | ms/batch 72.48461 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   199200 | 199200 batches | lr 1.97e-08 | ms/batch 71.97769 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   199400 | 199400 batches | lr 1.11e-08 | ms/batch 72.58529 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   199600 | 199600 batches | lr 4.93e-09 | ms/batch 70.12556 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step   199800 | 199800 batches | lr 1.23e-09 | ms/batch 70.27533 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step   200000 | 200000 batches | lr 0 | ms/batch 70.14164 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [ 4  9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10\n",
      "  1 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9]\n",
      "Target: [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9  4]\n",
      "Teacher forcing: acc:0.10216145833333333\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10216145833333333\n",
      "Preds:  [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  50 at step   200000 | time: 293.12s | valid loss 2.30257 | valid ppl    9.9999 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "|\n",
      "Source: [11  9 10  6  2  7  9  5 10  3  7  4  7  8  3  6  9  6 10  3  9 11  9 11\n",
      "  1 11  9 11  9  3 10  6  9  6  3  8  7  4  7  3 10  5  9  7  2  6 10  9]\n",
      "Target: [ 9 10  6  2  7  9  5 10  3  7  4  7  8  3  6  9  6 10  3  9 11  9 11  1\n",
      " 11  9 11  9  3 10  6  9  6  3  8  7  4  7  3 10  5  9  7  2  6 10  9 11]\n",
      "Teacher forcing: acc:0.10192708333333333\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10192708333333333\n",
      "Preds:  [ 9 10  6  2  7  9  5 10  3  7  4  7  8  3  6  9  6 10  3  9 11  9 11  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "| End of training | test loss 2.30258 | test ppl   9.99990\n",
      " | test acc 0.102\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "!bash run_reverse-debug.sh train --work_dir ../evaluation/tl24xl0mt24_debug --tgt_len 24 --eval_tgt_len 24 --mem_len 0 --num_mem_tokens 24 --device_ids 3 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea539b73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/home/admin/TXL/transformer-xl/evaluation/tl12xl0mt6-reverse/20220107-135100/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl12xl0mt6-reverse/20220105-133841/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl12xl0mt6-reverse/20220107-165847/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24-reverse/20220108-174137/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24-reverse/20220108-160157/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24-reverse/20220108-152407/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24-reverse/20220105-121837/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24-reverse/20220108-173854/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24-reverse/20220108-160306/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24-reverse/20220108-155300/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24-reverse/20220107-135033/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24-reverse/20220108-173917/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24-reverse/20220108-160030/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24-reverse/20220108-160205/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24-reverse/20220108-164449/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl48xl0mt24-reverse/20220108-155928/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl48xl0mt24-reverse/20220105-112634/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24_debug-reverse/20220108-174026/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24_debug-reverse/20220108-165557/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24_debug-reverse/20220108-165621/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24_debug-reverse/20220108-164945/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt24_debug-reverse/20220108-174053/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl12xl0mt0-reverse/20220105-102019/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl8xl12mt0-reverse/20220105-123810/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt0-reverse/20220105-102015/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt0-reverse/20220105-102012/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl8xl0mt0-reverse/20220105-102032/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl48xl0mt0-reverse/20220105-102004/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl24mt0-reverse/20220107-135022/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl24mt0-reverse/20220105-111749/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt12-reverse/20220108-181416/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt12-reverse/20220105-121609/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt12-reverse/20220107-135050/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt12-reverse/20220108-152440/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt12-reverse/20220108-160342/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl0mt12-reverse/20220107-154342/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl8xl0mt12-reverse/20220105-145709/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl48xl24mt0-reverse/20220105-105256/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl24xl12mt0-reverse/20220105-111704/log.txt',\n",
       " '/data/home/admin/TXL/transformer-xl/evaluation/tl12xl6mt0-reverse/20220105-120041/log.txt']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path = '/data/home/admin/TXL/transformer-xl/evaluation'\n",
    "\n",
    "def get_log_names(path):\n",
    "    logs = []\n",
    "    _, folders, names = next(os.walk(path))\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        logs += get_log_names(folder_path)\n",
    "\n",
    "    names = [n for n in names if '.txt' in n]\n",
    "    for name in names:\n",
    "        filepath = os.path.join(path, name)\n",
    "        logs.append(filepath)\n",
    "    \n",
    "    return logs\n",
    "\n",
    "get_log_names(path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08085efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[1]\n",
      "Experiment dir : ../evaluation/tl8xl0mt0-reverse/20220105-102032\n",
      "====================================================================================================\n",
      "    - data : /home/admin/x-transformers/data24\n",
      "    - dataset : reverse\n",
      "    - n_layer : 4\n",
      "    - n_head : 4\n",
      "    - d_head : 64\n",
      "    - d_embed : 128\n",
      "    - d_model : 128\n",
      "    - d_inner : 256\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.00025\n",
      "    - mom : 0.0\n",
      "    - scheduler : cosine\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 0.0\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 100000\n",
      "    - batch_size : 32\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 8\n",
      "    - eval_tgt_len : 8\n",
      "    - ext_len : 0\n",
      "    - mem_len : 0\n",
      "    - num_mem_tokens : 0\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : False\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : ../evaluation/tl8xl0mt0-reverse/20220105-102032\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : 50\n",
      "    - sample_softmax : -1\n",
      "    - patience : 0\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [1]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : False\n",
      "    - read_mem_from_cache : True\n",
      "    - answer_size : 24\n",
      "    - tied : True\n",
      "    - ntokens : 12\n",
      "    - n_all_param : 923148\n",
      "    - n_nonemb_param : 921088\n",
      "====================================================================================================\n",
      "#params = 923148\n",
      "#non emb params = 921088\n",
      "/home/admin/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 81.50588 | loss 2.33552 | ppl    10.335\n",
      "| epoch   1 step      400 |    400 batches | lr 0.00025 | ms/batch 78.82018 | loss 2.30620 | ppl    10.036\n",
      "| epoch   1 step      600 |    600 batches | lr 0.00025 | ms/batch 80.77456 | loss 2.30574 | ppl    10.032\n",
      "| epoch   1 step      800 |    800 batches | lr 0.00025 | ms/batch 80.90716 | loss 2.30483 | ppl    10.022\n",
      "| epoch   1 step     1000 |   1000 batches | lr 0.00025 | ms/batch 83.03707 | loss 2.30433 | ppl    10.018\n",
      "| epoch   1 step     1200 |   1200 batches | lr 0.00025 | ms/batch 80.82282 | loss 2.30407 | ppl    10.015\n",
      "| epoch   1 step     1400 |   1400 batches | lr 0.00025 | ms/batch 81.67199 | loss 2.30383 | ppl    10.012\n",
      "| epoch   1 step     1600 |   1600 batches | lr 0.00025 | ms/batch 79.19711 | loss 2.30370 | ppl    10.011\n",
      "| epoch   1 step     1800 |   1800 batches | lr 0.00025 | ms/batch 81.97739 | loss 2.30364 | ppl    10.011\n",
      "| epoch   1 step     2000 |   2000 batches | lr 0.00025 | ms/batch 79.68727 | loss 2.30356 | ppl    10.010\n",
      "| epoch   1 step     2200 |   2200 batches | lr 0.00025 | ms/batch 81.03377 | loss 2.30326 | ppl    10.007\n",
      "| epoch   1 step     2400 |   2400 batches | lr 0.00025 | ms/batch 80.65010 | loss 2.30326 | ppl    10.007\n",
      "| epoch   1 step     2600 |   2600 batches | lr 0.00025 | ms/batch 79.10358 | loss 2.30331 | ppl    10.007\n",
      "| epoch   1 step     2800 |   2800 batches | lr 0.00025 | ms/batch 79.62933 | loss 2.30317 | ppl    10.006\n",
      "| epoch   1 step     3000 |   3000 batches | lr 0.000249 | ms/batch 79.70617 | loss 2.30312 | ppl    10.005\n",
      "| epoch   1 step     3200 |   3200 batches | lr 0.000249 | ms/batch 79.82495 | loss 2.30307 | ppl    10.005\n",
      "| epoch   1 step     3400 |   3400 batches | lr 0.000249 | ms/batch 80.56196 | loss 2.30310 | ppl    10.005\n",
      "| epoch   1 step     3600 |   3600 batches | lr 0.000249 | ms/batch 81.16094 | loss 2.30301 | ppl    10.004\n",
      "| epoch   1 step     3800 |   3800 batches | lr 0.000249 | ms/batch 81.66781 | loss 2.30303 | ppl    10.004\n",
      "| epoch   1 step     4000 |   4000 batches | lr 0.000249 | ms/batch 82.72939 | loss 2.30299 | ppl    10.004\n",
      "|\n",
      "Source: [ 4  9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10\n",
      "  1 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9]\n",
      "Target: [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9  4]\n",
      "Teacher forcing: acc:0.10208333333333333\n",
      "Preds:  [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "No teacher forcing: acc:0.10208333333333333\n",
      "Preds:  [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step     4000 | time: 331.85s | valid loss 2.30285 | valid ppl   10.0027 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     4200 |   4200 batches | lr 0.000249 | ms/batch 124.64931 | loss 2.30301 | ppl    10.004\n",
      "| epoch   1 step     4400 |   4400 batches | lr 0.000249 | ms/batch 82.97793 | loss 2.30308 | ppl    10.005\n",
      "| epoch   1 step     4600 |   4600 batches | lr 0.000249 | ms/batch 79.71124 | loss 2.30295 | ppl    10.004\n",
      "| epoch   1 step     4800 |   4800 batches | lr 0.000249 | ms/batch 80.16981 | loss 2.30307 | ppl    10.005\n",
      "| epoch   1 step     5000 |   5000 batches | lr 0.000248 | ms/batch 79.26097 | loss 2.30299 | ppl    10.004\n",
      "| epoch   1 step     5200 |   5200 batches | lr 0.000248 | ms/batch 79.66849 | loss 2.30286 | ppl    10.003\n",
      "| epoch   1 step     5400 |   5400 batches | lr 0.000248 | ms/batch 80.24145 | loss 2.30290 | ppl    10.003\n",
      "| epoch   1 step     5600 |   5600 batches | lr 0.000248 | ms/batch 81.03819 | loss 2.30291 | ppl    10.003\n",
      "| epoch   1 step     5800 |   5800 batches | lr 0.000248 | ms/batch 83.24645 | loss 2.30286 | ppl    10.003\n",
      "| epoch   1 step     6000 |   6000 batches | lr 0.000248 | ms/batch 81.15407 | loss 2.30284 | ppl    10.003\n",
      "| epoch   1 step     6200 |   6200 batches | lr 0.000248 | ms/batch 82.73577 | loss 2.30294 | ppl    10.004\n",
      "| epoch   1 step     6400 |   6400 batches | lr 0.000247 | ms/batch 79.94914 | loss 2.30279 | ppl    10.002\n",
      "| epoch   1 step     6600 |   6600 batches | lr 0.000247 | ms/batch 79.18615 | loss 2.30281 | ppl    10.002\n",
      "| epoch   1 step     6800 |   6800 batches | lr 0.000247 | ms/batch 79.30349 | loss 2.30289 | ppl    10.003\n",
      "| epoch   1 step     7000 |   7000 batches | lr 0.000247 | ms/batch 78.71959 | loss 2.30278 | ppl    10.002\n",
      "| epoch   1 step     7200 |   7200 batches | lr 0.000247 | ms/batch 78.91868 | loss 2.30287 | ppl    10.003\n",
      "| epoch   1 step     7400 |   7400 batches | lr 0.000247 | ms/batch 79.08301 | loss 2.30280 | ppl    10.002\n",
      "| epoch   1 step     7600 |   7600 batches | lr 0.000246 | ms/batch 79.13613 | loss 2.30280 | ppl    10.002\n",
      "| epoch   1 step     7800 |   7800 batches | lr 0.000246 | ms/batch 81.03372 | loss 2.30287 | ppl    10.003\n",
      "| epoch   1 step     8000 |   8000 batches | lr 0.000246 | ms/batch 80.96705 | loss 2.30283 | ppl    10.002\n",
      "maslina\n",
      "|\n",
      "Source: [ 4 10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8\n",
      "  1  8  5  8  2  9 10 10  8  2  6  6  5  8 10  2  7  9  3  6  6  5  5 10]\n",
      "Target: [10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8  1\n",
      "  8  5  8  2  9 10 10  8  2  6  6  5  8 10  2  7  9  3  6  6  5  5 10  4]\n",
      "Teacher forcing: acc:0.10212053571428571\n",
      "Preds:  [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5]\n",
      "\n",
      "No teacher forcing: acc:0.10212053571428571\n",
      "Preds:  [10  5  5  6  6  3  9  7  2 10  8  5  6  6  2  8 10 10  9  2  8  5  8  1\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   2 at step     8000 | time: 329.87s | valid loss 2.30261 | valid ppl   10.0002 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     8200 |   8200 batches | lr 0.000246 | ms/batch 122.55710 | loss 2.30286 | ppl    10.003\n",
      "| epoch   1 step     8400 |   8400 batches | lr 0.000246 | ms/batch 78.60262 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step     8600 |   8600 batches | lr 0.000245 | ms/batch 79.15566 | loss 2.30280 | ppl    10.002\n",
      "| epoch   1 step     8800 |   8800 batches | lr 0.000245 | ms/batch 80.43553 | loss 2.30281 | ppl    10.002\n",
      "| epoch   1 step     9000 |   9000 batches | lr 0.000245 | ms/batch 80.16320 | loss 2.30276 | ppl    10.002\n",
      "| epoch   1 step     9200 |   9200 batches | lr 0.000245 | ms/batch 79.79276 | loss 2.30273 | ppl    10.001\n",
      "| epoch   1 step     9400 |   9400 batches | lr 0.000245 | ms/batch 83.99317 | loss 2.30278 | ppl    10.002\n",
      "| epoch   1 step     9600 |   9600 batches | lr 0.000244 | ms/batch 80.96254 | loss 2.30278 | ppl    10.002\n",
      "| epoch   1 step     9800 |   9800 batches | lr 0.000244 | ms/batch 79.92572 | loss 2.30275 | ppl    10.002\n",
      "| epoch   1 step    10000 |  10000 batches | lr 0.000244 | ms/batch 79.48343 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    10200 |  10200 batches | lr 0.000244 | ms/batch 80.13164 | loss 2.30278 | ppl    10.002\n",
      "| epoch   1 step    10400 |  10400 batches | lr 0.000243 | ms/batch 78.87416 | loss 2.30275 | ppl    10.002\n",
      "| epoch   1 step    10600 |  10600 batches | lr 0.000243 | ms/batch 82.49534 | loss 2.30275 | ppl    10.002\n",
      "| epoch   1 step    10800 |  10800 batches | lr 0.000243 | ms/batch 81.29780 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    11000 |  11000 batches | lr 0.000243 | ms/batch 79.45869 | loss 2.30282 | ppl    10.002\n",
      "| epoch   1 step    11200 |  11200 batches | lr 0.000242 | ms/batch 82.64913 | loss 2.30277 | ppl    10.002\n",
      "| epoch   1 step    11400 |  11400 batches | lr 0.000242 | ms/batch 83.17999 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    11600 |  11600 batches | lr 0.000242 | ms/batch 84.83984 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    11800 |  11800 batches | lr 0.000242 | ms/batch 82.62191 | loss 2.30275 | ppl    10.002\n",
      "| epoch   1 step    12000 |  12000 batches | lr 0.000241 | ms/batch 83.89720 | loss 2.30274 | ppl    10.002\n",
      "|\n",
      "Source: [ 9  7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3\n",
      "  1  3  3  4 11  6  5  9  9  8  3  7  6  2  9 10 10 11  4 11  3  3 10  7]\n",
      "Target: [ 7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3  1\n",
      "  3  3  4 11  6  5  9  9  8  3  7  6  2  9 10 10 11  4 11  3  3 10  7  9]\n",
      "Teacher forcing: acc:0.10182291666666667\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.10182291666666667\n",
      "Preds:  [ 7 10  3  3 11  4 11 10 10  9  2  6  7  3  8  9  9  5  6 11  4  3  3  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   3 at step    12000 | time: 333.61s | valid loss 2.30265 | valid ppl   10.0006 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    12200 |  12200 batches | lr 0.000241 | ms/batch 130.64669 | loss 2.30270 | ppl    10.001\n",
      "| epoch   1 step    12400 |  12400 batches | lr 0.000241 | ms/batch 84.27224 | loss 2.30276 | ppl    10.002\n",
      "| epoch   1 step    12600 |  12600 batches | lr 0.00024 | ms/batch 85.86292 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    12800 |  12800 batches | lr 0.00024 | ms/batch 82.93634 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    13000 |  13000 batches | lr 0.00024 | ms/batch 81.78989 | loss 2.30270 | ppl    10.001\n",
      "| epoch   1 step    13200 |  13200 batches | lr 0.000239 | ms/batch 80.90746 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    13400 |  13400 batches | lr 0.000239 | ms/batch 83.21971 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    13600 |  13600 batches | lr 0.000239 | ms/batch 80.98788 | loss 2.30274 | ppl    10.002\n",
      "| epoch   1 step    13800 |  13800 batches | lr 0.000238 | ms/batch 81.15152 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    14000 |  14000 batches | lr 0.000238 | ms/batch 83.05962 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    14200 |  14200 batches | lr 0.000238 | ms/batch 80.77031 | loss 2.30277 | ppl    10.002\n",
      "| epoch   1 step    14400 |  14400 batches | lr 0.000237 | ms/batch 79.17638 | loss 2.30272 | ppl    10.001\n",
      "| epoch   1 step    14600 |  14600 batches | lr 0.000237 | ms/batch 79.04110 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    14800 |  14800 batches | lr 0.000237 | ms/batch 79.96225 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    15000 |  15000 batches | lr 0.000236 | ms/batch 79.87646 | loss 2.30272 | ppl    10.001\n",
      "| epoch   1 step    15200 |  15200 batches | lr 0.000236 | ms/batch 81.03386 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    15400 |  15400 batches | lr 0.000236 | ms/batch 79.00179 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    15600 |  15600 batches | lr 0.000235 | ms/batch 81.49523 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    15800 |  15800 batches | lr 0.000235 | ms/batch 79.51298 | loss 2.30268 | ppl    10.001\n",
      "| epoch   1 step    16000 |  16000 batches | lr 0.000235 | ms/batch 80.14719 | loss 2.30274 | ppl    10.002\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9\n",
      "  1  9  6  4  6  2  6 10 10 10  8  8  3  9  7 11  2 11  3  3  9  4  6  3]\n",
      "Target: [ 3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9  1\n",
      "  9  6  4  6  2  6 10 10 10  8  8  3  9  7 11  2 11  3  3  9  4  6  3  7]\n",
      "Teacher forcing: acc:0.09909119897959184\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.09909119897959184\n",
      "Preds:  [ 3  6  4  9  3  3 11  2 11  7  9  3  8  8 10 10 10  6  2  6  4  6  9  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   4 at step    16000 | time: 334.52s | valid loss 2.30268 | valid ppl   10.0009 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    16200 |  16200 batches | lr 0.000234 | ms/batch 123.05666 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    16400 |  16400 batches | lr 0.000234 | ms/batch 78.28741 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    16600 |  16600 batches | lr 0.000233 | ms/batch 79.95809 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    16800 |  16800 batches | lr 0.000233 | ms/batch 78.75848 | loss 2.30271 | ppl    10.001\n",
      "| epoch   1 step    17000 |  17000 batches | lr 0.000233 | ms/batch 80.35281 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    17200 |  17200 batches | lr 0.000232 | ms/batch 81.65395 | loss 2.30273 | ppl    10.001\n",
      "| epoch   1 step    17400 |  17400 batches | lr 0.000232 | ms/batch 80.40331 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    17600 |  17600 batches | lr 0.000231 | ms/batch 79.56107 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    17800 |  17800 batches | lr 0.000231 | ms/batch 79.55628 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    18000 |  18000 batches | lr 0.000231 | ms/batch 80.45958 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    18200 |  18200 batches | lr 0.00023 | ms/batch 79.57383 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    18400 |  18400 batches | lr 0.00023 | ms/batch 79.85736 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    18600 |  18600 batches | lr 0.000229 | ms/batch 79.54570 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    18800 |  18800 batches | lr 0.000229 | ms/batch 79.86441 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    19000 |  19000 batches | lr 0.000228 | ms/batch 78.49162 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    19200 |  19200 batches | lr 0.000228 | ms/batch 79.89661 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    19400 |  19400 batches | lr 0.000227 | ms/batch 79.46487 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    19600 |  19600 batches | lr 0.000227 | ms/batch 80.44454 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    19800 |  19800 batches | lr 0.000227 | ms/batch 80.62970 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    20000 |  20000 batches | lr 0.000226 | ms/batch 81.34587 | loss 2.30263 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10\n",
      "  1 10  8  3  2  6  6  3  3  6  5 10  8  4  2  3  4  6  2 11  7  9  9  3]\n",
      "Target: [ 3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10  1\n",
      " 10  8  3  2  6  6  3  3  6  5 10  8  4  2  3  4  6  2 11  7  9  9  3  2]\n",
      "Teacher forcing: acc:0.101328125\n",
      "Preds:  [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "No teacher forcing: acc:0.101328125\n",
      "Preds:  [ 3  9  9  7 11  2  6  4  3  2  4  8 10  5  6  3  3  6  6  2  3  8 10  1\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   5 at step    20000 | time: 328.21s | valid loss 2.30264 | valid ppl   10.0005 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    20200 |  20200 batches | lr 0.000226 | ms/batch 125.19822 | loss 2.30267 | ppl    10.001\n",
      "| epoch   1 step    20400 |  20400 batches | lr 0.000225 | ms/batch 81.29922 | loss 2.30269 | ppl    10.001\n",
      "| epoch   1 step    20600 |  20600 batches | lr 0.000225 | ms/batch 82.45178 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    20800 |  20800 batches | lr 0.000224 | ms/batch 81.12384 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    21000 |  21000 batches | lr 0.000224 | ms/batch 79.29517 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    21200 |  21200 batches | lr 0.000223 | ms/batch 79.45794 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    21400 |  21400 batches | lr 0.000223 | ms/batch 80.57265 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    21600 |  21600 batches | lr 0.000222 | ms/batch 79.41765 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    21800 |  21800 batches | lr 0.000222 | ms/batch 79.95999 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    22000 |  22000 batches | lr 0.000221 | ms/batch 82.29152 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    22200 |  22200 batches | lr 0.000221 | ms/batch 81.50737 | loss 2.30266 | ppl    10.001\n",
      "| epoch   1 step    22400 |  22400 batches | lr 0.00022 | ms/batch 80.83641 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    22600 |  22600 batches | lr 0.00022 | ms/batch 81.86805 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    22800 |  22800 batches | lr 0.000219 | ms/batch 80.50797 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    23000 |  23000 batches | lr 0.000219 | ms/batch 78.77052 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    23200 |  23200 batches | lr 0.000218 | ms/batch 78.55848 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    23400 |  23400 batches | lr 0.000218 | ms/batch 78.64478 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    23600 |  23600 batches | lr 0.000217 | ms/batch 79.70843 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    23800 |  23800 batches | lr 0.000217 | ms/batch 79.44008 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    24000 |  24000 batches | lr 0.000216 | ms/batch 79.55768 | loss 2.30262 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3\n",
      "  1  3  3 10  2  5  6  4  2 11 10  6  6  6  7  6  4  7  8  7  2  2 11  3]\n",
      "Target: [ 3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3  1\n",
      "  3  3 10  2  5  6  4  2 11 10  6  6  6  7  6  4  7  8  7  2  2 11  3  2]\n",
      "Teacher forcing: acc:0.10182291666666667\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10182291666666667\n",
      "Preds:  [ 3 11  2  2  7  8  7  4  6  7  6  6  6 10 11  2  4  6  5  2 10  3  3  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   6 at step    24000 | time: 329.77s | valid loss 2.30257 | valid ppl    9.9998 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    24200 |  24200 batches | lr 0.000216 | ms/batch 123.43751 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    24400 |  24400 batches | lr 0.000215 | ms/batch 78.77999 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    24600 |  24600 batches | lr 0.000214 | ms/batch 78.83327 | loss 2.30265 | ppl    10.001\n",
      "| epoch   1 step    24800 |  24800 batches | lr 0.000214 | ms/batch 79.99242 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    25000 |  25000 batches | lr 0.000213 | ms/batch 79.48025 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    25200 |  25200 batches | lr 0.000213 | ms/batch 79.41262 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    25400 |  25400 batches | lr 0.000212 | ms/batch 79.41742 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    25600 |  25600 batches | lr 0.000212 | ms/batch 78.79305 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    25800 |  25800 batches | lr 0.000211 | ms/batch 79.86284 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    26000 |  26000 batches | lr 0.000211 | ms/batch 76.93088 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    26200 |  26200 batches | lr 0.00021 | ms/batch 77.74186 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    26400 |  26400 batches | lr 0.000209 | ms/batch 77.65923 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    26600 |  26600 batches | lr 0.000209 | ms/batch 79.19753 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    26800 |  26800 batches | lr 0.000208 | ms/batch 77.20137 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    27000 |  27000 batches | lr 0.000208 | ms/batch 77.22580 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    27200 |  27200 batches | lr 0.000207 | ms/batch 79.14847 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    27400 |  27400 batches | lr 0.000206 | ms/batch 79.42962 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    27600 |  27600 batches | lr 0.000206 | ms/batch 79.80387 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    27800 |  27800 batches | lr 0.000205 | ms/batch 81.35469 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    28000 |  28000 batches | lr 0.000205 | ms/batch 79.51757 | loss 2.30264 | ppl    10.001\n",
      "maslina\n",
      "|\n",
      "Source: [ 9 10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7\n",
      "  1  7 11  3  6 10 11  5  3  9  2 11  9  7  4 10  6  8  4  4  3  4  9 10]\n",
      "Target: [10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7  1\n",
      "  7 11  3  6 10 11  5  3  9  2 11  9  7  4 10  6  8  4  4  3  4  9 10  9]\n",
      "Teacher forcing: acc:0.0975233843537415\n",
      "Preds:  [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "No teacher forcing: acc:0.0975233843537415\n",
      "Preds:  [10  9  4  3  4  4  8  6 10  4  7  9 11  2  9  3  5 11 10  6  3 11  7  1\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   7 at step    28000 | time: 324.77s | valid loss 2.30258 | valid ppl   10.0000 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    28200 |  28200 batches | lr 0.000204 | ms/batch 122.28331 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    28400 |  28400 batches | lr 0.000203 | ms/batch 77.32981 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    28600 |  28600 batches | lr 0.000203 | ms/batch 78.27964 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    28800 |  28800 batches | lr 0.000202 | ms/batch 80.48076 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    29000 |  29000 batches | lr 0.000202 | ms/batch 80.14170 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    29200 |  29200 batches | lr 0.000201 | ms/batch 78.64661 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    29400 |  29400 batches | lr 0.0002 | ms/batch 80.19853 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    29600 |  29600 batches | lr 0.0002 | ms/batch 79.91031 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    29800 |  29800 batches | lr 0.000199 | ms/batch 80.19686 | loss 2.30264 | ppl    10.001\n",
      "| epoch   1 step    30000 |  30000 batches | lr 0.000198 | ms/batch 78.96221 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    30200 |  30200 batches | lr 0.000198 | ms/batch 79.46669 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    30400 |  30400 batches | lr 0.000197 | ms/batch 80.80179 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    30600 |  30600 batches | lr 0.000197 | ms/batch 81.70584 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    30800 |  30800 batches | lr 0.000196 | ms/batch 82.52143 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    31000 |  31000 batches | lr 0.000195 | ms/batch 81.94853 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    31200 |  31200 batches | lr 0.000195 | ms/batch 83.71577 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    31400 |  31400 batches | lr 0.000194 | ms/batch 84.00488 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    31600 |  31600 batches | lr 0.000193 | ms/batch 87.66706 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    31800 |  31800 batches | lr 0.000193 | ms/batch 79.13148 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    32000 |  32000 batches | lr 0.000192 | ms/batch 78.20338 | loss 2.30262 | ppl    10.000\n",
      "|\n",
      "Source: [ 2  5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8\n",
      "  1  8  2  8  3  3 11  4  4 10  5  2  3  7  3  3  5  7  6  6  8  6  5  5]\n",
      "Target: [ 5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8  1\n",
      "  8  2  8  3  3 11  4  4 10  5  2  3  7  3  3  5  7  6  6  8  6  5  5  2]\n",
      "Teacher forcing: acc:0.101015625\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.101015625\n",
      "Preds:  [ 5  5  6  8  6  6  7  5  3  3  7  3  2  5 10  4  4 11  3  3  8  2  8  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   8 at step    32000 | time: 332.95s | valid loss 2.30266 | valid ppl   10.0007 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    32200 |  32200 batches | lr 0.000191 | ms/batch 130.97756 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    32400 |  32400 batches | lr 0.000191 | ms/batch 78.45993 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    32600 |  32600 batches | lr 0.00019 | ms/batch 80.50013 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    32800 |  32800 batches | lr 0.000189 | ms/batch 79.62842 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    33000 |  33000 batches | lr 0.000189 | ms/batch 79.36490 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    33200 |  33200 batches | lr 0.000188 | ms/batch 78.86214 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    33400 |  33400 batches | lr 0.000187 | ms/batch 78.44648 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    33600 |  33600 batches | lr 0.000187 | ms/batch 77.76099 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    33800 |  33800 batches | lr 0.000186 | ms/batch 79.54967 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    34000 |  34000 batches | lr 0.000185 | ms/batch 79.97833 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    34200 |  34200 batches | lr 0.000185 | ms/batch 78.67355 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    34400 |  34400 batches | lr 0.000184 | ms/batch 77.54943 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    34600 |  34600 batches | lr 0.000183 | ms/batch 77.40946 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    34800 |  34800 batches | lr 0.000182 | ms/batch 78.85564 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    35000 |  35000 batches | lr 0.000182 | ms/batch 78.95364 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    35200 |  35200 batches | lr 0.000181 | ms/batch 78.32572 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    35400 |  35400 batches | lr 0.00018 | ms/batch 79.23590 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    35600 |  35600 batches | lr 0.00018 | ms/batch 78.25112 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    35800 |  35800 batches | lr 0.000179 | ms/batch 78.75858 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    36000 |  36000 batches | lr 0.000178 | ms/batch 78.76559 | loss 2.30262 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 5  3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3\n",
      "  1  3  6  5  9  4  5 11  4  7  7  2  4  3  5  9 11  7  8 11 11 10 10  3]\n",
      "Target: [ 3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3  1\n",
      "  3  6  5  9  4  5 11  4  7  7  2  4  3  5  9 11  7  8 11 11 10 10  3  5]\n",
      "Teacher forcing: acc:0.09941007653061225\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09941007653061225\n",
      "Preds:  [ 3 10 10 11 11  8  7 11  9  5  3  4  2  7  7  4 11  5  4  9  5  6  3  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   9 at step    36000 | time: 323.74s | valid loss 2.30264 | valid ppl   10.0005 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    36200 |  36200 batches | lr 0.000178 | ms/batch 120.45729 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    36400 |  36400 batches | lr 0.000177 | ms/batch 79.24766 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    36600 |  36600 batches | lr 0.000176 | ms/batch 79.59370 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    36800 |  36800 batches | lr 0.000175 | ms/batch 79.45024 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    37000 |  37000 batches | lr 0.000175 | ms/batch 80.54309 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    37200 |  37200 batches | lr 0.000174 | ms/batch 78.97650 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    37400 |  37400 batches | lr 0.000173 | ms/batch 77.71574 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    37600 |  37600 batches | lr 0.000172 | ms/batch 77.72069 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    37800 |  37800 batches | lr 0.000172 | ms/batch 79.58035 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    38000 |  38000 batches | lr 0.000171 | ms/batch 79.82318 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    38200 |  38200 batches | lr 0.00017 | ms/batch 79.20724 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    38400 |  38400 batches | lr 0.00017 | ms/batch 79.35624 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    38600 |  38600 batches | lr 0.000169 | ms/batch 78.73891 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    38800 |  38800 batches | lr 0.000168 | ms/batch 78.56695 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    39000 |  39000 batches | lr 0.000167 | ms/batch 78.13605 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    39200 |  39200 batches | lr 0.000167 | ms/batch 77.78023 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    39400 |  39400 batches | lr 0.000166 | ms/batch 78.24588 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    39600 |  39600 batches | lr 0.000165 | ms/batch 77.76148 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    39800 |  39800 batches | lr 0.000164 | ms/batch 78.97500 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    40000 |  40000 batches | lr 0.000164 | ms/batch 79.86508 | loss 2.30261 | ppl    10.000\n",
      "|\n",
      "Source: [ 4 11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2\n",
      "  1  2  3  8  7  8 10  7 11  7  2  5  6 10  8  2 11  9  3  8 11  7  9 11]\n",
      "Target: [11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2  1\n",
      "  2  3  8  7  8 10  7 11  7  2  5  6 10  8  2 11  9  3  8 11  7  9 11  4]\n",
      "Teacher forcing: acc:0.09830729166666667\n",
      "Preds:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9]\n",
      "\n",
      "No teacher forcing: acc:0.09830729166666667\n",
      "Preds:  [11  9  7 11  8  3  9 11  2  8 10  6  5  2  7 11  7 10  8  7  8  3  2  1\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  10 at step    40000 | time: 326.39s | valid loss 2.30262 | valid ppl   10.0003 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    40200 |  40200 batches | lr 0.000163 | ms/batch 136.21455 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    40400 |  40400 batches | lr 0.000162 | ms/batch 79.59609 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    40600 |  40600 batches | lr 0.000161 | ms/batch 79.03058 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    40800 |  40800 batches | lr 0.000161 | ms/batch 79.18024 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    41000 |  41000 batches | lr 0.00016 | ms/batch 77.83293 | loss 2.30263 | ppl    10.000\n",
      "| epoch   1 step    41200 |  41200 batches | lr 0.000159 | ms/batch 79.20338 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    41400 |  41400 batches | lr 0.000158 | ms/batch 78.72326 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    41600 |  41600 batches | lr 0.000158 | ms/batch 79.72599 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    41800 |  41800 batches | lr 0.000157 | ms/batch 78.55251 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    42000 |  42000 batches | lr 0.000156 | ms/batch 79.00943 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    42200 |  42200 batches | lr 0.000155 | ms/batch 79.21732 | loss 2.30262 | ppl    10.000\n",
      "| epoch   1 step    42400 |  42400 batches | lr 0.000155 | ms/batch 79.54940 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    42600 |  42600 batches | lr 0.000154 | ms/batch 79.99470 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    42800 |  42800 batches | lr 0.000153 | ms/batch 78.45217 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    43000 |  43000 batches | lr 0.000152 | ms/batch 78.72912 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    43200 |  43200 batches | lr 0.000152 | ms/batch 78.76436 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    43400 |  43400 batches | lr 0.000151 | ms/batch 78.26147 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    43600 |  43600 batches | lr 0.00015 | ms/batch 77.76677 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    43800 |  43800 batches | lr 0.000149 | ms/batch 79.02915 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    44000 |  44000 batches | lr 0.000148 | ms/batch 82.08204 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [ 8  3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9\n",
      "  1  9  7  9 10  3  7  9  6  4  4  4  8  8  4  5  9  7  8  6  5  8  6  3]\n",
      "Target: [ 3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9  1\n",
      "  9  7  9 10  3  7  9  6  4  4  4  8  8  4  5  9  7  8  6  5  8  6  3  8]\n",
      "Teacher forcing: acc:0.098671875\n",
      "Preds:  [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "No teacher forcing: acc:0.098671875\n",
      "Preds:  [ 3  6  8  5  6  8  7  9  5  4  8  8  4  4  4  6  9  7  3 10  9  7  9  1\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  11 at step    44000 | time: 325.73s | valid loss 2.30263 | valid ppl   10.0005 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    44200 |  44200 batches | lr 0.000148 | ms/batch 124.96807 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    44400 |  44400 batches | lr 0.000147 | ms/batch 80.84941 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    44600 |  44600 batches | lr 0.000146 | ms/batch 79.40963 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    44800 |  44800 batches | lr 0.000145 | ms/batch 78.31076 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    45000 |  45000 batches | lr 0.000145 | ms/batch 79.24590 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    45200 |  45200 batches | lr 0.000144 | ms/batch 80.09414 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    45400 |  45400 batches | lr 0.000143 | ms/batch 78.86590 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    45600 |  45600 batches | lr 0.000142 | ms/batch 78.46247 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    45800 |  45800 batches | lr 0.000141 | ms/batch 77.53002 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    46000 |  46000 batches | lr 0.000141 | ms/batch 79.11053 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    46200 |  46200 batches | lr 0.00014 | ms/batch 79.19628 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    46400 |  46400 batches | lr 0.000139 | ms/batch 78.54704 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    46600 |  46600 batches | lr 0.000138 | ms/batch 83.36137 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    46800 |  46800 batches | lr 0.000138 | ms/batch 84.19798 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    47000 |  47000 batches | lr 0.000137 | ms/batch 83.25118 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    47200 |  47200 batches | lr 0.000136 | ms/batch 84.78106 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    47400 |  47400 batches | lr 0.000135 | ms/batch 82.79104 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    47600 |  47600 batches | lr 0.000134 | ms/batch 84.39496 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    47800 |  47800 batches | lr 0.000134 | ms/batch 83.38443 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    48000 |  48000 batches | lr 0.000133 | ms/batch 81.83475 | loss 2.30260 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [11 10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4\n",
      "  1  4 10  4  4  3 11  9  7  8  9  7 10  7  8  3  3  6 10 11  7  7  5 10]\n",
      "Target: [10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4  1\n",
      "  4 10  4  4  3 11  9  7  8  9  7 10  7  8  3  3  6 10 11  7  7  5 10 11]\n",
      "Teacher forcing: acc:0.10196109693877552\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10196109693877552\n",
      "Preds:  [10  5  7  7 11 10  6  3  3  8  7 10  7  9  8  7  9 11  3  4  4 10  4  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  12 at step    48000 | time: 332.59s | valid loss 2.30260 | valid ppl   10.0001 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    48200 |  48200 batches | lr 0.000132 | ms/batch 127.53304 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    48400 |  48400 batches | lr 0.000131 | ms/batch 80.85706 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    48600 |  48600 batches | lr 0.00013 | ms/batch 80.40431 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    48800 |  48800 batches | lr 0.00013 | ms/batch 80.27839 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    49000 |  49000 batches | lr 0.000129 | ms/batch 78.73704 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    49200 |  49200 batches | lr 0.000128 | ms/batch 79.92017 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    49400 |  49400 batches | lr 0.000127 | ms/batch 80.49272 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    49600 |  49600 batches | lr 0.000127 | ms/batch 80.75609 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    49800 |  49800 batches | lr 0.000126 | ms/batch 80.93659 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    50000 |  50000 batches | lr 0.000125 | ms/batch 79.91601 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    50200 |  50200 batches | lr 0.000124 | ms/batch 80.35058 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step    50400 |  50400 batches | lr 0.000123 | ms/batch 81.33337 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    50600 |  50600 batches | lr 0.000123 | ms/batch 81.89373 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    50800 |  50800 batches | lr 0.000122 | ms/batch 79.23577 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    51000 |  51000 batches | lr 0.000121 | ms/batch 81.38277 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    51200 |  51200 batches | lr 0.00012 | ms/batch 80.80936 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    51400 |  51400 batches | lr 0.00012 | ms/batch 79.17244 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    51600 |  51600 batches | lr 0.000119 | ms/batch 81.00124 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    51800 |  51800 batches | lr 0.000118 | ms/batch 82.16324 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    52000 |  52000 batches | lr 0.000117 | ms/batch 79.74028 | loss 2.30260 | ppl    10.000\n",
      "|\n",
      "Source: [ 4  7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10\n",
      "  1 10 10  7  6  8 10  9  4  6 11 11 11  9  2  4  2  6 10 11  8  6  3  7]\n",
      "Target: [ 7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10  1\n",
      " 10 10  7  6  8 10  9  4  6 11 11 11  9  2  4  2  6 10 11  8  6  3  7  4]\n",
      "Teacher forcing: acc:0.103203125\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.103203125\n",
      "Preds:  [ 7  3  6  8 11 10  6  2  4  2  9 11 11 11  6  4  9 10  8  6  7 10 10  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  13 at step    52000 | time: 331.29s | valid loss 2.30255 | valid ppl    9.9997 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    52200 |  52200 batches | lr 0.000116 | ms/batch 124.49888 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    52400 |  52400 batches | lr 0.000116 | ms/batch 81.11075 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    52600 |  52600 batches | lr 0.000115 | ms/batch 79.66585 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    52800 |  52800 batches | lr 0.000114 | ms/batch 79.73779 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    53000 |  53000 batches | lr 0.000113 | ms/batch 81.36987 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    53200 |  53200 batches | lr 0.000112 | ms/batch 79.32876 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    53400 |  53400 batches | lr 0.000112 | ms/batch 77.93870 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    53600 |  53600 batches | lr 0.000111 | ms/batch 79.23075 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    53800 |  53800 batches | lr 0.00011 | ms/batch 78.74743 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    54000 |  54000 batches | lr 0.000109 | ms/batch 78.55517 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    54200 |  54200 batches | lr 0.000109 | ms/batch 77.95527 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    54400 |  54400 batches | lr 0.000108 | ms/batch 78.73972 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    54600 |  54600 batches | lr 0.000107 | ms/batch 79.65574 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    54800 |  54800 batches | lr 0.000106 | ms/batch 79.15312 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    55000 |  55000 batches | lr 0.000105 | ms/batch 78.40885 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    55200 |  55200 batches | lr 0.000105 | ms/batch 77.60487 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    55400 |  55400 batches | lr 0.000104 | ms/batch 77.38396 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    55600 |  55600 batches | lr 0.000103 | ms/batch 78.97342 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    55800 |  55800 batches | lr 0.000102 | ms/batch 79.80518 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    56000 |  56000 batches | lr 0.000102 | ms/batch 80.24259 | loss 2.30257 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 7  7  6  9  6  4  3  2  8  5  3  8  7  7  4  8  5 11  2  3  8  4  9 11\n",
      "  1 11  9  4  8  3  2 11  5  8  4  7  7  8  3  5  8  2  3  4  6  9  6  7]\n",
      "Target: [ 7  6  9  6  4  3  2  8  5  3  8  7  7  4  8  5 11  2  3  8  4  9 11  1\n",
      " 11  9  4  8  3  2 11  5  8  4  7  7  8  3  5  8  2  3  4  6  9  6  7  7]\n",
      "Teacher forcing: acc:0.09837372448979592\n",
      "Preds:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9]\n",
      "\n",
      "No teacher forcing: acc:0.09837372448979592\n",
      "Preds:  [ 7  6  9  6  4  3  2  8  5  3  8  7  7  4  8  5 11  2  3  8  4  9 11  1\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  14 at step    56000 | time: 325.66s | valid loss 2.30259 | valid ppl   10.0001 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    56200 |  56200 batches | lr 0.000101 | ms/batch 127.39277 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    56400 |  56400 batches | lr 0.0001 | ms/batch 81.81375 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    56600 |  56600 batches | lr 9.93e-05 | ms/batch 81.66804 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    56800 |  56800 batches | lr 9.85e-05 | ms/batch 82.90344 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    57000 |  57000 batches | lr 9.77e-05 | ms/batch 84.12019 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    57200 |  57200 batches | lr 9.7e-05 | ms/batch 82.79266 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    57400 |  57400 batches | lr 9.62e-05 | ms/batch 81.79470 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    57600 |  57600 batches | lr 9.54e-05 | ms/batch 81.42994 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    57800 |  57800 batches | lr 9.47e-05 | ms/batch 80.35483 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    58000 |  58000 batches | lr 9.39e-05 | ms/batch 79.94809 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    58200 |  58200 batches | lr 9.32e-05 | ms/batch 81.39392 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    58400 |  58400 batches | lr 9.24e-05 | ms/batch 81.91552 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    58600 |  58600 batches | lr 9.16e-05 | ms/batch 82.74391 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    58800 |  58800 batches | lr 9.09e-05 | ms/batch 81.61336 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    59000 |  59000 batches | lr 9.01e-05 | ms/batch 78.79681 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    59200 |  59200 batches | lr 8.94e-05 | ms/batch 82.14852 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    59400 |  59400 batches | lr 8.86e-05 | ms/batch 78.38551 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    59600 |  59600 batches | lr 8.79e-05 | ms/batch 79.77702 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step    59800 |  59800 batches | lr 8.71e-05 | ms/batch 80.64309 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    60000 |  60000 batches | lr 8.64e-05 | ms/batch 79.90108 | loss 2.30258 | ppl    10.000\n",
      "|\n",
      "Source: [11  8  3  2  3  9 10  5  9 10  6 10 11  9  6 11  2  9  7  5  7  5  3 11\n",
      "  1 11  3  5  7  5  7  9  2 11  6  9 11 10  6 10  9  5 10  9  3  2  3  8]\n",
      "Target: [ 8  3  2  3  9 10  5  9 10  6 10 11  9  6 11  2  9  7  5  7  5  3 11  1\n",
      " 11  3  5  7  5  7  9  2 11  6  9 11 10  6 10  9  5 10  9  3  2  3  8 11]\n",
      "Teacher forcing: acc:0.09791666666666667\n",
      "Preds:  [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7]\n",
      "\n",
      "No teacher forcing: acc:0.09791666666666667\n",
      "Preds:  [ 8  3  2  3  9 10  5  9 10  6 10 11  9  6 11  2  9  7  5  7  5  3 11  1\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  15 at step    60000 | time: 336.28s | valid loss 2.30264 | valid ppl   10.0005 | valid acc 0.098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    60200 |  60200 batches | lr 8.56e-05 | ms/batch 135.37318 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    60400 |  60400 batches | lr 8.49e-05 | ms/batch 78.85402 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    60600 |  60600 batches | lr 8.41e-05 | ms/batch 79.35967 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    60800 |  60800 batches | lr 8.34e-05 | ms/batch 79.56891 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    61000 |  61000 batches | lr 8.27e-05 | ms/batch 82.57083 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    61200 |  61200 batches | lr 8.19e-05 | ms/batch 82.40951 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    61400 |  61400 batches | lr 8.12e-05 | ms/batch 79.80954 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    61600 |  61600 batches | lr 8.04e-05 | ms/batch 80.82134 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    61800 |  61800 batches | lr 7.97e-05 | ms/batch 80.55365 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    62000 |  62000 batches | lr 7.9e-05 | ms/batch 79.38400 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    62200 |  62200 batches | lr 7.83e-05 | ms/batch 78.88546 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    62400 |  62400 batches | lr 7.75e-05 | ms/batch 80.02457 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    62600 |  62600 batches | lr 7.68e-05 | ms/batch 79.27808 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    62800 |  62800 batches | lr 7.61e-05 | ms/batch 82.59795 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    63000 |  63000 batches | lr 7.54e-05 | ms/batch 80.54687 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    63200 |  63200 batches | lr 7.46e-05 | ms/batch 79.64186 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    63400 |  63400 batches | lr 7.39e-05 | ms/batch 78.73818 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    63600 |  63600 batches | lr 7.32e-05 | ms/batch 79.88991 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    63800 |  63800 batches | lr 7.25e-05 | ms/batch 81.67922 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    64000 |  64000 batches | lr 7.18e-05 | ms/batch 80.39324 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 8  6  9  4 11  4  3  8  4 11 10  6  2  6  6  5  4  4  2 11  4  8  5  3\n",
      "  1  3  5  8  4 11  2  4  4  5  6  6  2  6 10 11  4  8  3  4 11  4  9  6]\n",
      "Target: [ 6  9  4 11  4  3  8  4 11 10  6  2  6  6  5  4  4  2 11  4  8  5  3  1\n",
      "  3  5  8  4 11  2  4  4  5  6  6  2  6 10 11  4  8  3  4 11  4  9  6  8]\n",
      "Teacher forcing: acc:0.10257227891156463\n",
      "Preds:  [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6]\n",
      "\n",
      "No teacher forcing: acc:0.10257227891156463\n",
      "Preds:  [ 6  9  4 11  4  3  8  4 11 10  6  2  6  6  5  4  4  2 11  4  8  5  3  1\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  16 at step    64000 | time: 330.01s | valid loss 2.30256 | valid ppl    9.9998 | valid acc 0.103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    64200 |  64200 batches | lr 7.11e-05 | ms/batch 123.63227 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    64400 |  64400 batches | lr 7.04e-05 | ms/batch 78.86342 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    64600 |  64600 batches | lr 6.97e-05 | ms/batch 78.52920 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step    64800 |  64800 batches | lr 6.9e-05 | ms/batch 80.34789 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    65000 |  65000 batches | lr 6.83e-05 | ms/batch 80.01918 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    65200 |  65200 batches | lr 6.76e-05 | ms/batch 79.32179 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    65400 |  65400 batches | lr 6.69e-05 | ms/batch 79.43820 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    65600 |  65600 batches | lr 6.62e-05 | ms/batch 78.07499 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    65800 |  65800 batches | lr 6.55e-05 | ms/batch 80.62817 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    66000 |  66000 batches | lr 6.48e-05 | ms/batch 79.64553 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    66200 |  66200 batches | lr 6.41e-05 | ms/batch 80.09971 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    66400 |  66400 batches | lr 6.34e-05 | ms/batch 78.86528 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    66600 |  66600 batches | lr 6.27e-05 | ms/batch 80.00235 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    66800 |  66800 batches | lr 6.2e-05 | ms/batch 80.18157 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    67000 |  67000 batches | lr 6.14e-05 | ms/batch 78.06759 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    67200 |  67200 batches | lr 6.07e-05 | ms/batch 78.08503 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    67400 |  67400 batches | lr 6e-05 | ms/batch 78.83035 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    67600 |  67600 batches | lr 5.94e-05 | ms/batch 77.64636 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    67800 |  67800 batches | lr 5.87e-05 | ms/batch 78.33533 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    68000 |  68000 batches | lr 5.8e-05 | ms/batch 80.10521 | loss 2.30259 | ppl    10.000\n",
      "|\n",
      "Source: [10 10  2 11  7  2  7 10  5 10  3  7  2  9 10 11  5  4  5 10  5  8 10  6\n",
      "  1  6 10  8  5 10  5  4  5 11 10  9  2  7  3 10  5 10  7  2  7 11  2 10]\n",
      "Target: [10  2 11  7  2  7 10  5 10  3  7  2  9 10 11  5  4  5 10  5  8 10  6  1\n",
      "  6 10  8  5 10  5  4  5 11 10  9  2  7  3 10  5 10  7  2  7 11  2 10 10]\n",
      "Teacher forcing: acc:0.101015625\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 8 8 8 8 8 8 8 8 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.101015625\n",
      "Preds:  [10  2 11  7  2  7 10  5 10  3  7  2  9 10 11  5  4  5 10  5  8 10  6  1\n",
      "  8  8  8  8  8  8  8  8  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  17 at step    68000 | time: 325.39s | valid loss 2.30256 | valid ppl    9.9998 | valid acc 0.101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    68200 |  68200 batches | lr 5.74e-05 | ms/batch 122.38928 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    68400 |  68400 batches | lr 5.67e-05 | ms/batch 79.91697 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    68600 |  68600 batches | lr 5.6e-05 | ms/batch 84.54778 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    68800 |  68800 batches | lr 5.54e-05 | ms/batch 78.30137 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    69000 |  69000 batches | lr 5.47e-05 | ms/batch 80.74397 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    69200 |  69200 batches | lr 5.41e-05 | ms/batch 80.28744 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    69400 |  69400 batches | lr 5.34e-05 | ms/batch 81.68147 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    69600 |  69600 batches | lr 5.28e-05 | ms/batch 80.84827 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    69800 |  69800 batches | lr 5.22e-05 | ms/batch 80.28017 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    70000 |  70000 batches | lr 5.15e-05 | ms/batch 80.50870 | loss 2.30261 | ppl    10.000\n",
      "| epoch   1 step    70200 |  70200 batches | lr 5.09e-05 | ms/batch 78.08058 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    70400 |  70400 batches | lr 5.03e-05 | ms/batch 78.99670 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    70600 |  70600 batches | lr 4.96e-05 | ms/batch 78.03570 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    70800 |  70800 batches | lr 4.9e-05 | ms/batch 79.93234 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    71000 |  71000 batches | lr 4.84e-05 | ms/batch 79.19506 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    71200 |  71200 batches | lr 4.78e-05 | ms/batch 80.28888 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    71400 |  71400 batches | lr 4.72e-05 | ms/batch 79.83624 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    71600 |  71600 batches | lr 4.65e-05 | ms/batch 79.77947 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    71800 |  71800 batches | lr 4.59e-05 | ms/batch 78.10742 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    72000 |  72000 batches | lr 4.53e-05 | ms/batch 79.35134 | loss 2.30257 | ppl    10.000\n",
      "|\n",
      "Source: [ 6 11  9 11  9  2  3  9 10  6 10  5  7  3  8  8  2  6 11 11 10 11  4  6\n",
      "  1  6  4 11 10 11 11  6  2  8  8  3  7  5 10  6 10  9  3  2  9 11  9 11]\n",
      "Target: [11  9 11  9  2  3  9 10  6 10  5  7  3  8  8  2  6 11 11 10 11  4  6  1\n",
      "  6  4 11 10 11 11  6  2  8  8  3  7  5 10  6 10  9  3  2  9 11  9 11  6]\n",
      "Teacher forcing: acc:0.09942708333333333\n",
      "Preds:  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3]\n",
      "\n",
      "No teacher forcing: acc:0.09942708333333333\n",
      "Preds:  [11  9 11  9  2  3  9 10  6 10  5  7  3  8  8  2  6 11 11 10 11  4  6  1\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  18 at step    72000 | time: 328.58s | valid loss 2.30261 | valid ppl   10.0002 | valid acc 0.099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    72200 |  72200 batches | lr 4.47e-05 | ms/batch 122.59060 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    72400 |  72400 batches | lr 4.41e-05 | ms/batch 79.46592 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    72600 |  72600 batches | lr 4.35e-05 | ms/batch 79.73478 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    72800 |  72800 batches | lr 4.29e-05 | ms/batch 79.45573 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    73000 |  73000 batches | lr 4.23e-05 | ms/batch 82.09143 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    73200 |  73200 batches | lr 4.17e-05 | ms/batch 78.94409 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    73400 |  73400 batches | lr 4.12e-05 | ms/batch 82.00033 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    73600 |  73600 batches | lr 4.06e-05 | ms/batch 80.45589 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    73800 |  73800 batches | lr 4e-05 | ms/batch 77.92396 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    74000 |  74000 batches | lr 3.94e-05 | ms/batch 78.83196 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    74200 |  74200 batches | lr 3.89e-05 | ms/batch 80.29774 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    74400 |  74400 batches | lr 3.83e-05 | ms/batch 80.92235 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    74600 |  74600 batches | lr 3.77e-05 | ms/batch 79.74113 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    74800 |  74800 batches | lr 3.72e-05 | ms/batch 79.48398 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step    75000 |  75000 batches | lr 3.66e-05 | ms/batch 77.63689 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    75200 |  75200 batches | lr 3.61e-05 | ms/batch 78.35720 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step    75400 |  75400 batches | lr 3.55e-05 | ms/batch 78.05737 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    75600 |  75600 batches | lr 3.5e-05 | ms/batch 75.56521 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    75800 |  75800 batches | lr 3.44e-05 | ms/batch 76.02964 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    76000 |  76000 batches | lr 3.39e-05 | ms/batch 81.51219 | loss 2.30259 | ppl    10.000\n",
      "maslina\n",
      "|\n",
      "Source: [ 8  2 10  3  4  3 10  8  8 11 10  6  3 11  9  8  9  3  4  7 11  5  4  6\n",
      "  1  6  4  5 11  7  4  3  9  8  9 11  3  6 10 11  8  8 10  3  4  3 10  2]\n",
      "Target: [ 2 10  3  4  3 10  8  8 11 10  6  3 11  9  8  9  3  4  7 11  5  4  6  1\n",
      "  6  4  5 11  7  4  3  9  8  9 11  3  6 10 11  8  8 10  3  4  3 10  2  8]\n",
      "Teacher forcing: acc:0.1015359268707483\n",
      "Preds:  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4]\n",
      "\n",
      "No teacher forcing: acc:0.1015359268707483\n",
      "Preds:  [ 2 10  3  4  3 10  8  8 11 10  6  3 11  9  8  9  3  4  7 11  5  4  6  1\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval  19 at step    76000 | time: 327.58s | valid loss 2.30257 | valid ppl    9.9998 | valid acc 0.102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step    76200 |  76200 batches | lr 3.33e-05 | ms/batch 133.91438 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    76400 |  76400 batches | lr 3.28e-05 | ms/batch 77.44234 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    76600 |  76600 batches | lr 3.23e-05 | ms/batch 75.94038 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    76800 |  76800 batches | lr 3.18e-05 | ms/batch 78.39857 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    77000 |  77000 batches | lr 3.12e-05 | ms/batch 76.64702 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    77200 |  77200 batches | lr 3.07e-05 | ms/batch 77.41487 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    77400 |  77400 batches | lr 3.02e-05 | ms/batch 81.09875 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    77600 |  77600 batches | lr 2.97e-05 | ms/batch 76.80863 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    77800 |  77800 batches | lr 2.92e-05 | ms/batch 75.64153 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    78000 |  78000 batches | lr 2.87e-05 | ms/batch 78.65867 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    78200 |  78200 batches | lr 2.82e-05 | ms/batch 77.05258 | loss 2.30257 | ppl    10.000\n",
      "| epoch   1 step    78400 |  78400 batches | lr 2.77e-05 | ms/batch 76.09848 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    78600 |  78600 batches | lr 2.72e-05 | ms/batch 81.40035 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    78800 |  78800 batches | lr 2.67e-05 | ms/batch 80.53361 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    79000 |  79000 batches | lr 2.62e-05 | ms/batch 80.89810 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    79200 |  79200 batches | lr 2.58e-05 | ms/batch 81.85778 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    79400 |  79400 batches | lr 2.53e-05 | ms/batch 85.14575 | loss 2.30260 | ppl    10.000\n",
      "| epoch   1 step    79600 |  79600 batches | lr 2.48e-05 | ms/batch 82.21174 | loss 2.30258 | ppl    10.000\n",
      "| epoch   1 step    79800 |  79800 batches | lr 2.43e-05 | ms/batch 83.58645 | loss 2.30259 | ppl    10.000\n",
      "| epoch   1 step    80000 |  80000 batches | lr 2.39e-05 | ms/batch 82.25547 | loss 2.30258 | ppl    10.000\n"
     ]
    }
   ],
   "source": [
    "!bash run_reverse.sh train --work_dir ../evaluation/tl8xl0mt0 --tgt_len 8 --eval_tgt_len 8 --mem_len 0 --num_mem_tokens 0 --device_ids 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f55a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash run_reverse.sh train --work_dir ../evaluation/tl8xl12mt0 --tgt_len 8 --eval_tgt_len 8 --mem_len 12 --num_mem_tokens 0 --device_ids 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215edd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash run_reverse.sh train --work_dir ../evaluation/tl8xl0mt12 --tgt_len 8 --eval_tgt_len 8 --mem_len 0 --num_mem_tokens 12 --device_ids 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7363d0d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[0, 1]\n",
      "Experiment dir : ../reverse/debug-reverse/20211229-144300\n",
      "====================================================================================================\n",
      "    - data : /home/admin/x-transformers/data24\n",
      "    - dataset : reverse\n",
      "    - n_layer : 4\n",
      "    - n_head : 4\n",
      "    - d_head : 64\n",
      "    - d_embed : 128\n",
      "    - d_model : 128\n",
      "    - d_inner : 256\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.00025\n",
      "    - mom : 0.0\n",
      "    - scheduler : cosine\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 0.0\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 150000\n",
      "    - batch_size : 32\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 24\n",
      "    - eval_tgt_len : 24\n",
      "    - ext_len : 0\n",
      "    - mem_len : 0\n",
      "    - num_mem_tokens : 24\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : False\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : ../reverse/debug-reverse/20211229-144300\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : 50\n",
      "    - sample_softmax : -1\n",
      "    - patience : 0\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [0, 1]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : True\n",
      "    - read_mem_from_cache : True\n",
      "    - answer_size : 24\n",
      "    - tied : True\n",
      "    - ntokens : 12\n",
      "    - n_all_param : 926220\n",
      "    - n_nonemb_param : 921088\n",
      "====================================================================================================\n",
      "#params = 926220\n",
      "#non emb params = 921088\n",
      "/home/admin/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 94.03120 | loss 2.32838 | ppl    10.261\n",
      "| epoch   1 step      400 |    400 batches | lr 0.00025 | ms/batch 77.12999 | loss 2.30646 | ppl    10.039\n",
      "| epoch   1 step      600 |    600 batches | lr 0.00025 | ms/batch 76.53622 | loss 2.30560 | ppl    10.030\n",
      "| epoch   1 step      800 |    800 batches | lr 0.00025 | ms/batch 76.38231 | loss 2.30490 | ppl    10.023\n",
      "| epoch   1 step     1000 |   1000 batches | lr 0.00025 | ms/batch 76.13207 | loss 2.30443 | ppl    10.019\n",
      "| epoch   1 step     1200 |   1200 batches | lr 0.00025 | ms/batch 76.84865 | loss 2.30411 | ppl    10.015\n",
      "| epoch   1 step     1400 |   1400 batches | lr 0.00025 | ms/batch 76.48667 | loss 2.30376 | ppl    10.012\n",
      "| epoch   1 step     1600 |   1600 batches | lr 0.00025 | ms/batch 77.27463 | loss 2.30356 | ppl    10.010\n",
      "| epoch   1 step     1800 |   1800 batches | lr 0.00025 | ms/batch 76.99379 | loss 2.30366 | ppl    10.011\n",
      "| epoch   1 step     2000 |   2000 batches | lr 0.00025 | ms/batch 76.70399 | loss 2.30356 | ppl    10.010\n",
      "| epoch   1 step     2200 |   2200 batches | lr 0.00025 | ms/batch 75.57015 | loss 2.30324 | ppl    10.007\n",
      "| epoch   1 step     2400 |   2400 batches | lr 0.00025 | ms/batch 76.52216 | loss 2.30324 | ppl    10.007\n",
      "| epoch   1 step     2600 |   2600 batches | lr 0.00025 | ms/batch 75.85119 | loss 2.30322 | ppl    10.006\n",
      "| epoch   1 step     2800 |   2800 batches | lr 0.00025 | ms/batch 77.08647 | loss 2.30324 | ppl    10.007\n",
      "| epoch   1 step     3000 |   3000 batches | lr 0.00025 | ms/batch 77.39906 | loss 2.30308 | ppl    10.005\n",
      "| epoch   1 step     3200 |   3200 batches | lr 0.00025 | ms/batch 76.97128 | loss 2.28513 | ppl     9.827\n",
      "| epoch   1 step     3400 |   3400 batches | lr 0.00025 | ms/batch 77.22969 | loss 2.25637 | ppl     9.548\n",
      "| epoch   1 step     3600 |   3600 batches | lr 0.00025 | ms/batch 77.59959 | loss 2.21332 | ppl     9.146\n",
      "| epoch   1 step     3800 |   3800 batches | lr 0.00025 | ms/batch 76.91777 | loss 2.16516 | ppl     8.716\n",
      "| epoch   1 step     4000 |   4000 batches | lr 0.00025 | ms/batch 78.31947 | loss 2.11481 | ppl     8.288\n",
      "|\n",
      "Source: [ 4  9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10\n",
      "  1 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9]\n",
      "Target: [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      " 10  4  4  6 11  4  8  7  9  6  5 10  3  8  9  2  5  3  4 11  4  9  9  4]\n",
      "Teacher forcing: acc:0.16028645833333333\n",
      "Preds:  [ 7  7  7  3  3  3  3  5  7  7  7  7  7  8  6  8  2  8  8  8  7  7  7  7\n",
      "  8  8  8  8  8 11  7  5  7  7  7  5  7  2  7  7  7  9  2  8  6  6  9  9]\n",
      "\n",
      "No teacher forcing: acc:0.12432291666666667\n",
      "Preds:  [ 9  9  4 11  4  3  5  2  9  8  3 10  5  6  9  7  8  4 11  6  4  4 10  1\n",
      "  8  8  8  8  8  8  7  2  7  2  2  7  2  4  7  2  2  6  6  7 10 10 10  7]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step     4000 | time: 321.82s | valid loss 2.06949 | valid ppl    7.9208 | valid acc 0.124\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| epoch   1 step     4200 |   4200 batches | lr 0.00025 | ms/batch 133.99891 | loss 2.08668 | ppl     8.058\n",
      "| epoch   1 step     4400 |   4400 batches | lr 0.000249 | ms/batch 77.51574 | loss 2.06605 | ppl     7.894\n",
      "| epoch   1 step     4600 |   4600 batches | lr 0.000249 | ms/batch 77.19888 | loss 2.04305 | ppl     7.714\n",
      "| epoch   1 step     4800 |   4800 batches | lr 0.000249 | ms/batch 76.34016 | loss 2.02303 | ppl     7.561\n",
      "| epoch   1 step     5000 |   5000 batches | lr 0.000249 | ms/batch 77.83952 | loss 2.00448 | ppl     7.422\n",
      "| epoch   1 step     5200 |   5200 batches | lr 0.000249 | ms/batch 77.86907 | loss 1.98575 | ppl     7.284\n",
      "| epoch   1 step     5400 |   5400 batches | lr 0.000249 | ms/batch 76.57243 | loss 1.96158 | ppl     7.111\n",
      "| epoch   1 step     5600 |   5600 batches | lr 0.000249 | ms/batch 77.10822 | loss 1.94517 | ppl     6.995\n",
      "| epoch   1 step     5800 |   5800 batches | lr 0.000249 | ms/batch 78.24240 | loss 1.92959 | ppl     6.887\n",
      "| epoch   1 step     6000 |   6000 batches | lr 0.000249 | ms/batch 76.97701 | loss 1.91526 | ppl     6.789\n",
      "^C\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "!bash run_reverse-debug.sh train --device_ids 0 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcdca97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training...\n",
      "[0, 1]\n",
      "Experiment dir : ../reverse/tl24_xl12_2912-reverse/20211229-140431\n",
      "====================================================================================================\n",
      "    - data : /home/admin/x-transformers/data24\n",
      "    - dataset : reverse\n",
      "    - n_layer : 4\n",
      "    - n_head : 4\n",
      "    - d_head : 64\n",
      "    - d_embed : 128\n",
      "    - d_model : 128\n",
      "    - d_inner : 256\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : adam\n",
      "    - lr : 0.00025\n",
      "    - mom : 0.0\n",
      "    - scheduler : cosine\n",
      "    - warmup_step : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 0.0\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_step : 120000\n",
      "    - batch_size : 32\n",
      "    - batch_chunk : 1\n",
      "    - tgt_len : 24\n",
      "    - eval_tgt_len : 24\n",
      "    - ext_len : 0\n",
      "    - mem_len : 12\n",
      "    - num_mem_tokens : 0\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - cuda : True\n",
      "    - adaptive : False\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - varlen : False\n",
      "    - multi_gpu : True\n",
      "    - log_interval : 200\n",
      "    - eval_interval : 4000\n",
      "    - work_dir : ../reverse/tl24_xl12_2912-reverse/20211229-140431\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : 50\n",
      "    - sample_softmax : -1\n",
      "    - patience : 0\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1\n",
      "    - dynamic_loss_scale : False\n",
      "    - device_ids : [0, 1]\n",
      "    - mem_backprop_depth : 0\n",
      "    - bptt_bp : False\n",
      "    - mem_at_end : True\n",
      "    - read_mem_from_cache : True\n",
      "    - answer_size : 24\n",
      "    - tied : True\n",
      "    - ntokens : 12\n",
      "    - n_all_param : 923148\n",
      "    - n_nonemb_param : 921088\n",
      "====================================================================================================\n",
      "#params = 923148\n",
      "#non emb params = 921088\n",
      "/home/admin/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 92.36856 | loss 2.33350 | ppl    10.314\n",
      "| epoch   1 step      400 |    400 batches | lr 0.00025 | ms/batch 76.54996 | loss 2.19138 | ppl     8.948\n",
      "| epoch   1 step      600 |    600 batches | lr 0.00025 | ms/batch 77.81847 | loss 1.97095 | ppl     7.177\n",
      "| epoch   1 step      800 |    800 batches | lr 0.00025 | ms/batch 74.62629 | loss 1.87090 | ppl     6.494\n",
      "| epoch   1 step     1000 |   1000 batches | lr 0.00025 | ms/batch 76.02644 | loss 1.83002 | ppl     6.234\n",
      "| epoch   1 step     1200 |   1200 batches | lr 0.00025 | ms/batch 78.14714 | loss 1.80880 | ppl     6.103\n",
      "| epoch   1 step     1400 |   1400 batches | lr 0.00025 | ms/batch 75.61209 | loss 1.78275 | ppl     5.946\n",
      "| epoch   1 step     1600 |   1600 batches | lr 0.00025 | ms/batch 76.48042 | loss 1.76888 | ppl     5.864\n",
      "| epoch   1 step     1800 |   1800 batches | lr 0.00025 | ms/batch 76.08393 | loss 1.75713 | ppl     5.796\n",
      "| epoch   1 step     2000 |   2000 batches | lr 0.00025 | ms/batch 76.15026 | loss 1.74777 | ppl     5.742\n",
      "| epoch   1 step     2200 |   2200 batches | lr 0.00025 | ms/batch 76.08583 | loss 1.73691 | ppl     5.680\n",
      "| epoch   1 step     2400 |   2400 batches | lr 0.00025 | ms/batch 76.76009 | loss 1.72392 | ppl     5.606\n",
      "| epoch   1 step     2600 |   2600 batches | lr 0.00025 | ms/batch 76.55529 | loss 1.70920 | ppl     5.525\n",
      "| epoch   1 step     2800 |   2800 batches | lr 0.00025 | ms/batch 75.44199 | loss 1.68607 | ppl     5.398\n",
      "| epoch   1 step     3000 |   3000 batches | lr 0.00025 | ms/batch 75.65708 | loss 1.66033 | ppl     5.261\n",
      "| epoch   1 step     3200 |   3200 batches | lr 0.00025 | ms/batch 75.81477 | loss 1.61817 | ppl     5.044\n",
      "| epoch   1 step     3400 |   3400 batches | lr 0.00025 | ms/batch 76.14886 | loss 1.59749 | ppl     4.941\n",
      "^C\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Exiting from training early\n",
      "Traceback (most recent call last):\n",
      "  File \"train_synthetic.py\", line 786, in <module>\n",
      "    with open(os.path.join(args.work_dir, 'model.pt'), 'rb') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../reverse/tl24_xl12_2912-reverse/20211229-140431/model.pt'\n"
     ]
    }
   ],
   "source": [
    "!bash run_reverse_larger.sh train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
